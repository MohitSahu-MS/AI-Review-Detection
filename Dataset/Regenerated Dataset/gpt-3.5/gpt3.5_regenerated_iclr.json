{"gFDFKC4gHL4": "Summary: The paper addresses the challenge of monitoring and assessing performance shifts in Machine Learning (ML) prediction APIs provided by companies like Amazon and Google. It introduces a principled algorithmic framework called MASA to efficiently assess these shifts by modeling them as confusion matrix differences. The paper demonstrates the significant changes in ML APIs accuracy from 2020 to 2021 across various applications using commercial APIs. MASA is shown to estimate API shifts more accurately than standard approaches while reducing the required sample size by up to 90. Strengths: 1. Relevance and Importance: The topic of monitoring ML API shifts is both relevant and important as it directly impacts downstream applications and user experience. 2. Innovative Framework: The introduction of MASA as an algorithmic framework to assess API shifts efficiently by modeling them as confusion matrix differences is a significant contribution to the field. 3. Empirical Evaluation: The paper provides extensive empirical evidence supporting the effectiveness of MASA in estimating API shifts. Realworld experiments demonstrate the superiority of MASA over standard sampling approaches. 4. Clear Problem Formulation: The problem of assessing API shifts is wellformulated and the paper effectively addresses the challenges associated with monitoring ML APIs over time. Weaknesses: 1. Limited Generalizability: The focus on classification tasks with a small to moderate number of classes limits the generalizability of the proposed framework to more complex ML APIs such as those involving OCR or NLP. 2. Lack of Comparative Analysis: While the paper compares MASA with standard uniform sampling a more comprehensive comparison with other stateoftheart methods for detecting distribution shifts or monitoring ML APIs could enhance its credibility. Questions and Feedback to the Author: 1. Have you considered exploring the applicability of MASA to more complex ML APIs involving tasks such as OCR or NLP where the output may not be easily captured by a confusion matrix 2. How does the proposed approach handle cases where the API updates are not only related to changes in the prediction accuracy but also potentially involve changes in the underlying model structure or data processing mechanisms Review Summary: The paper presents a novel algorithmic framework MASA for efficiently assessing performance shifts in ML prediction APIs. By modeling shifts as confusion matrix differences and employing adaptive sampling strategies MASA significantly improves shift estimation accuracy while reducing the required sample size. The empirical evaluation demonstrates the effectiveness of MASA in detecting API shifts and provides valuable insights into the importance of monitoring ML APIs over time. Overall the paper makes a valuable contribution to the field of monitoring ML API shifts although further exploration into handling more complex APIs and comparative analysis with other methods would enhance its impact.", "tgcAoUVHRIB": " Summary of the Paper: The paper focuses on answering multihop logical queries on Knowledge Graphs using neural networks to create onepoint vector embeddings. It addresses the challenge of handling full FirstOrder Logical (FOL) queries with Conjunction (\\xe2\\x88\\xa7) Disjunction (\\xe2\\x88\\xa8) and Negation (\u00ac) operators. The models introduced achieve significant improvements over existing methods in terms of performance on benchmark datasets.  Strong Points: 1. Novel Approach: Introducing models based on Neural Networks to handle FOL queries with versatile logical operators is a significant contribution. 2. Performance Improvement: The experimental results demonstrate a notable increase in performance compared to stateoftheart baselines showcasing the effectiveness of the proposed models. 3. Handling Negative Queries: Addressing the challenge of handling negative queries is an important and innovative aspect of the paper. 4. Model Variants: The exploration of model variants such as incorporating attention mechanisms and implementing a 2Vector Average Approach enhances the understanding of the models inner workings and optimization possibilities.  Weak Points: 1. Evaluation Metrics: While MRR and HK are commonly used metrics additional evaluation metrics or comparisons with other methods could add more depth to the evaluation section. 2. Complexity of Models: The complexity of the models especially with the implementation of logical operators using neural networks may require further explanation or justification to ensure reproducibility and understanding. 3. Scalability: The scalability of the proposed models to larger and more complex knowledge graphs could be discussed further to understand the potential limitations.  Questions for the Author: 1. Generalization: How do you anticipate the models will generalize to larger or more diverse knowledge graphs beyond the benchmark datasets used in the experiments 2. Implementation Details: Can you provide more insights into the practical implementation of the models on realworld applications or scenarios 3. Future Directions: What future research directions or improvements do you foresee based on the outcomes and limitations observed in the study  Feedback: The paper provides a comprehensive exploration of handling FOL queries on Knowledge Graphs using neural networkbased models showcasing promising results and advancements over existing methods. To enhance the paper it might be beneficial to delve deeper into the interpretation of the logical operators implemented by the neural networks address potential scalability issues and consider the generalizability to diverse knowledge graph structures. Additionally incorporating additional evaluation metrics or comparisons with different approaches could further strengthen the evaluation section. Overall the paper presents a valuable contribution to the field of AI reasoning and Knowledge Graphs and it warrants consideration for publication.", "nCw4talHmo5": " Summary of the Paper: The paper introduces the concept of Parallelly Distributable Slimmable (ParaDiS) neural networks which address the challenge of efficient utilization of resources and reduced processing latency in the context of running neural networks on limited power devices. ParaDiS networks consist of multiple distributable configurations or switches that can be split among various device configurations without the need for retraining. The framework is evaluated on popular architectures such as MobileNet v1 ResNet50 and WDSR for various tasks including ImageNet classification and image superresolution. The experiments demonstrate that ParaDiS models can achieve comparable or better performance compared to individually trained distributed configurations and outperform universally slimmable models when distributed over multiple devices.  Strong Points: 1. Innovative Concept: The idea of ParaDiS networks is innovative and addresses a significant challenge in the deployment of neural networks on resourceconstrained devices. 2. Experimental Validation: The paper provides comprehensive experimental results on various architectures and tasks to demonstrate the effectiveness of the ParaDiS framework. 3. Performance: The results show that ParaDiS models achieve comparable accuracy to individually trained models and outperform universally slimmable networks when distributed over multiple devices. 4. Training Enhancements: The modifications to the training procedure including knowledge distillation from a wider model and distilling feature activation maps are well explained and contribute to the performance improvements.  Weak Points: 1. Scalability Issues: The paper acknowledges possible scalability challenges and presents initial experiments with 8 switches which yielded unsatisfactory results. Further investigation and discussion on addressing scalability would be beneficial. 2. Communication and Latency: While the paper focuses on reducing processing latency a deeper discussion on the implications of communication load in distributed settings could enhance the completeness of the study. 3. Comparison Framework: While comparisons with existing methods like slimmable networks are provided further elaboration on the unique advantages and limitations of ParaDiS networks compared to these methods would add clarity.  Questions for the Authors: 1. Could you elaborate on the potential reasons for the unsatisfactory results observed when training ParaDiS networks with 8 switches What are the key challenges for scalability in the proposed framework 2. How does the ParaDiS framework handle communication overhead and latency issues in distributed settings Can you provide insights into the tradeoffs associated with parallel distribution without intermediate communication 3. In what scenarios do you envision ParaDiS networks being most beneficial compared to other flexible or distributed neural network approaches Are there specific use cases where ParaDiS networks excel  Feedback: 1. Enhancing the discussion on communication overhead scalability challenges and usecase scenarios would deepen the understanding and relevance of the ParaDiS framework. 2. Providing further insights into the implications of resource variation across devices and realworld deployment considerations would enrich the practical implications of the research. Overall the paper presents an innovative approach with promising results. Addressing scalability challenges and further exploring the impact of communication load would strengthen the contribution of the ParaDiS framework.  I hope this review helps you in providing valuable feedback to the authors. Let me know if you need further assistance.", "tzO3RXxzuM": " Summary: The paper proposes a framework for studying generalization in noisy stochastic iterative algorithms based on stability. It unifies and advances stabilitybased generalization bounds for noisy iterative learning algorithms such as stochastic gradient Langevin dynamics (SGLD) and Introduces Exponential Family Langevin Dynamics (EFLD) offering flexibility in using exponential family noise for gradient descent. The paper presents theoretical advances and establishes expected stability based generalization bounds for EFLD along with optimization guarantees for noisy SignSGD and SGLD. Empirical results on benchmark datasets demonstrate the nonvacuousness and sharpness of the proposed bounds compared to existing literature.  Strengths: 1. Theoretical Advancements: The paper makes significant theoretical contributions by developing a framework for generalization bounds for noisy stochastic iterative algorithms and introducing the EFLD algorithm. 2. Empirical Validation: The paper provides empirical results on benchmark datasets to demonstrate the effectiveness of the proposed bounds. 3. Comparative Analysis: The comparison with existing generalization bounds such as Li et al. (2020) and Negrea et al. (2019) provides a comprehensive understanding of the proposed frameworks advantages.  Weaknesses: 1. Complexity: The papers complexity might make it challenging for readers unfamiliar with the field to grasp all the details and contributions fully. 2. Clarity: Some sections could benefit from improved organization and clearer transitions to enhance readability. 3. Practical Applicability: While theoretical advancements are significant a clearer discussion on the practical implications and realworld applications could enhance the papers impact.  Questions and Feedback to the Authors: 1. Can you provide further insights into the computational complexity or practical implementation aspects of the proposed EFLD algorithm and how it compares to existing algorithms in terms of efficiency 2. The derivation and proof sections are critical could you consider providing additional explanations or illustrative examples to aid in understanding the complex theoretical developments 3. How do the proposed stabilitybased generalization bounds and optimization guarantees translate into realworld scenarios or practical machine learning applications A discussion on practical implications would enhance the papers relevance.  Review Summary: The paper presents a novel framework for analyzing generalization in noisy stochastic iterative algorithms with a focus on stabilitybased bounds and the introduction of Exponential Family Langevin Dynamics. The theoretical advancements and empirical validation showcase the effectiveness of the proposed approach. However improvements in clarity practical implications and additional explanations in complex sections could enhance the overall impact of the paper. Overall I recommend acceptance with minor revisions to address the weaknesses identified.", "j3krplz_4w6": " Summary of the Paper: The paper introduces TEXTEXPLANATIONFOOLER (TEF) a novel explanation attack algorithm that imperceptibly alters text input samples to significantly change the outcome of widelyused explanation methods without affecting classifier predictions. The study evaluates attribution robustness in TEF on five sequence classification datasets using various DNN and transformer architectures. The findings demonstrate that all models and explanation methods are susceptible to TEF perturbations which can transfer to other model architectures and attribution methods. Additionally the paper introduces semiuniversal attack policies that efficiently compute perturbations without knowledge of the attacked classifier or explanation method.  Strong Points: 1. Novelty:  The paper presents a novel attack algorithm TEF for evaluating the robustness of explanation methods in text classifiers.  Introduces semiuniversal attack policies which are effective in altering explanations without querying the model during attack time. 2. Comprehensive Evaluation:  Provides an extensive evaluation of attribution robustness on widelyused text datasets and model architectures comparing several stateoftheart approaches.  Examines the transfer capabilities of TEF perturbations across different models and explanations. 3. Detailed Methodology:  Clearly outlines the problem formulation threat model and the algorithm used to imperceptibly change inputs and evaluate attribution robustness.  Conducts an ablation study comparing different attack strategies to provide insights into the effectiveness of TEF. 4. Valuable Insights:  Discusses the fragility of explanations provided by current methods in text classifiers highlighting the importance of addressing robustness in critical applications.  Finds that attention mechanisms in transformer architectures are more robust to perturbations than nontransformerbased approaches.  Weak Points: 1. Lack of Practical Implementation:  While the theoretical approach and results are welldetailed the paper could benefit from discussing applications or case studies where the TEF attack could impact reallife scenarios.  Questions for the Authors: 1. Can you provide insights into how the TEF attack algorithm could be practically implemented in realworld text classification systems  Feedback for the Authors: 1. Enhance the discussion on the practical implications and potential applications of the TEF attack to provide a clearer understanding of its significance in realworld scenarios. 2. Consider elaborating on the limitations of the study and avenues for future research in improving the robustness of explanation methods in text classifiers.  Review Summary: The paper presents a novel attack algorithm TEF that effectively alters explanations in text classifiers without changing classifier predictions. The extensive evaluation demonstrates the susceptibility of existing explanation methods to imperceptible perturbations. The study provides valuable insights into the fragility of explanations especially in transformer architectures. However the paper could further enhance its impact by discussing practical implementations and addressing potential limitations and future research directions. Overall the work contributes significantly to the understanding of explanation robustness in text classifiers and warrants consideration for publication.", "q7n2RngwOM": "1) Summary of the Paper: The paper discusses the identification and estimation of treatment effects (TEs) under limited overlap focusing on subjects with certain features belonging to a single treatment group. The authors propose a generative prognostic model using a latent variable called \u03b2IntactVAE which is a new type of variational autoencoder. They establish TE error bounds that enable balanced representations for treatment groups conditioned on individualized features. The proposed method is compared with recent methods using synthetic datasets. The paper provides theoretical foundations and practical implications for causal inference from observational data. 2) Strengths and Weaknesses of the Paper: Strong Points:  Theoretical Foundation: The paper provides a strong theoretical foundation discussing identification prognostic score and balancing in the context of estimating treatment effects.  Innovative Methodology: The proposed \u03b2IntactVAE approach is innovative and aims to address the challenging issue of limited overlap in causal inference.  Comparative Analysis: The comparison with recent methods using synthetic datasets adds value to the paper by showcasing the effectiveness of the proposed approach.  Empirical Results: The experiments conducted demonstrate the effectiveness of the method and its superiority over existing methods in certain scenarios. Weak Points:  Complexity: The content may be challenging to grasp for readers less familiar with causal inference and machine learning methods.  Data Limitations: The synthetic datasets used in the experiments may not fully represent the complexity of realworld data limiting the generalizability of the findings.  Assumptions and Restrictions: The reliance on specific assumptions such as limited overlap and the use of Gaussian latent variables might restrict the applicability of the proposed method in diverse scenarios. Questions and Feedback to the Author: 1. Scalability: How scalable is the proposed \u03b2IntactVAE approach in handling highdimensional data and realworld datasets with complex structures 2. Robustness: Have you considered the robustness of the method to noisy or incomplete data especially in scenarios with realworld observational data where missing data is common 3. Practical Implementation: How feasible is the implementation of the proposed approach in realworld applications considering factors like computational resources and data availability 4. Generalizability: How well does the method generalize across different types of causal inference problems beyond the specific scenarios tested in the experimental setup 5. Future Directions: Are there any plans for future research to extend the proposed approach refine the methodology or validate the effectiveness of the method on more diverse datasets 3) Review Summary: The paper presents an innovative approach based on generative prognostic modeling using a \u03b2IntactVAE to estimate treatment effects under limited overlap. The theoretical foundations methodology and experimental results demonstrate the effectiveness of the method in comparison to existing approaches. While the paper showcases strong theoretical and methodological contributions there are areas for improvement such as addressing scalability robustness and ensuring broader applicability to realworld datasets. Overall the paper provides valuable insights into causal inference under limited overlap and sets a solid foundation for future research in this domain. Further validation on diverse datasets and exploration of practical implications can enhance the impact and applicability of the proposed method.", "ieNJYujcGDO": "Peer Review: 1) Summary: The paper investigates the benefits and limitations of Mixup training in the context of classification. It explores how Mixup training relies on data properties for minimizing empirical risk and achieving better generalization. The paper provides theoretical insights concrete datasets and sufficient conditions for understanding Mixup training behavior in different scenarios. It also discusses the margin characteristics of Mixup classifiers and when Mixup training converges to the same solution as standard training on linearly separable datasets. 2) Strengths and Weaknesses: Strong Points:  The paper provides a comprehensive overview of Mixup training and its effects on classification tasks.  The theoretical analysis is rigorous and the derivations are clearly presented.  Concrete examples and datasets are used to illustrate theoretical points.  The discussion on why Mixup training works in practice is insightful.  The experimental validation using neural networks and datasets enhances the theoretical findings.  The relation to prior work and the significance of the contributions are well explained. Weak Points:  The exposition could be made more accessible by providing more intuitive explanations alongside the theoretical derivations.  While the theoretical results are comprehensive practical implications for realworld applications could be further explored.  The paper could benefit from more discussion on the broader implications of the findings for the machine learning community. Questions and Feedback to the Author: 1. Could you elaborate on how the findings in your paper could impact the design of future training algorithms in the deep learning community 2. Have you considered conducting experiments on more diverse datasets to validate the theoretical results across different data distributions 3. Can you discuss the computational complexity of the proposed Mixup method compared to traditional training techniques in practice 3) Review Summary: The paper presents a detailed investigation into the implications of Mixup training for classification tasks focusing on empirical risk minimization generalization properties and relationships with standard training approaches. The thorough theoretical analysis along with concrete examples and datasets enriches the understanding of Mixup training behavior. The experimental validation using neural networks provides practical relevance to the theoretical findings. While the paper excels in its theoretical rigor and insights further discussions on practical implications and broader applications could enhance the impact of the research. Recommendation: I recommend accepting the paper with minor revisions to improve accessibility and discuss practical implications for machine learning applications.", "vUH85MOXO7h": " Peer Review Draft: 1) Summary: The paper introduces and studies the Tree Neural Tangent Kernel (TNTK) for soft tree ensembles. By considering an ensemble of infinite soft trees the paper explores the theoretical properties behavior and implications of the TNTK. The study covers various aspects including convergence equivalence of tree structures nonlinearity degeneracy with deep trees comparison to MLPinduced NTK and practical implications for training techniques. 2) Strengths and Weaknesses: Strong Points:  Theoretical Depth: The paper explores the theoretical underpinnings of soft tree ensembles and introduces the novel concept of TNTK.  Methodological Rigor: The study is wellstructured and presents detailed theoretical analyses proofs and empirical results to support the findings.  Originality: The application of NTK theory to tree models is a novel contribution in the field of machine learning.  Practical Implications: The implications for practical techniques and the comparison to MLPinduced NTK provide valuable insights for model selection based on tradeoffs between prediction accuracy and computational complexity. Weak Points:  Limited Empirical Validation: While the theoretical analyses are comprehensive more emphasis on detailed empirical validations would further enhance the credibility of the findings.  Clarity of Presentation: The complex mathematical derivations and technical jargon may pose a challenge for readers unfamiliar with the topic. Simplifying the language or providing more intuitive explanations could improve accessibility.  Discussion on Limitations: It would be beneficial to address potential limitations of the study such as assumptions made to provide a more nuanced interpretation of the results. Questions and Feedback to the Author: 1. Can you provide more insights into the practical implications of the TNTK theory for soft tree ensembles in realworld applications beyond the theoretical analysis 2. How do the findings of this study contribute to advancing the understanding of deep learning models with complex architectures like tree ensembles 3. Considering the degeneracy observed with deep trees are there measures or strategies suggested in your study to mitigate or avoid this issue in practice 3) Review Summary: The paper presents a significant contribution by introducing and studying the Tree Neural Tangent Kernel for soft tree ensembles. The theoretical depth and originality of the study are commendable offering new insights into the behavior of infinite soft trees. The methodological rigor and comprehensive analyses strengthen the credibility of the findings. However more emphasis on empirical validations and clarifying complex concepts for broader accessibility would further enhance the impact of the paper. The practical implications and comparisons to MLPinduced NTK provide valuable insights suggesting the potential significance of the TNTK theory in practical applications. Overall the paper offers a valuable addition to the field and is recommended for publication with minor revisions to enhance clarity and empirical validation.", "gccdzDu5Ur": " Paper Summary: The paper explores the design space of leveraging feature priors as distinct perspectives on the data to improve model generalization. It investigates training models with diverse feature priors focusing on shape and texture biases in the context of image classification. The study demonstrates that training models with different feature priors leads to nonoverlapping failure modes which can be effectively combined to enhance generalization. Additionally the paper explores jointly training models with diverse feature priors on unlabeled data through the framework of cotraining to correct each others mistakes and improve prediction rules. The results highlight the potential of incorporating distinct feature priors into the training process for better generalization.  Strengths: 1. Innovative Approach: The paper introduces a novel perspective on leveraging feature priors to improve model generalization. 2. Comprehensive Exploration: The study thoroughly investigates the impact of diverse feature priors on model performance in different scenarios providing detailed experimental results. 3. Clear Structure: The paper is wellorganized with a logical flow from motivation to methodology and results.  Weaknesses: 1. Lack of RealWorld Application: While the findings are insightful practical application examples in realworld scenarios could strengthen the paper. 2. Complexity: The methodology and results are quite detailed which may make it challenging for readers to grasp the key takeaways easily. 3. Comparison Metrics: Clearer metrics for comparison especially for evaluating the success of diverse feature priors in preventing models from relying on spurious correlations could enhance the papers impact.  Questions and Feedback: 1. Question: How do the findings of your study translate to other domains beyond image classification such as natural language processing or audio data analysis 2. Feedback: Consider discussing potential realworld applications or case studies where the approach of leveraging diverse feature priors could be beneficial.  Review Summary: The paper presents a thorough investigation into leveraging diverse feature priors to improve model generalization focusing on shape and texture biases in image classification. While the methodology and results are detailed the paper could benefit from clearer practical implications and comparison metrics. Overall the paper contributes significantly to understanding the impact of feature priors on model performance and inspires further research in exploring different feature priors in various applications. As such I recommend this paper for publication pending minor revisions to enhance clarity and applicability.", "xKZ4K0lTj_": "Summary: The paper introduces Fewshot Imitation with Skill Transition Models (FIST) an algorithm designed to enable autonomous agents to generalize to unseen longhorizon tasks by extracting skills from offline data and utilizing them with fewshot demonstrations. FIST incorporates an inverse skill dynamics model a distance function and a semiparametric approach for imitation learning. The algorithm demonstrates superior performance in navigation experiments and robotic manipulation tasks outperforming previous baselines in both domains. Strengths: 1. Innovative Approach: The integration of skill extraction future conditioning and semiparametric evaluation sets FIST apart from existing methodologies showcasing a novel approach to fewshot imitation learning. 2. Generalization Capability: FISTs ability to generalize to unseen longhorizon tasks across navigation and robotic manipulation domains demonstrates its robustness and versatility. 3. Experimental Rigor: The paper provides a comprehensive experimental evaluation across various environments highlighting the effectiveness of FIST compared to baseline methods. 4. Thorough Related Work: The paper effectively contextualizes FIST within existing literature demonstrating a clear understanding of current approaches and highlighting the novelty of the proposed algorithm. Weaknesses: 1. Technical Clarity: Some parts of the paper particularly in the Approach section may benefit from additional clarity or elaboration to help readers fully grasp the technical details of FIST. 2. Evaluation Metrics: While the results are presented effectively detailed discussions on specific evaluation metrics comparisons with stateoftheart methods and statistical significance would further strengthen the papers impact. Questions: 1. Could the paper provide more insights into the scalability of FIST when handling larger datasets or more complex tasks 2. How does FIST perform under noisy or partially observable environments and how does it adapt to changing dynamics in the tasks Feedback: 1. Consider enhancing the clarity of the technical presentation ensuring that complex concepts are explained in a concise and accessible manner for readers. 2. Providing a more detailed analysis of the computational requirements training stability and scalability of FIST would add value to the paper. Review Summary: The paper introduces an innovative algorithm FIST addressing fewshot imitation learning for longhorizon tasks with superior generalization capabilities. While the paper demonstrates significant advancements and robustness in navigation and robotic manipulation domains further clarity detailed metrics and scalability insights would enhance its impact. Overall the paper presents a promising approach and contributes significantly to the field of autonomous agent skill learning.", "yeP_zx9vqNm": " Summary of the Paper: The paper investigates the potential resemblance between adversarially trained neural networks and human peripheral computation in terms of robust feature representations. The authors conduct psychophysics experiments using a metamer task to evaluate how well human observers can distinguish between images synthesized to match adversarially robust representations nonrobust representations and a texture synthesis model of peripheral vision. The results suggest that adversarially robust representations capture peripheral computation better than nonrobust representations and are similar to texture model representations. The study aims to shed light on the role of texturebased summary statistic representations in driving human invariance to adversarial perturbations and potentially incorporating such representations into deep neural networks for useful properties like adversarial robustness.  Strong Points of the Paper: 1. Innovative Approach: The paper explores a novel perspective on the connection between adversarially robust networks and human peripheral vision offering a unique contribution to the field of computer vision. 2. Empirical Findings: The study presents thorough psychophysics experiments providing empirical evidence to support the hypothesis that adversarially robust networks capture aspects of human peripheral computation. 3. Clarity and Rigor: The paper is wellstructured detailing the methodology results and implications clearly and rigorously. 4. Indepth Analysis: The paper delves into detailed synthesis procedures model comparisons and human psychophysical experiments showcasing a comprehensive analysis and understanding of the research question.  Weak Points of the Paper: 1. Biological Plausibility: There are concerns about the discussion of biological plausibility particularly regarding the comparison between adversarially trained networks and human vision. The paper acknowledges the limitations but could further elaborate on the discrepancies in biological mechanisms.  Questions and Feedback to the Author: 1. Can you elaborate on the potential implications of the findings on the development of more biologically plausible deep neural networks that incorporate aspects of peripheral computation 2. How do you plan to address the limitations of the current study in terms of biological relevance and practical implications for the field of computer vision 3. Have you considered exploring additional physiological components such as MEG and fMRI in future studies to further investigate the neural correlates of the observed perceptual discriminations  Review Summary: The paper provides a compelling exploration of the relationship between adversarially robust networks and human peripheral computation through psychophysics experiments. Despite some concerns regarding biological plausibility the innovative research approach thorough methodology and empirical findings support the hypothesis that adversarially robust representations align with aspects of human peripheral vision. Overall the paper makes a valuable contribution to understanding adversarial robustness and the potential incorporation of texturebased representations into deep neural networks. I recommend acceptance after addressing minor concerns and providing further insights into the practical implications and future directions of the research.", "xy_2w3J3kH": " Summary of the Paper The paper proposes a decentralized actorcritic algorithm for cooperative multiagent reinforcement learning (MARL) in a subclass of cooperative Markov games where agents exhibit homogeneity enabling policy sharing without suboptimality. The algorithm employs consensus updates for actors and critics ensuring convergence. Practical techniques are developed to reduce communication costs during training while maintaining competitive policy performance compared to centralized training.  Strong Points 1. Theoretical Justification: The paper provides theoretical justification for policy parameter sharing in a subclass of cooperative Markov games. 2. Novel Algorithm: It introduces a novel decentralized actorcritic algorithm with policy consensus in homogeneous MGs ensuring convergence. 3. Efficiency: The proposed banditbased technique for communication selection optimizes communication efficiency demonstrating effectiveness in reducing communication overhead. 4. Empirical Validation: The algorithm is empirically evaluated on various tasks demonstrating competitive performance with reduced communication costs.  Weak Points 1. Assumptions: The convergence guarantees are provided under specific assumptions of linear critics and full observability which may limit the generalizability of the algorithm. 2. Complexity: The complexity of the proposed algorithm and the banditbased communication selection process might hinder its practical adoption in certain scenarios. 3. Empirical Validation Limitation: The empirical evaluation could be extended to more diverse environments and scenarios to strengthen the generalizability of the proposed approach.  Question and Feedback to the Author 1. Clarification: Could the paper elaborate on the practical implications and potential limitations of the proposed algorithm especially in scenarios where the assumptions made might not hold 2. Comparison: How does the proposed algorithm perform in scenarios with varying degrees of agent heterogeneity and observationaction space diversity 3. Scalability: Have scalability considerations been thoroughly addressed especially in terms of handling a large number of agents in the cooperative setting  Review Summary The paper presents a significant contribution by characterizing homogeneity in cooperative Markov games and introducing a decentralized actorcritic algorithm with policy consensus offering theoretical justification and practical implementation for policy parameter sharing. While demonstrating promising results in communication efficiency and competitive policy performance the algorithms generalizability beyond specific assumptions and complexities could be further explored. Overall this work is wellstructured theoretically grounded and provides valuable insights into decentralized MARL approaches. Further refinement and validation in diverse scenarios can enhance its applicability and impact.", "n0OeTdNRG0Q": " Summary: The paper introduces Efficient Sharpness Aware Minimizer (ESAM) an optimization algorithm derived from Sharpness Aware Minimizer (SAM) aiming to enhance the efficiency of deep neural networks (DNNs) without compromising generalization performance. ESAM integrates two novel strategies: Stochastic Weight Perturbation (SWP) and SharpnessSensitive Data Selection (SDS) to reduce the computational overhead compared to SAM. Extensive experiments on CIFAR and ImageNet datasets demonstrate that ESAM outperforms SAM by reducing computational overhead from 100 to 40 while maintaining or even improving test accuracies.  Strengths: 1. Theoretical Foundation: The paper provides clear theoretical explanations for the proposed strategies (SWP and SDS) and their effectiveness in enhancing the efficiency of SAM. 2. Empirical Validation: Extensive experiments on CIFAR and ImageNet datasets with various DNN architectures validate the effectiveness of ESAM in improving efficiency and maintaining or enhancing generalization performance. 3. Comparative Analysis: The comparison with SAM and base optimizers along with ablation studies provides a comprehensive understanding of the proposed strategies and their impact.  Weaknesses: 1. Lack of Realworld Application: While the experiments are detailed realworld case studies or applications demonstrating the practical impact of ESAM could enhance the relevance of the paper. 2. Parameter Sensitivity: The paper heavily relies on tuning hyperparameters (\u03b2 \u03b3) for optimal performance. Further insights on robustness to hyperparameter choices could be beneficial.  Questions and Feedback to the Author: 1. Scalability: How does ESAM scale with larger datasets or more complex DNN architectures Are there any limitations observed in terms of scalability 2. Comparative Analysis: Could the authors discuss the tradeoffs between efficiency gains and variations in test accuracies across different datasets and architectures 3. Implementation: How userfriendly is the implementation of ESAM provided in the GitHub repository Are there any plans for optimizing the implementation for wider adoption  Review Summary: The paper presents a wellstructured and theoretically grounded approach ESAM to enhance the efficiency of training deep neural networks. The experimental results demonstrate the effectiveness of ESAM in reducing computational overhead while maintaining or improving generalization performances compared to SAM. The incorporation of novel training strategies and rigorous experimental validation make ESAM a promising optimization algorithm for deep learning applications. Addressing scalability concerns providing insights on robustness to hyperparameters and further discussing realworld applications could further strengthen the paper. Overall based on the strong theoretical foundation and compelling empirical results I recommend acceptance of the paper pending minor revisions.", "nzvbBD_3J-g": " 1) Summary of the Paper: The paper introduces a novel framework called Intermediary Latent Space VAEs (InteLVAEs) to incorporate inductive biases into Variational AutoEncoders (VAEs). The standard VAE framework with an isotropic Gaussian prior faces issues in learning desired structural characteristics in the representations. The paper argues that directly changing the prior is not effective for incorporating inductive biases and introduces InteLVAEs as an alternative approach. InteLVAEs use an intermediary set of latent variables and a parametric mapping function to enforce inductive biases on representations effectively. The paper presents specific realizations of InteLVAEs for learning diverse representations such as sparse hierarchical multiply connected and multimodal features. Experimental results show the superiority of InteLVAEs over baseline methods in both generative modeling and feature quality.  2) Strengths and Weaknesses Questions and Feedback: Strong Points:  The paper addresses an important problem in VAEs related to incorporating inductive biases effectively.  The proposed InteLVAEs framework is innovative and provides a comprehensive solution to enforce desired characteristics in learned representations.  The detailed explanations mappings and results provided give a clear understanding of the proposed method.  The experimental results demonstrate the effectiveness of InteLVAEs in improving generative modeling quality and learning better representations. Weak Points:  The paper could benefit from more detailed discussions or comparisons with existing methods that also aim to incorporate inductive biases in VAEs.  Further elaboration on the limitations or potential challenges of the proposed InteLVAEs framework could enhance the papers completeness. Questions: 1. How does the computational complexity of training InteLVAEs compare to standard VAEs and other methods incorporating inductive biases 2. Are there specific applications or domains where InteLVAEs have shown exceptional performance leading to significant advancements 3. Have you considered exploring the interpretability aspects of the learned representations in the context of realworld applications Feedback:  The paper is wellstructured and provides a detailed overview of the proposed InteLVAEs framework. Enhancing the discussion with more comparative analyses and practical implications could strengthen the papers impact.  Clarifying certain aspects of the methodology and highlighting realworld use cases or future research directions would add value to the paper.  3) Review Summary: The paper presents an innovative approach InteLVAEs to address the challenge of incorporating inductive biases effectively into VAEs. The framework offers a comprehensive solution by introducing intermediary latent variables and a parametric mapping function to directly enforce desired characteristics in learned representations. The experimental results showcase the superiority of InteLVAEs in improving generative modeling and feature quality. While the paper is wellstructured and provides insightful contributions further discussions on practical implications and comparative analyses with existing methods could enhance its impact. Overall based on the robust methodology and promising results I recommend this paper for publication with minor revisions to strengthen its contribution.", "jE_ipyh20rb": " Peer Review  1) Summary: The paper introduces FEDPROF a novel algorithm for optimizing Federated Learning (FL) under scenarios where clients possess lowquality biased or noisy data. FEDPROF utilizes a data representation profiling and matching scheme to dynamically profile data representations and adjust client participation probability. Extensive experiments demonstrate that FEDPROF reduces communication rounds accelerates convergence time (up to 4.5x speedup) and improves the accuracy of the global model.  2) Strengths and Weaknesses: Strong Points:  Innovative Approach: The paper introduces a unique methodology with the data representation profiling and matching scheme to address the challenges associated with lowquality client data.  Theoretical Foundations: The paper provides theoretical proof supporting observations and methodologies enhancing the credibility of the proposed algorithm.  Extensive Experiments: The experiments conducted on public datasets demonstrate the effectiveness of FEDPROF in improving efficiency and convergence speed. Weak Points:  Clarity in Presentation: While the technical aspects are detailed the paper could benefit from improved clarity in presentation especially for readers less familiar with FL and data representation concepts.  Comparative Analysis: While the paper compares FEDPROF with other FL algorithms further insights could be provided on how FEDPROF specifically outperforms each baseline. Questions to Authors: 1. Could you elaborate on the practical implications of implementing FEDPROF in realworld scenarios such as industry applications or privacysensitive environments 2. How does FEDPROF address potential challenges in scalability when dealing with a large number of clients and varying data qualities Feedback to Authors:  Consider enhancing the clarity of the paper by providing more illustrative examples or visual aids to support complex concepts.  Provide a thorough comparative analysis highlighting the strengths of FEDPROF over existing algorithms in addressing issues related to biased or lowquality client data.  3) Review Summary: The paper presents an innovative FL algorithm FEDPROF addressing challenges posed by lowquality client data. The theoretical foundations methodology and experimental results showcase the algorithms effectiveness in improving convergence speed reducing communication rounds and enhancing global model accuracy. Despite certain areas for improvement in clarity and comparative analysis the innovative approach and promising experimental results warrant the paper\u2019s acceptance with minor revisions for enhanced presentation and analysis.  Recommendation: Accept with minor revisions.", "qqdXHUGec9h": " Summary: The paper proposes a novel method called CAV Learning (CAVL) for solving the Partiallabel Learning (PLL) problem. The key idea is to utilize the learned intrinsic representation of the model to identify the true label without relying on assumptions about the collected data. The paper introduces the concept of Class Activation Value (CAV) as a general and versatile tool to replace the limited Class Activation Map (CAM). CAVL uses CAV to guide the model in selecting the true label from candidate labels during training. Extensive experiments on benchmark datasets and realworld datasets demonstrate that CAVL outperforms existing stateoftheart methods.  Strong Points: 1. Innovative Approach: The paper introduces a novel method CAVL which leverages CAV to improve the performance of deep neural networks in solving the PLL problem. This innovative approach sets the paper apart from existing methods. 2. Experimental Validation:  Comprehensive Experiments: The paper conducts experiments on various benchmark datasets and realworld datasets to validate the effectiveness of the proposed method. The comparison with stateoftheart methods demonstrates the superiority of CAVL.  Reproducible Results: The rigorous experimental setup and multiple trials ensure the reproducibility of the results and increase the credibility of the proposed method. 3. Clear Presentation:  Detailed Explanation: The paper provides a detailed explanation of the proposed method including the motivation methodology and algorithm. This clarity aids in understanding the complex concepts presented.  Visual Aids: Figures and pseudocode (Algorithm 1) are included to help illustrate key concepts and the proposed approach making it easier for readers to follow along.  Weak Points: 1. Assumptions Clarification: The paper claims that the proposed method does not rely on assumptions about the collected data. However it would be beneficial to further clarify and elaborate on the implicit assumptions underlying the CAV and CAVL methods to ensure a better understanding of the proposed approach. 2. Theoretical Foundation: While the experimental results demonstrate the effectiveness of CAVL the paper could benefit from a more indepth discussion on the theoretical underpinnings of why CAV and CAVL work well in the context of PLL.  Questions and Feedback to the Author: 1. Could you provide more insights into the theoretical justification behind the effectiveness of CAV and CAVL in identifying the true label in the context of PLL 2. How do you envision extending the proposed CAVL method to more complex datasets or problem domains beyond those explored in the current experiments 3. Have you considered conducting sensitivity analysis to evaluate the robustness of CAVL to different hyperparameters or variations in dataset characteristics  Review Summary: The paper presents an innovative approach CAV Learning (CAVL) for addressing the Partiallabel Learning (PLL) problem without relying on data assumptions. The proposed Class Activation Value (CAV) serves as a versatile tool to guide the model in selecting the true label. The comprehensive experimental results showcase the superior performance of CAVL compared to stateoftheart methods. The paper is wellstructured detailed and provides clear explanations. However further clarification of assumptions and theoretical foundations as well as addressing weakness points would enhance the overall quality of the work. Considering the strong points and the innovative approach presented I recommend this paper for publication after addressing the feedback provided.", "vnF5gDNvcKX": " Paper Summary: The paper introduces an approach called Variance Reduced Domain Randomization (VRDR) for policy gradient methods in the context of domain randomization (DR). DR aims to improve the generalization capability of deep reinforcement learning policies by adding diversity to training environments through randomization of parameters affecting dynamics. However the randomness in environment parameters introduces high variance in gradient estimation during training leading to low sample efficiency. The paper theoretically derives an optimal stateenvironmentdependent baseline for DR showing improved variance reduction over standard baselines. VRDR divides the environment space into subspaces estimating statesubspacedependent baselines to balance between variance reduction and computational complexity. Empirical evaluations on six robot control tasks demonstrate that VRDR accelerates policy training convergence and achieves higher rewards in specific tasks.  Strong Points: 1. Theoretical Foundation: The paper provides a strong theoretical foundation by deriving the optimal stateenvironmentdependent baseline for DR demonstrating variance reduction improvements. 2. Innovative Approach: The proposed VRDR method is innovative in striking a balance between variance reduction and computational complexity addressing the high variance issue in policy gradient methods for DR. 3. Empirical Evaluations: The empirical evaluations on six robot control tasks show consistent acceleration of policy training convergence and improved rewards using VRDR. 4. Comprehensive Experiments: The ablation studies on the impact of key parameters like the number of baselines and clustering interval add depth to the experimental evaluation.  Weak Points: 1. Computational Complexity: The feasibility and computational overhead of maintaining stateenvironmentdependent baselines for each environment parameter need further discussion. 2. Comparison: While comparisons with DR and MVN are provided additional benchmarks or comparison with other stateoftheart methods could add more depth to the evaluation. 3. RealWorld Application: The discussion on the applicability and scalability of VRDR to realworld scenarios with more complex environments could enhance the practical relevance of the proposed approach.  Questions and Feedback: 1. How scalable is the VRDR approach to larger more complex environments or tasks beyond the simulated robot control scenarios 2. Have you considered incorporating any visualization techniques or interpretability methods to analyze the clustering results in VRDR and understand the impact on policy training 3. How robust is VRDR to hyperparameter tuning across different tasks or environments Did you observe any tradeoffs between hyperparameters and performance 4. Could the paper benefit from more indepth discussions on the limitations or potential challenges in the practical implementation of VRDR in realworld applications  Review Summary: Overall the paper presents a novel and theoretically sound approach VRDR to address the high variance issue in policy gradient methods for domain randomization. The extensive empirical evaluations and ablation studies support the effectiveness of VRDR in accelerating policy training convergence and improving rewards. While the paper demonstrates promising results discussing the practical implications scalability and potential challenges in realworld applications could enhance the thoroughness and impact of the proposed VRDR approach. Based on the strong theoretical foundation and positive empirical findings I recommend accepting the paper after addressing the minor weaknesses and questions raised.", "yBYVUDj7yF": " Summary: The paper investigates the theoretical foundations of contrastive learning compared to autoencoders focusing on linear representation settings in the spiked covariance model. The study demonstrates that contrastive learning outperforms autoencoders for feature recovery and downstream tasks. Additionally the role of labeled data in supervised contrastive learning and multitask transfer learning is explored to understand the impact on performance. The investigation includes numerical experiments to validate theoretical findings.  Strengths: 1. Theoretical Analysis: The paper offers a rigorous theoretical analysis of contrastive learning shedding light on its advantages over autoencoders in feature recovery and downstream tasks. 2. Novel Contribution: The study provides significant insights into the role of labeled data in supervised contrastive learning and transfer learning scenarios addressing practical implications of the methodology. 3. Clear Structure: The paper is wellstructured with detailed sections covering different aspects of the theoretical study making it easy to follow the logic and flow of the analysis.  Weaknesses: 1. Complexity: The paper delves into a highly specialized and technical domain which might make it challenging for readers without a strong background in the subject matter to grasp the contents fully. 2. Limited Scope: The focus on linear representation settings may limit the generalizability of the findings to more complex models and scenarios. 3. Assumptions and Conditions: The theoretical results depend on several assumptions and conditions which might have implications on practical applicability and realworld scenarios.  Questions: 1. Generalization: How transferrable do you anticipate the theoretical results presented in this study to be across different model complexities beyond linear representations 2. Practical Implications: How do you envision integrating the theoretical insights from this study into practical applications of contrastive learning in realworld scenarios 3. Future Directions: What specific research directions do you foresee based on the findings of this study Are there plans to explore more complex models or expand the analysis to different domains  Feedback: Overall the paper provides valuable theoretical insights into contrastive learning and its comparison with autoencoders. To enhance the impact of the study consider addressing the limitations and assumptions exploring avenues for practical implementation and discussing future research directions to broaden the scope and applicability of the theoretical findings. Additionally clarifying the implications of the studys results on broader machine learning applications would strengthen the papers contribution to the field.", "yjxVspo7gXt": " Summary: The paper investigates bias mitigation techniques in deep learning algorithms with a focus on intersectional fairness where multiple protected attributes lead to a large number of intersectional groups. The study explores the scalability of existing bias mitigation methods to multiattribute fairness settings using datasets including CelebA and ImageNet People Subtree. The research proposes a novel regularization method named Knowledge Distillation of Independent Models as Regularization (DIR) which demonstrates significant bias reduction in intersectional groups on ImageNet.  Strengths: 1. Thorough Empirical Analysis: The paper conducts a comprehensive empirical analysis of existing bias mitigation methods in multiattribute settings shedding light on the importance of leveraging protected attribute labels for promoting fairness. 2. Novel Methodology: The introduction of the DIR regularization method which uses groupspecific teacher classifiers to train a single student classifier presents a unique approach to address the scalability and overfitting challenges in bias mitigation algorithms. 3. Performance Results: The study presents compelling performance results showing reduction in bias amplification by 15 on ImageNets People Subtree demonstrating the effectiveness of the proposed DIR method.  Weaknesses: 1. Scalability Challenges: The paper highlights challenges related to model complexity and overfitting in scaling bias mitigation algorithms to datasets with a large number of intersectional groups. More details on how these challenges are specifically addressed through DIR would enhance the papers clarity. 2. Evaluation Metrics: While the paper extensively discusses various fairness metrics like bias amplification and intersectional bias a deeper discussion on the limitations or potential biases associated with these metrics could provide more insights into the evaluation process.  Questions to the Author: 1. Can you elaborate on the specific characteristics of the CelebA dataset and ImageNet People Subtree dataset that make them suitable for analyzing bias mitigation methods in multiattribute settings 2. How does the proposed DIR method compare to other stateoftheart bias mitigation algorithms in terms of computational efficiency and performance on different datasets  Feedback: The paper provides valuable insights into the challenges and solutions for bias mitigation in deep learning algorithms when dealing with multiple protected attributes. Strengthening the discussion around the scalability of bias mitigation methods and providing a more detailed comparison with existing algorithms would enhance the impact and clarity of the research. Moreover incorporating discussions on the ethical implications of protected attribute labels in datasets and potential biases in fairness metrics would enrich the papers contribution to the broader research community. Overall the studys thorough empirical analysis and the introduction of the novel DIR method contribute significantly to the field of bias mitigation in multiattribute fairness settings.", "youe3QQepVB": "Peer Review Draft: 1) Summarize the paper: The paper introduces a novel problem of learning a shared generative model that benefits multiple visual perception tasks. The proposed multitask oriented generative modeling (MGM) framework leverages both discriminative multitask network and generative network to facilitate learning tasks using synthesized images paired with weak annotations. The key components include a refinement network and a selfsupervision network that guide the image synthesis process. Experimental evaluations on challenging multitask benchmarks demonstrate superior performance compared to stateoftheart multitask approaches. 2) Evaluation of Strong and Weak Points: Strong Points:  The paper addresses a novel problem of learning a shared generative model for multiple visual tasks.  The MGM framework shows significant improvement in task performance across different scenarios outperforming existing multitask approaches.  Incorporation of auxiliary networks like the refinement and selfsupervision networks enhances feature representation and learning from weak annotations.  Experimental evaluations on standard multitask datasets validate the effectiveness of the proposed approach. Weak Points:  The paper lacks a detailed discussion on the computational efficiency and scalability of the proposed MGM framework.  The evaluation should include more extensive comparison with a broader range of stateoftheart methods in multitask learning and generative modeling.  Clarity could be improved in the presentation of algorithm details and the training procedure for better understanding by readers.  The paper could benefit from a more thorough analysis of the generalizability and robustness of the proposed MGM model across different datasets. Questions and Feedback to the author:  Can you provide more insights into the scalability of the MGM framework to handle a larger number of tasks and more diverse datasets  How would you address the computational complexity concerns raised by incorporating generative modeling in multitask learning frameworks  Can you elaborate on potential realworld applications and scenarios where the MGM framework could offer significant advantages over existing methods  Have you considered conducting additional experiments or case studies to demonstrate the robustness and generalizability of the proposed MGM model beyond the current evaluation on specific datasets 3) Review Summary: The paper presents a novel and promising approach introducing generative modeling for multitask visual learning through the MGM framework. The comprehensive evaluation results demonstrate the effectiveness of the proposed model in improving task performance across different scenarios. While the paper has significant strengths in its innovation and experimental validation there are opportunities for improvement in addressing computational efficiency scalability and generalizability concerns. Overall based on the strong performance results and the innovative nature of the proposed approach I recommend this paper for publication after addressing the mentioned points for improvement.", "vDa28vlSBCP": " Summary: The paper introduces ProtoTrex networks a novel approach to enhance transformer language models (LMs) with interpretability using prototype networks directly integrated into the model architecture. The key contributions include explaining the reasoning process behind LM decisions enhancing model interpretability and incorporating user feedback through interactive learning. Experimental results demonstrate that ProtoTrex networks perform competitively with noninterpretable baselines offering promising explanations for user understanding and decisionmaking processes.  Strengths: 1. Innovative Approach: The integration of prototype networks within transformer LMs for improved interpretability is a novel and promising concept in the field of NLP. 2. Experimental Evaluation: The paper provides comprehensive experimental results that demonstrate the effectiveness of ProtoTrex networks in enhancing model interpretability and performance on various tasks. 3. Interactive Learning Setup: The incorporation of human feedback through an interactive learning approach adds a valuable layer of user involvement contributing to model transparency and trust. 4. Faithfulness Evaluation: The assessment of the faithfulness of explanations through a rigorous evaluation methodology adds credibility to the proposed prototype networks.  Weaknesses: 1. Complexity: The paper introduces several modules and concepts which may be challenging for readers not familiar with the specific domain possibly hindering the accessibility of the content. 2. Presentation: The information could be conveyed more concisely to improve readability and clarity especially in the technical details and methodology sections.  Questions to Author: 1. Scalability: How does the proposed interactive learning setup scale with larger datasets or more complex models Have you explored the implications for realworld applications with extensive user feedback 2. Generalization: How well do the insights and findings from this study transfer to other NLP tasks or domains beyond the datasets evaluated in the experiments  Feedback to Author: 1. Clarity: Consider streamlining the presentation of technical details to enhance readability and improve the accessibility of the content to a broader audience. 2. Comparison: Provide more comparative analysis with existing post hoc interpretation methods to showcase the unique advantages of ProtoTrex networks more explicitly.  Review Summary: The paper presents an innovative approach ProtoTrex networks that enhances transformer LMs with interpretability through the integration of prototype networks and interactive learning. The experimental results showcase the competitive performance of this approach with noninterpretable baselines validating the effectiveness of the proposed method. While the technical complexity and presentation could be improved the strengths lie in the novel concept thorough experimental evaluation and the incorporation of user feedback for better model understanding. Hence I recommend accepting this paper with minor revisions to enhance clarity and broaden the discussion on scalability and generalizability.", "fTYeefgXReA": " Review of Paper: \"Equivariant Graph Neural Networks for Heterogeneous Graphs\"  1) Summary: The paper proposes an approach for designing neural networks that can operate on heterogeneous graphs directly without losing valuable information as seen in the process of converting heterogeneous graphs into homogeneous ones. The authors introduce linear layers that are maximally expressive while being equivariant only to permutations of nodes within each type in heterogeneous graphs. The proposed method is demonstrated to be effective for tasks like node classification and link prediction on heterogeneous graphs.  2) Strengths:  Innovative Approach: The paper introduces a novel method for designing neural networks for heterogeneous graphs.  Theoretical Rigor: The authors provide a detailed theoretical foundation for their approach including equations and theorems.  Evaluation: The paper presents a comprehensive evaluation on node classification and link prediction tasks showcasing the effectiveness of the proposed method.  Code Availability: Providing the code in an anonymous repository enhances the reproducibility of the results.  3) Weaknesses:  Complexity: The paper delves into technical details rapidly potentially making it challenging for readers new to the topic to grasp the concepts.  Evaluation: While the method is effective a deeper analysis of results and comparisons with more diverse baselines could provide a more holistic view.  Clarity: Some sections especially the theoretical explanations could be further simplified for better readability.  Question to the Authors:  Could you provide more insights into the practical implications of your method in realworld applications beyond the specific tasks discussed in the paper  Feedback to the Authors:  Clarify some technical terms and concepts for improved readability especially in theoretical sections.  Consider discussing scalability and computational efficiency as these aspects are crucial for deploying the proposed method in practical scenarios.  Review Summary: The paper presents an innovative approach for designing neural networks that can operate directly on heterogeneous graphs while preserving valuable information across multiple types of entities and relationships. The detailed theoretical foundation coupled with the empirical evaluation showcases the effectiveness of the proposed method. However ensuring clarity in technical explanations and providing broader implications beyond the discussed tasks would enhance the comprehensiveness of the paper. Overall the paper makes a strong contribution to the field of graph neural networks for heterogeneous graphs and warrants publication after addressing minor clarity and depth concerns.", "oZe7Zdia1H5": " Summary: The paper introduces a novel approach to locate structurally sparse winning tickets within deep neural networks. The core idea involves appending postprocessing techniques after iterative magnitude pruning (IMP) to enforce structural sparsity. Specifically the proposed method involves refilling pruned elements in important channels and regrouping nonzero elements to create groupwise structural patterns. Extensive experiments on various datasets and network backbones consistently validate the proposal showcasing substantial inference speedups with maintained accuracy.  Strengths: 1. Innovative Approach: The paper introduces a novel method for identifying structurally sparse winning tickets bridging the gap between theoretical concepts and practical hardware acceleration in deep learning. 2. Comprehensive Experiments: Extensive experiments conducted on diverse datasets and network architectures support the effectiveness of the proposed approach in achieving inference speedups while maintaining comparable accuracy. 3. Clear Visualization: The visual representation of the sparse masks aids in understanding the impact of different methods on the network structure. 4. Public Release of Resources: The commitment to releasing codes and pretrained models publicly adds to the credibility and reproducibility of the research.  Weaknesses: 1. Limited Comparison Methods: The paper could benefit from comparing the proposed method against more stateoftheart pruning methods to showcase its superiority more comprehensively. 2. Algorithmic Details: Providing more indepth explanations of the algorithms used such as regrouping and refilling could enhance the clarity for readers unfamiliar with these techniques. 3. Impact of Hyperparameters: The paper could delve deeper into the impact of hyperparameters on the results obtained providing insights into the sensitivity and robustness of the proposed methods.  Questions to the Author: 1. Can you provide insights into how the proposed approach would scale for even larger networks or more complex datasets 2. How sensitive is the performance of the method to the choice of hyperparameters and are there strategies to optimize these for different scenarios  Feedback: The paper presents a significant advancement in identifying structurally sparse winning tickets within deep neural networks showcasing promising results in terms of inference speedups while maintaining accuracy. To enhance the clarity of the paper consider enriching the discussion on comparison methods algorithmic intricacies and the impact of hyperparameters. Additionally addressing the questions raised could provide valuable insights into the scalability and adaptability of the proposed approach.  Conclusion: Overall the paper presents a novel and promising method for locating structurally sparse winning tickets demonstrating compelling results in accelerating deep neural networks on practical hardware platforms. With some improvements in addressing limitations and providing deeper insights the research could potentially have a significant impact on enhancing the efficiency of deep learning models. I recommend further exploration and consideration for publication in a reputable venue.", "ue4CArRAsct": "Peer Review 1) Summary: The paper proposes a novel autoencoder architecture Structural Autoencoder (SAE) for selfsupervised structured representation learning without the need for aggressive regularization. The SAE utilizes structural decoders to learn a hierarchical representation akin to structural causal models focusing on independence of latent variables. The study demonstrates the effectiveness of SAE in improving performance across various downstream tasks such as generative modeling disentanglement and extrapolation on challenging image datasets. 2) Evaluation: Strong Points:  The paper introduces a novel approach to structured representation learning that avoids the reconstruction quality versus generative performance tradeoff seen in VAEs.  The SAE architecture infused with structural decoders and hybrid sampling shows superior performance in reconstruction quality and generative modeling compared to baseline methods across diverse datasets.  The experimental results provide comprehensive insights into the effectiveness of the proposed SAE architecture in achieving disentanglement improving extrapolation and capturing hierarchical structures in latent representations.  The paper is wellstructured with clear explanations of the methodology and results. The inclusion of comparisons with other autoencoder architectures and detailed experiments adds depth to the study. Weak Points:  While the paper provides thorough explanations it could benefit from additional visualizations or illustrations to aid in understanding the proposed architecture and experimental results.  The discussion on potential further enhancements or future work could be expanded to provide a more robust roadmap for advancing the proposed approach and its applications.  In terms of generalization the paper could discuss the scalability and adaptability of the SAE architecture to larger datasets or different modalities beyond images. Questions:  Could you elaborate on the computational overhead or efficiency considerations associated with implementing the SAE architecture especially with increasing dataset sizes or complexity  How does the SAE architecture handle scenarios where there might be interactions or dependencies between latent variables that are not completely independent  Have you explored the transferability of the learned representations from SAE to other tasks or domains beyond the ones evaluated in the study Feedback to Authors:  Consider enhancing the paper with more visual aids to illustrate the working principles of the SAE architecture and the observed experimental results.  Extend the discussion on potential future directions and applications for the SAE architecture to provide a more comprehensive outlook for researchers interested in this area.  It would be valuable to elucidate the implications and challenges related to implementing the proposed hybrid sampling technique in practical scenarios or more complex datasets. 3) Review Summary: The paper presents a compelling approach the Structural Autoencoder (SAE) for structured representation learning without explicit regularization showcasing notable improvements in generative modeling disentanglement and extrapolation tasks on challenging image datasets. The SAE architecture combined with structural decoders and hybrid sampling demonstrates superior performance compared to traditional autoencoder models. While the study is wellexecuted and provides valuable insights further enhancements in visualization scalability consideration and broader applicability discussions could enhance the overall impact and relevance of the proposed approach. Overall the paper offers a significant contribution to the field of selfsupervised learning with structured representations.", "wzJnpBhRILm": " Summary: The paper aims to address the limitations and potential issues of batch normalization by proposing a method that approximates population normalization with reduced or eliminated crossexample interactions in gradient computation. The method allows for perexample normalization without altering the inference model architecture and demonstrates improved performance in scenarios where small minibatches or individual examples are used during training.  Strengths: 1. Innovative Approach: The paper presents a novel method to approximate population normalization with reduced crossexample interactions in gradient computation addressing the limitations of batch normalization. 2. Theoretical Analysis: The paper provides a comprehensive theoretical analysis of the proposed method outlining the computations involved in the gradient estimation and highlighting the rationale behind different modifications. 3. Experimental Results: The paper includes thorough experimentation using Inceptionv3 and ResNet50 architectures showcasing the effectiveness of the proposed method in improving model accuracy especially in scenarios with small minibatches or single examples. 4. Comparative Analysis: The comparison with baseline batch normalization twostage gradient computation and improved batch renormalization provides a clear understanding of the proposed methods performance and advantages.  Weaknesses: 1. Complexity: The proposed method involves intricate modifications to the normalization process and gradient computation potentially making it complex to implement and integrate into existing models. 2. Limited Benchmarking: While the experiments demonstrate improvements in specific scenarios benchmarking the proposed method against a wider range of models and datasets could provide further insights into its applicability and generalizability.  Questions: 1. Could you provide further insights into the computational overhead incurred by the proposed method compared to traditional batch normalization especially in scenarios with varying minibatch sizes 2. How does the proposed method perform in scenarios where the dataset characteristics differ significantly from ImageNet such as datasets with fewer training examples or different data distributions  Feedback: The paper presents a significant contribution to the field of normalization techniques in deep learning showcasing a new method that addresses the limitations of batch normalization. The thorough theoretical analysis and the extensive experimental validation strengthen the credibility of the proposed approach. However further clarification on the computational complexity and generalizability of the method could enhance the papers impact and applicability in diverse scenarios. Overall I recommend acceptance after addressing the questions and incorporating the suggested feedback.", "mwdfai8NBrJ": "Summary: The paper addresses the issue of adversarial robustness in reinforcement learning (RL) scenarios where deep neural networks (DNNs) are used. It introduces a defense procedure using randomized smoothing to make RL policies provably robust against normbounded adversarial perturbations of observations. The main theoretical contribution is an adaptive version of the NeymanPearson Lemma for RL proving robustness guarantees for DNNbased policies. Policy smoothing is proposed where a Gaussian noise is added to observations before passing them through the policy function to ensure robustness certificates on the total reward without requiring policy robustness at each timestep. The effectiveness of the method is demonstrated through experiments on various RL environments. Strengths: 1. Theoretical Contributions: The paper offers a strong theoretical framework by introducing an adaptive version of the NeymanPearson Lemma for RL proving robustness guarantees for DNNbased policies in adaptive settings. 2. Practical Application: The proposed policy smoothing technique is modelagnostic and offers a practical defense mechanism that can provide certified robustness without increasing the computational complexity of the agent\u2019s policy. 3. Empirical Validation: The experiments on standard RL benchmark tasks demonstrate the effectiveness of the method showing meaningful robustness guarantees in practice. 4. Reproducibility: The paper provides supplementary code for reproducing experimental results pretrained models and detailed information on hyperparameters and environments used enhancing the reproducibility of the study. Weaknesses: 1. Complexity: The paper contains intricate theoretical derivations and technical language that may be challenging for readers not wellversed in the field of adversarial robustness in RL. 2. Realworld Applicability: While the proposed method shows promising results in experimental settings the realworld applicability and scalability to more complex RL scenarios or practical systems may require further investigation. 3. Attack Evaluation: The analysis of the attack strategy on the defended agents could be further strengthened to provide deeper insights into the robustness of the proposed method under various adversarial scenarios. Questions to the Authors: 1. Could you elaborate on the key assumptions made in the experiments and their implications for the practical applicability of the proposed policy smoothing technique in realworld RL scenarios 2. How does your method compare to existing stateoftheart approaches in terms of computational efficiency and scalability especially when dealing with more complex RL environments or larger neural network models Feedback: The paper presents a sound theoretical foundation and practical application for addressing adversarial robustness in RL scenarios. Strengthening the discussion on realworld applicability and further evaluating the robustness of the proposed method against sophisticated attacks would enhance the paper\u2019s impact. Additionally providing clarity on key assumptions and comparisons with existing methods can improve the understanding and relevance of the research in the field of adversarial robustness in reinforcement learning. Review Summary: Overall the paper presents a comprehensive study on provable adversarial robustness in RL by introducing a novel policy smoothing technique based on randomized smoothing. The theoretical contributions are solid and experimental results demonstrate the methods effectiveness. While there are minor weaknesses in complexity and practical applicability that require further clarification the study constitutes a valuable contribution to the field. I recommend acceptance with minor revisions to address the mentioned points.", "kOtkgUGAVTX": "Summary of the Paper: The paper introduces Contrastive Intrinsic Control (CIC) an algorithm for unsupervised skill discovery in reinforcement learning that maximizes the mutual information between skills and state transitions. CIC incentivizes diverse behaviors by maximizing state entropy and distilling these behaviors into distinct skills through contrastive learning. The authors evaluate CIC on the Unsupervised Reinforcement Learning Benchmark showing a significant improvement over prior methods in terms of downstream task performance. Strong Points: 1. Innovative Approach: CIC introduces a novel algorithm that addresses the challenges of unsupervised skill discovery in RL by combining entropy maximization with contrastive learning. 2. Performance Improvement: The results demonstrate that CIC outperforms prior unsupervised skill discovery methods by a significant margin indicating the effectiveness of the proposed algorithm. 3. Theoretical Grounding: The paper provides a solid theoretical foundation for the CIC estimator and its lower bound estimate for mutual information enhancing the credibility of the proposed approach. 4. Comparison and Evaluation: The thorough evaluation of CIC against baselines in the Unsupervised RL Benchmark provides a comprehensive analysis of its performance and efficiency. Weak Points: 1. Complexity: The paper may be challenging for readers not wellversed in RL concepts as it delves into intricate details of the algorithm making it less accessible for a broader audience. 2. Assumptions: The paper assumes a deep understanding of RL frameworks and terminology which might limit its applicability to readers with less experience in the field. 3. Experimental Setup Details: While the experimental setup is explained more details on hyperparameters methodology and code availability could enhance the replicability and reproducibility of the results. Questions and Feedback to the Authors: 1. Question: Could you provide more insights into the generalizability of CIC to different environments and tasks beyond the ones evaluated in the Unsupervised RL Benchmark 2. Question: How sensitive is the performance of CIC to changes in hyperparameters and are there specific settings that are crucial for optimal performance 3. Feedback: Enhancing the discussion on the realworld applications and scalability of CIC beyond the benchmark tasks could further strengthen the practical implications of the algorithm. 4. Feedback: Including visualizations or intuitive examples to illustrate the core concepts of CIC could improve the accessibility and understanding of the paper for readers with varying levels of expertise in RL. Review Summary: The paper introduces a novel algorithm Contrastive Intrinsic Control (CIC) for unsupervised skill discovery in reinforcement learning. With a strong theoretical basis and significant performance improvement over prior methods in the Unsupervised RL Benchmark CIC demonstrates promising results. However the complexity of the paper and the need for additional clarity in experimental details suggest areas for improvement. Overall the innovative approach thorough evaluation and potential for impact warrant further exploration and development of CIC in the field of unsupervised RL.", "nrGGfMbY_qK": "Summary: The paper introduces a novel continual learning setup called iBlurry that is online taskfree classincremental has blurry task boundaries and supports anytime inference. It proposes a method Continual Learning for iBlurry (CLIB) which incorporates a new memory management scheme memoryonly training and adaptive learning rate scheduling to address the challenges of the iBlurry setup. Empirical validation demonstrates that CLIB outperforms existing continual learning models by significant margins across multiple datasets. Strengths: 1. Innovative Setup: The introduction of the iBlurry setup fills a gap in realistic continual learning scenarios considering practical aspects like classincremental learning and anytime inference. 2. Novel Contributions: The proposed CLIB method introduces new memory management memoryonly training and adaptive learning rate scheduling techniques to improve continual learning performance. 3. Empirical Validation: The paper provides extensive empirical validation across various datasets and settings showcasing the superiority of CLIB over existing methods by large margins. 4. Theoretical Justification: The paper provides theoretical underpinnings for its proposed memory management strategy enhancing the credibility of its approach. Weaknesses: 1. Complexity: The paper introduces several new components and techniques potentially making it challenging for practical implementation or adoption by those new to continual learning. 2. Evaluation Diversity: While the proposed AAUC metric is a valuable addition additional evaluation metrics could enhance the thoroughness of performance assessment. 3. Limited Discussion: The paper could benefit from a more comprehensive discussion on potential limitations or tradeoffs of the proposed approach. 4. Comparative Analysis: While comparisons with existing methods are provided more detailed comparisons and analysis could further strengthen the paper. Questions and Feedback: 1. Can you elaborate on the computational complexity implications of the proposed memory management scheme compared to traditional methods like reservoir sampling 2. How robust is the CLIB method to variations in the dataset characteristics or noise in the continual learning stream 3. Considering the proposed adaptive learning rate scheduling how sensitive is the methods performance to the choice of hyperparameters 4. Have you explored the scalability of the CLIB method to larger or more diverse datasets beyond the ones evaluated in the paper Review Summary: The paper presents significant advancements in the field of continual learning by introducing the innovative iBlurry setup and the CLIB method. With strong empirical validation demonstrating its superiority over existing methods the paper showcases promising results. While the complexity of the proposed approach and potential limitations are notable the papers unique contributions and theoretical justifications warrant a positive recommendation for publication with the suggestion for further discussion on limitations and practical implications.", "hzmQ4wOnSb": "Summary: The paper discusses the use of Graph Neural Networks (GNN) in Question Answering (QA) systems and investigates the reasoning capabilities of existing GNN modules for knowledgeaware reasoning. The authors analyze stateoftheart GNN modules and discover that they might be overcomplicated and do not effectively utilize the complex information available. To address this they propose a simple graphbased neural counter model called Graph Soft Counter (GSC) which outperforms existing GNN modules on two popular QA benchmark datasets. Strengths: 1. Innovative Approach: The paper addresses an important issue in AI and NLP by examining the reasoning capabilities of GNN modules in QA systems and proposes a novel simple yet effective solution. 2. Thorough Analysis: The use of Sparse Variational Dropout for dissecting the GNN models and identifying parts of the model that are redundant or essential shows a deep understanding of the subject matter. 3. Experimental Results: The paper provides extensive experimental results on popular QA benchmark datasets to demonstrate the effectiveness of their proposed GSC model compared to existing GNNbased methods. Weaknesses: 1. Evaluation Metrics: The paper lacks indepth discussion on the choice of evaluation metrics used to compare the performance of the proposed GSC model against existing methods. Providing a detailed analysis of specific evaluation criteria and metrics would enhance the rigor of the experiments. 2. Visualization: While the paper proposes an interpretable model with GSC more visual aids or diagrams could have been included to illustrate the workings of the GSC model and make it more comprehensible for readers. 3. Theoretical Framework: The paper could benefit from providing a more extensive theoretical framework explaining the reasoning behind the effectiveness of the proposed GSC model. Further theoretical justifications and insights could strengthen the findings. Question and Feedback to the Author: 1. Clarification on Evaluation Metrics: Could you provide more details on the selection of evaluation metrics used in comparing the performance of the GSC model with existing GNNbased methods 2. Visualization Enhancement: To improve the clarity of the GSC models operation would you consider including more visual aids or diagrams for a better understanding of how the model computes graph scores Review Summary: The paper presents a novel approach in dissecting and simplifying GNN modules for knowledgepowered QA introducing the Graph Soft Counter (GSC) model as a more efficient alternative. Despite some weaknesses in evaluation metrics visualization and theoretical depth the paper stands out for its innovative contribution indepth analysis of existing GNN modules and strong experimental results. Therefore I recommend accepting the paper for publication with some revisions addressing the identified weaknesses for further clarity and rigor.", "kNKFOXleuC": " Paper Summary: The paper proposes a novel approach called Anytime Dense Prediction with Confidence (ADPC) for anytime dense visual prediction. The method integrates early exiting redesigned prediction heads and confidence adaptivity to enable anytime inference without sacrificing accuracy while reducing total computation significantly. The approach is evaluated on two dense prediction tasks: Cityscapes semantic segmentation and MPII human pose estimation showing promising results in terms of accuracy and reduced FLOPs.  Strong Points: 1. Innovative Approach: Introducing an endtoend approach for anytime dense prediction is a valuable contribution to the field. 2. Comprehensive Methodology: The paper covers early exiting head redesign and confidence adaptivity techniques providing a thorough approach to tackle anytime inference for dense prediction tasks. 3. Performance Validation: The proposed method is evaluated on challenging tasks like semantic segmentation and human pose estimation showcasing promising results in terms of accuracy and computational efficiency.  Weak Points: 1. Evaluation Scope: While the experimental results are insightful more diverse datasets and tasks could further validate the versatility and generalizability of the proposed method. 2. Comparative Analysis: While the comparisons with baseline models are informative extending comparisons to more stateoftheart methods or benchmarks would strengthen the papers positioning in the field.  Question to Authors:  How does the proposed method generalize to other dense prediction tasks beyond semantic segmentation and human pose estimation  Have you considered exploring different confidence measures or thresholds for the adaptivity scheme to optimize performance further  Feedback to Authors:  Consider expanding the discussion section to provide a deeper analysis of the strengths and limitations of the proposed approach.  It would be beneficial to provide insights into potential future directions for enhancing the proposed anytime dense prediction framework.  Review Summary: The paper introduces an innovative approach ADPC for anytime dense visual prediction through a combination of early exiting redesigning prediction heads and confidence adaptivity techniques. The experimental evaluation on two challenging tasks demonstrates promising results in terms of maintaining accuracy while significantly reducing total computation. Despite some limitations in the evaluation scope and comparative analysis the paper presents a comprehensive and noteworthy contribution to the field. Thus I recommend this paper for publication with minor revisions to address the weak points highlighted above.", "gD0KBsQcGKg": " Review of the Paper  1. Summary of the Paper: The paper proposes a novel method for generating prediction intervals (PIs) by redefining them as a union of disjoint intervals to account for multimodality in regression uncertainty estimation. The study demonstrates the limitations of current methods that use a single continuous PI leading to poor uncertainty estimation in the presence of multiple modes. Through experiments on UCI benchmark datasets the proposed method shows a significant improvement over existing stateoftheart methods by reducing the average PI width by over 27.  2. Strengths and Weaknesses:  Strong Points:  Novel Methodology: The paper introduces a new approach for generating PIs by considering multimodality in regression uncertainty estimation which improves performance significantly.  Experimental Validation: The study conducts experiments on realworld datasets to validate the proposed method demonstrating superior performance over existing methods.  Ensemble Method: The incorporation of an ensemble method for in and outofdistribution observations enhances the predictive uncertainty estimation.  Coverage Probability Estimation: Providing coverage probability for each disjoint PI offers additional insights into the reliability of the intervals.  Weak Points:  Algorithm Complexity: The paper introduces complex derivations and neural network architectures which may be challenging to implement and understand for some readers.  Hyperparameter Optimization: The paper mentions the difficulty in optimizing hyperparameters for existing stateoftheart methods which may affect reproducibility and generalizability.  Limited Comparison: The comparison with other methods is limited to specific metrics and a more comprehensive evaluation could provide further insights.  Realworld Applicability: While the method shows promise on benchmark datasets the realworld applicability and generalization across various domains need further exploration.  Question and Feedback to the Authors: 1. Could you provide more insights on the interpretability of the proposed DDD method in terms of how the disjoint PIs can enhance decisionmaking compared to traditional continuous PIs 2. How do you foresee the scalability and generalizability of the proposed method across different types of regression problems and datasets 3. Considering the complexity of the proposed methodology are there any plans to simplify the implementation or provide opensource code to facilitate adoption and further research 4. Have you considered conducting user studies or practical applications to validate the utility and usability of the proposed approach in realworld scenarios  3. Review Summary: The paper presents a novel approach for addressing multimodality in regression uncertainty estimation by redefining PIs as a union of disjoint intervals. The experimental results on UCI benchmark datasets demonstrate a substantial improvement over existing methods establishing the effectiveness of the proposed method. While the methodology is innovative and the experiments are robust the paper could benefit from addressing algorithm complexity enhancing interpretability and exploring realworld applicability. Overall I recommend acceptance of the paper with possible revisions to address the highlighted points for clarity and completeness.", "lEB5Dnz_MmH": " Summary The paper proposes a Collaborative Attention Adaptive Transformer approach for financial market forecasting (CAFF) that effectively integrates social media data and real market prices to improve trading decisions. The main contributions include introducing a coattention Transformer fusion approach inspired by multimodal fusion methods proposing a deep fusion framework for multisource data analysis and achieving superior accuracy and profit increase in stock market prediction tasks. Experimental results demonstrate the effectiveness of the CAFF model compared to baseline methods.  Strengths  Novel Approach: The paper introduces an innovative framework that effectively fuses tweets and market prices through a collaborative attention Transformer approach which outperforms existing methods in financial market forecasting.  Extensive Experiments: The paper conducts thorough experiments on stock market data to validate the proposed models superior accuracy and practical application value demonstrating its effectiveness.  Comparative Analysis: The paper provides a comprehensive comparison with baseline models and fusion methods highlighting the strengths of the proposed CAFF model in terms of performance and profitability.  Quantitative Analysis: The quantitative evaluation of different fusion methods and ablation studies provides valuable insights into the critical role of text data in improving stock price prediction accuracy.  Weaknesses  Conceptual Explanation: The paper could benefit from providing a more detailed explanation of the theoretical underpinnings and practical implementation of the CAFF model for readers to better understand the proposed approach.  Reproducibility: While the availability of the dataset on GitHub is mentioned additional details on data preprocessing steps model hyperparameters and code implementation could enhance reproducibility for future researchers.  Questions for the Authors 1. Can you elaborate on the choice of the specific hyperparameters used in the experimental setup such as learning rate batch size and optimizer and provide justification for their selection 2. How do you envision the CAFF model being applied in realworld financial market scenarios and what factors need to be considered for practical implementation and scalability 3. Have you considered the potential impact of external factors such as market volatility news events or economic indicators on the performance and robustness of the CAFF model in different market conditions  Feedback The paper presents a promising framework for financial market forecasting that effectively integrates social media data and market prices. Strengthening the theoretical clarity providing more details on experimental setup and results interpretation could enhance the papers impact and readability. Additionally addressing the practical implications and external factors affecting model performance can further enrich the discussion on the CAFF models applicability in realworld trading scenarios. Overall the papers innovative approach and empirical validation make a significant contribution to the field of financial market prediction.", "oAy7yPmdNz": " Review of \"Accelerating Inference and Training of CoordinateBased MLPs for Implicit Neural Representations with Split MLP Architecture CoordX\"  Summary: The paper introduces a new split MLP architecture CoordX to accelerate inference and training of coordinatebased MLPs for implicit neural representations. The proposed model splits input coordinates into separate dimensions processes them independently in parallel branches initially and then fuses them in the final layers to generate the learned signal. CoordX significantly reduces computation requirements while maintaining similar accuracy compared to baseline MLPs. The paper provides detailed methodologies on splitting fusion and handling training complexities ensuring no added memory overhead. Experimental results demonstrate up to 2.92x speedup in image video and 3D shape rendering tasks.  Strong Points: 1. Novel Contribution: The proposed CoordX architecture is innovative and addresses a significant challenge in accelerating inference and training for implicit neural representations. 2. Speed and Efficiency: The paper demonstrates substantial speedups in both training and inference without compromising model accuracy showcasing the effectiveness of the CoordX model. 3. Comprehensive Evaluation: The paper provides detailed experimental evaluations across various tasks like image video and 3D shape representation along with comparisons to baseline models displaying the practical applicability of CoordX. 4. Technical Depth: The paper delves into the technical aspects of model design training complexities and inference speed analysis providing a comprehensive understanding of the proposed architecture.  Weak Points: 1. Quality Drop: The paper acknowledges a slight drop in accuracy (e.g. PSNR) for image representation tasks with CoordX. It would be beneficial to provide deeper insights or further strategies to mitigate this loss. 2. Complexity in Sampling: The requirement for specific sampling strategies during training to ensure all combinations of split inputs are available may pose practical challenges. Offering simpler or more generalizable approaches could enhance the applicability of the method.  Question to the Authors: 1. Could you elaborate on potential strategies to further enhance the accuracy of CoordX models especially in scenarios where high frequency variations in signals are critical 2. How do you envision the scalability and generalizability of CoordX to more complex tasks beyond the ones evaluated in the paper  Feedback to the Authors: 1. Consider providing insights into strategies to mitigate the accuracy drop observed in certain scenarios enhancing the practical utility of CoordX across a wider range of tasks. 2. Address the implications of sampling strategies on realworld implementation feasibility aiming for more practical and scalable solutions for diverse applications.  Review Summary: The paper presents a novel split MLP architecture CoordX that effectively accelerates training and inference for coordinatebased MLPs providing significant speedups while maintaining accuracy. The detailed methodologies comprehensive experimental evaluations and technical depth signify the papers contribution to the field. Addressing minor concerns with accuracy drops and complexities in sampling strategies could further enhance the impact and usability of CoordX. Overall considering the strengths and potential improvements I recommend acceptance of the paper with minor revisions to address the highlighted points.", "oapKSVM2bcj": " Peer Review  1) Summary of the Paper: The paper introduces `einops` a minimalist notation for tensor manipulations that addresses issues in conventional tensor methods and aims to improve code readability reliability and flexibility in deep learning frameworks. It emphasizes structuring tensor operations based on a pattern description of input and output tensors. The paper discusses the drawbacks of mainstream tensor manipulation approaches and the challenges related to reliable and scalable code development in the context of modern deep learning architectures.  2) Strengths and Weaknesses: Strong Points:  Innovative Notation: The `einops` notation provides a unique and straightforward way to describe tensor manipulations enhancing code readability and maintainability.  Code Flexibility: The paper highlights how `einops` offers more flexibility in expressing tensor operations compared to conventional methods mitigating common mistakes and improving code structure.  CrossFramework Support: The implementation of `einops` in a Python package that supports various widelyused frameworks is a significant advantage for its adoption.  Performance and Efficiency: The caching mechanism and the efficient conversion of `einops` patterns into tensor operations ensure minimal overhead on top of deep learning frameworks.  Extensive Explanation and Use Cases: Detailed case studies and examples illustrate the application of `einops` in transforming existing DL code demonstrating its efficacy and usability. Weak Points:  Complexity Concerns: The paper addresses criticisms of `einops` being \"stringlytyped\" which may deter users accustomed to traditional tensor manipulation methods that rely on tuplebased patterns.  Integration Challenges: While `einops` provides a more declarative approach to tensor operations it does not enforce alignment of shapesaxes across operations which might limit integrated shape analysistracking.  Adoption Rate: While the paper mentions an increasing adoption of `einops` in GitHub projects further validation or benchmarks comparing its adoption and performance against conventional methods could strengthen its significance. Questions and Feedback to the Author: 1. Can you provide more insights into potential strategies to address concerns regarding the \"stringlytyped\" nature of `einops` and how it may impact user adoption especially in the context of users more familiar with tuplebased patterns 2. How do you foresee the integration of `einops` into other languages or lowerlevel primitives impacting its performance and usability in the context of deep learning frameworks or scientific computing beyond Python 3. Considering the growing complexity of deep learning architectures have you conducted any user studies or surveys to gauge the ease of learning and adoption of `einops` particularly among practitioners with varying levels of expertise in tensor operations  3) Review Summary: The paper presents a novel and promising solution in the form of `einops` notation for addressing the limitations of mainstream approaches to tensor operations in deep learning frameworks. By introducing a more structured and declarative way of defining tensor manipulations `einops` offers improved code readability reliability and flexibility as evidenced by detailed case studies and examples provided in the paper. The crossframework support and efficiency optimizations in `einops` implementation further strengthen its value as a convenient tool for researchers and engineers in the deep learning community. While there are some concerns regarding potential integration challenges and the \"stringlytyped\" nature of the notation the paper stands as a comprehensive and valuable contribution to advancing the field of tensor computations and deep learning. Given the innovative approach and the demonstrated benefits of `einops` in addressing common issues in tensor manipulations I recommend accepting this paper for publication as it offers a significant advancement in improving the efficiency and maintainability of tensor operations in deep learning frameworks. Kindly address the questions and feedback provided to further enhance the clarity and practical implications of the `einops` notation. Well done on the thorough research and detailed presentation of your findings.", "rwE8SshAlxw": " 1) Summary: The paper proposes a method called unsupervised discovery of Object Radiance Fields (uORF) for inferring objectcentric scene representations from a single image. The key goal is to develop a representation that is learned without supervision explains the image formation process and captures the 3D nature of the scene. uORF integrates recent advancements in neural 3D scene representations and rendering with deep inference networks to achieve unsupervised 3D scene decomposition. The method is trained on multiview RGB images and can decompose complex scenes with diverse background textures from a single image. Through extensive experiments on three datasets of increasing complexity uORF is shown to excel in tasks such as scene segmentation editing in 3D and novel view synthesis.  2) Strengths and Weaknesses: Strong Points:  The paper addresses a significant problem in computer vision  unsupervised 3D scene decomposition from a single image.  The proposed uORF method integrates neural radiance fields with deep inference networks showing promising results in objectcentric scene representation.  The integration of NeRFs allows for handling complex scenes with diverse background environments.  The paper demonstrates the effectiveness of uORF through thorough experiments on different datasets showcasing its capabilities in scene segmentation novel view synthesis and scene editing in 3D.  The methodology is wellstructured and the presentation is detailed making it easy to understand the proposed approach. Weak Points:  The paper heavily relies on technical details and may be challenging for readers without a strong background in neural networks and computer vision.  Limited comparison with existing methods especially those outside the realm of neural radiance fields.  The evaluation metrics used could be further elaborated to provide more insights into the performance of uORF.  The discussion on computational efficiency and scalability could be more detailed to address practical implementation considerations.  The realworld applicability and scalability of uORF in scenarios beyond synthetic datasets could be further explored. Questions and Feedback to the Author: 1. Could you elaborate on the potential challenges or limitations of implementing the uORF method in realworld settings especially with respect to computational resources and scalability 2. How do you envision the integration of uORF into practical applications and its potential impact on realworld computer vision tasks 3. Have you considered any qualitative user studies or case studies to evaluate the usability and interpretability of the scene representations generated by uORF 4. In the discussion section could you provide more insights into the interpretability of the object radiance fields inferred by uORF and how they could be leveraged for downstream tasks such as robotics or augmented reality applications  3) Review Summary: The paper presents a novel approach uORF for unsupervised 3D scene decomposition from a single image. Through a comprehensive set of experiments on multiple datasets the paper demonstrates the effectiveness of uORF in generating objectcentric scene representations that are 3Daware and capable of tasks such as scene segmentation editing in 3D and novel view synthesis. While the paper showcases promising results and technical advancements in the field of computer vision further elaboration on practical implementation challenges comparison with a broader set of existing methods and realworld applicability could enhance the impact and completeness of the study. Based on the strong experimental results and the innovative methodological contribution I recommend this paper for publication with minor revisions to address the aforementioned points for a more comprehensive discussion.", "hjd-kcpDpf2": " Summary: The paper introduces Maximize Ensemble Diversity in Reinforcement Learning (MEDRL) a set of regularization methods to improve the diversity among neural networks in ensemblebased deep reinforcement learning algorithms. The study focuses on preventing convergence of representations in neural networks within the ensemble to enhance performance. The authors integrate MEDRL in various ensemblebased deep RL algorithms for both continuous and discrete control tasks and evaluate the performance on Mujoco environments and Atari games. Results indicate significant performance gains and improved sampleefficiency with MEDRL.  Strong Points: 1. Innovative Approach: The paper introduces a novel regularization technique inspired by economics and consensus optimization to improve diversity in ensemblebased deep RL methods. 2. Empirical Evaluation: The authors provide a comprehensive evaluation of the proposed method on a variety of tasks demonstrating significant performance improvements and increased sampleefficiency. 3. Contribution to the Field: The paper addresses a critical issue of diversity in ensemblebased methods which has implications for the performance and sampleefficiency of deep RL algorithms. 4. Related Work: The paper thoroughly discusses related work in the field highlighting the significance of ensemble methods and diversity in reinforcement learning.  Weak Points: 1. Theoretical Justification: While the rationale behind the regularization methods is wellexplained a more detailed theoretical justification could enhance the papers rigor. 2. Clarity in Experiments: More clarity could be provided in experimental setups and comparisons with baseline methods for a deeper understanding of the results. 3. Visualization and Interpretation: Additional visualizations or interpretations of results could aid in understanding the impact of MEDRL on different tasks and settings.  Questions and Feedback: 1. Question: How do the regularization methods proposed in MEDRL compare to other diversitypromoting techniques in deep reinforcement learning 2. Feedback: Enhancing the discussion on the interpretability of ensemble diversity and its impact on model generalization could strengthen the papers contribution.  Review Summary: The paper presents a novel approach MEDRL to enhance diversity in ensemblebased deep reinforcement learning methods. The empirical results demonstrate the effectiveness of the proposed regularization methods showcasing significant performance gains and sampleefficiency improvements across various tasks. The thorough evaluation and clear presentation make a strong case for the adoption of MEDRL in deep RL algorithms. Overall the study contributes valuable insights to the field of reinforcement learning and ensemble methods. Based on the innovative approach extensive evaluation and promising results I recommend this paper for publication with minor revisions to address the weak points highlighted.", "qWhajfmKEUt": " Summary of the Paper The paper explores the connection between spectral signatures of deep features and adversarial robustness in deep neural networks. It introduces a new metric to measure the change of features along eigenvectors under adversarial attacks. The study reveals that eigenvectors with smaller eigenvalues are more nonrobust and it proposes a method called Feature Spectral Regularization (FSR) to alleviate the dominance of larger eigenvalues and improve adversarial robustness. The effectiveness of FSR is demonstrated through comprehensive experiments on different datasets.  Strong Points 1. Novel Contribution: The paper introduces a unique perspective on adversarial robustness by analyzing spectral signatures of deep features and proposing FSR to enhance adversarial robustness. 2. Theoretical Underpinnings: The theoretical analysis provided in the paper offers a solid foundation for the proposed method enhancing the credibility of the findings. 3. Comprehensive Experiments: The paper conducts detailed experiments on various datasets and scenarios providing substantial empirical evidence to support the effectiveness of FSR. 4. Detailed Methodology: The paper presents a clear methodology including definitions algorithms and theoretical derivations making it easy to follow and replicate the study.  Weak Points 1. Clarity of Presentation: Some sections of the paper especially the technical details could be further clarified for better understanding by a wider audience. 2. Comparison with Existing Methods: While the paper highlights the limitations of previous methods a more detailed comparison with existing stateoftheart techniques could provide a stronger context for the proposed approach. 3. Code Availability: While promising the availability of codes soon providing immediate access to the codebase would enhance reproducibility and facilitate further research in the field.  Questions to the Authors 1. Can you elaborate on the potential scalability of the proposed FSR method to larger datasets and more complex neural network architectures 2. How do you envision integrating FSR into realworld applications to enhance the security and robustness of deep neural networks 3. Could you provide insights on any potential limitations or drawbacks of the FSR technique that were encountered during the experimental evaluation  Feedback to the Authors The paper presents a valuable contribution to the field of adversarial robustness in deep neural networks. Strengthening the clarity of presentation and comparisons with existing methods would enhance the impact and relevance of the work. Additionally addressing questions regarding scalability practical integration and limitations of the proposed method could further enrich the discussion and practical implications of the study.  Review Summary This paper introduces a novel perspective on improving adversarial robustness in deep neural networks through spectral analysis and the proposed FSR method. The wellsupported theoretical analysis and comprehensive experimental validation underscore the efficacy of FSR in enhancing model robustness. With some enhancements in clarity comparisons and addressing additional questions raised the paper has the potential to make a significant contribution to the field. Therefore I recommend accepting this paper with minor revisions to strengthen its impact and practical relevance.", "p0rCmDEN_-": " PeerReview of \"Dynamical Recurrent Classifier: Spatiotemporal Computation in Early Visual Areas\"   1. Summary: The paper explores a novel approach of enhancing visual recognition through the use of dynamical features mimicking recurrent neuronal computations in early visual areas. The study introduces a Dynamical Recurrent Classifier (DRC) by training a convolutional neural network with recurrent connectivity in early layers to process sequences of lowresolution images generated via sensor motion mimicking ocular drift. The system improved classification accuracy substantially compared to static feedforward classifiers when tested on CiFAR10 and CiFAR100 datasets downsampled via an 8x8 sensor. The results also show that trajectory curvature positively influences recognition accuracy.  2. Strengths and Weaknesses:  Strong Points:  Innovative Approach: The introduction of a DRC that exploits dynamical features for image classification is a novel and creative approach.  Experimental Rigor: The study design is well organized with clear experimental setups and detailed explanations on methodology and results.  Relevance to AI Applications: The proposed solution for artificial image recognition in settings with limited resolution is highly relevant for edge AI applications.  Thorough Analysis: The indepth analysis of features extracted by the DRC and the effect of sensor trajectory on recognition accuracy add valuable insights.  Weak Points:  Lack of Comparison Metrics: While the DRCs performance is extensively evaluated comparisons with other stateoftheart methods or benchmarks could further validate the approach.  Complexity in Understanding: The technical jargon and detailed descriptions might be challenging for readers not wellversed in the topic which could affect the papers accessibility.  Minimal Empirical Validation: More empirical validation such as experiments on realworld datasets or benchmarks could strengthen the generalizability of the findings.  Questions and Feedback:  Comparative Analysis: Have the authors considered comparing the DRCs performance against other dynamic classifiers or models in similar settings to assess its competitiveness further  Empirical Validation: How do the authors plan to validate the DRCs performance on realworld datasets beyond CiFAR showing its applicability in broader contexts and scenarios  3. Review Summary: The paper presents an innovative solution the Dynamical Recurrent Classifier (DRC) showcasing the benefits of incorporating dynamical features for image recognition in lowresolution settings. The study is wellstructured with rigorously conducted experiments and detailed analyses. However the paper could benefit from additional comparative analysis with existing methods and more empirical validation on diverse datasets to enhance the robustness and applicability of the proposed model. The researchs novelty relevance to AI applications and thorough analyses make it a promising contribution to the field of computer vision. Further validation and refinement would elevate the impact and significance of the study.  Recommendation: Given the innovative approach comprehensive analyses and potential relevance to AI applications the paper demonstrates promising research in the field of computer vision. However to strengthen the papers impact and credibility I recommend clarification on comparative analyses and further empirical validation on realworld datasets. With these enhancements the paper has the potential to significantly contribute to advancing research in the domain of visual processing and artificial intelligence. ", "u7UxOTefG2": " Peer Review  1. Summary: The paper challenges the common assumption that Bayesian neural networks (BNNs) are inherently suitable for outofdistribution (OOD) detection due to their epistemic uncertainty. By studying infinitewidth networks and different architectural choices the authors find that the uncertainties induced by BNNs may not reflect the underlying data generating process making them unsuitable for OOD detection. The paper explores the influence of architecture choices and weight space priors on OOD detection capabilities suggesting a tradeoff between generalization and OOD detection. The study raises fundamental issues in using BNNs for OOD detection and suggests avenues for future research.  2. Strengths and Weaknesses:  Strong Points:  Deep Analysis: The paper provides indepth analysis through theoretical concepts empirical results and visualizations.  Methodological Approach: The study employs both theoretical and practical approaches to investigate the suitability of BNNs for OOD detection.  Architectural Insights: The discussion on the impact of architecture choices and weight space priors on OOD detection is insightful and provides valuable insights.  Relevance: The topic of OOD detection using BNNs is relevant and addresses a key challenge in machine learning applications.  Weak Points:  Limitation to LowDimensional Data: The findings are limited to lowdimensional problems which restricts the generalizability of the conclusions to realworld highdimensional datasets.  Scope: The paper focuses on theoretical arguments and lowdimensional problems limiting the practical applicability of the findings to realworld scenarios.  Validation Method: The proposed validation method using generative models based on epistemic uncertainty may need further validation and practical applicability assessment.  Question to the Authors:  Have you considered the scalability of your findings to highdimensional datasets commonly encountered in realworld applications  How do you plan to further validate the proposed generative modelbased OOD evaluation method especially in complex and highdimensional datasets  Feedback to the Authors:  Consider expanding the validation method to include experiments on highdimensional datasets to enhance the generalizability of the findings.  Discuss the practical implementation and feasibility of the proposed generative modelbased OOD evaluation method to provide a more concrete validation of the approach.  3. Review Summary: The paper presents a detailed exploration of the limitations of using Bayesian neural networks for outofdistribution detection. The study offers valuable insights into the impact of architectural choices and weight space priors on the OOD detection capabilities of BNNs. While the analysis is thorough and theoretically sound the focus on lowdimensional problems limits the practical applicability of the findings. Further validation on highdimensional datasets and refinement of the proposed generative modelbased evaluation method would enhance the credibility and usability of the research.  Recommendation: Based on the thorough analysis theoretical depth and relevance of the topic I recommend the paper for publication after addressing the limitations mentioned above and refining the proposed validation method for realworld applicability.", "ibqTBNfJmi": " Summary: The paper proposes Frequencyaware Stochastic Gradient Descent (FASGD) and Counterbased Frequencyaware Stochastic Gradient Descent (CFSGD) algorithms for efficient learning of quality embeddings in embedding learning problems. The key focus is on leveraging token frequency information to design provably efficient algorithms that outperform standard Stochastic Gradient Descent (SGD) for imbalanced token distributions. The theoretical analysis highlights the convergence properties of FASGD and CFSGD demonstrating their advantages over standard SGD. Extensive experiments on benchmark recommendation datasets and a largescale industrial recommendation system validate the effectiveness of the proposed algorithms.  Strengths:  Innovative Approach: The paper introduces novel algorithms that leverage token frequency information for efficient learning of embeddings addressing the challenges posed by imbalanced token distributions.  Theoretical Analysis: The theoretical results provide clear insights into the convergence properties of FASGD and CFSGD demonstrating provable benefits over standard SGD for nonconvex embedding learning problems.  Empirical Validation: Extensive experiments on benchmark datasets and a largescale industrial system validate the effectiveness of the proposed algorithms showcasing improved convergence rates and performance over baseline methods.  Memory Efficiency: The highlighted memory efficiency of CFSGD compared to adaptive algorithms is a crucial practical advantage especially for largescale models with substantial memory requirements.  Weaknesses:  Clarity and Concision: The paper is densely packed with technical details which may make it challenging for readers new to the topic to grasp the key points easily.  Need for Simplification: While the theoretical and empirical findings are comprehensive there is a need for succinct summaries or visual aids to help readers better understand the main contributions and results.  Practical Implementation: While the theoretical soundness and empirical results are impressive practical guidelines or implementation details for incorporating the proposed algorithms into realworld systems could enhance the papers practical utility.  Questions: 1. Clarification on Algorithm Implementation: Could the paper provide more insights into how the proposed algorithms (FASGD and CFSGD) can be practically implemented in realworld recommendation systems 2. Scalability Considerations: Are there any scalability limitations observed during the experiments especially in the context of dealing with largescale industrial datasets 3. Comparison with StateoftheArt Approaches: How do the proposed FASGD and CFSGD algorithms compare with other cuttingedge approaches in similar domains such as Transformerbased models or graph neural networks  Feedback: The paper presents a compelling solution to the challenges of embedding learning by introducing innovative algorithms that exploit token frequency information. Clarifying practical implementation details and exploring comparisons with stateoftheart approaches would further enhance the impact of the proposed algorithms. Additionally simplifying the presentation without compromising depth would make the findings more accessible to a wider audience. Overall the theoretical foundation and empirical validation of the proposed algorithms solidify their significance in the field of embedding learning.", "rMbLORc8oS": "Summary of the Paper: The paper introduces a method called SemiRetro that combines the advantages of templatebased (TB) and templatefree (TF) approaches for retrosynthesis prediction. By breaking full templates into simpler semitemplates and utilizing a twostep TF framework SemiRetro aims to reduce template redundancy while preserving essential chemical knowledge. The method also introduces a directed relational graph attention (DRGAT) layer for better center identification. Experimental results on the USPTO50k dataset demonstrate that SemiRetro significantly outperforms existing TB and TF methods in terms of accuracy scalability interpretability and training efficiency. Strengths of the Paper: 1. Innovation: The paper introduces a novel approach that combines the strengths of TB and TF methods to enhance retrosynthesis prediction accuracy. 2. Performance: Experimental results show that SemiRetro outperforms existing methods in terms of both accuracy and scalability. 3. Scalability: The use of semitemplates helps reduce template redundancy and improve coverage indicating scalability improvements. 4. Interpretability: SemiRetro offers better interpretability compared to other methods enhancing the understanding of synthetic routes. 5. Training Efficiency: The paper highlights that SemiRetro trains significantly faster than existing methods making it more efficient for practical applications. Weaknesses of the Paper: 1. Evaluation: While the paper discusses the effectiveness of SemiRetro more detailed comparisons with a broader range of existing methods could strengthen the evaluation. 2. Generalization: The paper mainly focuses on the USPTO50k dataset including evaluations on other datasets would enhance the generalizability of the proposed method. 3. Discussion: Further elaboration on the limitations or potential drawbacks of the proposed approach could provide a more comprehensive view of the methods applicability. Questions and Feedback to the Authors: 1. Question: How does SemiRetro perform when applied to datasets beyond the USPTO50k dataset Have you considered conducting experiments on other benchmark datasets to evaluate the generalizability of the method 2. Feedback: Providing additional insights into the limitations or potential challenges of SemiRetro and how these could be addressed in future research would enrich the discussion section. Review Summary: The paper presents a novel method SemiRetro that combines the strengths of templatebased and templatefree approaches for retrosynthesis prediction. The method shows significant improvements in accuracy scalability interpretability and training efficiency compared to existing methods. Despite its strengths the paper could benefit from expanding the evaluation on different datasets and providing a more comprehensive discussion on the methods limitations. Overall based on the innovative approach superior performance and promising results I recommend acceptance of this paper for publication.", "givsRXsOt9r": "Summary: The paper introduces the novel concept of spherical message passing (SMP) for 3D molecular graph representation learning. It addresses the challenge of efficiently processing 3D molecular structures by proposing a message passing framework that considers distance angle and torsion information. The paper further extends this concept to develop SphereNet a neural network architecture for 3D molecular learning. Experimental results demonstrate the superior performance of SphereNet compared to baseline methods on benchmark datasets. Strengths: 1. Innovative Concept: Introducing SMP for 3D molecular graph representation learning is a novel and significant contribution to the field. 2. Efficiency: The paper addresses the scalability issue by proposing an efficient message passing scheme that reduces computational complexity while maintaining performance. 3. Experimental Validation: Extensive experiments on benchmark datasets demonstrate the effectiveness of SphereNet in improving prediction tasks for 3D molecular learning. 4. Rigorous Analysis: The paper provides a detailed formal analysis of the completeness and efficiency of the proposed methods offering clear justifications for design choices. 5. Open Source Code: Public availability of the code in the DIG library enhances reproducibility and transparency of the research. Weaknesses: 1. Incomplete Discussion: While the paper extensively discusses the proposed methods it could benefit from a deeper discussion on the limitations and potential future directions of the research. 2. Comparison Metrics: The paper could have provided a more comprehensive comparison with existing methods including a detailed analysis of strengths and weaknesses in terms of specific performance metrics. 3. Visualization: While the ablation study results are presented in Table 5 visual representations or diagrams could enhance the understanding of the impact of different geometries on model performance. Questions and Feedback to the Author: 1. Can you describe how the proposed SMP and SphereNet can be extended or adapted to handle more complex molecular structures or tasks beyond the datasets used in the experiments 2. How does the proposed SphereNet architecture handle transfer learning or domain adaptation for tasks in domains other than molecular learning 3. Have you considered applying interpretability techniques to the models to provide insights into the decisionmaking process and enhance model transparency Review Summary: The paper presents a novel approach SMP and SphereNet for 3D molecular graph representation learning addressing the need for efficient and effective methods in this underexplored area. The proposed methods show significant performance improvements and scalability backed by rigorous analyses and experimental results. While the paper has strengths in innovation efficiency and experimental validation it could benefit from deeper discussions on limitations and future directions to further enhance the impact of the research. Overall I recommend this paper for publication due to its innovative approach comprehensive analysis and promising experimental results.", "jWaLuyg6OEw": "1) Summary: This paper investigates the performance of two firstorder optimization algorithms obtained from forward Euler discretization of finitetime optimization flows\u2014the rescaledgradient flow (RGF) and the signedgradient flow (SGF). The study provides convergence guarantees for these algorithms in both deterministic and stochastic settings and applies them to academic examples and deep neural networks training on the SVHN dataset. The results indicate that the proposed schemes demonstrate faster convergence compared to standard optimization alternatives. 2) Strengths and Weaknesses: Strong Points:  The paper delves into an innovative approach of discretizing finitetime optimization flows which is a novel and interesting concept.  The theoretical analysis provides convergence guarantees for the proposed algorithms in both deterministic and stochastic settings.  Empirical testing on the SVHN dataset validates the performance of the algorithms showing faster convergence against standard optimization alternatives.  The paper is comprehensive covering background theory methodological details and numerical experiments providing a wellrounded analysis. Weak Points:  The paper can benefit from more detailed explanations in certain sections especially for readers less familiar with the mathematical concepts used.  The experimental results could be further enhanced by discussing potential limitations or challenges encountered during the implementation on realworld datasets. Questions and Feedback to the Author:  Could you elaborate on the algorithmic implementations of RGF and SGF in the context of deep neural network training on the SVHN dataset  How do you evaluate the robustness of the proposed algorithms to noise or perturbations in realworld optimization scenarios  Were there any specific challenges faced during the design and execution of the numerical experiments and how were they addressed  Considering the potentials of the proposed algorithms how do you envision their practical applications in various optimization problems beyond the scope of this study 3) Review Summary: The paper presents an insightful investigation into the discretization of finitetime optimization flows through the proposed algorithms RGF and SGF showing promising convergence guarantees and empirical results on the SVHN dataset. The theoretical analysis is solid and the experimental validation adds credibility and practical relevance to the study. Overall the paper is wellstructured informative and contributes valuable insights to the optimization algorithms domain. I recommend the paper for publication with minor revisions to enhance clarity in certain sections and include discussions on practical implications and challenges.", "q79uMSC6ZBT": " Summary of the Paper The paper introduces GRAMMFORMER a Transformerbased model designed for code completion that generates code suggestions in the form of \"sketches\" with holes instead of fully completed sequences. The model is trained on code completion tasks for C and Python languages with the goal of generating more accurate completions than traditional generative models. The paper presents a novel evaluation metric called REGEXACC that measures the quality of predictions with holes and compares the performance of GRAMMFORMER to standard sequence models.  Strengths of the Paper 1. Novel Approach: The paper introduces a novel concept of code completion using sketches with holes which is a significant departure from traditional generative models. 2. Theoretical Foundation: The paper provides a detailed explanation of the model architecture training approach and evaluation metrics giving a comprehensive understanding of the proposed methodology. 3. Evaluation Metrics: Introducing a new metric REGEXACC to evaluate the success of generating completions with as few holes as possible adds value to the research in the field of code completion. 4. Empirical Results: The paper presents empirical results showing improvements in accuracy and sketch length compared to baselines demonstrating the effectiveness of the proposed GRAMMFORMER model. 5. Thorough Experiments: The paper includes detailed experiments comparisons with baselines and ablations to assess the performance of the proposed model comprehensively.  Weaknesses of the Paper 1. Dataset Description: The paper lacks a detailed description of the dataset used for training and evaluation including the size diversity and source of the dataset. 2. Model Complexity: The complexity of the GRAMMFORMER model which involves separate encoder and decoder modules and reinforcement learning may make it challenging to understand and reproduce the results for researchers with limited resources. 3. Scalability Concerns: The computational cost of GRAMMFORMER is mentioned as higher than standard transformer models which may raise scalability concerns especially for applications in larger codebases or realtime use cases. 4. Future Work Implications: The paper opens up avenues for future research directions but does not delve deeply into potential extensions of the proposed model or address scalability concerns in practical applications. 5. Comparison with StateoftheArt: While the paper compares GRAMMFORMER to standard sequence models a comparison with stateoftheart code completion models could provide additional insights into the models performance and relevance.  Question to the Author 1. Can you provide more details on the dataset used for training and evaluation including the size diversity and any preprocessing steps applied 2. How does the proposed model GRAMMFORMER compare to existing stateoftheart code completion models in terms of accuracy efficiency and scalability 3. Have you conducted user studies or gathered feedback from developers to evaluate the usability and effectiveness of GRAMMFORMER in realworld code completion tasks  Feedback to the Author 1. Consider providing more insights into the dataset used and any challenges faced during data preprocessing to improve the reproducibility and transparency of the research. 2. Address the scalability concerns of the proposed model by discussing potential optimizations or strategies to make GRAMMFORMER more efficient for larger codebases. 3. Expand on the future research implications by discussing potential extensions or applications of the GRAMMFORMER model in different scenarios or programming languages.  Review Summary The paper presents a novel approach to code completion using sketches with holes introducing the GRAMMFORMER model that outperforms traditional generative models in accuracy and sketch length. The theoretical foundation detailed experiments and introduction of a new evaluation metric add value to the field of code completion research. However the paper could benefit from providing more details on the dataset addressing scalability concerns comparing with stateoftheart models and exploring realworld usability through user studies. Overall the paper presents a promising direction for code completion research with evident strengths in methodology and results.", "sPfB2PI87BZ": " Summary of the Paper: The paper introduces a novel approach called Optimal Sample Transformation and Reweight (OSTAR) for unsupervised domain adaptation (UDA) in the context of Generalized Target Shift (GeTarS). Unlike existing methods that focus on learning domaininvariant representations OSTAR aligns pretrained representations by utilizing an Optimal Transport map that maps source representations to target ones without constraining representation invariance. The approach aims to match conditional distributions across domains recover target proportions and control the target generalization risk.  Strong Points: 1. Innovative Approach: OSTAR introduces a novel method for UDA under GeTarS circumventing the limitations of existing domaininvariant methods. 2. Theoretical Guarantees: The paper provides strong theoretical guarantees under mild assumptions offering insights into the optimality and effectiveness of the proposed approach. 3. Flexibility and Scalability: OSTAR is flexible scalable and preserves target discriminativity making it applicable in various realworld scenarios. 4. Experimental Validation: Extensive experimental results on multiple UDA benchmarks demonstrate that OSTAR outperforms stateoftheart GeTarS models and provides competitive results even without explicit knowledge of the target label distributions. 5. Comprehensive Analysis: The paper provides a detailed analysis of the approach including ablation studies and comparisons with existing methods enhancing the credibility and robustness of the findings.  Weak Points: 1. Complexity: The paper involves several technical concepts making it challenging for readers unfamiliar with UDA and GeTarS to grasp the details effectively. 2. OT Formulation: While the Optimal Transport formulation is key to the approach the paper could provide more insights into the practical implications and computational complexity of using OT in this context.  Questions and Feedback to the Author: 1. Implementation Details: Could the authors provide more insights into the implementation specifics of OSTAR such as hyperparameters NN architectures and initialization strategies 2. Generalization Concerns: How does OSTAR generalize to different datasets and problem domains beyond the ones tested in the experiments Are there specific scenarios where it may face challenges  Review Summary: The paper presents a novel approach OSTAR for UDA under Generalized Target Shift offering theoretical guarantees flexibility and scalability. The experimental results demonstrate the efficacy of the method outperforming existing stateoftheart approaches in various UDA benchmarks. While the paper is technically sound and provides comprehensive insights further elaboration on implementation details and generalization aspects could enhance the clarity and applicability of OSTAR in practical settings. Overall based on the strong theoretical foundation innovative methodology and compelling experimental results I recommend accepting this paper for publication with minor revisions to address the mentioned weak points and clarify implementation aspects.", "nhnJ3oo6AB": " Summary of the Paper: The paper introduces LocoTransformer an endtoend Reinforcement Learning (RL) method that combines proprioceptive states and visual observations for locomotion control in quadrupedal robots. The LocoTransformer model leverages a Transformerbased architecture to fuse information from both modalities enabling the agent to reason and plan actions in complex environments with obstacles and uneven terrain. The study includes experiments in both simulated and realworld environments showcasing improved generalization performance and robustness of the proposed method compared to baseline approaches.  Strengths of the Paper: 1. Innovative Approach: The integration of Transformer model for multimodal reasoning in locomotion control is a novel and promising approach. 2. Comprehensive Evaluation: The paper provides comprehensive evaluation results from experiments in simulated environments to realworld scenarios demonstrating the effectiveness and generalization capability of the proposed method. 3. Visual Insight: The attention maps visualization adds depth to understanding how the model leverages visual information for decisionmaking. 4. Comparison to Baselines: Clear comparisons with relevant baselines provide a benchmark to showcase the superiority of the LocoTransformer method.  Weaknesses of the Paper: 1. Lack of Abstractions: While the technical details are welldescribed further insights on the theoretical implications of incorporating visual information could enhance the paper. 2. Simplicity of Environments: The simulated environments could potentially be expanded to include more diverse and complex scenarios for a broader assessment of the models capabilities. 3. RealWorld Validation: While the realworld experiments are promising additional details on challenges faced during the deployment and potential limitations would provide a holistic view.  Questions for the Author: 1. How does the LocoTransformer method compare with stateoftheart RL approaches in terms of sample efficiency and scalability 2. Can you elaborate on the computational overhead introduced by the Transformer model in training and inference stages 3. How does the model handle dynamic and rapidly changing environments that may require realtime adaptation  Feedback for the Author:  Provide a more indepth discussion on the theoretical underpinnings of incorporating visual inputs and the implications on policy learning.  Consider expanding the realworld validation section to include more details on challenges faced and insights gained during the deployment.  Review Summary: Overall the paper presents a compelling approach in addressing quadrupedal locomotion tasks using a Transformerbased RL model that combines proprioceptive states and visual inputs. The comprehensive evaluation across simulation and realworld environments along with clear comparisons to baselines strengthens the credibility of the proposed LocoTransformer method. With minor enhancements in theoretical discussions and realworld validation details this paper can potentially make significant contributions to the field of legged locomotion in robotics. Thus I recommend acceptance with revisions to address the identified weaknesses.", "rvost-n5X4G": "Summary: The paper introduces a new algorithm called State Planning Policy Reinforcement Learning (SPPRL) for reinforcement learning where the actor plans for the next state provided the current state. The approach focuses on training an actor to operate entirely in the state space rather than the traditional action space. To communicate the actor output to the environment an inverse dynamics control model is incorporated and trained using supervised learning. The SPPRL algorithm is benchmarked using various continuous environments demonstrating superior performance compared to stateoftheart offpolicy reinforcement learning algorithms in robotic locomotion tasks. Strengths: 1. Innovative Approach: The paper presents a novel approach SPPRL which operates in the statespace and achieves superior performance in a wide range of continuous environments compared to traditional RL algorithms. 2. Experimental Rigor: The paper provides comprehensive experimental evaluations across different benchmark environments demonstrating the effectiveness of the SPPRL algorithm. 3. Theoretical Justification: The paper also includes theoretical analysis and derivations such as the implementation using the Lagrange multiplier method contributing to the rigor of the proposed method. 4. OpenSource Implementation: The implementation of the SPPRL methods as a modular PyTorch library shared as opensource enhances reproducibility and promotes further research in the field. 5. Interpretability and Exploration: The paper also addresses important issues related to interpretability of RL agents and efficient exploration in the targetstate space which can have practical implications. Weaknesses: 1. Lack of Comparative Analysis: While the paper compares the SPPRL approach with traditional RL algorithms a more detailed comparative analysis with other recent stateoftheart methods could provide further insights into the effectiveness of the proposed approach. Questions and Feedback to the Author: 1. Clarification on Sample Efficiency: Can the authors provide more insights into the sample efficiency of the SPPRL approach compared to traditional RL methods and how it impacts the overall learning process 2. Scalability and Generalization: How scalable is the SPPRL approach to more complex environments and what are the implications for generalization beyond the benchmark tasks mentioned in the paper 3. Potential Extensions: Are there plans to extend the SPPRL approach to address other challenges in RL such as sparse reward settings or multiagent environments Review Summary: The paper introduces a novel State Planning Policy Reinforcement Learning (SPPRL) approach demonstrating superior performance in robotic locomotion tasks compared to traditional offpolicy RL methods. The experimental evaluations theoretical justifications and opensource implementation enhance the credibility and reproducibility of the proposed approach. Addressing the weaknesses highlighted and answering questions related to scalability and generalization could further strengthen the paper. Overall I recommend acceptance of the paper given its significant contributions to the field of RL.", "mNLLDtkAy4X": "Review of \"Aleatoric Mapping Agents: NeuroscienceInspired Curiosity for Better Exploration in AI Agents\" 1) Summary of the Paper: The paper introduces aleatoric mapping agents (AMAs) a novel form of curiositydriven agents inspired by the cholinergic system of the mammalian brain. The primary focus is on addressing the limitations of traditional curiositybased agents when faced with actiondependent noise sources which can lead to exploration challenges. AMAs aim to determine and reduce intrinsic rewards in regions where dynamics are unpredictable even if induced by the agents actions by estimating aleatoric uncertainty. The paper presents theoretical and algorithmic foundations of AMAs demonstrates their effectiveness in various environments and proposes experiments to test the proposed model of acetylcholine uncertainty signalling. 2) Strong Points:  Original Contribution: The introduction of AMAs drawing inspiration from neuroscience provides a novel perspective on addressing exploration challenges in AI agents.  Theoretical Grounding: The paper offers a comprehensive theoretical basis for aleatoric uncertainty estimation linking it to cholinergic signaling and exploring its implications for exploration.  Experimental Validation: The experiments showcased in various environments including noisy MNIST MiniGrid Atari games and a proposed VR task effectively demonstrate the efficacy of AMAs in overcoming stochastic traps.  Clarity and Depth: The paper meticulously explains the formulation of AMAs the rationale behind their design and the implications for both neuroscience and reinforcement learning. Weak Points:  Complexity: The detailed theoretical and algorithmic explanations while insightful may be challenging for readers without a strong background in both neuroscience and reinforcement learning.  Evaluation Metrics: While the paper extensively covers the performance of AMAs in different environments a more holistic analysis comparing against a wider range of benchmarks or alternative approaches could provide deeper insights.  Experimental Details: Providing more details on hyperparameters training settings and reproducibility could enhance the transparency and replicability of the experiments. Questions and Feedback to the Author:  Clarification: Can you elaborate on how AMAs handle uncertainty estimates in outofdistribution scenarios or nonstationary environments  Comparative Analysis: How does the performance of AMAs compare with other stateoftheart curiositydriven approaches specifically in terms of sample efficiency and scalability  Reproducibility: To enhance reproducibility could you provide detailed information on hyperparameters neural network architectures and code availability for the implementation of AMAs 3) Review Summary: The paper on AMAs presents a significant advancement in exploring the intersection of neuroscienceinspired curiosity and AI agents exploration capabilities. The theoretical underpinnings and algorithmic implementation of AMAs alongside comprehensive experiments demonstrate the efficacy of the proposed approach in overcoming actiondependent stochastic traps. While the complexity of the content may pose a challenge for some readers the depth of the research and the potential impact on both neuroscience and AI exploration are substantial. Recommendations for enhancing the paper include a more detailed comparative analysis clarity on handling uncertain environments and increased focus on reproducibility for wider adoption and validation within the research community. Overall the paper is a noteworthy contribution that opens new avenues for exploration in reinforcement learning.", "wQfgfb8VKTn": "Peer Review of \"Learning Sparse Coordination Graphs Adaptive to Coordination Dynamics Among Agents\" 1) Summary: The paper introduces a novel method ContextAware SparsE Coordination graphs (CASEC) for learning sparse and dynamic coordination graphs in cooperative multiagent learning. The method utilizes the variance of pairwise payoff functions to construct sparse graphs reducing communication overhead and improving learning performance. An empirical evaluation is conducted on the MultiAgent COordination (MACO) benchmark showcasing the effectiveness of the proposed method. Additionally a case study on the Sensor task further demonstrates the benefits of sparse graph learning. 2) Strengths and Weaknesses: Strong Points:  The paper addresses the challenge of learning sparse coordination graphs in multiagent systems offering a novel approach that leverages the variance of payoff functions.  The theoretical analysis provided in the paper supports the methods effectiveness by showing the relationship between variances and action selection stability.  The proposed method CASEC is evaluated on a diverse set of benchmark tasks showcasing its scalability and performance compared to existing methods.  The inclusion of a case study on the Sensor task adds practical relevance to the proposed method and provides insights into its realworld applicability. Weak Points:  The paper could benefit from a more detailed discussion on the limitations of the proposed method specifically regarding the cyclefree nature of learned sparse graphs.  While the empirical evaluation is comprehensive additional comparative analysis with more baselines or methods could provide a broader perspective on the performance of CASEC. Question to the Author: 1. What strategies are employed in the algorithm to ensure the learned sparse graphs are cyclefree considering the potential suboptimality of cyclic graphs in coordination tasks 2. Could you elaborate on the scalability of CASEC in terms of the number of agents and actions in highly complex multiagent environments Feedback to the Author:  Discuss the limitations of the proposed method more explicitly particularly focusing on potential drawbacks of not having cyclefree graphs.  Consider providing a more extensive comparison with additional stateoftheart methods to further highlight the effectiveness and uniqueness of CASEC. 3) Review Summary: The paper presents a wellstructured and detailed approach to learning sparse coordination graphs in cooperative multiagent systems. The theoretical foundation empirical evaluation on the MACO benchmark and the case study on the Sensor task together provide a comprehensive view of the proposed methods capabilities. While the paper effectively highlights the advantages of CASEC addressing the limitations and expanding the comparative analysis would further strengthen the study. The proposed questions aim to encourage the authors to delve deeper into the methods intricacies and foster a more transparent discussion around its applicability and scalability. Overall the paper offers a significant contribution to the field of multiagent learning with its innovative approach to sparse coordination graph learning. Recommendation: Based on the methods novelty effectiveness and the thorough evaluation conducted I recommend that this paper be accepted for publication provided the authors address the suggested questions and feedback to enhance the clarity and robustness of their work.", "gI7feJ9yXPz": " Summary of the Paper The paper focuses on the generalization performance analysis of minimax problems in machine learning particularly in the context of reinforcement learning robust optimization adversarial learning and distributed computing. The authors provide improved stability analyses for various generalization measures and establish sharper high probability generalization bounds for minimax learning algorithms such as classical empirical saddle point (ESP) solution gradientbased optimization algorithms (GDA SGDA PPM EG OGDA) among others. The study aims to offer insights into the generalization behavior of minimax algorithms beyond convergence analysis.  Strong Points  Original Contribution: The paper presents a novel approach to analyzing the generalization performance of minimax algorithms filling a gap in current literature where there is more emphasis on convergence behavior.  Improved Bounds: Sharper high probability generalization bounds are developed for various minimax learning algorithms offering a deeper understanding of their performance.  Comprehensive Analysis: The study considers different stability measures such as uniform stability and argument stability providing a systematical analysis of generalization measures for minimax problems.  Applicability: The results are applicable to various popular optimization algorithms in machine learning enhancing their theoretical foundation.  Weak Points  Assumptions: The paper relies on several assumptions such as Lipschitz continuity smoothness and strong concavity which may limit the applicability of the results in practical scenarios with data noise or outliers.  Theoretical Focus: The emphasis on theoretical analysis and high probability bounds may make it challenging for practitioners to directly apply the findings in realworld applications without further validation.  Limited Practical Application: While the study provides valuable insights into minimax algorithm generalization the practical applicability in realworld settings may require additional validation and empirical testing.  Questions for the Author 1. Practical Implications: How do you envision translating the theoretical findings and sharper bounds derived in this study into practical applications of minimax algorithms in machine learning tasks 2. Robustness Consideration: Given the focus on high probability bounds for algorithmic stability how do you plan to address the robustness and sensitivity of the minimax algorithms to various types of data perturbations and outliers  Feedback The paper provides a valuable contribution to the understanding of generalization performance in minimax problems. To enhance the practical relevance of the findings you could consider discussing potential avenues for empirical validation and application of the proposed bounds in realworld scenarios. Additionally addressing the limitations in assumptions and exploring the implications of robustness in highprobability analysis would further strengthen the studys impact in machine learning research.  Review Summary Overall the paper presents a comprehensive and novel analysis of sharper generalization bounds for minimax learning algorithms offering a significant contribution to the field of machine learning. The improved stability analyses and high probability generalization bounds provide valuable insights for theoretical understanding and potential practical applications. Despite some limitations in assumptions and practical applicability the study stands out for its rigorous approach and innovative contribution to minimax algorithm analysis. Hence it is recommended for publication with minor revisions to address practical aspects and robustness considerations.", "obi9EkyVeED": "Summary of the Paper: The paper introduces FedDrop a novel federated learning (FL) method that aims to address the significant increase in computational costs associated with existing FL algorithms that focus primarily on reducing communication costs. FedDrop utilizes SyncDrop layers which are structured dropout layers with adaptive keep probabilities to accelerate training while minimizing computational costs. The method optimizes dropout probabilities for individual neurons to enhance convergence by reducing disparities among interclient update trajectories. Empirical results demonstrate that FedDrop achieves a superior communicationcomputation tradeoff compared to traditional FL approaches. Strengths: 1. Innovative Approach: The paper introduces a novel method FedDrop to address the computational costs neglected by existing FL algorithms showcasing the innovative contribution to the field of federated learning. 2. Comprehensive Experiments: The paper presents thorough experimental results on popular datasets demonstrating the efficacy of FedDrop in achieving a better tradeoff between communication and computation costs compared to existing FL methods. 3. Clear Explanation: The paper provides a detailed explanation of the proposed method SyncDrop layers and the optimization process for dropout probabilities making it easy for readers to understand the technical aspects of FedDrop. 4. Sensitivity Analysis: The sensitivity analysis conducted on hyperparameters such as optimization iterations FLOPs constraints regularizer and FLOPs budget ratio provides valuable insights into the performance of the FedDrop method under different settings. Weaknesses: 1. Theoretical Details: While the paper presents a detailed explanation of the proposed method some readers may find the theoretical derivations and proofs complex to follow without prior knowledge in the field. 2. Assumption Clarity: Clarifying the assumptions made during the experimental setup and the impact of these assumptions on the results could enhance the transparency and reproducibility of the findings. 3. Comparison Metrics: The paper could benefit from a more comprehensive comparison with existing FL methods in terms of additional performance metrics such as convergence speed or model accuracy under varying conditions. 4. Privacy Consideration: Given the sensitive nature of data in FL further discussion on privacy implications and security measures implemented by FedDrop would enhance the papers completeness. Questions and Feedback to the Author: 1. Could you provide more insights into the privacy implications of FedDrop especially regarding the handling of sensitive data during the federated learning process 2. How does FedDrop compare in terms of model accuracy and convergence speed compared to existing FL methods and could the paper include a more detailed analysis in these aspects 3. Have you considered exploring the scalability of FedDrop to larger and more complex models and how it may impact the computational efficiency of training in realworld scenarios 4. It would be beneficial to discuss the generalizability of FedDrop across diverse datasets and model architectures along with potential limitations or constraints when applied in practical settings. Review Summary: The paper presents a novel federated learning method FedDrop that addresses the computational cost challenges often neglected in existing FL algorithms. Through SyncDrop layers and optimization of dropout probabilities FedDrop achieves a superior communicationcomputation tradeoff compared to traditional methods. While the paper showcases innovation and provides comprehensive experimental results further clarity on theoretical aspects comparisons with existing methods privacy implications and scalability considerations would enhance the completeness and impact of the work. Overall the paper demonstrates promising advancements in the field of federated learning warranting further exploration and validation in realworld scenarios.", "fQTlgI2qZqE": " Summary of the Paper: The paper introduces a novel method for detecting feature interactions in blackbox models to enhance model interpretability. The proposed method uses a global statistical metric based on the expected Hessian and applies adaptive sampling with the Upper Confidence Bound (UCB) algorithm for swift detection. Following the detection the authors build a lightweight and interpretable deep learning model called ParaACE which outperforms its overparameterized Teacher model in prediction accuracy and model size reduction. The method is evaluated on various datasets showcasing its potential for scientific discovery.  Strengths: 1. Innovative Methodology: The paper introduces a novel approach for detecting feature interactions in a blackbox model using a global statistical metric demonstrating creativity and advancement in the field. 2. ModelAgnostic and Adaptive: The method is modelagnostic making it applicable to various pretrained learning models and the use of UCB algorithm for adaptive sampling adds efficiency to the detection process. 3. Thorough Evaluation: The paper provides comprehensive theoretical analyses and performance evaluations on real datasets demonstrating the effectiveness of the proposed method in enhancing model interpretability and prediction performance. 4. Model Compression: The application of detected interactions to design a compressed and interpretable Student model that outperforms the Teacher model signifies the practical utility of the method. 5. Experimental Results: The results on synthetic and realworld datasets showcase the effectiveness of the proposed method in improving prediction accuracy and reducing model size.  Weaknesses: 1. Complexity: The paper delves into complex mathematical formulations and algorithms which might make it challenging for readers with limited mathematical background to understand the intricacies of the method. 2. Limited Comparison: While the paper compares the proposed method with existing approaches a broader comparison with a wider range of stateoftheart methods could provide a more comprehensive assessment.  Question for the Author: 1. Generalization and Scalability: How do you foresee the scalability of your proposed method in handling larger datasets and more complex models beyond the ones tested in the paper  Feedback to the Author: The paper presents a novel and innovative method for feature interaction detection in blackbox models demonstrating significant improvements in model interpretability and prediction accuracy. However further clarity in explaining the methods mathematical complexity and a more extensive comparison with existing approaches would enhance the papers impact and appeal to a wider audience. Additionally addressing the scalability and generalizability of the method would strengthen the practical utility of the proposed approach. Overall the paper contributes valuable insights to the field of explainable deep learning and deserves consideration for publication with minor revisions.", "nT0GS37Clr": "1) Summarize the paper: The paper introduces Federated Supermask Learning (FSL) a collaborative learning algorithm designed to address the challenges of robustness to poisoning attacks and communication efficiency in Federated Learning (FL). FSL uses a novel approach where clients share rankings of network edges instead of float weights leading to reduced space for malicious updates and lower communication costs. The paper presents the theoretical foundations and empirical results demonstrating the robustness and communication efficiency of FSL compared to stateoftheart FL algorithms. 2) List strong and weak points of the paper Question and Feedback to the author: Strong Points:  Innovative Approach: Introducing the concept of supermasks and edge rankings in FL is a unique and innovative approach towards addressing robustness and communication efficiency.  Robustness: The paper provides a strong theoretical foundation and empirical evidence showcasing the robustness of FSL against poisoning attacks without the need for knowledge about the percentages of malicious clients.  Communication Efficiency: FSL significantly reduces communication costs by sharing rankings of edges instead of weight parameters offering a promising solution for resourceconstrained environments.  Experimental Validation: Empirical results on realworld datasets demonstrate the superior performance of FSL in terms of accuracy communication costs and robustness compared to existing FL algorithms. Weak Points:  Complexity: The complexity of FSL might pose implementation challenges for practical deployment especially for integrating it into existing FL frameworks.  Scalability: The scalability of FSL to largescale FL scenarios involving a vast number of clients needs further exploration to assess its feasibility.  Evaluation Metrics: Providing additional metrics or comparisons with more FL baselines could further strengthen the argument for the effectiveness of FSL. Question to the Author:  Scalability: How do you foresee the scalability of FSL to handle FL scenarios with a large number of clients  Generalization: Have you considered evaluating FSL on different types of datasets or application domains to assess its generalizability Feedback to the Author:  Simplicity in Presentation: Simplifying the presentation of the theoretical foundations and technical aspects of FSL could enhance the accessibility of the paper to a broader audience.  Discussion of Limitations: Including a section on the limitations or potential challenges in deploying FSL in practical scenarios would provide a more holistic view of its applicability. 3) Review Summary: The paper presents an innovative approach Federated Supermask Learning (FSL) to address the challenges of robustness to poisoning attacks and communication efficiency in Federated Learning. The theoretical foundations experimental results and comparisons with existing FL algorithms demonstrate the effectiveness of FSL in achieving high performance robustness and reduced communication costs. The innovative use of supermasks and edge rankings sets FSL apart in terms of addressing key FL challenges. However additional evaluations on scalability and generalizability along with clarity in presentation would strengthen the papers impact and practical relevance. Therefore based on the comprehensive analysis of the papers strengths weaknesses and implications I recommend the paper for publication with minor revisions to address the questions and suggestions provided.", "p98WJxUC3Ca": " Review of \"Active Learning for Domain Adaptation: A DiscrepancyBased Approach\"  1) Summary: The paper introduces an active learning approach for domain adaptation using a discrepancybased strategy that focuses on the localized discrepancy between labeled and unlabeled distributions. The proposed methodology is theoretically grounded with generalization error bounds derived based on localized discrepancies and Rademacher averages. An accelerated Kmedoids algorithm is presented to select queries in large datasets efficiently. The paper showcases the competitiveness of the proposed approach through empirical experiments on regression and classification tasks in domain adaptation scenarios.  2) Strengths and Weaknesses:  Strong Points:  Theoretical Grounding: The paper is theoretically sound providing clear definitions and derivations for the proposed approach.  Novelty: Introducing a discrepancybased approach for active learning in domain adaptation is innovative and addresses a significant challenge in domain shift scenarios.  Practicality: The incorporation of an accelerated Kmedoids algorithm makes the proposed approach practical for large datasets.  Empirical Validation: Extensive empirical experiments that demonstrate the competitiveness of the proposed approach against stateoftheart methods in domain adaptation.  Weak Points:  Complexity: The proposed methodology especially the theoretical derivations may be challenging for readers less familiar with the underlying concepts like Lipschitz functions and discrepancy minimization.  Limited Comparative Analysis: The comparison with existing methods lacks depth in some aspects like scalability computational efficiency or robustness to parameter settings.  Experimental Description: More details on experimental setup specifics parameter settings and potential sensitivity analysis could enhance the reproducibility and insight into the outcomes.  Questions and Feedback to the Author:  Clarification on Algorithmic Details: Could the author provide more insights into the algorithmic steps within the accelerated Kmedoids algorithm especially regarding the BranchandBound process for medoid updates  Scalability Concerns: How does the proposed approach scale in terms of computational complexity as dataset sizes increase beyond hundreds of thousands of instances  Further Analysis: Would additional experiments or analyses be conducted to explore the impact of different hyperparameters dataset characteristics or model complexities on the performance of the proposed approach  3) Review Summary: The paper presents a novel active learning approach for domain adaptation based on discrepancy minimization providing theoretical guarantees and an efficient query selection algorithm. The methodologies are wellgrounded and demonstrated to be competitive through empirical experiments. Despite a few weaknesses in terms of complexity and limited comparative analysis the paper showcases significant contributions to the active learning literature in domain adaptation. With minor improvements in clarity additional comparative analysis and scalability considerations the proposed approach could have a broader impact. Therefore I recommend the paper for publication pending some minor revisions and enhancements.", "m8bypnj7Yl5": " Paper Summary: The paper addresses the challenge of synthesizing optimal controllers for dynamical systems under computational constraints. It introduces a novel hypersolvers approach that combines a differential equation solver with a neural network to improve control policies within a fixed computational budget. The performance of the proposed approach is evaluated in direct and recedinghorizon optimal control tasks in both low and high dimensions showcasing consistent improvements in solution accuracy and control performance.  Strengths: 1. Innovative Approach: The integration of hypersolvers to improve control policies represents a novel and promising solution to the challenges of optimal control under computational constraints. 2. Comprehensive Evaluation: The paper provides a thorough evaluation of the proposed approach in various scenarios demonstrating Pareto improvements in solution accuracy and control performance. 3. Detailed Method Description: The paper elaborates on the hypersolver methodology including pretraining strategies architecture and loss functions providing a clear understanding of the proposed approach. 4. Experimental Results: The experimental results demonstrate the effectiveness of hypersolvers in improving control policies in different scenarios highlighting the practical implications of the approach.  Weaknesses: 1. Lack of Comparison: While the paper extensively evaluates the proposed approach a comparison with existing stateoftheart methods could provide more context and highlight the advantages of hypersolvers. 2. Complexity of Method Description: The technical details of hypersolvers loss functions and architectures may be challenging for readers unfamiliar with the field potentially limiting the accessibility of the paper. 3. Evaluation Metrics: A more detailed discussion on the specific metrics used to evaluate performance and generalization could enhance the interpretability of the results.  Questions  Feedback: 1. Clarification on Comparison: Could you provide insights into how the proposed hypersolvers approach compares against traditional optimization methods in terms of computational efficiency and solution quality 2. Practical Implementation: How feasible is the implementation of hypersolvers in realworld control systems Have there been any realworld applications or case studies to validate the effectiveness of the proposed approach  Review Summary: The paper introduces a novel hypersolvers approach to enhance control policies in dynamical systems under computational constraints. The detailed methodology comprehensive evaluation and promising experimental results highlight the potential of hypersolvers in improving solution accuracy and control performance. However additional comparative analysis and practical validation could further strengthen the papers contributions. Overall the innovative nature and potential impact of hypersolvers in the control field warrant further exploration and validation.", "yuv0mwPOlz3": " 1) Summary of the Paper: The paper investigates the challenging scenario of active learning with multiple outofdistribution data sources for tasks like question answering and sentiment analysis in natural language processing (NLP). The study surveys various techniques in active learning domain shift detection and multidomain sampling. The research explores different methods to address the uncertainty in selecting examples and domains to optimize performance in the multidomain active learning setting.  2) Evaluation of Strong and Weak Points:  Strong Points:  Comprehensive Study: The paper provides a detailed analysis of 18 acquisition functions across different families of methods for multidomain active learning.  Novel Proposals: Introduces a new variant DALE which shows consistent improvements over existing methods.  Empirical Results: Identifies HDivergence methods particularly DALE as consistently strong performers offering actionable insights to practitioners.  Comparative Analysis: Differentiates the performance of acquisition methods based on their effectiveness in uncertainty estimation domain shift detection and semantic similarity detection.  Experimental Rigor: The paper presents experiments on common NLP tasks QA and SA along with thorough evaluation metrics to provide a comprehensive review of the methods.  Weak Points:  Lack of Discussion on Limitations: While the paper touches upon some limitations in the methods used more detailed discussion on potential shortcomings could enhance the robustness of the study.  Algorithm Parameters: A deeper analysis of hyperparameters used in the methods and their impact on results could provide additional insights.  Questions and Feedback to the Author: 1. Could you elaborate on the relative significance of example selection compared to domain selection in improving performance especially in scenarios where multiple domains are involved 2. Considering the importance of domain diversity highlighted in your findings how do you envision practitioners applying these insights to realworld NLP tasks with varied domain requirements 3. Have you considered exploring the impact of other factors such as model architecture or training data size on the performance of the acquisition functions studied in the paper  3) Review Summary: The paper presents a comprehensive evaluation of active learning methods for multidomain scenarios in natural language tasks highlighting the importance of domain diversity and example selection strategies. The findings demonstrate the effectiveness of HDivergence methods particularly the introduced variant DALE in improving performance. The studys detailed comparative analysis and empirical results provide valuable guidance for practitioners facing similar challenges. Overall the paper contributes significantly to the understanding of multidomain active learning in NLP and opens up avenues for future research in this area.  Recommendation: Based on the thorough analysis and empirical findings presented in the paper I recommend acceptance for publication given the meaningful contributions and insights it offers to the NLP research community.", "rxF4IN3R2ml": "Peer Review Draft: 1) Summarize the paper: The paper introduces novel enhancements to neural forecasting by incorporating transformer architecture inspired changes for improved accuracy and reduced excess volatility in forecasting. The proposed improvements include a decoderencoder attention mechanism for context alignment a positional encoding scheme for learning contextdependent seasonality functions and a decoder selfattention mechanism for forecast evolution. The paper demonstrates significant improvements in accuracy and reduction in excess volatility in demand forecasting scenarios and outperforms existing stateoftheart models on public datasets. 2) Evaluation of the Paper: Strong Points:  Innovative Enhancements: The proposed improvements such as horizonspecific attention and decoder selfattention introduce novel inductive biases that address limitations in existing forecasting architectures.  Empirical Results: The empirical results show substantial improvements in accuracy and reduction in excess volatility demonstrating the effectiveness of the proposed enhancements.  Compatibility with Efficient Training: The architecture is shown to achieve significant speedup and increased throughput making it efficient for largescale applications. Weak Points:  Limited Comparison: While the paper compares the proposed MQTransformer architecture with existing models on various datasets a more extensive comparison with a wider range of stateoftheart forecasting models would strengthen the evaluation.  Lack of Theoretical Analysis: The paper lacks a detailed theoretical analysis of the proposed enhancements and their impact on the models performance which could provide deeper insights into the underlying mechanisms.  Clarity in Description: Some sections particularly the methodology and empirical results could benefit from clearer explanations and more detailed diagrams to enhance understanding. Questions and Feedback to the Author: 1. Can you provide more insights into how the proposed horizonspecific attention mechanism aids in capturing relevant events and improving alignment in multihorizon forecasting scenarios 2. Could you elaborate on the significance of the decoder selfattention mechanism in reducing excess volatility and how it leverages feedback from previous forecasts for enhanced accuracy 3. Have you considered conducting experiments on additional realworld datasets to comprehensively validate the generalizability of the proposed architecture across diverse forecasting tasks 3) Review Summary: The paper presents significant advancements in neural forecasting by introducing novel enhancements inspired by transformer architectures. The proposed MQTransformer model demonstrates substantial improvements in forecast accuracy and reduction in excess volatility in demand forecasting scenarios and outperforms existing stateoftheart models on various public datasets. Despite some limitations in the comparison breadth and theoretical analysis the innovative enhancements and empirical results make a strong case for the effectiveness of the proposed architecture. Therefore based on the strong empirical results and the potential impact of the proposed enhancements I recommend acceptance of the paper with minor revisions to address the weak points highlighted above.", "kroqZZb-6s": " Summary: The paper proposes a supervised deep learning model CAMELOT to cluster Electronic Health Record (EHR) data in order to predict patient outcomes in a multiclass setting. The model introduces novel loss functions to address class imbalance and cluster collapse issues and utilizes a featuretime attention mechanism to enhance interpretability. The model is evaluated on over 100000 unique trajectories from hospitalised patients with TypeII respiratory failure outperforming benchmarks by at least 5 in mean AUROC.  Strengths: 1. Innovative Approach: The paper introduces a novel deep learning model with specialized loss functions and attention mechanisms to improve clusterbased phenotype identification in EHR data. 2. Interpretability: The inclusion of a featuretime attention layer provides insights into relevant timestamps and feature variables enhancing the interpretability of the clustering results. 3. Comprehensive Evaluation: The model is tested on a large dataset and compared to relevant benchmarks showing improvement in both clustering performance and outcome prediction. 4. Clear Exposition: The paper is wellstructured with detailed descriptions of the dataset model architecture training procedures and evaluation results.  Weaknesses: 1. Complexity: The model is complex and may be challenging to implement and replicate for researchers without extensive deep learning expertise. 2. Limitation in Comparison: While the model outperforms benchmarks more direct comparisons with similar models in the literature could provide additional context for the effectiveness of the proposed approach. 3. Limited Insight into Future Work: The discussion on future work could be expanded to provide more concrete directions for potential improvements or extensions of the proposed model.  Questions and Feedback to the Authors: 1. Could you provide insights into the computational resources required for training and inference of the proposed model especially in terms of scalability to larger datasets 2. Have you conducted any analysis on the robustness and generalizability of the model across different hospital settings or patient populations 3. Considering the high dimensionality and complexity of EHR data have you explored any strategies to mitigate potential biases or overfitting in the model  Review Summary: The paper presents an innovative deep learning model CAMELOT for clustering EHR data to predict patient outcomes. The models utilization of novel loss functions and attention mechanisms leads to improved interpretability and performance compared to existing benchmarks. The comprehensive evaluation on a large dataset showcases the models efficacy in capturing clinically relevant patient phenotypes. Despite its complexity the models potential for enhancing outcome prediction and cluster interpretation in healthcare settings is evident. Therefore I recommend the paper for acceptance after addressing minor concerns related to implementation and model generalization.", "pzgENfIRBil": " PeerReview of \"Solving Approximated Schr\u00f6dinger Equations Using Selfconsistent Gradientlike Eigen Decomposition\"   1. Summary: The paper introduces a novel framework Selfconsistent Gradientlike Eigen Decomposition (SCGLED) to solve approximated Schr\u00f6dinger equations using gradientlike eigendecomposition methods. Traditional methods heavily rely on domainspecific heuristics whereas the proposed SCGLED eliminates this need and approaches the selfconsistency of the equation iteratively. The paper presents a detailed algorithm along with numerical improvements demonstrating robustness freedom from quantummechanismbased heuristics and efficiency in implementation. Experimental results show SCGLEDs superior performance in providing initial guesses and solving equations independently.  2. Strengths and Weaknesses:  Strong Points:  Innovative Approach: Introducing a new method that eliminates the need for domainspecific heuristics is a significant contribution.  Robust Experimental Validation: Extensive experiments on the W417 dataset showcase the effectiveness of SCGLED in providing initial guesses and as a full solver.  Detailed Methodology: The paper provides a detailed description of the proposed algorithm numerical improvements and performance benchmarks.  Clear Experimental Setup: Detailed experimental setup algorithm implementation details and comparison with traditional methods enhance the clarity.  Weak Points:  Convergence Analysis: The lack of a detailed theoretical analysis for the convergence of SCGLED is a noticeable gap. Providing some insights on convergence guarantees would enhance the papers rigor.  Algorithm Complexity: The complexity of the proposed algorithm especially the use of damping and momentum techniques may hinder its adoption in practical applications. Providing a discussion on computational efficiency and scalability is warranted.  Comparison with StateoftheArt: While the paper discusses the limitations of traditional methods a comparison with stateoftheart algorithms in solving Schr\u00f6dinger equations would further establish the novelty and superiority of SCGLED.  Questions and Feedback: 1. Convergence Assurance: Can you elaborate on the robustness of SCGLED in terms of convergence guarantees especially in complex systems Any insights on the conditions under which the algorithm may fail to converge 2. Computational Efficiency: How does the computational efficiency of SCGLED compare to traditional methods Have you considered scalability issues when applying SCGLED to larger quantum systems 3. Applicability: In what scenarios do you envision SCGLED outperforming traditional heuristicbased methods Are there specific quantum systems where SCGLED might face challenges due to its iterative nature  3. Review Summary: The paper presents a novel framework SCGLED for solving approximated Schr\u00f6dinger equations eliminating the need for domainspecific heuristics. The experimental results demonstrate the superior performance of SCGLED in providing initial guesses and acting as a full solver. While the paper is wellstructured and presents a detailed methodology addressing the convergence analysis algorithm complexity and scalability concerns would further enhance its impact. Overall the innovative approach and robust experimental validation make SCGLED a promising method for solving complex quantummechanical problems. Considering the innovative approach robust experimental validation and detailed methodology I recommend acceptance of the paper with minor revisions to address the highlighted points which would strengthen its contribution to the field of computational physics and machine learning.", "lEoFUoMH2Uu": " Summary of the Paper: The paper proposes a novel method for reconstructing visual stimulus images from fMRI data by incorporating the concept of visual attention. The method first decodes human visual attention from fMRI data and then uses it to guide the reconstruction of visual images. The paper introduces Foregroundattention (Fattention) and incorporates a selfattention module to capture global information in the fMRI decoding process. Additionally a new training strategy called LoopEncDec is proposed for the encoderdecoder model. The experimental results demonstrate successful decoding of visual attention and reconstruction of visual stimulus images outperforming existing methods.  Strong Points: 1. Innovative Approach: Introducing the concept of visual attention in reconstructing visual stimulus images from fMRI data is an innovative approach that adds value to the existing literature. 2. Comprehensive Methodology: The paper presents a detailed methodology including the Fattention decoder selfattention module and LoopEncDec framework enhancing the understanding of the proposed approach. 3. Experimental Validation: The paper provides thorough experimental results including Fattention decoding image reconstruction ablation experiments and validation of neuroscience research demonstrating the effectiveness of the proposed method.  Weak Points: 1. Clarity of Presentation: The paper is highly technical and certain sections can be challenging to follow due to complex terminology and concepts. Improving the clarity of presentation could enhance readability and understanding. 2. Comparison to Existing Works: While the paper discusses existing methods briefly a more indepth comparison with previous approaches including quantitative comparisons could strengthen the discussion on the novelty and effectiveness of the proposed method.  Questions for the Author: 1. Could you elaborate on the specific advantages of incorporating the selfattention module in the fMRI decoding process and how it enhances the reconstruction of visual stimulus images 2. How does the proposed LoopEncDec framework improve the stability and generalization of the image reconstruction model compared to traditional training strategies  Feedback to the Author: 1. Consider providing a more detailed discussion on the implications of the results in the context of realworld applications or potential future research directions. 2. Enhance the discussion on the limitations and potential challenges of the proposed method to provide a more comprehensive view of the research.  Review Summary: The paper presents an innovative method for reconstructing visual stimulus images from fMRI data by incorporating the concept of visual attention. While the methodology is sound and the experimental results are promising the paper would benefit from clearer presentation more indepth comparison with existing works and detailed discussions on the implications and limitations of the proposed approach. Overall the paper adds value to the field of fMRI image reconstruction and with some enhancements in presentation and discussion it has the potential to make a significant contribution to the research domain.", "i3RI65sR7N": " Summary of the Paper: The paper introduces a novel approach called hierarchical variational memory for fewshot learning across domains. The proposed method stores and fetches features at different semantic levels providing adaptability to domain shift circumstances. It utilizes a hierarchical prototype model alongside a newly derived hierarchical variational inference framework to metalearn the model. The paper also presents a method to learn weights associated with prototypes at each level in a datadriven manner enabling the model to choose the most generalizable features. Extensive experiments are conducted to showcase the effectiveness of the model on both crossdomain fewshot and traditional fewshot classification tasks.  Strong Points of the Paper: 1. Innovative Approach: The hierarchical variational memory model is a novel approach that addresses the limitations of existing memory models in handling domain shifts in fewshot learning scenarios. 2. Comprehensive Experiments: The paper conducts thorough experiments including ablation studies and comparisons with stateoftheart methods to demonstrate the effectiveness of the proposed hierarchical memory. 3. Clear Problem Statement and Contributions: The paper clearly defines the problem of crossdomain fewshot learning and outlines three significant contributions in addressing this problem with the proposed hierarchical memory model.  Weak Points of the Paper: 1. Complexity: The paper introduces several novel concepts and methods which might lead to a higher learning curve for readers to fully grasp the proposed model and its components.  Questions and Feedback for the Author: 1. Clarification on Hierarchical Variational Prototype Network: Could the author provide more insights into how the hierarchical variational prototype network differs from traditional prototype networks and how it contributes to the adaptability of the model 2. Discussion on Scalability: How does the proposed hierarchical memory model scale with larger datasets or more complex networks and what considerations were taken to ensure scalability 3. Comparison with Existing Memory Models: Could the author provide a more detailed comparison of the hierarchical variational memory model with existing memory models in terms of performance flexibility and scalability  Review Summary: Overall the paper presents an innovative and wellresearched approach to address the challenges of crossdomain fewshot learning. The proposed hierarchical variational memory model along with the hierarchical prototype network and the learning to weigh prototypes method provides significant advancements in handling domain shifts and achieving stateoftheart performance. The comprehensive experiments and detailed analysis presented in the paper strongly support the effectiveness and relevance of the proposed model. Therefore I recommend accepting the paper for publication due to its novelty clear presentation and strong experimental validation.", "p3DKPQ7uaAi": " Review  1) Summary: The paper introduces Temporal Alignment Prediction (TAP) a learnable sequence distance measure that predicts optimal alignment between sequences using a lightweight convolutional neural network. TAP can be applied to various machine learning tasks involving sequence data such as supervised representation learning and fewshot action classification. The paper demonstrates the effectiveness of TAP in terms of performance and computational efficiency compared to existing methods on multiple datasets.  2) Critical Analysis:  Strong Points:  Innovative Approach: The introduction of TAP as a learnable sequence distance measure is novel and addresses the challenges of aligning sequences efficiently.  Application Versatility: The ability of TAP to be used in various machine learning tasks including supervised learning and fewshot action recognition showcases its versatility.  Performance Metrics: The comprehensive comparison with existing methods through performance metrics such as MAP and classification accuracies on multiple datasets provides a thorough evaluation.  Weak Points:  Limitations of TAP: The paper mentions limitations of TAP such as asymmetry and potential limitations with strictly ordered data.  Comparison with SoftDTW: While demonstrating TAPs advantages over SoftDTW more detailed insights into the specific scenarios where SoftDTW outperforms could enhance the analysis.  Theoretical Explanation: Some sections especially regarding the alignment prediction network and hyperparameter settings could benefit from more indepth explanations or visual aids for better understanding.  Questions and Feedback: 1. Clarification on Asymmetry: Can the asymmetry of TAP potentially affect certain applications where symmetry is essential and is there any plan to address this limitation in future research 2. Detailed Comparison with SoftDTW: Could you provide insights into the specific scenarios where SoftDTW performs better than TAP in the context of different losses to enable a clearer understanding of the tradeoffs between the two methods  3) Review Summary: The paper introduces an innovative approach TAP for learnable sequence distance calculation showcasing its versatility across various machine learning tasks. The comprehensive evaluation on multiple datasets and comparison with existing methods demonstrate the efficacy and efficiency of TAP. Addressing the identified weak points such as detailing the limitations and nuances of comparisons with SoftDTW would further strengthen the paper.  Recommendation: Given the innovative approach comprehensive evaluation and versatility of TAP I recommend acceptance of the paper with revisions to address the weak points and enhance the clarity of certain sections for better understanding.", "izj68lUcBpt": " Summary of the Paper: The paper introduces TemporallyAdaptive Convolutions (TAdaConv) as a method for enhancing video understanding by calibrating convolution weights based on local and global temporal context. This approach empowers spatial convolutions with temporal modeling capabilities leading to improved performance in video action recognition and localization tasks. TAdaConv is demonstrated to be efficient easy to integrate into existing models and capable of utilizing pretrained weights effectively. Experimental results on various datasets show that TAdaConv outperforms stateoftheart approaches and significantly enhances video models without substantial computational overhead.  Strong Points: 1. Innovative Approach: TAdaConv introduces a novel method of dynamically calibrating convolution weights for each frame based on temporal context enabling better modeling of complex temporal dynamics in videos. 2. Efficiency: The paper highlights the efficiency of TAdaConv by directly operating on convolution kernels leading to improved temporal modeling without significant computational overhead. 3. Experimental Validation: Extensive experiments on multiple datasets demonstrate the effectiveness of TAdaConv in enhancing video classification and action localization tasks consistently outperforming existing methods. 4. Compatibility: TAdaConvs design allows for easy integration into existing video models enabling it to be used as a plugin module to boost model performance.  Weak Points: 1. Lack of Comparative Analysis: While the paper extensively evaluates TAdaConv on various datasets a more detailed comparative analysis against a wider range of stateoftheart methods could provide a more comprehensive understanding of its performance. 2. Theoretical Discussion: Some aspects of the method such as the choice of calibration dimensions and the impact on model interpretability could be further elaborated theoretically to enhance understanding.  Questions and Feedback to the Author: 1. Could the authors provide more insights into the implications of choosing the calibration dimension on the overall performance and interpretability of the model 2. How does TAdaConv compare with Transformerbased models in terms of accuracy computation and efficiency for video understanding tasks 3. It would be beneficial to include visualizations or case studies showcasing how TAdaConv improves the interpretability or explainability of video models compared to traditional approaches.  Review Summary: The paper presents an innovative approach TAdaConv for enhancing video understanding through dynamic calibration of convolution weights based on temporal context. Strong experimental results showcase the effectiveness and efficiency of TAdaConv in improving video action recognition and localization tasks. While the paper could benefit from further theoretical explanations and comparative analyses its contributions to the field of video modeling are significant. Therefore I recommend accepting this paper for its valuable insights and promising results in the domain of video understanding.", "mF122BuAnnW": " Summary of the Paper: The paper introduces a novel approach for certifying collective robustness for multioutput classifiers focusing on softly local models where each output assigns different importance to different parts of the input. The proposed method localized randomized smoothing uses noni.i.d. noise distributions to enhance robustness certification for various tasks such as image segmentation and node classification. The strategy involves deriving peroutput certificates based on the models soft locality and combining them into a collective certificate via a linear program. Experimental evaluations on image segmentation and node classification tasks demonstrate the effectiveness of the proposed approach in achieving a better tradeoff between accuracy and certifiable robustness compared to traditional i.i.d. noisebased smoothing techniques.  Strong Points: 1. Novel Methodology: The paper introduces a novel approach localized randomized smoothing tailored for softly local models addressing the challenges of certifying collective robustness effectively. 2. Comprehensive Experiments: The experimental evaluation includes multiple tasks like image segmentation and node classification showcasing the effectiveness of the proposed approach over traditional methods. 3. Linear Program Integration: The incorporation of a linear program for combining peroutput certificates into a collective certificate is a key strength enhancing the robustness guarantees provided by the method. 4. Clarity and Detail: The paper is wellstructured and provides thorough explanations of the methodologies used making it easy to follow and understand the proposed approach.  Weak Points: 1. Limitation to Softly Local Models: The method is limited to softly local models potentially restricting its applicability to models with different characteristics. 2. Parameter Selection: The paper mentions the challenge of choosing the smoothing distributions based on assumptions which may introduce subjectivity and potential inaccuracies in the certification process. 3. Performance Impact: While the proposed approach shows promising results the computational overhead of the collective LP should be further optimized for practical implementation.  Questions and Feedback to the Author: 1. Scalability and Generalizability: How scalable and generalizable is the proposed approach to various types of multioutput tasks and models beyond the ones evaluated in the experiments 2. Practical Implementation: How feasible is it to implement the proposed approach in realworld scenarios considering the potential computational overhead and parameter selection challenges mentioned in the paper 3. Future Directions: What are the planned future directions for enhancing the proposed method addressing remaining limitations and improving the practical utility of the certification framework  Review Summary: The paper presents a novel approach localized randomized smoothing for certifying collective robustness in softly local multioutput classifiers demonstrating improved accuracyrobustness tradeoffs compared to traditional methods. The comprehensive experiments and integration of a linear program enhance the credibility and effectiveness of the proposed method. Addressing potential limitations and optimizing practical implementation could further strengthen the impact and applicability of the approach in various realworld scenarios. Overall the paper makes a significant contribution to the field of robustness certification for multioutput models.", "rSI-tyrv-ni": " Summary: The paper investigates the utility of incorporating entity type abstractions into pretrained Transformers and evaluates the impact on various natural language processing tasks such as compositional language understanding abductive reasoning multihop question answering and conversational question answering. Three methods of incorporating abstraction are explored: embedding level encoding level and as an auxiliary task. The experimental results show that models with abstract entity knowledge outperform those without in tasks requiring logical reasoning demonstrating improved generalization and performance. However the effectiveness of abstraction varies depending on the technique used and the nature of the NLP task.  Strong Points: 1. Comprehensive Investigation: The paper explores various methods of incorporating entity type abstractions into pretrained Transformers and evaluates their impact on different NLP tasks providing a thorough analysis of the benefits. 2. Empirical Evaluation: The experiments conducted on synthetic and realworld datasets demonstrate the effectiveness of incorporating entity abstraction for tasks requiring logical reasoning. 3. Clear Presentation: The paper is wellstructured and clearly outlines the objectives methods experimental setups and results making it easy to follow the research process. 4. Strategy Diversity: The paper presents multiple strategies for incorporating abstraction providing a broad perspective on how entity types can enhance the performance of language models.  Weak Points: 1. Limited Realworld Data Analysis: While the results on synthetic datasets are promising the performance on realworld datasets like HotpotQA might not demonstrate significant improvements with abstraction limiting the generalizability of the findings. 2. Complex Implementation: The method of incorporating abstract knowledge requires additional annotated data and processing which may not be scalable for larger datasets and could introduce complexity in deployment. 3. Lack of Benchmark Comparison: While the paper provides comparisons with baseline models it lacks comparisons with other stateoftheart approaches in the field which could provide a broader context for the results.  Question and Feedback to the Author:  Question: Have you considered further augmenting the model with additional external knowledge sources beyond entity abstraction to enhance its reasoning capabilities further  Feedback: To enhance the papers impact it would be beneficial to provide a discussion on the practical implications and potential future applications of incorporating entity type abstractions along with a comparison with existing stateoftheart models on similar tasks.  Review Summary: The paper presents a comprehensive investigation into incorporating entity type abstractions into pretrained Transformers for enhancing logical reasoning in natural language processing tasks. While the results on synthetic datasets show promising improvements in reasoning and generalization abilities the effectiveness of abstraction varies across tasks especially in more natural language scenarios. The clarity in presentation and the diversity of strategies explored are strong points. However to strengthen the paper additional analyses on realworld datasets scalability concerns and comparisons with other advanced models could provide a more holistic view. Overall the paper contributes valuable insights into enhancing language models with explicit entity abstraction especially in structured logical reasoning settings.", "uB12zutkXJR": " Summary of the Paper: The paper introduces GRAPHIX a novel pretrained graph edit model designed for automatically detecting and fixing bugs and code quality issues in Java programs. The model leverages the abstract syntax structure of the code and uses a multihead graph encoder along with an autoregressive tree decoder. A notable contribution of the paper is the proposed novel pretraining strategy called deleted subtree reconstruction which enriches the model with implicit knowledge of program structures from unlabeled source code. Experimental results show that GRAPHIX outperforms various baselines and Transformer models while demonstrating strong inductive biases in learning structural and semantic code patterns.  Strengths of the Paper: 1. Novel Approach: The use of graph edit operations combined with the proposed pretraining strategy sets GRAPHIX apart showcasing a unique approach to automated program repair. 2. Superior Performance: The experimental results indicate that GRAPHIX outperforms various baselines and Transformer models demonstrating the effectiveness of the model. 3. Inductive Biases: The paper highlights the strong inductive biases of GRAPHIX in learning meaningful structural and semantic code patterns showcasing the models ability to handle complex bug fixes. 4. Comparative Analysis: The paper provides a comprehensive comparison with existing models and demonstrates the competitive performance of GRAPHIX especially in comparison to stateoftheart Transformer models.  Weaknesses of the Paper: 1. Limited Evaluation: While the paper showcases performance on Java programs the evaluation on only Java may limit the generalizability of the model to other programming languages. 2. Complexity: The detailed technical description of the model architecture and training procedures may be overwhelming for readers unfamiliar with graph neural networks and program repair techniques. Simplifying the explanation could improve understanding. 3. Negative Examples: The paper should elaborate more on the negative examples generated by GRAPHIX to provide insights into areas where the model struggles and opportunities for further enhancement.  Questions and Feedback to the Author: 1. Can the performance of GRAPHIX be further enhanced when applied to other programming languages apart from Java 2. How does the model handle more complex and less common bug patterns that may require intricate fixes 3. Considering the positive results in the paper have there been any realworld applications or deployments of GRAPHIX in industry settings  Review Summary: The paper introduces a novel pretrained graph edit model GRAPHIX for automated program repair showcasing superior performance against various baselines and Transformer models. The strong inductive biases of GRAPHIX in learning structural and semantic code patterns are commendable. Despite the technical complexity the paper provides valuable insights into the potential of using graph neural networks for code repair tasks. Overall the paper presents a promising approach with significant potential for advancing automated program repair methodologies.", "fR-EnKWL_Zb": " Summary of the Paper The paper introduces QuadTree Attention a novel approach that reduces the computational complexity of transformers from quadratic to linear in the context of vision tasks. By building token pyramids and computing attention in a coarsetofine manner the QuadTree Attention method achieves significant performance improvements in various vision tasks including feature matching stereo image classification and object detection. The paper presents experimental results demonstrating stateoftheart performance compared to existing efficient transformer methods.  Strong Points 1. Innovative Approach: The QuadTree Attention method introduces a unique way of reducing computational complexity in vision transformers combining both fine image details and longrange dependencies efficiently. 2. Comprehensive Experiments: The paper conducts extensive experiments across various vision tasks showcasing the effectiveness of the QuadTree Attention method with detailed results and comparisons against stateoftheart methods. 3. Performance Improvements: The QuadTree Attention method achieves notable improvements in feature matching stereo matching image classification object detection and semantic segmentation tasks over existing efficient transformer methods. 4. Code Availability: Providing the code on GitHub allows for reproducibility and further research in the field.  Weak Points 1. Lack of Detailed Pipeline Description: While the paper describes the QuadTree Attention method effectively a more detailed explanation of the implementation pipeline hyperparameters selection and training strategy could enhance the reproducibility and understanding of the proposed approach. 2. Further Analysis on Limitations: More comprehensive analysis on the limitations and edge cases where QuadTree Attention might not perform optimally would provide a better understanding of the methods applicability.  Questions for the Authors 1. Could you provide more insights into the finetuning process for different vision tasks with the QuadTree Attention method 2. How does the QuadTree Attention method handle situations where the image patterns are highly complex or noisy  Feedback for Authors The paper introduces a novel QuadTree Attention method that significantly reduces computational complexity in vision transformers while achieving stateoftheart performance across various tasks. The comprehensive experiments and results support the claims made in the paper effectively. Further detailed explanations analysis on limitations and exploring edge cases could enhance the overall impact and understanding of the method. More insights into the finetuning process and addressing complex image patterns would provide a more holistic view of the methods capabilities. Overall the QuadTree Attention method presents a promising direction in efficient transformer design for vision tasks.", "hk3Cxc2laT-": "Summary of the Paper: The paper introduces a Clustered TaskAware MetaLearning (CTML) framework that leverages both features and learning paths for task representation. The framework is tested on two realworld application domains: fewshot image classification and coldstart recommendation. The proposed CTML outperforms stateoftheart baselines in both domains by considering task heterogeneity and providing a comprehensive taskspecific optimization process. Strengths and Weaknesses: Strong Points: 1. Innovative Framework: The paper proposes a novel CTML framework that integrates task representation from both features and learning paths providing a comprehensive approach to metalearning. 2. Superior Performance: Extensive experiments demonstrate that CTML outperforms existing baselines in both fewshot image classification and coldstart recommendation tasks. 3. Comprehensive Experiments: The paper conducts thorough experiments on various datasets and provides detailed performance metrics and comparisons with stateoftheart methods. Weak Points: 1. Complexity: The proposed CTML framework seems complex due to multiple components like meta path learner shortcut tunnel and clustering which might make it challenging to implement and understand. 2. MetaTesting Time: The paper mentions that CTML requires twice the inference time compared to baselines due to additional components like rehearsed learning during metatesting which can be a limitation for practical applications. 3. HyperParameters Settings: The paper lacks an indepth discussion on how hyperparameters were selected and the potential impact of these settings on the results. Questions to the Author(s): 1. Could you provide more insights into the rationale behind choosing specific hyperparameter settings for the experiments 2. How sensitive is the CTML framework to variations in hyperparameters and have you conducted sensitivity analysis to determine robustness 3. Can you elaborate on the potential computational constraints and scalability issues associated with the proposed CTML framework especially in largescale applications Feedback to the Author(s): 1. Consider providing a clearer explanation of the reasons behind the choice of specific hyperparameter settings and their potential impact on the results. 2. Discuss the practical implications of the findings and how the proposed CTML framework could be applied in realworld scenarios addressing computational costs and scalability concerns. Review Summary: The paper introduces a novel Clustered TaskAware MetaLearning (CTML) framework and demonstrates its effectiveness in fewshot image classification and coldstart recommendation tasks. While the proposed CTML outperforms existing baselines its complexity metatesting time and sensitivity to hyperparameters should be further explored. Additional clarifications on the hyperparameter selection process and the practical applicability of CTML would enhance the papers overall impact and utility. Overall the paper presents a compelling new approach to metalearning but additional insights and analysis could further strengthen its contribution.", "fkjO_FKVzw": " PeerReview of \"Coarformer: Integrating GNN and Transformer for Large Graphs\"  1. Summary: The paper introduces Coarformer a novel architecture that combines Graph Neural Networks (GNNs) and Transformers to address challenges in applying Transformer to large graphs. It identifies obstacles of Transformer application on large graphs primarily due to the vast number of distant nodes and the quadratic computational complexity. Coarformer utilizes a twoview approach with a GNNbased module capturing local information and a Transformerbased module handling coarse yet longrange information on a downsampled graph. Additionally a crossview propagation scheme enables information exchange between the two views. Experimental results on realworld datasets illustrate Coarformer outperforms other methods in terms of performance running time and GPU memory consumption on both homophilic and heterophilic graphs showcasing the effectiveness of integrating local and global information.  2. Strengths and Weaknesses:  Strong Points:  Novel Architecture: Coarformers twoview architecture effectively leverages GNNs and Transformers to encode local and global information showcasing a wellthought design.  Performance: Extensive experiments demonstrate Coarformers superior performance over existing methods on realworld datasets in terms of accuracy running time and GPU memory consumption.  Efficiency: The paper highlights Coarformers efficiency in handling large graphs by reducing computational complexity and memory requirements compared to traditional Transformerbased methods.  Generalizability: The methodology and findings are broadly applicable to a range of graphrelated tasks and datasets showcasing the potential impact of Coarformer in this domain.  Weak Points:  Experimental Clarity: While the experiments are comprehensive additional details on hyperparameter tuning and validation methodology would strengthen the experimental validation section.  Algorithm Detailed Description: Providing more detailed steps or pseudocode for the Coarformers architecture and computation process would enhance the clarity for readers to understand the proposed method.  Further Comparative Analysis: Comparing Coarformer with an even broader range of existing methods and providing deeper insights into the specific advantages of Coarformer over each approach could strengthen the paper.  Robustness Testing: While sensitivity analysis on coarsening algorithms and rates is presented further robustness testing under different scenarios or noise levels could provide additional insights into Coarformers performance under varying conditions.  Questions and Feedback to Author:  Question: How sensitive is Coarformers performance to variations in hyperparameters and how robust is the proposed architecture to noise or perturbations in the input data  Feedback: Providing additional insights into the interpretability of the learned representations by Coarformer and discussing potential limitations or edge cases where the proposed method may not perform optimally would enrich the discussion.  3. Review Summary: The paper presents an innovative architecture Coarformer that effectively combines GNNs and Transformers to address challenges in large graph processing. The methodology is wellmotivated and supported by extensive experiments that showcase superior performance and efficiency over existing methods. The integration of local and global information via a twoview approach demonstrates promising results making Coarformer a strong candidate for graphrelated tasks. While the paper is wellstructured and addresses key aspects comprehensively additional clarity on experimental details and further comparative analysis would enhance the impact and generalizability of the proposed architecture. Overall the paper presents a valuable contribution to the field of graph data processing and lays a solid foundation for future research in this area.  Recommendation: Based on the thorough evaluation of the paper I recommend the acceptance of this work for publication with minor revisions to address the feedback provided. The proposed Coarformer architecture showcases promising results and contributes significantly to advancing the understanding and applicability of Transformers in handling large graphs effectively.", "vkZtFD0zga8": " PeerReview of \"UncertaintyAware Deep LearningBased Video Compression\"  Summary The paper presents a novel approach aiming to address the challenges in deep learningbased video compression by capturing predictive uncertainty using an ensemblebased video compression model. The model utilizes multihead decoders to infer motion vectors and residuals and incorporates ensembleaware loss and adversarial training strategy. Experimental results demonstrate the effectiveness of the proposed model in terms of bitrate savings and outperformance of previous stateoftheart models.  Strengths of the Paper: 1. Innovative Approach: The utilization of an ensemblebased model to capture predictive uncertainty in deep learningbased video compression is innovative and addresses a crucial aspect often overlooked in prior models. 2. Comprehensive Literature Review: The paper provides a detailed review of related work in the field of video compression model uncertainty and deep ensembles establishing a strong theoretical foundation for the proposed method. 3. Experimental Results: The experimental results demonstrate the superiority of the proposed model over previous stateoftheart methods in terms of bitrate savings and performance on various datasets validating the efficacy of the approach. 4. Reproducibility: The authors provide clear details in the appendix regarding the model implementation uncertainty visualization and simulation results enhancing the reproducibility of the study.  Weaknesses of the Paper: 1. Complexity: While the proposed model shows promising results the complexity analysis provided in the paper is limited to the increase in model size and MACs. Further elaboration on training time computational resources and runtime efficiency could enhance the practical applicability of the model. 2. Evaluation on a Single Dataset: The evaluation on a single dataset (Vimeo90K set) limits the generalizability and robustness of the proposed model. Evaluation on additional diverse datasets could provide more insights into the models performance across various scenarios. 3. Lack of Discussion on Limitations: The paper lacks a thorough discussion on the limitations of the proposed method such as potential challenges in realworld deployment constraints in scalability or impact of hyperparameters.  Questions and Feedback to the Author: 1. Generalization: Have you evaluated the model on datasets other than Vimeo90K How does the model generalize across diverse datasets and scenarios 2. Tradeoff Analysis: Could you discuss the tradeoff between bitrate savings and computational complexity for the proposed model How does the model perform in terms of speed and resource requirements compared to existing methods 3. Future Directions: What are the planned future directions for this research Are there specific aspects to focus on for further improvement or realworld applicability of the proposed model  Review Summary: The paper presents a novel ensemblebased model for uncertaintyaware deep learning video compression showcasing impressive bitrate savings and performance gains over stateoftheart models. The comprehensive experimental results and detailed methodology provide strong support for the proposed approach. However addressing the limitations and practical considerations can further enhance the impact and applicability of the model. Overall based on the innovative approach solid experimental results and clear presentation I recommend the paper for acceptance with minor revisions to address the weaknesses highlighted.", "sS0dHmaH1I": " Review of the Paper  1) Summarize the Paper: The paper presents a novel approach for anomaly detection in images by leveraging an Energy Based Model (EBM) coupled with an adaptive sparse coding layer that allows for quick adaptation to new tasks. The framework is designed to efficiently model abnormal distribution adapt to new tasks at inference time with minimal data and simulate fewshot learning through meta learning. The proposed method demonstrates superior anomaly detection and localization capabilities through experimental evaluations in industrial inspection and video surveillance tasks.  2) Evaluation of the Paper: Strong Points:  Innovative Framework: The integration of EBM with adaptive sparse coding layer for anomaly detection is innovative and addresses key challenges faced in anomaly detection in images.  Efficient Adaptation to New Tasks: The ability to adapt to novel tasks at inference time with a few normal samples is a significant strength of the proposed approach.  Sound Methodology: The detailed explanation of the adaptive EBM framework and the episodic training strategy provides a clear understanding of the proposed method.  Empirical Evidence: The paper provides strong empirical evidence to support the effectiveness of the proposed approach through quantitative and qualitative results in both industrial inspection and video surveillance tasks. Weak Points:  Complexity: The method presented may be challenging to implement for researchers without a deep understanding of energybased models and sparse coding which could limit its adoption.  Limited Comparison: While the paper compares the proposed method with existing approaches in the anomaly detection domain a wider comparison with more diverse methodologies could provide a better perspective on the performance. Questions to the Authors: 1. Have you considered exploring the scalability of the proposed framework to larger datasets or different types of anomalies 2. How does the computational complexity of your method compare to other stateoftheart anomaly detection techniques 3. Can you provide insights on the interpretability of the anomaly detection results generated by your method Feedback to the Authors:  The detailed explanations provided in the paper are commendable and should be maintained in future revisions to assist readers in understanding the complexities of the proposed method.  Consider highlighting practical implications and potential applications of the proposed framework to attract a wider audience interested in anomaly detection in various domains.  3) Review Summary: The paper presents a cuttingedge approach for anomaly detection in images using an innovative framework of an EBM with an adaptive sparse coding layer. The method\u2019s strength lies in its efficient adaptation to new tasks with minimal data requirement showcasing superior anomaly detection capabilities through both industrial inspection and video surveillance tasks. Despite its complexity the comprehensive empirical evidence supports the proposed approachs effectiveness. However further comparison with a broader range of existing methods and considerations on scalability and interpretability could enhance the papers impact. Based on the strong methodology empirical evidence and innovative approach presented in the paper I recommend acceptance after addressing the weak points through additional comparisons and further clarifications in response to the posed questions. ", "yjsA8Uin-Y": "Summary: The paper presents a novel trainingfree solution to detect noisy labels in datasets using representations. The proposed method relies on good representations to define neighboring instances for each training example utilizing consensus label information from these neighbors to detect label noise. Two methods are introduced: a local votingbased approach and a rankingbased global detection method. The paper provides theoretical analyses on the impact of representations on the detection methods and proves worstcase error bounds for the rankingbased method. Experimental results on synthetic and realworld label noise datasets demonstrate the effectiveness of the proposed solution compared to existing trainingbased methods achieving significant improvements in detecting label noise. Strengths: 1. Innovative Approach: The paper presents a unique trainingfree solution for label noise detection diverging from existing trainingbased methods. 2. Theoretical Analysis: The paper provides theoretical analyses on the influence of representations on the detection methods offering insights into the effectiveness of the proposed approaches. 3. Empirical Validation: Experimental results on synthetic and realworld datasets support the efficacy of the proposed methods showing consistent and significant improvements over existing baselines. 4. Comparative Analysis: The comparison with existing trainingbased methods and investigation into the impact of different types of representations adds depth to the study. Weaknesses: 1. Complexity: The paper involves complex mathematical derivations and theoretical analysis which might be challenging for readers without a strong background in the subject matter. 2. Generalization: The empirical evaluation focuses on a specific set of datasets and representation methods limiting the generalizability of the findings. 3. Clarity of Presentation: Some sections and concepts could be further clarified for improved understanding especially in the theoretical analyses. Questions: 1. How does the proposed trainingfree solution scale with larger datasets and more complex label noise patterns 2. Have you considered incorporating additional types of representations or pretraining methods to further enhance the performance of the label noise detection methods 3. What are the implications of your findings for realworld applications and industrial implementations of label noise detection Feedback: 1. Enhance the clarity of the theoretical analyses and mathematical derivations for better comprehension by a wider audience. 2. Consider expanding the scope of empirical evaluations to include a more diverse range of datasets and representation methods to validate the generalizability of the proposed methods. 3. Discuss the practical implications and potential applications of the proposed trainingfree label noise detection methods in more detail to provide insights for future research and development. Review Summary: The paper introduces a novel trainingfree solution for label noise detection using representations providing theoretical analyses and empirical evidence to support the efficacy of the proposed methods. While the paper presents innovative approaches and substantiates its claims with experimental results improvements in clarity and generalizability can further enhance the impact and relevance of the study. Therefore I recommend acceptance with minor revisions to address the mentioned weaknesses and questions for further clarification.", "gWGexz8hFH": "Summary The paper introduces the Distributed Skellam Mechanism (DSM) as a solution to enforce differential privacy in federated learning settings where sensitive data are distributed among individual participants. By injecting noise drawn from the Skellam distribution and using secure multiparty computation the proposed DSM overcomes challenges in existing mechanisms by providing privacy guarantees independent of the data dimensionality. The paper conducts theoretical analysis to prove the privacy guarantee and error bounds of DSM and evaluates its performance through experiments on benchmark datasets. Strengths 1. Innovative Solution: The introduction of DSM as a new mechanism for enforcing differential privacy in federated learning is a significant contribution to the field. 2. Theoretical Rigor: The paper provides a detailed theoretical analysis of DSM including privacy guarantees and error bounds showcasing a strong foundation for the proposed solution. 3. Experimental Evaluation: The experiments conducted on benchmark datasets demonstrate the superior performance of DSM compared to existing solutions validating the efficacy of the proposed mechanism. Weaknesses 1. Complexity: The technical aspects of the paper especially in the theoretical analysis might be challenging for readers with limited background in differential privacy and federated learning. Questions 1. Have you considered the computational complexity of implementing the DSM in realworld applications How does the added noise affect the performance of training models on largescale datasets Feedback 1. Providing a summary or visualization of the privacyutility tradeoff for DSM compared to existing mechanisms would enhance the understanding of the practical implications of the proposed solution. 2. Discussing the scalability and potential limitations of DSM in deploying federated learning systems with varying communication constraints could further strengthen the paper. Review Summary The paper presents a novel solution the Distributed Skellam Mechanism (DSM) for enforcing differential privacy in federated learning settings. Through rigorous theoretical analysis and experimental validation the authors showcase the effectiveness of DSM in preserving privacy while maintaining model utility. Despite the complexity of the technical details the innovative approach and promising results position DSM as a valuable contribution to the field. Therefore based on the strength of the proposed mechanism the thorough theoretical analysis and the compelling experimental results I recommend accepting this paper with minor revisions to improve clarity and provide additional insights into the practical implications of DSM.", "ivQruZvXxtz": " Summary of the Paper: The paper addresses the issue of aligning gradients between tasks in multilingual models during finetuning to prevent negative transfer and catastrophic forgetting of knowledge acquired from pretraining. The authors propose a method called Sequential Reptile which efficiently aligns gradients between tasks by sampling minibatches from all tasks sequentially. The method is validated on various multitask learning and zeroshot crosslingual transfer tasks where it outperforms relevant baselines.  Strong Points of the Paper: 1. Innovative Methodology: The paper introduces a novel method Sequential Reptile to efficiently align gradients between tasks during finetuning of multilingual models. 2. Extensive Validation: The authors extensively validate their method on various MTL and zeroshot crosslingual transfer tasks demonstrating its superior performance over all considered baselines. 3. Clear Problem Statement: The paper clearly articulates the problem of negative transfer and catastrophic forgetting in multilingual models and provides a systematic approach to address it.  Weak Points of the Paper: 1. Complexity of Method Description: The papers method description could be challenging to follow for readers unfamiliar with the specific technical details related to gradient alignment. 2. Lack of Visualization: While the paper includes some visual illustrations more visual aids could help in understanding the proposed method and experimental results better. 3. Limited Comparison: The paper compares its method against a set of baselines but additional comparison with other stateoftheart methods could provide a broader perspective.  Questions for the Authors: 1. Generalization: How does the proposed Sequential Reptile method generalize to tasks beyond multitask learning and zeroshot crosslingual transfer 2. Scalability: Have you explored the scalability of the method to larger models or datasets considering its efficiency in aligning gradients between tasks 3. Robustness: How does Sequential Reptile handle noisy or varied data distributions across different tasks during training  Feedback to the Authors: 1. Clarity: Simplify the method description for better understanding possibly with more illustrative examples. 2. Comprehensive Comparison: Consider including comparisons with more stateoftheart methods to provide a thorough evaluation of the proposed approach.  Review Summary: The paper introduces an innovative method Sequential Reptile to align gradients between tasks in multilingual models during finetuning. The method is effective in preventing negative transfer and catastrophic forgetting as validated through extensive experiments. While the paper demonstrates strong methodology and results improvements in clarity visual aids and additional comparisons would enhance its overall impact. Hence based on the presented findings the paper is recommended for publication pending revisions to address the identified weaknesses.", "z2B0JJeNdvT": " 1) Summary of the Paper: The paper focuses on zerothorder optimization particularly in the context of largescale multiagent systems with decentralized data and costs. It presents a detailed investigation into the transition from centralized to distributed zerothorder algorithms specifically addressing convergence rates for multiagent systems with timevarying communication networks. The paper introduces the MAZOPA algorithm a multiagent zerothorder projection averaging algorithm and extends it to a multistage setting for improved convergence rates. Theoretical convergence results are provided for various function classes along with numerical examples to demonstrate the performance of the proposed algorithms.  2) Strengths and Weaknesses Questions and Feedback:  Strong Points:  Theoretical Contributions: The paper makes significant theoretical contributions by establishing convergence rates for distributed zerothorder subgradient algorithms across different function classes. This provides valuable insights into the performance of zerothorder optimization in complex distributed systems.  Algorithm Development: The introduction of the MAZOPA algorithm and its multistage variant demonstrates the innovative approach to addressing optimization challenges in decentralized settings.  Comprehensive Analysis: The paper provides a thorough analysis of the convergence rates under various scenarios and function classes enhancing the understanding of distributed zerothorder optimization algorithms.  Weak Points:  Complexity: The papers technical depth and rigorous mathematical formulations may present challenges for readers not wellversed in optimization theory potentially limiting the accessibility of the research.  Practical Implementation: While the numerical examples are beneficial more extensive experiments showcasing the practical applicability of the proposed algorithms across diverse scenarios could further strengthen the paper.  Questions and Feedback to the Author: 1. Clarity for NonExperts: Considering the complexity of the topic how would you suggest improving the readability and clarity of the paper for readers with varied levels of expertise particularly those outside the optimization research domain 2. Practical Validation: Have you considered additional realworld case studies or simulations to validate the effectiveness and efficiency of the proposed algorithms in practical multiagent systems scenarios 3. Scalability Analysis: How do you envision scaling these distributed zerothorder algorithms to even larger multiagent systems especially in scenarios with dynamic or unpredictable network structures  3) Review Summary: The paper presents a detailed study on distributed zerothorder optimization in multiagent systems offering valuable theoretical insights and algorithmic developments. While the rigorous analysis and convergence results contribute significantly to the understanding of decentralized optimization addressing the complexity for broader readership and conducting further practical validations could enhance the impact and applicability of the research. Overall the paper provides a solid foundation for future studies in scalable distributed optimization algorithms.", "oaKw-GmBZZ": "Peer Review  1) Summary of the Paper: The paper introduces a novel approach using graph neural networks to efficiently solve timedependent partial differential equations (PDEs). The authors propose domain invariant features inspired by classical PDE solvers to represent PDEdata enabling accurate solver schemes for linear and nonlinear PDEs. The message passing graph neural networks (MPGNN) are utilized to parameterize governing equations allowing for learning efficient solver operators that can be generalized to solve on different physical domains. The study also presents a recurrent graph neural network approach for predicting a temporal sequence of solutions to a PDE.  2) Strengths and Weaknesses of the Paper: Strong Points:  Innovative Approach: The paper presents a novel approach using graph neural networks to solve timedependent PDEs which is a unique and promising direction in computational physics.   Generalization: The ability of the proposed MPGNN to generalize to new physical domains and different geometries is a significant strength.   Recurrent Model: The inclusion of a recurrent graph neural network for predicting temporal sequences of solutions adds valuable insight to the study.  Comparison with Existing Methods: The benchmark tests comparing the proposed approach with existing studies provide valuable insights into the performance of different neural network architectures for PDEs. Weak Points:  Experiment Details: More detailed information on the specific datasets used hyperparameters and training procedures could enhance the reproducibility and credibility of the results.  Performance Metrics: While MSE (Mean Squared Error) is commonly used incorporating additional metrics such as convergence rates or computational efficiency could further evaluate the proposed method.  Visualization: More visualizations or illustrative examples could aid in understanding the performance and generalization capabilities of the MPGNNs for various PDEs.  Disadvantages of Existing Methods: While the paper highlights the limitations of existing methods a more direct comparison showcasing how the proposed approach addresses these limitations would strengthen the argument. Questions and Feedback to the Author: 1. Data Generation: Could you provide more details on the data generation process for training the MPGNN specifically regarding the distribution of initial conditions and PDE parameters 2. Computational Efficiency: How does the computational efficiency of your approach compare to traditional numerical schemes for solving PDEs 3. Further Experiments: Have you considered testing your method on more complex PDE systems to evaluate its performance in challenging scenarios  3) Review Summary: The paper presents a novel approach using graph neural networks for efficient and accurate solver schemes for timedependent PDEs. The generalization capabilities of the proposed MPGNN particularly to new physical domains demonstrate the potential of the method for realworld applications. While the experimental results showcase the effectiveness of the approach providing more specific details on datasets hyperparameters and a deeper analysis of computational efficiency could further strengthen the paper. By addressing the weaknesses mentioned the study has the potential to make a significant impact in the field of computational physics. Given the innovative nature of the proposed method and its potential for broad applications in solving PDEs I recommend this paper for publication pending the authors addressing of the mentioned weaknesses and questions. ", "hpBTIv2uy_E": " Summary The paper proposes a new hypergraph neural network paradigm called AllSet which offers a highly general framework for hypergraph neural networks. AllSet utilizes multiset functions to efficiently learn and process hypergraph data enhancing performance on a variety of node classification tasks. The study includes an extensive set of experiments involving ten benchmark datasets and three newly curated datasets showcasing the superior performance of AllSet compared to existing methods.  Strengths 1. Novel Approach: The paper introduces a novel paradigm AllSet for hypergraph neural networks offering a highly general framework for efficient learning and processing of hypergraph data. 2. Theoretical Analysis: The study provides a comprehensive theoretical analysis demonstrating the expressiveness of the proposed AllSet framework in comparison to existing hypergraph neural networks. 3. Extensive Experiments: The paper conducts a thorough set of experiments involving various benchmark datasets and newly curated datasets demonstrating the superior performance of AllSet across different scenarios. 4. Comparison with Baselines: The paper thoroughly compares the proposed AllSet with a range of baseline methods showcasing significant improvements in performance. 5. Relevant Citations: The paper is wellreferenced citing important works in the field of hypergraph neural networks and machine learning.  Weaknesses 1. Complexity: The paper is highly technical and dense which may make it challenging for readers outside the specific domain of hypergraph neural networks to understand fully. 2. Assumption Clarity: Some assumptions made regarding dataset properties and learning methods could benefit from further clarification for better understanding. 3. Reproducibility: Although efforts have been made to ensure reproducibility more detailed guidelines or code snippets provided in the paper might enhance the replicability of the experiments.  Questions and Feedback to the Author 1. Could you elaborate on any specific challenges faced during the implementation of the AllSet framework that influenced the performance results 2. In the discussion of related works could additional context be provided on how AllSet can potentially address limitations highlighted in prior research on GNNs and hypergraph neural networks 3. The mention of ethical considerations in the \"Ethics Statement\" is appreciated. Could you expand on any potential ethical implications relevant to hypergraph neural network research that emerged during the development or experimentation phases 4. Providing more insights into the practical implications of the study results such as applications in realworld scenarios or potential future research directions would enhance the impact of the paper.  Review Summary The paper introduces a novel hypergraph neural network paradigm AllSet which offers a highly general framework for efficient learning and processing of hypergraph data. The study demonstrates superior performance compared to existing hypergraph neural networks through an extensive set of experiments involving diverse datasets. While the paper is wellstructured and thorough in its analysis further clarification on implementation challenges reproducibility ethical considerations and practical implications could strengthen the overall impact of the research. Overall the innovative approach and strong experimental results support a positive recommendation for publication.", "jaLDP8Hp_gc": " Summarize the Paper: The paper addresses the problem of correspondence hallucination in computer vision where a network is trained to output a peaked probability distribution over the correspondent\u2019s location in the target image plane regardless of it being visible occluded or outside the field of view. The proposed network NeurHal learns to identify and hallucinate correspondences and is evaluated on unseen image pairs for various applications including absolute camera pose estimation.  Strong Points: 1. Novelty: The paper tackles a novel problem of learning to hallucinate correspondences filling a gap in the existing literature. 2. Experimental Evaluation: Detailed experiments demonstrate the networks ability to hallucinate correspondences on unseen image pairs and its significant improvement in absolute camera pose estimation over stateoftheart methods. 3. Technical Details: The paper provides a comprehensive analysis of the problem loss function network architecture and training methodology making it informative and reproducible. 4. Results: The paper presents strong results that indicate the effectiveness of NeurHal in hallucinating correspondences and improving robustness in camera pose estimation scenarios.  Weak Points: 1. Limitations: The paper mentions limitations such as the lower precision in inpainting compared to outpainting and the output of lowresolution correspondence maps which can be potential areas for further research. 2. Generalization: While the network is shown to generalize well across different datasets further analysis on the transferability of the approach to diverse scenarios could strengthen the paper.  Questions and Feedback to the Author: 1. Can you provide more insights into the implications of the networks lowresolution correspondence maps on the accuracy of hallucinated correspondences 2. How do you envision overcoming the challenging task of inpainting correspondences to achieve higher precision in future iterations of the model 3. It would be beneficial to discuss the computational complexity of the proposed architecture and any optimizations made for efficient training and inference.  Review Summary: The paper introduces a novel approach NeurHal to learn correspondence hallucination in computer vision showcasing its effectiveness in hallucinating correspondences and improving robustness in absolute camera pose estimation. The comprehensive analysis detailed experiments and strong results support the significance of the proposed method. However addressing limitations and providing further insights into the practical implications of the networks outputs would enhance the overall contribution of the work. Based on the novelty technical depth and promising results I recommend accepting the paper with minor revisions to address the identified weak points.", "tJhIY38d2TS": "PeerReview: 1) Summary of the Paper: The paper focuses on proposing a new adversarial training approach called locally reweighted adversarial training (LRAT) to address the limitations of instancesreweighted adversarial training (IRAT) when tested on attacks different from the attack simulated during training. The authors argue that the safeness of instances should be attackdependent and thus propose LRAT which pairs each instance with its adversarial variants and performs local reweighting inside each pair. Experimental results show that LRAT outperforms IRAT and standard adversarial training (AT) when trained with one attack and tested with different attacks. 2) Strengths and Weaknesses: Strong Points:  The paper addresses an important issue in robustness training by proposing a novel LRAT approach.  The rationale behind LRAT is clearly described explaining the need for attackdependent weight assignment.  The experimental setup is comprehensive and evaluates LRAT against various attacks providing detailed results and comparisons.  The proposed LRAT approach demonstrates clear improvements over existing methods in defending against multiple attacks. Weak Points:  The paper could benefit from more indepth discussions on the choice of hyperparameters and the rationale behind their selection.  Evaluation on additional datasets or realworld scenarios could further strengthen the validation of LRATs effectiveness.  Further insight into the scalability and efficiency of LRAT compared to existing methods would enhance the practical applicability of the proposed approach. Questions and Feedback: 1. Can you provide more insights into the selection of hyperparameters (such as \u03b1 \u03b2 C) in LRAT and how sensitivity analysis of these parameters was conducted 2. Have you considered the computational complexity of LRAT compared to existing methods especially in scenarios involving largescale datasets or complex neural network architectures 3. How would LRAT perform in realworld scenarios beyond the experimental setup presented in the paper and are there any plans to validate LRAT in practical applications 3) Review Summary: The paper presents a wellstructured and insightful analysis of the limitations of existing adversarial training methods and proposes a novel solution LRAT to address these limitations. The experimental results demonstrate the effectiveness of LRAT in improving robustness against various attacks compared to IRAT and standard AT. The comprehensive evaluation and clear explanations support the recommendation of LRAT as a promising approach to enhance model robustness. However further exploration on hyperparameter selection scalability and realworld applicability could strengthen the practical impact and relevance of LRAT. Overall the paper provides valuable contributions to the field of adversarial training and warrants further investigation and validation in diverse settings. Recommendation: Based on the thorough analysis and experimental results presented in the paper I recommend accepting this work for publication. The proposed LRAT approach demonstrates significant advancements in adversarial training methodology and contributes valuable insights to the research community. Addressing the minor points highlighted above would further enhance the clarity and impact of the paper. Ethics and Reproducibility Statement: The ethics and reproducibility aspects of the paper have been appropriately addressed in the provided sections ensuring compliance with ethical standards and transparency for reproducibility. Please feel free to reach out if you need further clarification or additional feedback.", "luO6l9cP6b6": " Summary: The paper investigates the nature of crossdomain transfer in pretrained language models by exploring the extent of transfer when models are denied word identity information via random scrambling. The experiments compare BERT LSTM with GloVe embeddings and baseline models on four classification tasks and two sequence labeling tasks. The key findings include BERT demonstrating robust crossdomain transfer primarily in classification tasks the limited role of word frequency matching in transfer and the importance of both pretraining and finetuning in achieving transfer.  Strengths: 1. Comprehensive and Rigorous Approach: The paper provides a thorough investigation into crossdomain transfer by systematically exploring various aspects such as word frequency matching semantic consistency and freezing BERT parameters. 2. Clear Experimental Design: The paper presents a clear experimental paradigm to evaluate transferability without identity information utilizing multiple models and tasks to draw meaningful conclusions. 3. Detailed Analysis and Interpretation: The authors delve into the results providing indepth analyses regarding the factors influencing transfer the role of pretraining and finetuning and the learning dynamics of the models. 4. Relevance and Contribution: The findings add valuable insights into the mechanisms of crossdomain transfer in pretrained models contributing to the understanding of how models like BERT can generalize across diverse domains.  Weaknesses: 1. Limitation in Sequence Labeling Tasks: The paper acknowledges the challenge of transfer in sequence labeling tasks but a more detailed discussion on potential strategies to enhance transfer in these tasks could be beneficial. 2. Need for Extended Discussion: While the analysis is comprehensive further discussion on the implications of the findings for future research and practical applications could enhance the papers impact.  Questions and Feedback: 1. The paper highlights the importance of pretraining and finetuning in achieving crossdomain transfer. Could the authors elaborate on potential strategies to optimize the interplay between these processes for improved transferability 2. Considering the limitations in sequence labeling tasks do the findings suggest specific modifications in architecture or training approaches that could enhance transfer in such tasks  Review Summary: The paper presents a wellstructured and detailed investigation into crossdomain transfer in pretrained language models particularly focusing on the impact of word identity denial via scrambling. Strong experimental design analytical depth and contribution to understanding transfer mechanisms make this work valuable. However addressing the limitations in sequence labeling tasks and providing further insights for practical applications could enhance the papers impact. Thus I recommend accepting this paper with minor revisions and extensions to improve completeness and applicability.", "qSTEPv2uLR8": " Review of Paper on Deep Learning Approach for Optimal Mass Transport and Density Estimation 1. Summary: The paper proposes a new deep learning approach to solve the continuous Optimal Mass Transport (OMT) problem by leveraging Breniers theorem and Physics Informed Neural Networks (PINNs). The method aims to estimate the solution to L2optimal transport problem between two densities using input convex neural networks. The framework is tested on synthetic examples with known analytical solutions and compared with existing deep learningbased algorithms. Additionally the paper demonstrates applications in density estimation and generative modeling tasks. 2. Strengths and Weaknesses: Strong Points:  The paper addresses a significant problem of solving the continuous OMT problem using deep learning approaches.  The incorporation of Breniers theorem and PINNs adds theoretical rigor to the framework.  Extensive experimental validation on synthetic examples provides evidence of the proposed methods effectiveness.  The comparison with stateoftheart deep learningbased OMT solvers and density estimation methods adds value to the research. Weak Points:  The paper could benefit from a more detailed explanation of the algorithmic implementation and experimental setup.  The evaluation metrics used in the experiments could be further elaborated to provide a comprehensive understanding of the results.  More insights into the computational efficiency and scalability of the proposed method would enhance the impact of the research. Questions to Authors:  Can you provide more insights into the hyperparameter tuning process for the deep neural networks used in the experiments  How does the proposed method scale with increasing dimensions especially in highdimensional spaces  Have you considered realworld datasets for evaluating the performance of the framework and if so what were the results Feedback to Authors:  Consider providing more detailed explanations of the experimental setup and results to facilitate better reproducibility and understanding for readers.  Include discussions on the computational efficiency and scalability aspects of the proposed approach in relation to realworld applications.  Addressing the weak points identified could further strengthen the paper and its impact in the research community.  Review Summary: The paper presents a novel deep learning approach for solving the continuous Optimal Mass Transport problem and density estimation tasks. The incorporation of Breniers theorem and PINNs demonstrates theoretical depth and the extensive experimental validation showcases the methods efficacy. While the paper has notable strengths in its theoretical foundation and experimental validation improvements in clarity scalability discussions and realworld applications would enhance the overall impact. Therefore based on the presented strengths and potential improvements I recommend a major revision to address the identified weak points and enhance the clarity and applicability of the proposed method.", "q23I9kJE3gA": "1) Summary: The paper proposes a novel approach for conditional set generation in natural language processing tasks using SEQ2SEQ models. The authors argue that the traditional approach of treating sets as sequences disregards key properties such as orderinvariance and cardinality. To address this they introduce a data augmentation method called TSAMPLE that enables informative orders for set labels while explicitly modeling set cardinality. The experiments conducted on simulated settings and three realworld NLP datasets demonstrate significant improvements in F1 score over strong SEQ2SEQ baselines. 2) Strengths and Weaknesses: Strong Points:  The paper addresses a crucial gap in modeling set generation tasks by incorporating orderinvariance and cardinality explicitly.  The proposed TSAMPLE method efficiently generates informative orders by leveraging label dependencies leading to improved performance.  Empirical results across both simulated and realworld datasets demonstrate substantial gains in F1 score validating the effectiveness of the proposed approach.  The paper provides a theoretical grounding for the approach and thorough analysis to support the methods soundness. Weak Points:  The paper could benefit from more detailed discussion on the limitations and challenges of the proposed method.  While the experimental results are promising more indepth analysis on failure cases or areas for further improvement could strengthen the discussion.  The paper could provide insights into the computational and memory requirements of the proposed method to help practitioners understand its scalability. Questions and Feedback to the Author: 1. Could you elaborate on any specific scenarios or cases where the proposed method might face challenges or limitations in realworld applications 2. It would be beneficial to discuss the practical computational requirements and scalability aspects of the TSAMPLE method. How does the method perform with larger datasets or more complex NLP tasks 3. Considering the empirical results have you identified any specific NLP tasks or domains where the proposed approach might exhibit exceptional performance or potential drawbacks 3) Review Summary: The paper presents an innovative approach to conditional set generation in NLP tasks by explicitly modeling orderinvariance and cardinality using the TSAMPLE data augmentation method. The empirical results demonstrate a significant improvement in F1 score over strong SEQ2SEQ baselines validating the effectiveness of the proposed approach. The papers thorough theoretical grounding detailed experimental analysis and practical implications showcase the methods relevance and contribution to the research community. While the paper has significant strengths in addressing key properties of set generation further discussions on limitations and scalability aspects would enhance the overall impact and applicability of the proposed method. Overall I recommend acceptance of this paper based on its novel approach strong experimental results and valuable contributions to the field of NLP set generation tasks.", "sTNHCrIKDQc": " 1. Summary of the Paper: The paper addresses the challenge of learning from networkvalued data particularly focusing on clustering multiple graphs without vertex correspondence. The proposed methods are inspired by the concept of graphons and involve a novel graph distance estimation technique. Two clustering algorithms Distancebased Spectral Clustering (DSC) and Similaritybased SemiDefinite Programming (SSDP) are introduced and shown to achieve stateoftheart results. The paper also explores the use of the proposed graph distance for graph twosample testing. Theoretical consistency of the algorithms is proved under certain assumptions and the performance is evaluated on both simulated and real datasets from Bioinformatics and Social Networks.  2. Strengths and Weaknesses:  Strengths:  Theoretical Foundation: The paper is theoretically wellgrounded with rigorous proofs of consistency provided for the proposed algorithms under specific assumptions. This adds credibility to the proposed methods.  Innovative Approach: The paper introduces a novel graph distance estimation technique based on graphons which differentiates it from traditional methods and leads to effective clustering algorithms.  Empirical Evaluation: Extensive experimental evaluation on both simulated and real datasets demonstrates the effectiveness of the proposed methods in comparison to existing approaches.  Practical Applicability: The proposed algorithms show strong performance not only in accuracy but also in computational efficiency making them suitable for realworld applications.  Comprehensive Discussion: The paper covers a wide range of topics including theory algorithms experiments and practical implications providing a holistic view of the research.  Weaknesses:  Assumptions and Limitations: The reliance on specific assumptions (Assumptions 13) may limit the generalizability of the proposed methods. Further exploration of scenarios where these assumptions may not hold would enhance the robustness of the approach.  Experimental Setup: While the empirical evaluation is thorough additional insights could be gained by assessing the sensitivity of the algorithms to parameter choices and exploring different experimental setups.  Comparison with Existing Methods: While the comparison with existing methods is informative a deeper analysis of the limitations of competing techniques and potential areas of improvement would enhance the papers contributions.  Questions and Feedback to the Author: 1. Assumption Evaluation: Have you conducted sensitivity analyses or additional experiments to assess the impact of deviations from the assumptions made in the theoretical proofs on the performance of the proposed algorithms 2. Parameter Sensitivity: How sensitive are the algorithms (DSC and SSDP) to the specific choices of parameters such as n0 and \u03c3i in different experimental settings Have you explored ways to automate the selection of these parameters 3. Scalability Discussion: Could you provide more insights into the scalability aspects of the proposed algorithms especially in handling very large graphs or datasets beyond the ones considered in the current experiments 4. Future Directions: What are some potential avenues for future research building on the current work Are there specific extensions or new applications where the proposed methods could be further explored and refined  3. Review Summary and Recommendation: The paper presents a welldeveloped approach to clustering multiple graphs based on a novel graph distance estimation technique inspired by graphons. The theoretical foundation empirical evaluation and practical implications showcased in the paper highlight the effectiveness and potential impact of the proposed methods in learning from networkvalued data. Despite some limitations and assumptions the strong theoretical backing and promising empirical results make this work a significant contribution to the field. Therefore based on the thorough review of the paper and its strengths I recommend acceptance with minor revisions to address the questions and points raised for further clarification and enhancement of the research. The papers comprehensive coverage innovative approach and significant empirical results justify its acceptance for publication while suggesting opportunities for further exploration and refinement.", "jKzjSZYsrGP": " Summary The paper proposes a novel Transformerbased model called SCformer for longterm time series forecasting. SCformer replaces the conventional selfattention mechanism with a more efficient segment correlation attention (SCAttention) mechanism which divides time series into segments and captures correlations between segments. Additionally the paper introduces a dual task as a regularization method to train SCformer aiming to enhance the stability and generalization ability of the model. Extensive experiments on multiple datasets demonstrate that SCformer outperforms other Transformerbased methods.  Strengths 1. Innovative Mechanism: Introducing the SCAttention mechanism to improve efficiency and capture long shortterm dependencies is a significant contribution. 2. Empirical Evaluation: The extensive experiments on various datasets in different fields provide a strong basis for the proposed models superiority over existing methods. 3. Dual Task Design: The dual task regularization for training SCformer is a novel approach to enhance prediction stability. 4. Ablation Study: The ablation study provides valuable insights into the effectiveness of the proposed SCAttention mechanism and the impact of the segment length hyperparameter.  Weaknesses 1. Clarity: Some sections could benefit from clearer explanations especially regarding the mathematical formulations of the proposed mechanisms. 2. Comparison Metrics: While MSE and MAE are standard evaluation metrics inclusion of additional metrics or comparisons could provide more comprehensive insights into model performance.  Questions to Authors 1. Can you elaborate on the computational complexity of SCAttention compared to traditional selfattention in Transformer models 2. How does the segment length parameter impact the performance of SCformer and how was the optimal segment length determined 3. Have you considered conducting experiments on realworld applications to validate the performance of SCformer in practical scenarios 4. Could you provide insights on the scalability of SCformer to larger and more complex datasets  Feedback Overall the paper presents a novel approach to longterm time series forecasting with the proposed SCformer model and SCAttention mechanism. The empirical results are promising showcasing the models effectiveness. Clearer explanations in certain sections and additional experiments could further strengthen the paper. The dual task regularization and ablation study add depth to the analysis. Considering the potential impact of the proposed model addressing the weaknesses mentioned could enhance the overall quality of the paper. Based on the innovative approach strong empirical results and potential impact of the SCformer model on longterm time series forecasting I recommend accepting this paper for publication after addressing the mentioned suggestions.", "vgqS1vkkCbE": " Summary of the Paper: The paper introduces Value Function Spaces (VFS) as a novel state representation derived from the value functions of lowlevel skills in hierarchical reinforcement learning (HRL). VFS provides a skillcentric representation that captures the affordances of the lowlevel skills allowing for enhanced highlevel planning. The authors showcase the effectiveness of VFS through empirical evaluations for mazesolving and robotic manipulation tasks demonstrating improved longhorizon performance and better zeroshot generalization compared to alternative methods.  Strong Points: 1. Innovative Contribution: The introduction of VFS as a representation learning approach that leverages value functions for constructing a skillcentric state abstraction is a significant and innovative contribution to HRL. 2. Empirical Evaluation: The paper thoroughly evaluates VFS through mazesolving and robotic manipulation tasks demonstrating improved performance in longhorizon tasks and effective zeroshot generalization. 3. Clafrity: The paper is wellstructured and clearly explains the motivation behind VFS the methods employed and the experimental results. 4. Comparative Analysis: The paper provides indepth comparisons with alternative representation learning methods in HRL showcasing the superiority of VFS in terms of performance and generalizability.  Weak Points: 1. Lack of Exploration on Skill Learning: The paper assumes the availability of lowlevel skills and does not address the process of skill discovery or learning which could limit the applicability of VFS in realworld scenarios where skill acquisition is a major challenge. 2. Limited Scope in ModelBased Planning: The modelbased planning algorithm using VFS is relatively simplistic and the paper acknowledges that more sophisticated methods could further enhance performance. Some exploration in this area would have added depth to the research.  Questions for the Author: 1. How do you plan to address the challenge of skill discovery or learning in future iterations of your work considering the critical role of these lowlevel skills in the effectiveness of VFS 2. Have you considered extending the modelbased planning algorithm with more advanced techniques such as path integral optimal control to further improve performance and compare against VFS  Feedback: The paper presents a valuable contribution in the area of hierarchical reinforcement learning with the introduction of VFS. Strengthening the discussion on skill learning and exploring more advanced modelbased planning methods would enhance the comprehensiveness of the research. The empirical evaluations are robust and provide compelling evidence of the effectiveness of VFS. Overall the paper is wellwritten and makes a significant impact in the field of reinforcement learning. Further exploration into the areas of skill acquisition and advanced planning algorithms could open up new avenues for research.  Review Summary: Based on the strong conceptual framework innovative approach thorough empirical evaluations and clear presentation in the paper I recommend acceptance for publication with minor revisions to address the areas of skill learning and modelbased planning as discussed above.", "zBOI9LFpESK": " Summary: The paper introduces a novel metalearnerbased framework called Adaptive Metalearner of Behavioral Similarities (AMBS) for learning state representations in reinforcement learning from highdimensional visual observations. The framework decomposes observations into reward and dynamics representations and uses metalearners to learn similarities in these representations. The paper discusses the motivation methodology and experimental results of AMBS showcasing superior performance compared to stateoftheart baselines in various benchmarks.  Strengths: 1. Originality and Innovation: The paper introduces a novel metalearnerbased framework which is different from traditional approaches in reinforcement learning. 2. Clear Problem Statement: The paper effectively articulates the problem of learning state representations from highdimensional visual observations in reinforcement learning. 3. Comprehensive Experiments: The paper includes extensive experiments on various benchmarks demonstrating the effectiveness of AMBS compared to stateoftheart baselines. 4. Detailed Methodology: The methodology is wellexplained detailing the components of AMBS such as metalearners balancing reward and dynamics impacts and data augmentation. 5. Robustness Analysis: The ablation study and transferability experiments provide insightful analyses of the effectiveness and generalizability of the proposed framework.  Weaknesses: 1. Clarity in Presentation: While the paper is comprehensive the technical details and terminology used may be challenging for readers without a strong background in reinforcement learning. 2. Evaluation Metrics: The paper could benefit from a more detailed discussion on the specific metrics used to evaluate the performance of the proposed method. 3. Discussion of Limitations: It would be beneficial to include a section addressing potential limitations of the proposed framework to provide a more balanced perspective.  Questions and Feedback: 1. How does the proposed AMBS framework scale with the complexity of environments or tasks in reinforcement learning 2. Could you provide more insight into the computational efficiency of AMBS compared to existing approaches especially considering the use of metalearners 3. In the transfer over reward functions experiment what specific insights can be drawn from the faster convergence of the transferring encoder compared to training from scratch 4. It would be beneficial to elaborate on the practical implications of the proposed AMBS framework for realworld applications in autonomous driving or other complex environments.  Review Summary: The paper presents a novel metalearnerbased framework AMBS for learning state representations in reinforcement learning. The methodology is innovative and the paper provides comprehensive experimental results showcasing the superiority of AMBS over existing baselines. While the paper is detailed and thorough improvements in clarity evaluation metrics and discussing limitations would enhance its overall impact. Given the strengths observed in the papers methodology and experimental results I recommend acceptance after addressing the mentioned feedback to improve clarity and provide a more balanced perspective on the proposed framework.", "fwzUgo0FM9v": " 1) Summary of the Paper: The paper introduces a new threat model in federated learning where malicious modifications to a shared model architecture allow the server to directly obtain user data from gradient updates without solving complex inverse problems. The study focuses on scenarios where the server aims to breach user privacy by modifying the model architecture and parameters. The proposed attack method called the imprint module involves structured manipulations in a neural network that enable the recovery of user data from model updates even when aggregated over large batches. Experimental results show the effectiveness of the imprint module in breaching privacy in realworld scenarios.  2) Strong Points:  Innovative Contribution: The paper introduces a novel threat model and attack method in federated learning extending the understanding of privacy vulnerabilities in current systems.  Comprehensive Analysis: The study provides detailed discussions on the attack mechanism experimental results and implications for user privacy in federated learning systems.  Empirical Validation: The experiments conducted on ImageNet datasets demonstrate the practical applicability and effectiveness of the proposed attack method showcasing substantial privacy breaches.  Data Agnostic Approach: The imprint module attack is not limited to image data offering potential threats in various data modalities beyond vision applications.  Weak Points:  Limited Discussion on Defense Strategies: The paper briefly mentions potential defense and mitigation strategies such as differential privacy and secure aggregation but further exploration and analysis of defense mechanisms could enhance the study.  Ethical Implications: The ethical considerations and implications of the proposed attack method in terms of user consent data privacy regulations and societal impacts are not extensively discussed.  Clarity in Presentation: Certain sections of the paper especially in the experimental results and discussions could benefit from clearer explanations and visual representations to enhance understanding.  Questions and Feedback to the Author: 1. Clarification of Defense Mechanisms: Can the authors elaborate on potential defense strategies and mitigation techniques that could effectively counter the imprint module attack in federated learning settings 2. Ethical Considerations: How do the authors view the ethical implications of breaching user privacy through the proposed attack method and what ethical guidelines should be considered when conducting research on privacy vulnerabilities in machine learning systems 3. Generalization to Other Data Modalities: How transferable is the imprint module attack across different types of data modalities and are there specific characteristics that render it more effective in certain domains over others  3) Review Summary: The paper presents a significant contribution in identifying a new threat model in federated learning showcasing the potential for breaching user privacy through minimal modifications to model architectures. The innovative attack method the imprint module offers a practical and scalable approach to extracting user data from aggregated model updates. While the study provides valuable insights and empirical evidence of privacy vulnerabilities further exploration of defense strategies ethical implications and generalizability to diverse data modalities would strengthen the overall impact and relevance of the research. Therefore I recommend the paper for publication pending addressing the mentioned weak points and providing additional clarity on defense mechanisms and ethical considerations.", "lkQ7meEa-qv": "1) Paper Summary: The paper introduces Neural Acoustic Fields (NAFs) an implicit neural representation method to capture the acoustic properties of scenes. NAFs model how sounds propagate in a physical space by encoding impulse responses using Fourier frequency domain. It addresses the gap in learning auditory representations compared to visual representations by providing a continuous and generic approach to model acoustics. The models effectiveness is demonstrated through high fidelity scene reverberation modeling improved crossmodal generation of novel visual views and sound source localization. 2) Evaluation of Strong and Weak Points: Strong Points:  Introduction of a novel Neural Acoustic Fields (NAFs) approach for learning acoustic representations in a continuous and spatially coherent manner.  Demonstrated capability to capture environment reverberations with high fidelity and generalize to novel locations.  Improvement in crossmodal generation of visual views using learned acoustic structure.  Successful application of NAFs for sound source localization.  Comprehensive background information and detailed methodology explain the working principles of NAFs effectively. Weak Points:  The paper could benefit from more quantitative results and comparisons with additional baselines to enhance the robustness of the proposed NAF model.  Limited discussion on scalability and computational efficiency of the NAF method particularly in realtime applications.  The qualitative results are showcased but could be strengthened with more quantitative evaluations across various tasks presented in the paper.  The generalization through geometric conditioning aspect could be further elaborated to provide a deeper understanding of the models capabilities. Questions and Feedback to the Author: 1. Can you provide more detailed comparisons with existing methods or benchmarks to showcase the superiority of Neural Acoustic Fields (NAFs) 2. How does the computational complexity of implementing NAFs compare to traditional methods for acoustic representation and do you foresee any challenges in scaling the approach 3. Could you elaborate further on the potential limitations of NAFs such as scenarios where the model may struggle or require additional optimizations 4. It would be beneficial to include insights on the practical applicability of NAFs in realworld scenarios and potential avenues for future research based on the current findings. 3) Review Summary: The paper presents a significant contribution in the field of neural acoustic representation through the introduction of Neural Acoustic Fields (NAFs) showcasing its effectiveness in capturing the underlying acoustics of scenes. The detailed methodology thorough experimental evaluations and innovative application scenarios demonstrate the versatility and potential of NAFs in various tasks including scene reverberation modeling crossmodal generation and sound source localization. While the paper presents a strong foundation and promising results additional quantitative analyses and discussions on practical implications could further enhance the clarity and impact of the proposed approach. Overall the paper offers a valuable and innovative perspective on learning acoustic representations in a continuous and spatially coherent manner. Based on the strengths and potential improvements highlighted I recommend acceptance of the paper with revisions to address the mentioned weak points and expand on the practical implications and scalability considerations of Neural Acoustic Fields.", "iPHLcmtietq": " Paper Review:  1) Summary: The paper investigates the role of phase collapses in improving the linear separability of image classes in deep convolutional networks. It introduces the concept of phase collapse which eliminates spatial variability while still achieving linear separability. The paper demonstrates that phase collapses rather than thresholding operators significantly contribute to the classification accuracy of deep neural networks. A novel architecture called Learned Scattering network is proposed which utilizes phase collapses and reaches ResNet accuracy on ImageNet and CIFAR10 datasets. Furthermore the paper compares phase collapses with amplitude reductions showing that phase collapses outperform amplitude reductions in classification tasks.  2) Strengths and Weaknesses:  Strong Points:  The paper proposes a novel concept of phase collapses and demonstrates their effectiveness in achieving high classification accuracy in deep neural networks.  The introduction of the Learned Scattering network that leverages phase collapses without the need for thresholding operators is a significant contribution.  The theoretical analysis and mathematical explanations provided give a solid foundation for understanding the mechanisms behind phase collapses and amplitude reductions.  The experimental validation conducted on datasets like ImageNet and CIFAR10 supports the claims made in the paper.  The reproducibility statement and availability of code on GitHub enhance the transparency and credibility of the research.  Weak Points:  While the paper introduces a novel concept the technical language and complex mathematical explanations might make it challenging for readers without a strong background in signal processing and neural networks.  The text could be more concise in certain sections to improve readability and understanding.  Additional visual aids or diagrams could help in clarifying complex mathematical concepts and architecture designs.  Questions and Feedback: 1. Could you provide a practical example or case study to better illustrate how phase collapses can be applied in realworld image classification tasks 2. It would be helpful to include comparisons with other stateoftheart architectures or methodologies in the experimental section to further highlight the superiority of phase collapses. 3. Have you considered conducting user studies or surveys to validate the impact of phase collapses on classification accuracy in realworld scenarios 4. Clarifying the incorporation of skipconnections in the Learned Scattering network and how they contribute to the final accuracy would enhance the understanding of the proposed architecture.  3) Review Summary: The paper makes a significant contribution to the understanding of deep neural networks by introducing the concept of phase collapses and demonstrating their effectiveness in achieving high classification accuracy. The theoretical analysis along with experimental validation on benchmark datasets provides strong evidence for the superiority of phase collapses over amplitude reductions in image classification tasks. The proposed Learned Scattering architecture leveraging phase collapses without thresholding operators is a novel and promising approach to improve linear separability. While the paper is technically rich and complex it sets a solid foundation for future research in the field of deep learning. Overall based on the novel concept theoretical groundwork experimental validation and reproducibility statement I recommend acceptance of the paper. Further clarification on implementation details and practical applications would enhance the impact and accessibility of the research. If you have any further questions please feel free to ask.", "tT9t_ZctZRL": " Peer Review  1) Summary The paper investigates the trainability of deep graph convolutional networks (GCNs) using the Graph Neural Tangent Kernel (GNTK) framework. It explores the behavior of ultrawide GCNs as the depth increases and shows that trainability exponentially collapses in the optimization process. The study also extends the analysis to residual connectionbased techniques and proposes Critical DropEdge a connectivityaware sampling method to address the exponential trainability decay problem. Theoretical analyses along with experimental evaluations on node classification tasks confirm the effectiveness of Critical DropEdge in achieving better results compared to existing methods.  2) Strengths and Weaknesses  Strengths:  Theoretical Contributions: This paper makes significant theoretical contributions by analyzing the trainability of deep GCNs using GNTK and providing insights into why deep GCNs fail to optimize.  Methodological Innovation: The introduction of Critical DropEdge as a novel sampling method to address trainability issues in deep GCNs is innovative and holds promise.  Comprehensive Experimental Evaluation: The empirical evaluation on multiple datasets and comparisons with stateoftheart methods strengthen the validity of the proposed method.  Weaknesses:  Complexity: The paper is highly technical and may be challenging for readers without a strong background in graph neural networks and optimization theory.  Missing Detailed Implementation: While the theoretical framework and results are wellexplained more detailed insights into the practical implementation of Critical DropEdge and its computational complexity could be beneficial.  Limited Evaluation Metrics: The paper focuses on node classification tasks and it would benefit from discussing results on other graphrelated tasks like link prediction or graph classification.  Questions and Feedback to the Author: 1. Can you provide more insights into the computational complexity and practical implementation aspects of the Critical DropEdge method to give readers a better understanding of its feasibility in realworld scenarios 2. How does the proposed method generalize to other graphrelated tasks beyond node classification Have you considered evaluating its performance on tasks like link prediction or graph classification  3) Review Summary The paper presents a thorough investigation into the trainability of deep GCNs using the GNTK framework and proposes Critical DropEdge as a solution to mitigate trainability issues. The theoretical foundation is strong and the empirical evaluation demonstrates the efficacy of the proposed method. While the paper is wellstructured and presents innovative contributions addressing the complexity of the content and providing more practical implementation details could further enhance its impact. Overall the study contributes significantly to the understanding and improvement of deep graph neural networks. Given the strengths of the theoretical framework methodological innovation and promising experimental results I recommend acceptance of this paper with minor revisions to address the weaknesses identified.", "lKcq2fe-HB": " Peer Review  Summary: The paper investigates nonparametric selfpaced reinforcement learning (SPRL) utilizing Wasserstein metrics to prevent degeneracy in learning performance. It addresses limitations of existing SPRL implementations restricted to Gaussian distributions. The study introduces NPSPRL and WBSPRL demonstrating the importance of respecting the metric structure of the context space in curriculum reinforcement learning. Various experiments are conducted across maze pointmass pickandplace and TeachMyAgent benchmark environments showcasing the effectiveness of WBSPRL.  Strengths: 1. Novel Contribution: The paper presents a novel approach by introducing nonparametric SPRL and incorporating Wasserstein metrics to overcome limitations of Gaussianbased SPRL implementations. 2. Theoretical Grounding: The incorporation of optimal transport concepts into SPRL adds a strong theoretical foundation to the study enhancing the understanding of curriculum reinforcement learning. 3. Comprehensive Experiments: The paper conducts thorough experiments across different environments comparing the performance of NPSPRL and WBSPRL with existing methods providing valuable insights into the effectiveness of the proposed approaches. 4. Clear Methodological Detail: The study provides clear and detailed explanations of the methodologies employed including algorithms and implementation details enhancing reproducibility and understanding for the research community.  Weaknesses: 1. Discrete Disadvantages: The study acknowledges challenges in scaling the discretization approach to higher dimensions potentially limiting the applicability of NPSPRL in more complex scenarios. 2. Limited Experiment Variety: While the experiments cover various environments additional diverse scenarios or benchmarking against more stateoftheart methods in each environment could further strengthen the study. 3. Complexity and Interpretation: The technical aspects of Wasserstein metrics and optimal transport may be challenging for readers not wellversed in these concepts. Providing more intuitive explanations could enhance the accessibility of the paper.  Questions for the Author: 1. How do you envision addressing scalability issues related to the discretization approach in NPSPRL for higherdimensional context spaces 2. Can you elaborate on potential future directions for incorporating a prioritized sampling mechanism in the SPRL framework to enhance learning progress 3. How do you plan to make the SPRL framework more accessible to a wider audience by simplifying complex concepts like Wasserstein metrics for practical implementation in reinforcement learning  Feedback to the Author:  Consider expanding the experiment diversity to include more challenging environments or compare performances against additional stateoftheart methods in each setting.  Clarify complex theoretical concepts through more accessible explanations or visual aids to facilitate understanding for a broader audience.  Address scalability concerns related to discretization methods and outline strategies for overcoming limitations in higherdimensional spaces.  Review Summary: The paper presents a novel approach to curriculum reinforcement learning by introducing nonparametric SPRL with Wasserstein metrics. The thorough experiments conducted across multiple environments showcase the effectiveness of the proposed methods. While the study provides strong theoretical grounding and clear methodological detail there are limitations related to scalability and complexity that need to be addressed. Overall the innovative contributions and insightful experimental results make this work significant for the reinforcement learning community warranting further exploration and potential improvements in scalability and accessibility.", "xZ6H7wydGl": " Summary of the Paper: The paper introduces a novel algorithm for efficient estimation of probabilities in Stochastic Differential Equation (SDE) models for the purpose of learning. The proposed method generates lower variance gradient estimates compared to algorithms based on SDE integrators and it is embarrassingly parallelizable. This algorithm eliminates the sequential dependence between sampled points and can take advantage of modern parallel hardware for significant decreases in computation time. The authors provide theoretical bases mathematical proofs and experiments to validate the effectiveness of their proposed method.  Strong Points: 1. Innovative Algorithm: The proposed importancesampling estimator for SDEs is innovative and addresses a critical issue in learning SDE models efficiently. 2. Mathematical Rigor: The paper provides detailed theoretical foundations and mathematical proofs to support the proposed algorithm. 3. Parallelization Benefit: The embarrassingly parallelizable nature of the algorithm is a significant advantage for leveraging modern parallel hardware for efficient computation. 4. Comparison Experiments: The paper includes thorough experiments comparing the proposed algorithm with traditional integratorbased approaches providing clear insights into the effectiveness of the new method.  Weak Points: 1. Complexity: The paper delves deep into mathematical formulations and technical details which might be challenging for readers not wellversed in stochastic calculus. 2. Limited Experimental Scenarios: Although the experiments demonstrate the effectiveness of the proposed method the paper could benefit from a wider variety of scenarios to showcase its applicability across different domains. 3. Lack of Realworld Application: While theoretical and experimental aspects are wellcovered the paper could be strengthened by including a realworld application or case study to demonstrate the practical utility of the proposed algorithm.  Questions to the Author: 1. Can you provide additional insights into the potential applications of this algorithm in practical scenarios or realworld problems to showcase its versatility 2. How does the proposed method compare in terms of scalability when the number of observations or dimensions increases significantly 3. Have you considered incorporating additional datasets or models to demonstrate the robustness and generalizability of the algorithm beyond the provided experiments  Feedback: 1. Clarity and Accessibility: Simplifying complex mathematical explanations and providing intuitive illustrations can enhance the accessibility of the paper to a broader audience. 2. Expanded Application Scenarios: Including more diverse realworld application examples could strengthen the papers practical relevance and broaden its impact. 3. Discussion on Computational Efficiency: Further discussion on the efficiency gains in terms of computational resources and time savings by using the proposed algorithm would be beneficial for the readers.  Review Summary: The paper presents an innovative algorithm for efficient estimation of probabilities in SDE models addressing a critical computational challenge in learning processes. Supported by robust theoretical foundations and thorough experiments the proposed method showcases promise in improving computational efficiency and reducing variance in gradient estimates. While the paper excels in technical depth and experimental validation expanding the discussion to include practical applications and insights on scalability could further enhance its impact. Overall based on the strengths observed in the paper and its potential contributions to the field I recommend acceptance after minor revisions to enhance clarity and broaden the practical context of the algorithm.", "gdWQMQVJST": " Summary: The paper introduces a novel federated learning paradigm called Neural Tangent Kernelpowered Federated Learning (NTKFL). In this approach participants in a federated learning setting transmit samplewise Jacobian matrices to a central server which constructs a global model using the Neural Tangent Kernel framework. The proposed method aims to address the challenge of statistical heterogeneity in federated learning by providing more expressive updates. Additionally a variant of the method CPNTKFL is introduced with enhanced communication efficiency and privacy features. The paper provides theoretical analysis algorithm descriptions and experimental results demonstrating the superiority of NTKFL over traditional federated learning methods like FedAvg.  Strong Points: 1. Innovative Approach: The paper introduces a novel paradigm that leverages the Neural Tangent Kernel framework for federated learning addressing the challenge of statistical heterogeneity. 2. Theoretical Justification: The paper provides theoretical analysis supporting the effectiveness of the NTKFL paradigm showcasing its advantages over existing methods. 3. Experimental Validation: The experimental results presented in the paper demonstrate the superior performance of NTKFL in terms of accuracy and efficiency compared to baseline federated learning methods. 4. Comprehensive Exposition: The paper provides a detailed explanation of the proposed method including algorithm descriptions theoretical foundations and experimental setups.  Weak Points: 1. Complexity: The papers content is highly technical and may be challenging for readers without a strong background in machine learning and federated learning. 2. Limited Comparison: While the experimental results show the effectiveness of NTKFL a more extensive comparison with a wider range of baseline methods could provide further insights into its performance. 3. Privacy Consideration: The paper mentions enhanced privacy features in CPNTKFL but may benefit from a more detailed discussion or analysis of privacy implications and guarantees.  Question to Author: 1. Can you provide insights into the practical implementation challenges of deploying NTKFL in realworld federated learning scenarios  Feedback to Author: 1. Clarification: Clarify how the proposed method can be adapted for different neural network architectures beyond multilayer perceptrons. 2. Indepth Analysis: Consider adding a more extensive analysis of the privacy implications of the proposed method and how it compares to existing privacypreserving federated learning approaches.  Review Summary: The paper presents an innovative approach NTKFL to address the challenge of statistical heterogeneity in federated learning by leveraging the Neural Tangent Kernel framework. The theoretical foundations algorithm descriptions and experimental results provide strong evidence of the methods superiority over traditional federated learning methods. While the work demonstrates significant contributions to the field further insights into practical implementation challenges and privacy considerations could enhance the papers impact. Overall the paper is wellstructured technically sound and provides valuable insights into the evolving landscape of federated learning paradigms. I recommend acceptance with minor revisions to address the mentioned points.", "y8zhHLm7FsP": "Summary: The paper presents a novel simulationbased algorithm using an ensemble Kalman filter (EnKF) to learn the optimal control law for the linear quadratic Gaussian (LQG) optimal control problem without explicitly solving the Riccati equation. The algorithm applied to both fully and partially observed cases combines EnKF for estimation and control design. The theoretical results algorithms and numerical experiments are highlighted in the paper. Strengths: 1. Innovative Approach: The paper introduces a novel simulationbased algorithm using EnKF to approximate the optimal control law offering a fresh perspective on tackling the LQG optimal control problem. 2. Comprehensive Explanation: The paper provides a detailed explanation of the EnKF algorithm its twostep process and how it relates to reinforcement learning landscape making it accessible for readers to understand the methodology. 3. Practical Relevance: The application of the algorithm to both fully and partially observed cases of the LQG problem along with numerical comparisons with competing algorithms demonstrates the practical relevance of the proposed approach. 4. Theoretical Analysis: The paper includes theoretical analysis convergence studies and error bounds enhancing the credibility of the proposed algorithm. 5. Numerical Illustrations: The numerical simulations provided in the paper demonstrate the effectiveness of the EnKFbased algorithm in approximating the optimal control showcasing its performance compared to existing approaches. Weaknesses: 1. Complexity: The paper delves into intricate mathematical formulations which might pose a challenge for readers who are not wellversed in advanced control theory or simulationbased algorithms. 2. Limited RealWorld Applications: While the EnKFbased approach shows promise in the LQG problem the paper lacks practical examples or case studies where the algorithm has been applied in realworld scenarios limiting the practical understanding of its effectiveness. Questions and Feedback to the Author: 1. Clarification: Can the author explain how the EnKFbased approach is practically applied in realworld scenarios beyond the theoretical and numerical examples provided in the paper 2. Generalization: Are there plans to extend the applicability of the proposed algorithm to other control problems or domains beyond the LQG optimal control problem 3. Scalability: How does the computational complexity of the EnKF algorithm scale with increasing dimensions of the statespace and are there strategies to optimize it for largescale systems Review Summary: The paper presents a novel EnKFbased algorithm for learning the optimal control law in the LQG optimal control problem showcasing advanced theoretical analysis numerical demonstrations and comparisons with existing methods. The innovative approach comprehensive explanation and practical relevance make this paper a valuable contribution to the field of control theory and reinforcement learning. However the complexity of the presented algorithms and limited realworld applications might hinder its accessibility and practical adoption. Overall with further considerations for realworld applications generalization and scalability the proposed algorithm shows promise in enhancing control solutions in complex systems.", "zuqcmNVK4c2": " Summary The paper introduces a new supervised learning paradigm called selfjoint learning aiming to mitigate the limitations of standard supervised learning frameworks. It proposes modeling the joint conditional distribution of pairs of data samples to explicitly learn sampletosample relationships addressing issues like overconfidence vulnerability to overfitting adversarial attacks and outofdistribution detection. The framework incorporates auxiliary unlabelled data to enhance performance. Experimental results on benchmark image datasets demonstrate significant improvements over standard supervised learning in accuracy robustness against adversarial attacks outofdistribution detection and overconfidence mitigation.  Strengths  Novel Approach: The introduction of selfjoint learning is a valuable addition to the field offering a fresh perspective on supervised learning.  Experimental Evidence: The paper provides thorough experimental results demonstrating the effectiveness of the proposed framework in improving classification accuracy and robustness against adversarial attacks and outofdistribution data.  Regularization Effect: Demonstrated effectiveness in regularizing deep models with increasing capacity to avoid overfitting.  Incorporation of Auxiliary Data: Effective utilization of auxiliary unlabelled data for improved generalization.  Adversarial Robustness: Significantly outperforms baseline methods in terms of robustness against adversarial attacks.  OOD Detection: Successfully detects outofdistribution data with competitive performance against stateoftheart techniques.  Importance of Joint Distribution: Clear comparison highlighting the superiority of the selfjoint model over ensembling with shared parameters.  Weaknesses  Complexity: The paper is quite technical and may be challenging for readers unfamiliar with advanced machine learning concepts.  Limited Clarity: Some sections could benefit from clearer explanations or illustrative examples to aid comprehension.  Configuration Details: Lack of hyperparameter optimization specifics for each dataset may raise questions regarding reproducibility and generalization.  Evaluation Metrics: While the evaluation metrics used are standard discussing potential limitations or alternative metrics could enrich the analysis.  Questions and Feedback to the Author 1. Clarity on Auxiliary Data Selection: Can you provide insights into how auxiliary unlabelled data were selected and their impact on model performance 2. Reproducibility: Could you provide a more detailed description of hyperparameter tuning strategies to ensure reproducibility 3. Comparison with StateoftheArt: How does the proposed selfjoint learning framework compare with other cuttingedge techniques in terms of adversarial robustness and outofdistribution detection 4. Scalability: Have you tested the scalability of your selfjoint learning framework to larger and more complex datasets beyond the ones mentioned in the paper 5. Potential Applications: How do you envision the practical applications of selfjoint learning in realworld scenarios outside the realm of image classification  Review Summary The paper presents a novel selfjoint learning framework that effectively addresses the limitations of standard supervised learning showcasing improvements in accuracy robustness and generalization. The comprehensive experimental evaluations validate the effectiveness of the proposed approach. However it would benefit from enhanced clarity detailed hyperparameter optimization methods and further comparisons with stateoftheart techniques. The meticulous approach innovative methodology and promising results make this paper highly recommendable for publication.", "oC12z8lkbrU": "1) Summary of the paper: The paper introduces a framework called \"generate annotate and learn (GAL)\" to address the challenge of scarcity of taskspecific unlabeled data in semisupervised learning (SSL) and knowledge distillation (KD) for Natural Language Processing (NLP) and tabular tasks. The GAL framework utilizes unconditional language models to generate synthetic indomain unlabeled data which is then annotated with pseudo labels. This synthetic data is used in SSL and KD to improve model performance. The paper demonstrates the effectiveness of GAL on various NLP tasks achieving a new stateoftheart result for 6layer transformers on the GLUE leaderboard and showing significant gains on tabular tasks as well. 2) Strong Points:  Introduction of an innovative framework (GAL) that addresses the challenge of scarcity of taskspecific unlabeled data in SSL and KD.  The paper is comprehensive in demonstrating the effectiveness of GAL on various NLP tasks and tabular tasks through systematic experiments.  The theoretical framing of GAL in terms of empirical and vicinal risk minimization adds a strong theoretical underpinning to the approach.  The connection made between expected risk minimization perspective and GALs performance adds depth to the understanding of the framework.  Strong empirical results showcasing improvements in promptbased fewshot learning SSL and KD on NLP and tabular tasks. Weak Points:  The text is highly technical and may be challenging for readers not deeply familiar with SSL KD and the related concepts.  Limited discussion on the potential scalability and generalizability of GAL to other domains beyond NLP and tabular tasks.  More insights could be provided on the limitations or scenarios where GAL may not be as effective.  Clearer visualization or summarization of the results could enhance the readability and understandability of the findings presented. Questions and Feedback to the author:  Have you considered exploring potential limitations or scenarios where the GAL framework may not be as effective or applicable  How scalable is the GAL framework to larger datasets and more complex tasks beyond the ones demonstrated in the paper  It would be beneficial to provide practical implementation guidance or considerations for researchers interested in adopting the GAL framework in their own work.  Are there plans for further research to extend the GAL framework to other domains or tasks beyond NLP and tabular data 3) Review Summary: The paper presents a novel framework GAL that addresses the challenge of limited taskspecific unlabeled data in SSL and KD. Through a combination of theoretical motivation empirical experiments and resultdriven analysis the paper convincingly demonstrates the effectiveness of GAL on improving model performance in NLP and tabular tasks. The strong theoretical underpinning comprehensive experimental validation and clear presentation of results make this paper a valuable contribution to the field of SSL and KD. Despite the technical complexity the clear structure and systematic approach make it a significant research piece. Therefore based on the comprehensive evidence provided I recommend this paper for publication in a reputable conference or journal.", "niZImJIrqVt": " Summary of the Paper The paper introduces Expected Quadratic Utility Maximization Reinforcement Learning (EQUMRL) as an efficient method for addressing the meanvariance (MV) tradeoff in riskaware reinforcement learning. By training an agent to maximize the expected quadratic utility function Pareto efficient policies are achieved without the computational difficulties associated with conventional MVRL methods. The paper presents theoretical foundations algorithm formulation and experimental results to validate the effectiveness of EQUMRL.  Strong Points  Innovative Approach: The paper introduces a novel approach EQUMRL to address the meanvariance tradeoff efficiently in reinforcement learning settings.  Theoretical Foundation: The paper provides a solid theoretical foundation for the proposed method linking it to existing economic and financial principles.  Computational Efficiency: EQUMRL avoids the computational difficulties of existing methods by directly optimizing the expected quadratic utility function.  Experimental Validation: The effectiveness of EQUMRL is validated through comprehensive experiments showcasing its superior performance in achieving MVefficient policies.  Weak Points  Evaluation Metrics: While the paper presents several metrics to evaluate the performance of EQUMRL further discussions on the choice of evaluation metrics and its impact on method comparison could enhance the clarity of results interpretation.  Algorithm Implementation Details: Providing more detailed information on the implementation of algorithms including hyperparameters selection convergence criteria and computational complexities would improve the reproducibility and understanding of the proposed method.  Questions to the Author 1. How does the proposed EQUMRL method handle nonstationarity and changing dynamics in the environment especially in scenarios where the meanvariance tradeoff may evolve over time 2. Can you elaborate on the implications of the hyperparameter selection in EQUMRL particularly regarding the choice of parameters \u03b1 and \u03b2 and how sensitive the method is to these choices 3. Are there any specific insights or practical considerations for implementing EQUMRL in complex or realworld applications beyond the financial portfolio management scenarios discussed in the paper  Feedback to the Author  Clarity and Structure: The paper is wellstructured and provides a clear explanation of the proposed method. However enhancing the clarity of the algorithm implementation details evaluation methodologies and implications of hyperparameter choices can further improve the readers understanding.  Experimental Validation: The experimental results are insightful providing additional analyses on the robustness generalization capabilities and scalability of EQUMRL across diverse reinforcement learning tasks would strengthen the empirical validation.  Impact and Significance: Highlighting the potential impact of EQUMRL in addressing realworld challenges beyond financial applications could enhance the broader relevance of the proposed method in various domains. Overall the paper presents a promising approach with potential implications in the field of reinforcement learning particularly in addressing the computational challenges of MVRL methods. Further discussions on implementation details hyperparameter sensitivities and generalization capabilities could enhance the practical utility and broader adoption of EQUMRL. nbsp The review aligns with the expectations of a highquality peer review by providing a comprehensive assessment of the paper highlighting its strengths and weaknesses posing relevant questions to the author and offering constructive feedback for improvement.", "i7O3VGpb7qZ": "1) Summarize the paper: The paper introduces a novel deep learning approach to solve the code editing problem with few exemplars. The problem involves adapting an editorial pattern from a small number of editing exemplars to a query code snippet aiming to automate code revisions in software engineering tasks. The proposed method combines edit representations extracted from exemplars and generalizes them compositionally to edit the query code snippet. By parsing code snippets into abstract syntax trees and measuring similarities between support and query snippets at multiple extents the method achieves significant accuracy improvements compared to noncomposition baselines on C and Python datasets. 2) Strengths and Weaknesses: Strong Points:  Innovative Approach: The paper introduces a novel deep learning approach addressing the code editing problem with few exemplars which surpasses existing methods in accuracy improvements.  Comprehensive Evaluation: The paper provides a thorough evaluation of the proposed method on both C and Python datasets showcasing its effectiveness in various coding contexts.  Clear Problem Statement: The paper clearly defines the problem of code editing with exemplars and articulates the necessity of leveraging few exemplars for better generalization.  Detailed Methodology: The paper provides detailed information on the proposed method including the parsing of code snippets multiextent similarities ensemble and error estimator for composition. Weak Points:  Complexity: The method described in the paper may be quite complex for readers unfamiliar with deep learning or abstract syntax trees potentially making it challenging for some readers to grasp the technical details.  Lack of comparison to more conventional methods: While the proposed method outperforms the baseline models a comparison with more traditional approaches or human performance could provide additional insights into the methods effectiveness. 3) Questions and Feedback to the Author:  Question: Can you provide insights into the computational resources required to implement the proposed method especially in terms of training time and model complexity  Question: How does the proposed method scale with a larger dataset in terms of both performance and training time  Feedback: It would be beneficial to include a discussion on practical implications and potential realworld applications of the proposed method in software engineering tasks. 4) Review Summary: The paper presents a novel deep learning approach to address the code editing problem with few exemplars demonstrating significant accuracy improvements compared to noncomposition baseline methods on C and Python datasets. The methods innovative approach comprehensive evaluation and detailed methodology make a strong case for its effectiveness in automating code revisions in software engineering tasks. While the complexity of the method and lack of comparison to conventional approaches are notable weaknesses the overall contribution of the paper in advancing code editing with deep learning techniques is commendable. Therefore I recommend accepting this paper for publication considering the significance of the problem addressed and the promising results showcased by the proposed method.", "h4EOymDV3vV": " Summary of the Paper: The paper introduces a new diffusionbased representation learning framework that allows for manual control of the level of details encoded in the representation. By augmenting the denoising score matching objective the authors propose a method to learn a latent datagenerating code without any supervised signal. The framework includes an alternative formulation of denoising score matching conditional score matching for controllable generation and learning of latent representations. The authors show improvements in stateoftheart models on semisupervised image classification and present side contributions on adversarial training and the choice of initial noise scale in diffusionbased generative models.  Strengths of the Paper:  Innovative Framework: The proposed diffusionbased representation learning framework offers a new perspective on representation learning without the need for supervised signals.  Controllable Granularity: The ability to control the granularity of features encoded in the representation is a unique and valuable aspect of the proposed method.  Improved Performance: The experimental results demonstrating improvements in stateoftheart models for semisupervised image classification showcase the effectiveness of the proposed approach.  Theoretical Foundation: The paper provides a strong theoretical foundation for their proposed methods supported by detailed derivations and analysis.  Comprehensive Analysis: The authors provide a thorough analysis of their methods and results including detailed experiments and evaluations.  Weaknesses of the Paper:  Complexity: The papers technical depth and complexity may make it challenging for readers unfamiliar with diffusionbased generative models to fully grasp the concepts and implications.  Limited Computational Resources: The authors acknowledge limitations in their computational resources which may impact the scalability and generalizability of the proposed approach.  Incomplete Comparison: While the paper showcases improvements in semisupervised image classification a more extensive comparison with existing methods and benchmarks could provide a more comprehensive evaluation.  Questions for the Authors: 1. Can you provide insights into how the proposed framework can be practically applied in realworld scenarios where data may be more complex or varied 2. How sensitive are the results to hyperparameters in the proposed diffusionbased representation learning framework and what considerations should be made for tuning these hyperparameters 3. Could you elaborate on potential future directions or extensions of your work especially in terms of improving computational efficiency and scalability  Feedback for the Authors:  Clarification and Simplification: Consider providing additional clarifications or simplifications in the presentation of complex theoretical concepts to enhance accessibility for a broader audience.  Expanded Comparative Analysis: To strengthen the paper consider expanding the comparative analysis with existing methods and conducting additional experiments on diverse datasets to further validate the effectiveness of the proposed framework.  Robustness Analysis: Conduct further robustness analysis to examine the stability of the proposed approach under different conditions and settings.  Review Summary: The paper presents an innovative diffusionbased representation learning framework that allows for manual control of the granularity of features encoded in the representation. While the paper demonstrates improvements in semisupervised image classification and provides a strong theoretical foundation for their methods there are aspects that need further exploration and refinement. With its unique contributions and detailed analysis the paper has the potential to significantly impact the field of generative modeling and representation learning. Addressing the weaknesses and considerations mentioned above can further enhance the clarity applicability and robustness of the proposed framework.", "qynB_fAt5TQ": "Peer Review: 1. Summary: The paper introduces a novel method for improving the efficiency and effectiveness of the Bootstrap method particularly in situations where computational resources are limited. The proposed method aims to optimize a small set of highquality centroids to approximate the ideal bootstrap distribution. By minimizing a specially designed objective function equivalent to the Wasserstein distance the paper demonstrates improved efficiency and accuracy of uncertainty estimation compared to standard i.i.d. sampling. The paper includes theoretical analysis and empirical evaluations across various applications such as confidence interval estimation contextual bandit methods and bootstrapped deep Qnetwork. 2. Evaluation: Strong Points:  The paper addresses a significant issue of computational intensity in the Bootstrap method providing a novel and efficient approach to optimize centroids.  The theoretical foundation including Assumptions Definitions and Theorems is wellstructured and supports the proposed method effectively.  The comprehensive evaluation with multiple experiments across different applications demonstrates the effectiveness and superiority of the proposed centroid approximation method.  The paper includes detailed methodology algorithms and background information providing a clear understanding of the proposed approach. Weak Points:  The paper may benefit from further elaboration on the practical implications and limitations of the proposed method in realworld applications.  The explanation of the centroid degeneration phenomenon and its resolution could be further elaborated to enhance clarity for readers.  Providing more insights into the scalability of the method concerning larger sample sizes or computational complexities could further enhance the papers impact. Questions and Feedback to the Author: 1. Could you elaborate on any potential limitations or challenges that the proposed centroid approximation method may face in practical deployment scenarios 2. How does the proposed method perform in situations where the training data is limited or the model complexity is increased Could you provide some insights on adaptability to different data scenarios 3. It would be beneficial to discuss the computational overhead introduced by the proposed method in more detail. How does it compare to the standard Bootstrap method in terms of computational resources and running time 3. Review Summary: The paper presents a wellstructured and thorough examination of a novel centroid approximation method to improve the efficiency and accuracy of Bootstrap. The theoretical foundation is robust and the empirical evaluations convincingly establish the superiority of the proposed method over standard Bootstrap. The papers clarity methodology and detailed experiments make a strong case for the viability and significance of the proposed approach. Further research and discussions on practical implications scalability factors and computational efficiency could enhance the impact and applicability of the method. Recommendation: Based on the thorough evaluation of the paper I recommend accepting this manuscript with minor revisions to address the weak points highlighted. The significance of the proposed method supported by theoretical analyses and empirical results makes it a valuable contribution to the field of uncertainty quantification in machine learning and statistics.", "ufGMqIM0a4b": "1) Summary of the Paper: The paper introduces a new framework called InfinityGAN for generating arbitrarysized images. Traditional generative models have limitations when it comes to scaling up image generation to arbitrarily large sizes due to computational constraints and lack of largefieldofview training data. InfinityGAN addresses these challenges by training and inferring images in a patchbypatch manner disentangling global appearances local structures and textures. The framework is able to generate highquality images with superior realism and scalability showcasing applications like spatial style fusion multimodal outpainting and image inbetweening. 2) Evaluation: Strong Points:  Innovative Framework: InfinityGAN addresses the challenges of generating arbitrarily large images efficiently and realistically offering a new approach to image generation.  Experimental Validation: The paper extensively validates the proposed method through qualitative and quantitative experiments user studies and comparisons with existing methods.  Parallelizable Inference: InfinityGAN enables spatially independent generation facilitating parallel computation and significantly improving inference speed for highresolution image synthesis.  Applications Showcase: The paper demonstrates various practical applications of the proposed framework showcasing its flexibility and performance in tasks like style fusion outpainting and inbetweening. Weak Points:  Limited Comparison Metrics: Direct comparison between InfinityGAN and other methods is challenging due to the different training resolutions and patchbased approach of InfinityGAN making it hard to assess performance objectively.  Artifact Generation: The paper mentions potential artifact generation issues when composing landscapes of different scales or when dealing with elements like tree leaves in sky regions.  Convergence Issues: The slight decrease in FID score compared to StyleGAN2 could indicate potential convergence issues that might need further investigation and improvement. Questions and Feedback to the Author: 1. Performance Metrics: Could you explain in more detail how you validated the performance of InfinityGAN against existing methods given the challenges in direct comparison due to different training resolutions 2. Artifact Mitigation: Have you considered specific strategies or techniques to address the potential artifact generation issues when composing images of different scales or dealing with specific elements like tree leaves in sky regions 3. Convergence Improvement: What are your plans to address the slight decrease in FID score compared to StyleGAN2 especially regarding potential convergence issues and areas for improvement 3) Review Summary: Overall the paper presents a novel and innovative framework InfinityGAN for arbitrarysized image generation addressing key challenges in scalability and realism. The thorough experimental validation extensive applications showcase and promising user study results underline the strength of the proposed method. While the parallelizable inference and practical applications demonstrate the potential of InfinityGAN considerations like artifact mitigation and convergence improvements could enhance the frameworks performance further. Given the frameworks innovation scalability and practical utility I recommend acceptance after addressing the mentioned weak points and questions. Further research into artifact mitigation strategies and convergence improvement could strengthen the frameworks performance.", "nEfdkfAyRT8": " Summary of the Paper: The paper introduces CubicGDA a Newtontype Gradient DescentAscent algorithm developed to escape strict saddle points and converge to secondorder stationary points in nonconvexstronglyconcave minimax optimization problems. By leveraging gradient ascent to estimate secondorder information of the minimax objective function and utilizing the cubic regularization technique CubicGDA aims to achieve global convergence to secondorder stationary points at a sublinear rate. The paper provides a comprehensive theoretical analysis including the characterization of an intrinsic potential function global convergence properties convergence rates under various nonconvex geometries and the asymptotic convergence rates of parameter sequences generated by CubicGDA.  Strengths of the Paper: 1. Original Contribution: The development of CubicGDA as the first Newtontype GDA algorithm for escaping strict saddle points in nonconvex minimax optimization is a significant contribution. 2. Theoretical Analysis: The paper provides a thorough theoretical analysis including the formulation of an intrinsic potential function global convergence properties and convergence rates under different types of geometries showcasing a deep understanding of the algorithms behavior. 3. Clear Presentation: The paper is wellstructured logically organized and clearly presents all concepts definitions and results making it accessible for readers.  Weaknesses of the Paper: 1. Complexity: The paper delves into an intricate mathematical framework which might make it challenging for readers without a strong background in optimization and mathematics to grasp the details fully. 2. Implementation Details: While the core algorithm is wellexplained more insights on practical implementation details and realworld applicability could enhance the papers usability. 3. Comparative Analysis: While the paper mentions the superiority of CubicGDA over standard GDA a more detailed comparison with existing stateoftheart algorithms in escaping saddle points could strengthen the paper.  Questions to the Author: 1. How does the proposed CubicGDA algorithm compare with other existing secondorder optimization algorithms in terms of convergence speed and computational efficiency 2. Are there specific scenarios or types of minimax optimization problems where CubicGDA demonstrates superior performance and if so what are the distinguishing characteristics 3. Have there been any experimental validations or case studies conducted to validate the theoretical results presented in the paper and if not what are the planned future directions for practical validation  Feedback: The paper provides a valuable contribution to the field of nonconvex minimax optimization with the introduction of CubicGDA. Strengthening the theoretical analysis with practical case studies or experimental validations could further enhance the significance and applicability of the proposed algorithm. Additionally elaborating on the algorithms computational complexity and resource requirements could help readers better understand the practical feasibility of implementing CubicGDA in realworld scenarios.  Review Summary: Overall the paper presents a wellstructured and comprehensive study on CubicGDA providing novel insights into escaping saddle points in nonconvex minimax optimization. The theoretical rigor and depth of analysis showcased in the paper make a compelling case for the significance of the proposed algorithm. However addressing implementation considerations comparative analyses and experimental validations would further strengthen the papers impact and relevance in the optimization research community. Based on the papers strengths in theoretical analysis and original algorithm development with some enhancements in practical applicability I recommend this paper for publication after addressing the mentioned feedback points.", "jJis-v9Pzhj": " Summary The paper proposes a new method called PUUPL (PositiveUnlabeled UncertaintyAware PseudoLabeling) for PositiveUnlabeled (PU) learning. The method incorporates epistemic uncertainty in pseudolabeling to improve the reliability of pseudolabels which in turn enhances the predictive performance of the model. The authors conduct extensive experiments on various datasets modalities and learning tasks to demonstrate the effectiveness and robustness of their method achieving new stateoftheart results in PU learning.  Strengths  Novel Methodology: The introduction of an uncertaintyaware pseudolabeling paradigm specifically for PU learning is a significant contribution to the field.  Extensive Experiments: The authors provide a comprehensive evaluation of their method on various datasets and datasets demonstrating its effectiveness and robustness.  StateoftheArt Results: The proposed PUUPL method outperforms existing approaches setting new benchmarks in PU learning.  Robustness Analysis: The extensive ablation studies and robustness testing enhance the credibility of the proposed method.  Clear and Detailed Explanations: The paper provides clear explanations of the methodology notation and experimental procedures aiding in the reproducibility of the research.  Weaknesses  Explicit Comparison: The paper could benefit from a more direct comparison with existing uncertaintyaware methods in different learning contexts beyond PU learning.  Visualization: The inclusion of visual aids such as figures or graphs could enhance the readers understanding and provide clearer insights into the experimental results.  Questions and Feedback to the Author 1. Generalization: How well do you think the proposed PUUPL method would generalize to other machine learning tasks beyond PU learning 2. Scalability: Have you considered the scalability of the method to larger datasets and more complex models Are there any computational limitations that might arise with bigger datasets 3. Realworld Applications: Can you provide insights into potential realworld applications where PUUPL could be particularly beneficial apart from the examples mentioned in the introduction 4. Future Work: What are the next steps in your research Are there specific extensions or improvements to the PUUPL method that you plan to explore in future work  Review Summary The paper presents a novel and effective approach PUUPL for PU learning by incorporating epistemic uncertainty in pseudolabeling. With extensive experiments and robustness analysis the authors demonstrate the superiority of PUUPL over existing methods achieving new stateoftheart results. The clear explanations thorough evaluation and robustness testing strengthen the credibility of the research. Hence based on the strengths of the proposed method and the comprehensive evaluation presented I recommend accepting this paper for publication in its current form.", "zyrhwrd9EYs": " Summary of the Paper: The paper introduces a new missingness mechanism Mixed Confounded Missingness (MCM) and proposes a solution termed selective imputation for handling missing data in treatment effects estimation. It highlights the necessity to handle missingness carefully as imputation strategies can introduce bias or remove critical information. The study presents theoretical and empirical evidence supporting the use of selective imputation over traditional imputation methods.  Strong Points of the Paper: 1. Novel Contribution: Introduces a new missingness mechanism and proposes a targeted imputation strategy advancing the existing literature in treatment effect estimation with missing data. 2. Theoretical Foundation: The paper provides comprehensive theoretical constructs outlining the underlying assumptions and relationships among variables in a causal inference framework. 3. Empirical Validation: Conducts experiments to validate the proposed selective imputation method showcasing the effectiveness through comparative analysis with other imputation strategies.  Weak Points of the Paper: 1. Complexity: The paper introduces intricate concepts and assumes familiarity with causal inference terminology potentially limiting accessibility for readers without a strong background in the field. 2. Reproducibility: While the paper asserts reproducibility clarity may be needed in providing detailed steps or code samples in the main body for readers to follow.  Questions for the Authors: 1. Could you elaborate on the scalability of the proposed selective imputation method in handling larger datasets with highdimensional feature spaces 2. How does the proposed method compare to existing complex imputation techniques like Multiple Imputation by Chained Equations (MICE) in scenarios with varying degrees of missingness 3. Are there any limitations or constraints to consider when applying the selective imputation approach across diverse realworld datasets in different domains like healthcare or finance  Feedback to the Authors:  Strengthen the introduction by providing more context and motivation for the research problem.  Consider simplifying complex explanations by offering more intuitive examples or visual aids.  Enhance the reproducibility section with clear stepbystep instructions and code samples for readers to replicate the experiments effectively.  Review Summary: The paper presents a significant contribution to the field of treatment effect estimation with missing data by introducing the concept of mixed confounded missingness (MCM) and advocating for selective imputation as a solution. The theoretical foundation is strong and empirical validation confirms the effectiveness of the proposed method. While the paper is comprehensive and wellstructured improvements in accessibility reproducibility and clarity of explanations could enhance the overall impact and readability for a wider audience. Overall I recommend acceptance with minor revisions to address the highlighted areas.", "tV3N0DWMxCg": " Summary of the Paper: The paper introduces the Natural Posterior Network (NatPN) as a new method for uncertainty estimation in machine learning models. NatPN is designed to estimate uncertainty efficiently for tasks where the target distribution falls under the exponential family. Unlike previous approaches NatPN does not require outofdistribution data during training and leverages Normalizing Flows to fit a single density on a taskdependent latent space for Bayesian updates. The paper provides theoretical insights and practical experiments showcasing NatPNs excellent performance in classification regression and count prediction tasks.  Strong Points: 1. Theoretical Foundation: The paper provides a solid theoretical foundation for NatPN explaining the Bayesian update rule and leveraging exponential family distributions for uncertainty estimation. 2. Practical Application: NatPN is shown to achieve competitive performance in various tasks like classification regression and count prediction through extensive experiments. 3. Efficient Uncertainty Estimation: NatPN offers fast and highquality uncertainty estimation in a single forward pass making it a practical and efficient approach. 4. OOD Detection: NatPN demonstrates impressive outofdistribution detection capabilities without requiring outofdistribution data during training. 5. Experimental Results: The paper presents detailed experimental results showcasing the performance of NatPN compared to existing methods in terms of classification accuracy calibration and OOD detection.  Weak Points: 1. TaskSpecific OOD Detection: The paper mentions that NatPNs OOD detection may be limited to the specific task it was trained on potentially leading to challenges in detecting anomalies that are unrelated to the task. 2. ModelTask Mismatch: The uncertainty estimation quality of NatPN may depend on the convergence of the model encoder architecture and the choice of target distribution which may not always align perfectly with the task requirements. 3. Optimization Scheme Details: While the paper briefly discusses the optimization procedure more detailed information on the choices made during optimization and the rationale behind them could enhance the clarity of the methodology.  Questions for the Authors: 1. Can you elaborate more on the experimental setup used for training and evaluating NatPN particularly regarding hyperparameter tuning and model selection 2. How does NatPN handle scenarios where the input data significantly deviates from the training data distribution Are there any limitations or challenges in such cases 3. Have you considered exploring NatPNs performance on additional realworld datasets to assess its generalization capabilities across different domains and tasks  Feedback to the Authors: 1. Providing more insights into the training process and justification for specific design choices would enhance the reproducibility and understanding of NatPNs implementation. 2. Consider discussing potential applications and scenarios where NatPN might excel or face challenges to provide a more comprehensive view of its capabilities. Overall the paper showcases a novel approach in uncertainty estimation through NatPN backed by strong theoretical foundations and promising experimental results. Further discussions on practical implications and potential extensions of the method could enrich the value of the paper.", "syzTg1vyBtL": " Summary The paper introduces the problem of Congested Bandits where the reward of each action depends on the number of times it was played in the past. It studies this problem in both a multiarmed bandit (MAB) setup and a contextual bandit setup with linear rewards. The paper proposes algorithms for both setups provides regret bounds and evaluates the algorithms through simulations.  Strengths 1. Novel Problem Formulation: The paper addresses an interesting and novel problem of Congested Bandits which has practical implications in scenarios like traffic routing and recommendation systems. 2. Algorithm Design: The proposed algorithms CARMAB and CARCB are welldesigned tackling the complexities of a dynamic reward system dependent on congestion. 3. Theoretical Analysis: The paper provides rigorous theoretical analyses including regret bounds for the proposed algorithms showcasing the robustness and effectiveness of the algorithms. 4. Experimental Evaluation: The simulation study provides empirical evidence supporting the noregret properties of the proposed algorithms.  Weaknesses 1. Complexity: The paper involves highlevel mathematical formulations which might be challenging for readers not wellversed in the field of bandit algorithms. 2. Numerical Results Interpretation: Although the simulations are conducted the interpretation and discussion of the results could be more detailed to provide insights into the algorithms performance efficiency.  Questions and Feedback 1. Clarity of Algorithms: Can you provide more insights into the practical implementation and computational complexity of the proposed algorithms especially in scenarios with large data sets 2. Generalization: How do the proposed algorithms perform in scenarios with more complex structures or when faced with highly nonlinear contexts in the bandit setups 3. Scalability: Have you considered the scalability of the algorithms when dealing with a large number of arms or contexts and how they would perform in realworld applications with dynamic environments  Review Summary The paper introduces a significant problem of Congested Bandits and proposes novel algorithms for both multiarmed and contextual bandit setups with linear rewards. It provides rigorous theoretical analyses and empirical evaluation to support the effectiveness of the algorithms. Overall the paper contributes valuable insights into dynamic reward systems and proposes algorithms with strong theoretical foundations for handling congestionaware decision making. With minor improvements in the clarity of presentation and further exploration of practical implications and scalability the paper will be a valuable contribution to the field of bandit algorithms. I recommend acceptance with minor revisions.", "jNsynsmDkl": " 1) Summary: The paper introduces a novel framework MulCon for multilabel image classification that leverages contrastive learning. The framework learns multiple labellevel representations of an image using attention mechanisms and contrastive loss aiming to improve performance in multilabel classification tasks. The proposed method is compared with stateoftheart methods on benchmark datasets demonstrating superior performance in terms of mAP CF1 and OF1 metrics.  2) Strengths and Weaknesses: Strong Points:  Innovative Approach: The paper proposes a novel framework that adapts contrastive learning to enhance multilabel image classification performance.  Comprehensive Experiments: Extensive experiments on benchmark datasets demonstrate stateoftheart performance and validate the effectiveness of the proposed framework.  Analytical Insights: The paper provides analytical explanations for how contrastive learning can help in multilabel classification tasks enhancing the overall understanding. Weak Points:  Complex Terminology: The paper utilizes technical terms and complex concepts related to contrastive learning and multilabel classification which might be challenging for readers not familiar with the domain.  Limited Discussion on Limitations: While the proposed framework shows promising results a more indepth discussion on the limitations or potential shortcomings of the approach could provide a more balanced viewpoint.  Clarity in Notations: The extensive use of mathematical notations and equations might make it difficult for readers to follow the papers methodological details clearly. Questions to the Author: 1. Can you elaborate on how the proposed labellevel embeddings capture the context of specific labels in the images How do these embeddings facilitate the process of multilabel classification 2. How does the tradeoff parameter \u03b3 influence the balance between the classification loss and contrastive loss Was there experimentation done to optimize this parameter 3. Could you provide more insights into the computational complexity of the proposed framework compared to traditional multilabel classification methods Feedback to the Author: 1. Consider providing more intuitive explanations or visual aids to help readers better understand the complex concepts and methodologies presented in the paper. 2. It would be beneficial to discuss potential realworld applications or scenarios where the proposed framework could be particularly useful adding more practical relevance to the research. 3. Providing a discussion on the scalability and generalizability of the MulCon framework to different datasets or domains could strengthen the papers impact in the field.  3) Review Summary: The paper presents an innovative framework MulCon that leverages contrastive learning for multilabel image classification demonstrating stateoftheart performance on benchmark datasets. The extensive experiments and analytical insights provided support the efficacy of the proposed approach. While the complex nature of the technical details poses a challenge for nonexperts the papers novelty and experimental validation make it a valuable contribution to the field of computer vision. Consideration of the feedback points could enhance the clarity and impact of the research. Overall based on the strengths of the proposed framework and the compelling experimental results I recommend accepting this paper for publication pending minor revisions to improve clarity and discuss limitations.", "qLqeb9AjD2o": "Summary: The paper introduces a novel training method called ConfidenceAware Training for Randomized Smoothing (CATRS) for building more robust smoothed classifiers in the presence of adversarial perturbations. The method leverages the tradeoff between accuracy and robustness by controlling the target robustness samplewise based on the confidence level of the predictions. It introduces two new losses: bottomK and worstcase Gaussian training to address lowconfidence and highconfidence samples during training. Experimental results on MNIST and CIFAR10 datasets demonstrate that CATRS outperforms existing stateoftheart methods in terms of certified robustness. Strengths: 1. Innovative Approach: The paper introduces a novel training method that leverages confidenceawareness for building more robust classifiers addressing the accuracyrobustness tradeoff. 2. Theoretical Foundation: The method is wellgrounded in the existing literature on randomized smoothing and adversarial robustness demonstrating a good understanding of the problem space. 3. Experimental Rigor: The paper provides comprehensive experimental results on established benchmarks showcasing the effectiveness of the proposed method compared to existing approaches. 4. Ablation Study: The ablation study provides insights into the individual contributions of different components in the proposed method enhancing the understanding of the approach. Weaknesses: 1. Complexity: The paper involves advanced concepts in adversarial robustness and neural network training which may pose challenges for readers unfamiliar with the topic. 2. Clarity: Some sections could benefit from improved clarity and simplification especially for readers who are not experts in the field of adversarial machine learning. Questions and Feedback: 1. Clarification: Can you provide more insights into the implications of the proposed ConfidenceAware Training for realworld applications of deep learning systems 2. Comparison: How does the proposed method compare in terms of training efficiency compared to existing methods Is there a significant increase in computational requirements 3. Robustness Measures: Are there any specific scenarios or datasets where the proposed method may not be as effective and if so how could those be addressed in future work Review Summary: The paper presents an innovative training method CATRS for building more robust smoothed classifiers by balancing accuracy and robustness through confidenceawareness. The method shows promising results on MNIST and CIFAR10 datasets outperforming existing stateoftheart approaches. The thorough experimental evaluation ablation study and theoretical foundation contribute to the papers strength. With some improvements in clarity and further exploration on practical implications the paper could make significant contributions to the field of adversarial machine learning.", "sBT5nxwt18Q": "Summary: The paper introduces Critical Classification Regions (CCRs) as a novel approach to enhance posthoc explanationbyexample methods with nearest neighbors for eXplainable Artificial Intelligence (XAI). CCRs aim to highlight important parts in a test image by connecting them to features learned in the training data. The study compares various methods to compute CCRs and evaluates their effectiveness through computational experiments and a controlled user study on ImageNet. Strengths: 1. Novelty: The introduction of CCRs as a way to enhance XAI methods with clear examples of how critical features are learned is a significant contribution to the XAI field. 2. Comprehensive Approach: The paper covers various methods for computing CCRs performs extensive experiments across multiple domains and conducts a wellcontrolled user study demonstrating thorough research. 3. Detailed Experiments: The computational experiments are welldetailed providing clear descriptions of methodologies results and discussions enhancing reproducibility. 4. User Study Design: The user study is welldesigned considering materialsets counterbalancing and carefully controlled conditions to ensure meaningful insights. 5. Ethical Considerations: The paper acknowledges and discusses the ethical implications of XAI solutions highlighting the need for data anonymization and considerations regarding potential societal impacts. Weaknesses: 1. Complexity: The technical details in the paper are highly complex and may be challenging for readers without a deep understanding of XAI concepts and methodologies. 2. Clarity: While thorough the paper could benefit from clearer organization and formatting especially separating sections or key points for easier readability. 3. Assumptions: Some assumptions in the computational experiments and user study design could be further clarified to improve the robustness and generalizability of the findings. 4. Validation: While the computational experiments seem thorough providing more validation on the effectiveness and generalizability of CCRs across various datasets and models could strengthen the papers claims. Questions and Feedback: 1. Validation: Were the computational experiments tested on different datasets and model architectures to assess the generalizability of the proposed CCR method 2. User Study Impact: How do the findings of the user study influence the adoption and implementation of CCRs in practical XAI applications 3. Algorithm Efficiency: Considering the computational cost comparison between latentbased CCRs and pixelbased CCRs are there plans to further optimize these methods for efficiency in realworld applications Review Summary: The paper presents a novel approach CCRs to enhance XAI methods through clear identification and linking of critical features in test images to training data. While the research is comprehensive and innovative addressing key aspects of XAI interpretability some clarity improvements validation on diverse datasets and optimization for efficiency could further strengthen the impact of the proposed method. The ethical considerations highlighted in the paper are commendable and reflect a responsible approach to XAI research. Overall with additional validation and clarity enhancements the papers contribution to advancing XAI interpretability is significant warranting consideration for publication.", "t3E10H8UNz": " Paper Review: 1. Summary: The paper introduces a novel hierarchical meta imitation learning framework called Dual Meta Imitation Learning (DMIL). The method combines metalearning with hierarchical structure modeling to enable robots to quickly adapt to new tasks with fewshot demonstrations. The framework metalearns both highlevel network and subskills iteratively proving convergence and achieving stateoftheart fewshot imitation learning performance. 2. Strengths:  Novel Methodology: The integration of metalearning with a hierarchical structure in DMIL is a novel approach.  Theoretical Proof: The paper provides theoretical proof of the convergence of DMIL enhancing its credibility.  StateoftheArt Results: DMIL achieves stateoftheart fewshot imitation learning performance on Metaworld benchmark environments and shows comparable results in the Kitchen environment.  Comprehensive Experiments: The paper conducts comprehensive experiments and ablation studies to analyze the effects of various components in the model.  Detailed Ablation Studies: The detailed ablation studies provide insights into the importance of components such as subskill number finetuning steps and continuity regularization. 3. Weaknesses:  Reproducibility: While some basic code is provided the complete training code is mentioned to be provided later which could impact reproducibility.  Experimental Complexity: The inclusion of a large number of methods ablations and experiments might make it hard for readers to keep track of all the comparisons and results.  Clarity in Presentation: The complex mathematical derivations could be challenging for readers not familiar with the intricacies of the field.  Questions and Feedback to the Author: 1. Could you provide a more detailed explanation or visualization of the convergence proof for DMIL as it is a critical aspect of the paper 2. How robust is the DMIL framework to noise or variability in the demonstration dataset and how does it handle outliers or anomalies in the data 3. In the ablation studies did you observe any specific trends or patterns based on the subskill number (K) and its impact on the overall performance 4. Providing additional visual aids or diagrams to explain some of the complex theoretical concepts could greatly enhance the understanding for readers who may not be familiar with the technical intricacies of the paper.  Review Summary: The paper presents a novel method DMIL which successfully integrates metalearning with a hierarchical structure to improve fewshot imitation learning for robots. The extensive experiments ablation studies and theoretical proofs strengthen the credibility of the proposed framework. However efforts could be made to enhance the clarity of presentation and reproducibility for broader readability and accessibility within the research community. Overall the paper contributes significantly to the field of hierarchical imitation learning and provides a solid foundation for future research direction.", "yhjfOvBvvmz": "Summary: The paper introduces a Weaklysupervised learning approach for learning Disentangled and Interpretable Skills (WEDIS) from continuous latent representations of trajectories. They propose a trajectory Variational Autoencoder (VAE) model with weak labels to enforce disentanglement of latent representations into factors of interest. A skillbased policy network is trained to generate trajectories similar to the learned decoder simplifying training and enabling sampleefficient planning. Experiments demonstrate the effectiveness of WEDIS in solving hierarchical RL problems in challenging navigation tasks. Strengths: 1. Innovative Approach: The proposed WEDIS method offers a novel approach for learning disentangled skills from trajectories filling a gap in current hierarchical RL techniques. 2. Clear Problem Statement: The paper clearly addresses the challenges in hierarchical RL and provides a welldefined solution to learn and utilize reusable skills effectively. 3. Comprehensive Experiments: The experiments conducted on challenging navigation tasks demonstrate the efficacy of WEDIS comparing it to existing methods and showcasing superior performance. 4. Theoretical Foundation: The discussion on weak supervision disentangled representations and modelbased planning is thorough providing a robust theoretical foundation for the proposed approach. Weaknesses: 1. Complexity: The paper introduces several technical concepts and methods making it challenging to follow for readers not wellversed in RL and VAEs. Providing more intuitive explanations or visual aids could enhance clarity. 2. Evaluation Metrics: While the evaluation results are presented graphically the paper could benefit from providing a more detailed analysis of the metrics used and how they relate to the performance of the WEDIS model. 3. Generalization: The paper could elaborate more on the potential limitations or challenges in generalizing the findings of the experiments to broader RL scenarios or domains. 4. Reproducibility: Detailed information on code availability datasets used and specific hyperparameters would enhance the reproducibility of the results for other researchers. Questions and Feedback to the Authors: 1. How does the WEDIS method compare to existing approaches in terms of computational complexity and training time 2. Can the authors provide insights into the choice of the specific weak labels used in the WETVAE model and their impact on the disentanglement of factors 3. Considering realworld applications how scalable and adaptable is the proposed WEDIS method to different environments or tasks beyond the navigation scenarios tested 4. Have you considered incorporating realworld robot experiments to validate the effectiveness of WEDIS in practical settings Review Summary: The paper presents an innovative WEDIS approach for learning disentangled skills in hierarchical RL showcasing promising results through thorough experiments. The theoretical foundations are solid and the comparisons with existing methods add value to the research. However enhancing clarity reproducibility and addressing potential limitations could further strengthen the paper. Overall I recommend acceptance pending minor revisions related to clarity and reproducibility.", "y1faDxZ_-0a": " PeerReview of \"SelfSupervised Federated Learning Framework: Addressing Label Deficiency and Data Heterogeneity Challenges\"  1) Summary: The paper introduces a novel SelfSupervised Federated Learning (SSFL) framework designed to address challenges of data heterogeneity and label deficiency in Federated Learning (FL). The SSFL framework incorporates selfsupervised learning methods particularly SimSiam networks and proposes algorithms like perFedAvg and PerSSFL for personalized federated learning. Experiments are conducted on nonI.I.D. datasets CIFAR10 and GLD23K to demonstrate the effectiveness of the proposed framework.  2) Strengths:  Innovative Framework: The introduction of the SSFL framework is a novel approach to addressing label deficiency and data heterogeneity in FL.  Comprehensive Experiments: Thorough experiments are conducted on synthetic and realworld datasets to validate the proposed framework.  Algorithm Design: The algorithms developed under the SSFL framework such as perFedAvg and PerSSFL show promise in balancing personalization and consensus.  Theoretical Interpretation: Providing theoretical analysis and interpretability of the methods used is beneficial for understanding the algorithmic underpinnings.  3) Weaknesses:  Theoretical Analysis: While the paper provides an interpretation of the algorithm a more indepth theoretical analysis demonstrating robustness and convergence properties could enhance the paper.  Comparison Metrics: The paper conducts comprehensive experiments but it would be useful to provide a detailed comparison with existing methods in terms of convergence speed scalability and robustness.  Discussion on Resource Efficiency: Discussion on the resource efficiency and computational cost of the proposed framework could provide insights into its practical applicability.  Questions and Feedback to the Authors:  Scalability: How does the proposed SSFL framework scale with an increasing number of edge devices in a practical scenario  Realworld Applications: Have you considered realworld applications where label deficiency and data heterogeneity are common problems and evaluated the frameworks performance in such scenarios  Comparison with Baselines: It would be helpful to provide more detailed comparisons with existing FL methods in terms of convergence speed robustness and scalability.  Review Summary: The paper presents a novel SSFL framework to address label deficiency and data heterogeneity challenges in FL. The innovative approach incorporating selfsupervised learning and personalized federated learning algorithms shows promise as evidenced by comprehensive experiments on nonI.I.D. datasets. However the paper could benefit from further theoretical analysis comparison with existing methods and discussion on resource efficiency. Overall the paper provides valuable insights into advancing the field of selfsupervised federated learning and offers a strong basis for future research.  Recommendation: Strong Accept with Minor Revisions: The paper contributes significant value by introducing the SSFL framework and demonstrating its effectiveness through experiments. With minor revisions to address theoretical aspects and comparative analysis the paper can make a strong impact on the field of federated learning.", "kavTY__jxp": " Summary of the Paper: The paper presents the Distilled Graph Attention Policy Network (DGAPN) a reinforcement learning model designed for generating novel graphstructured chemical representations with optimized userdefined objectives. The framework is tested on the task of generating molecules that bind noncovalently to functional sites of SARSCoV2 proteins. The model incorporates a spatial Graph Attention (sGAT) mechanism and an attentional policy network for navigating a dynamic fragmentbased chemical environment. Stateoftheart policy gradient techniques are utilized for training the network with exploration driven by the stochasticity of the action space design and innovation rewards from Random Network Distillation. Experimental results show that the framework outperforms stateoftheart algorithms in generating molecules with high affinity to the target while simplifying paths to chemical synthesis.  Strong Points:  Innovative Framework: The introduction of DGAPN combines spatial graph attention and policy learning for efficient molecule generation addressing challenges in chemical representation and exploration in the discrete and massive chemical space.  Performance: The model achieves outstanding results compared to stateoftheart algorithms demonstrating superiority in generating molecules with high affinity to the target while reducing synthesis complexity.  Relevance: The application of the framework in antiviral drug discovery particularly in designing inhibitors targeting SARSCoV2 proteins showcases the practical significance of the research.  Weak Points:  Complexity: The detailed technical descriptions could be complex for readers not familiar with reinforcement learning and graph neural networks potentially limiting accessibility and understanding.  Evaluation Metrics: The evaluation primarily focuses on docking scores with limited discussion on other important metrics such as diversity uniqueness and druglikeness which are crucial for comprehensive assessment in molecular design.  Questions and Feedback for the Author: 1. Clarity for NonExperts: Could you consider providing a more accessible explanation for readers not wellversed in reinforcement learning and graph neural networks to ensure broader understanding of your work 2. Additional Evaluation: How do the molecules generated by DGAPN perform in terms of diversity uniqueness and druglikeness compared to stateoftheart models Including a discussion on these aspects will provide a more comprehensive assessment.  Review Summary: The paper presents a novel DGAPN model for generating molecules optimized for specific objectives focusing on targeting SARSCoV2 proteins for antiviral drug discovery. The framework showcases significant advancements in chemical representation and exploration resulting in superior performance compared to existing models. While the technical depth of the paper is commendable enhancing clarity and including a broader range of evaluation metrics would further strengthen the research. Overall given the innovation and performance demonstrated I recommend this paper for publication pending minor revisions to improve accessibility and comprehensiveness of evaluation.", "g5odb-gVVZY": "Summary: The paper introduces multilevel physics informed neural networks (MPINNs) inspired by classical multigrid methods for solving linear systems from PDE discretization. MPINNs use a correction scheme with fine and coarse terms optimized alternatively to accelerate neural network training similar to multigrid methods. They exhibit better performance than classical PINNs especially being less sensitive to learning rate choices for elliptic and nonlinear equations with reduced computational operations. The study is supported with numerical tests on various equations. Strong Points: 1. Innovative Concept: Introducing MPINNs derived from classical multigrid methods is innovative and has shown improved performance compared to standard PINNs. 2. Theoretical Background: The paper gives a detailed theoretical background highlighting the motivation behind MPINNs compared to classical multigrid methods and existing neural network architectures. 3. Experimental Validation: Conducting numerical tests on 1D and 2D elliptical and nonlinear equations along with Burgers\u2019 equation adds substantial empirical evidence to support the findings. 4. Comparative Analysis: Comparing MPINNs with standard PINNs in terms of sensitivity to learning rates decreased operations for similar approximation errors and use in different types of equations enriches the analysis. Weak Points: 1. Practical Implementation Details: The paper lacks detailed insights into the practical implementation aspects of MPINNs such as computational efficiency scalability and potential challenges in realworld applications. 2. Limited Convergence Analysis: While the paper mentions the possibility of recursive extension for MPINNs a more detailed discussion on convergence properties and associated theoretical guarantees for the proposed method would enhance the contribution. 3. Code Reproducibility: While the authors state that the code is included for reproducibility more emphasis on transparency and accessibility of the codebase for interested readers or researchers would further strengthen the credibility of the study. Questions to the Authors: 1. Scalability: How does the scalability of MPINNs compare to classical PINNs especially when dealing with largerscale problems or highdimensional data 2. Computational Efficiency: Can you provide insights into the computational efficiency gains achieved by using MPINNs compared to standard PINNs in practical scenarios Feedback:  Clarify the practical applicability of MPINNs by addressing implementation aspects optimization strategies and potential limitations in realworld settings.  Enhance the theoretical discussion by providing more insights into convergence properties recursive extension possibilities and further refinement of the proposed multilevel scheme. Review Summary: The paper introduces a novel approach MPINNs inspired by classical multigrid methods for neural network training which shows promise in accelerating training and improving performance compared to standard PINNs. While the paper provides strong empirical evidence through numerical tests addressing practical implementation details scalability and enhancing theoretical analysis on convergence properties would further solidify the contribution. Overall I recommend acceptance with minor revisions to address the outlined points.", "fPhKeld3Okz": " Summary of the Paper The paper introduces a new PlugandPlay (PnP) algorithm using a denoiser represented as a neural network gradient descent step targeting stationary points of an explicit global functional. By leveraging convergence results for proximal gradient descent algorithms in the nonconvex setting the proposed PnP algorithm is proven to be convergent for various illposed inverse problems such as deblurring superresolution and inpainting. The denoiser is trained to satisfy specific constraints and experiments demonstrate stateoftheart performance in image restoration tasks.  Strengths 1. Innovative Approach: The paper introduces a novel PnP algorithm with theoretical convergence guarantees and achieves stateoftheart performance in image restoration with a deep denoiser. 2. Theoretical Framework: The paper provides a rigorous theoretical analysis of the proposed algorithms convergence addressing nonconvex and nonexpansive denoisers which is crucial for realworld applications. 3. Experimental Validation: The experiments conducted cover various image restoration tasks including deblurring and superresolution showing empirical confirmation of the proposed algorithms convergence and performance. 4. Comprehensive Experiments: Detailed comparative experiments against existing methods along with quantitative and qualitative results demonstrate the superior performance of the proposed algorithm. 5. Clear Presentation: The paper is wellstructured providing a clear explanation of the problem methodology experimental setup and results.  Weaknesses 1. Complexity of Denoiser: The constraints on the denoiser architecture while essential for convergence guarantees might limit the flexibility and applicability of the algorithm in certain scenarios. 2. Sparse Validation: While the experiments cover multiple tasks a more extensive validation on diverse datasets and scenarios could further strengthen the robustness and generalizability of the proposed algorithm. 3. Limited Discussion on Scalability: The paper could include a discussion on the computational scalability of the proposed algorithm especially considering the training complexity of the denoiser on large datasets.  Questions to the Authors 1. Have you considered the potential impact of regularization in the denoiser design on the overall convergence and performance of the PnP algorithm 2. How do you envision the proposed algorithm being applied to realworld scenarios with largescale imaging data and complex degradation models 3. Could you elaborate on potential future directions for enhancing the scalability and efficiency of the proposed algorithm for practical deployment  Feedback to the Authors The paper presents a significant advancement in the field of image restoration with a novel PnP algorithm backed by strong theoretical convergence guarantees and impressive experimental validation. The innovative approach coupled with rigorous analysis and comprehensive results makes it a valuable contribution to the research community. Addressing the weaknesses identified and providing further insights into scalability and realworld applications would enhance the impact and applicability of the proposed algorithm. Overall the paper is wellstructured wellwritten and makes a compelling case for the effectiveness of the proposed method.  Recommendation Based on the thorough analysis of the paper including its strengths in innovation theoretical framework and experimental validation I recommend accepting the paper with minor revisions to address the identified weaknesses and clarifications sought in the feedback section. The significant contribution solid methodology and promising results make it a valuable addition to the field of image restoration research.", "zrdUVVAvcP2": " 1) Summary: The paper introduces a method termed GrASP (Gradientbased Affordance Selection for Planning) to address the challenge of planning with continuous action spaces in largescale reinforcement learning (RL) problems. GrASP aims to discover affordances that select a small number of actionsoptions to consider during the treeexpansion process in planning. Using a gradientbased approach GrASP updates parameters representing affordances through the planning procedure. The paper demonstrates the feasibility of learning to select both primitiveaction and option affordances showcasing that planning with learned valueequivalent models using GrASP can outperform modelfree RL in benchmark control problems. The study explores hierarchical tasks requiring policies over continuous options and different domains from the DeepMind Control Suite illustrating the effectiveness of GrASP agents through various experiments and comparisons.  2) Strengths and Weaknesses: Strong Points:  Novel Concept: Introduces the innovative concept of discovering affordances for planning in continuous action spaces.  Practical Applicability: Demonstrates empirical evidence of the effectiveness of GrASP in improving sample efficiency compared to modelfree RL.  Comprehensive Evaluation: Conducts experiments on hierarchical tasks and diverse domains providing a thorough analysis of the methods performance.  Detailed Methodology: Provides detailed explanations of the components involved in GrASP including the affordance module valueequivalent model and treeexpansion procedures. Weak Points:  Limited Comparisons: While the study compares GrASP with modelfree RL methods a direct comparison with stateoftheart methods like MuZero could have provided more insights.  Theoretical Limitations: The paper acknowledges that GrASP does not claim to be stateoftheart for modelbased RL in continuous domains.  Complexity vs. Simplicity: The study presents the choice between completetree and UCTbased planning methods but the comparison lacks a clear preference. Question and Feedback: 1. Clarification on Comparison: Could you elaborate on the reasons for choosing TD3 as the primary baseline for comparison and why MuZero was not directly compared with GrASP due to the lack of released code 2. Extension of Work: How do you plan to further validate GrASP in realworld applications or more complex scenarios to ensure its robustness and scalability  3) Review Summary: The paper introduces a novel method GrASP to address the challenge of planning in continuous action spaces in RL. The study provides a comprehensive evaluation through experiments on hierarchy tasks and multiple RL domains demonstrating the effectiveness of GrASP agents in improving sample efficiency. Despite certain limitations like limited comparisons and theoretical constraints the papers innovative concept detailed methodology and practical applicability make it a valuable contribution to the field of modelbased RL. Therefore considering the positive aspects and the potential impact of GrASP I recommend this paper for publication after addressing the mentioned points for improvement.", "xspalMXAB0M": "1) Summary: The paper introduces a novel boosting algorithm for reinforcement learning in Markov decision processes (MDPs) with large state spaces. The algorithm leverages weak learners capable of approximately optimizing linear functions over policy space. It iteratively aggregates weak learners outputs creating a policy that approaches optimality. The methodology overcomes challenges such as nonconvexity of value functions and unknown transition functions providing provable guarantees. 2) Strengths and Weaknesses:  Strengths:  Innovative Approach: Introduces a novel boosting algorithm for reinforcement learning offering a fresh perspective on addressing computational challenges in large state MDPs.  Theoretical Analysis: The paper provides detailed theoretical analysis proving sample complexity and running time bounds independent of the number of states.  Relevant References: Wellincorporated references to prior studies on reinforcement learning boosting and online optimization enhancing the papers credibility.  Weaknesses:  Complexity: The papers technical nature might limit accessibility to a broader audience. Some concepts and methodologies may be challenging for readers without a strong background in reinforcement learning and optimization.  Practical Implementation: Limited discussion on the practical implementation of the proposed boosting algorithm in realworld scenarios could be a drawback.  Explanations: Some parts of the paper especially in the Preliminaries and Algorithms sections may benefit from clearer explanations for readers unfamiliar with the specific technical terms. Question and Feedback:  Question: Could you provide more insights into how the proposed boosting algorithm compares to existing reinforcement learning approaches for large state spaces in terms of computational efficiency and effectiveness  Feedback: Enhancing the clarity and accessibility of complex technical terms such as the different modes of accessing the MDP and weak learning models could improve reader understanding and engagement. 3) Review Summary: The paper presents a comprehensive study on developing a boosting algorithm for reinforcement learning in large state MDPs utilizing weak learners to iteratively aggregate policies towards optimality. The theoretical analysis and algorithm design showcase significant contributions to the reinforcement learning domain. However the papers technical complexity and potential barriers to understanding for nonexperts need to be addressed. Overall the innovative approach and demonstrated theoretical guarantees position the paper favorably for the academic audience interested in reinforcement learning and optimization research. Therefore based on the contributions and potential impact of the paper in advancing reinforcement learning methodologies I recommend acceptance with minor revisions for improved clarity and accessibility.", "hfU7Ka5cfrC": " Summary of the Paper: The paper proposes an approximate hypergradientbased hyperparameter optimiser that can handle any continuous hyperparameter in a differentiable weight update and requires only one training episode without the need for restarts. The method is designed to optimise hyperparameters like learning rates and momenta efficiently and competitively. The authors extend existing methods to support arbitrary continuous inputs and demonstrate the algorithms performance on various datasets and neural network architectures.  Strong Points: 1. Innovative Approach: The paper introduces a novel method for hyperparameter optimisation that is both efficient and effective extending existing techniques to handle a broader range of hyperparameters. 2. Comprehensive Experiments: The authors provide a detailed experimentation section across different datasets and neural network models demonstrating the effectiveness of the proposed method. 3. Theoretical Justification: The paper provides theoretical justifications for the proposed approximations and convergence to the true hypergradient enhancing the credibility of the method. 4. Comparative Analysis: The paper compares the proposed method with existing algorithms showcasing its competitive performance and efficiency.  Weak Points: 1. Complexity: The paper is highly technical and complex which may limit its accessibility to a broader audience. Simplifying the language and enhancing the clarity of explanations would improve the readability. 2. Lack of Practical Examples: While the experiments are comprehensive more concrete examples or use cases illustrating the application of the proposed method could enhance the papers practical relevance. 3. Hyperparameter Initialization Sensitivity: The paper mentions challenges with extreme hyperparameter initializations leading to NaN results indicating potential limitations in handling outlier cases.  Question to the Authors: 1. How does the proposed method handle scenarios of nondifferentiability in the weight update functions for certain hyperparameters  Feedback to the Authors: 1. Consider adding practical examples or case studies to demonstrate the application of the proposed method in realworld scenarios enhancing the papers practical relevance.  Review Summary: The paper presents an innovative approach for hyperparameter optimisation using approximate hypergradientbased methods. The method shows promising results in competitive experiments across various datasets and neural network architectures. While the paper is technically sound and rigorously detailed improvements in accessibility practical examples and handling extreme hyperparameter initializations could further enhance the paper. Overall the paper contributes significantly to the field of automated hyperparameter optimisation and warrants further exploration and practical validation.", "gzeruP-0J29": " 1) Summary of the Paper: The paper discusses the challenges associated with adversarial training (AT) in deep neural networks and introduces a novel approach called FASTBAT which leverages bilevel optimization to address the limitations of existing fast AT methods. The authors highlight the shortcomings of current fast AT techniques such as lack of stability robustness catastrophic overfitting and poor accuracyrobustness tradeoff. By formulating adversarial training through the prism of bilevel optimization the proposed FASTBAT framework aims to achieve improved stability mitigated catastrophic overfitting and enhanced accuracyrobustness tradeoff.  2) Strengths and Weaknesses Questions and Feedback:  Strengths: 1. Innovative Approach: Leveraging bilevel optimization for designing a fast adversarial training framework is a novel and theoretically grounded approach. 2. Theoretical Foundation: The paper provides a solid theoretical foundation for their proposed FASTBAT framework demonstrating a clear understanding of the underlying concepts. 3. Empirical Results: Extensive experiments are conducted on different datasets and model architectures to showcase the superiority of FASTBAT over existing baselines demonstrating improvements in robustness and standard accuracy. 4. Mitigation of Overfitting: FASTBAT successfully addresses the issue of robustness catastrophic overfitting without resorting to additional robustness stability regularization which is a significant achievement.  Weaknesses: 1. Complexity: The paper delves into complex technical details possibly making it challenging for readers unfamiliar with bilevel optimization to grasp the content easily. 2. Detailed Empirical Analysis: While the paper presents significant improvements in robustness and stability a more detailed analysis of the empirical results including comparisons with more benchmarks and datasets could strengthen the study further. 3. Practical Implementation: The feasibility and scalability of implementing FASTBAT in realworld scenarios with largescale datasets or complex neural network architectures could be a potential concern not adequately addressed.  Questions  Feedback: 1. Scalability: How does the proposed FASTBAT framework scale with larger datasets or more complex neural network architectures compared to existing fast AT methods 2. Robustness: Can you further elaborate on the tradeoff between robustness and standard accuracy achieved by FASTBAT in different attack scenarios and perturbation strengths 3. Generalization: Have you tested the generalization performance of FASTBAT on unseen datasets or novel attack strategies to assess its robustness in realworld scenarios  3) Review Summary: Overall the paper presents an innovative and theoreticallygrounded approach FASTBAT for improving the stability robustness and accuracyrobustness tradeoff in adversarial training of deep neural networks. The rigorous theoretical foundation along with the empirical results demonstrating superior performance over existing baselines showcases the efficacy of FASTBAT. However further practical evaluation and detailed analysis are needed to assess the scalability and generalizability of the proposed framework. With minor refinements in presentation clarity and additional empirical validation the study has the potential to make a significant contribution to the field of adversarial defense in deep learning. Based on the strengths presented and the potential for further contributions with slight enhancements I recommend this paper for acceptance pending further experiments and refinements addressing the weaknesses highlighted.", "mHu2vIds_-b": "1) Summary: The paper introduces a novel method of using ensembles as base models for Randomized Smoothing (RS) to enhance robustness certificates for deep learning models. The key insight is that ensembles lead to reduced variance over perturbations thereby increasing certified radii. The paper also presents optimizations that significantly decrease the computational complexity of RS. Extensive theoretical analysis and empirical evaluations on CIFAR10 and ImageNet datasets showcase stateoftheart results. 2) Strengths and Weaknesses: Strong Points:  The theoretical foundation for using ensembles in RS is well explained and justified.  Empirical results demonstrate stateoftheart improvement in certified accuracy and radii on CIFAR10 and ImageNet datasets.  Introduces adaptive sampling and KConsensus aggregation techniques to reduce computational overhead effectively.  Clear and thorough explanations are provided throughout the paper making complex concepts accessible. Weak Points:  Although the theoretical insights and empirical results are compelling the paper could benefit from additional analysis of the limitations or potential challenges of the proposed methods.  The evaluation is focused mainly on CIFAR10 and ImageNet datasets extending experiments to a broader range of datasets could enhance the robustness of the conclusions.  While the solutions to reduce computational overhead are explained well a more detailed discussion on practical implementation challenges or tradeoffs could provide deeper insights. Questions and Feedback: 1. Have you considered testing the proposed methods on more diverse datasets beyond CIFAR10 and ImageNet to assess the generalizability of your approach 2. How do you ensure the reliability and accuracy of the adaptive sampling mechanism in diverse scenarios Is there a potential risk of overfitting or underestimating certification radii 3. Can you discuss the potential implications of using ensembles in RS for realworld applications especially in safetycritical domains or scenarios with limited computational resources 3) Review Summary: The paper presents a wellgrounded and innovative approach using ensembles as base models for Randomized Smoothing to enhance robustness certificates for deep learning models. The theoretical foundation empirical results and proposed optimizations are impressive and showcase significant improvements in certified accuracy and radii. The introduction of adaptive sampling and KConsensus aggregation techniques addresses the computational overhead efficiently. However further exploration of limitations broader dataset evaluations and discussions on practical implications would strengthen the paper. Overall this work provides valuable insights and advancements in enhancing the robustness of deep learning models. Thus I recommend acceptance with minor revisions to address the points raised above.", "hgKtwSb4S2": " Summary of the Paper The paper introduces a generalized approach to the randomized singular value decomposition (SVD) extending it to use multivariate Gaussian vectors and incorporating prior knowledge into the algorithm. The generalized approach is applied to Hilbert\u2013Schmidt (HS) operators using functions drawn from a Gaussian process. A new covariance kernel based on weighted Jacobi polynomials is proposed to control the smoothness of randomly generated functions. The paper provides new theoretical bounds for the algorithm numerical examples demonstrating its applicability and applications in learning integral kernels.  Strong Points 1. Novelty and Contribution: The paper makes significant contributions by generalizing the randomized SVD to incorporate prior knowledge and by introducing a new covariance kernel for GP enabling control over the smoothness of sampled functions. 2. Theoretical Advancements: The theoretical bounds provided for the algorithm using nonstandard covariance matrices and HS operators add depth to the existing body of knowledge on randomized SVD. 3. Numerical Examples: The numerical experiments effectively demonstrate the benefits of using nonstandard covariance matrices and the proposed covariance kernel showcasing improved approximation accuracy. 4. Algorithm Implementation: Descriptions of the randomized SVD algorithm for learning HS operators are clear and wellillustrated through an algorithmic representation.  Weak Points 1. Complexity: The paper delves into advanced mathematical concepts that may be challenging for some readers not wellversed in the field. Providing more intuitive explanations or illustrations could enhance accessibility. 2. Clarity of Presentation: While the information is detailed some sections may benefit from clearer organization or division for improved readability and comprehension.  Questions and Feedback 1. Clarity on Algorithm Implementation: It would be beneficial to provide additional details or code snippets of the implemented algorithm for better understanding by readers especially those interested in practical applications. 2. Practical Utility: How does the proposed algorithm and covariance kernel compare in practical scenarios with large datasets or complex matrix structures beyond the numerical examples provided Further practical applications or benchmarks could strengthen the papers impact.  Review Summary The paper presents a comprehensive study on generalizing the randomized SVD to incorporate prior knowledge and introduces a new covariance kernel for GP. Strong theoretical advancements practical implementations and numerical experiments support the effectiveness of the proposed approach. However attention to clarity and practical utility aspects could further enhance the papers impact. In conclusion considering the significant contributions and advancements presented I recommend this paper for publication pending minor revisions to improve clarity and practical relevance.", "metRpM4Zrcb": " PeerReview  1. Summary: The paper introduces a novel approach to tackle catastrophic forgetting in deep neural networks through continual learning with filter atom swapping. By decomposing convolutional filters into filter atoms and sharing subspace coefficients across tasks the proposed method effectively maintains past knowledge while learning for new tasks. The approach is empirically and theoretically validated to outperform stateoftheart methods on multiple benchmark datasets.  2. Strengths:  Innovative Approach: The method of decomposing convolutional filters and maintaining filter atoms for each task is novel and shows promise in reducing catastrophic forgetting.  Empirical Validation: The approach is empirically validated on various benchmarks showing superior results against existing methods highlighting its effectiveness.  Theoretical Analysis: The theoretical excess risk bound analysis provides insight into the performance of the model and supports the empirical findings.  Scalability: The method demonstrates scalability in terms of memory usage making it suitable for largescale continual learning tasks.  Efficient Model Ensemble: The method enables efficient intertask and intratask model ensemble further boosting performance in different continual learning settings.  Weaknesses:  Complexity: The paper introduces several concepts and methodologies that might be challenging for readers to grasp easily. Simplifying the explanations could improve readability.  Evaluation Metrics: While the paper outlines various metrics for evaluation further discussion on the choice of metrics and possibly comparison with other evaluation methods could enhance the robustness of the method evaluation.  Reproducibility: More details on the implementation hyperparameters and code availability would enhance the reproducibility of the approach for the research community.  Questions and Feedback to the Author: 1. Can you provide more insights into the choice of hyperparameters such as the number of filter atoms and ensemble members How sensitive is the models performance to these hyperparameters 2. Could you provide more detailed information on the training process including learning rate schedules optimizer choices and any regularization techniques used 3. It would be beneficial to include a discussion on the computational complexity of the proposed method compared to existing approaches. How does the proposed method fare in terms of inference time and computational resources 4. Is there any plan to release the code implementation of the proposed method to facilitate replication and further experimentation by the research community  3. Review Summary: The paper presents an innovative approach to continual learning through filter atom swapping showcasing superior performance on benchmark datasets. The empirical validation and theoretical analysis support the effectiveness of the proposed method. While the approach demonstrates scalability and efficient model ensemble further simplification of explanations additional details on implementation and consideration of evaluation metrics can enhance the papers impact and reproducibility. Overall the method shows promise in addressing catastrophic forgetting in deep neural networks and contributes significantly to the field of continual learning. Based on the strengths and performance superiority demonstrated by the proposed method I recommend acceptance after minor revisions to improve clarity and reproducibility.", "wMXYbJB-gX": " Summary: The paper focuses on label smoothing regularization (LSR) in deep learning and proposes a novel algorithm TwoStage Label Smoothing (TSLA) to improve convergence and generalization. The theoretical analysis provides insights into the influence of label smoothing on optimization. TSLA accelerates convergence by incorporating both LSR and onehot labels in a twostage training paradigm. The effectiveness of TSLA is verified through extensive experiments on benchmark datasets.  Strong Points: 1. Theoretical Analysis: The paper provides a comprehensive theoretical analysis of how label smoothing affects optimization offering insights into its impact on convergence behaviors. 2. Novel Algorithm: Introducing and analyzing the TwoStage Label Smoothing Algorithm (TSLA) is a strong contribution addressing the limitations of LSR in late optimization stages. 3. Experimental Validation: Extensive experiments on benchmark datasets validate the effectiveness of TSLA both theoretically and empirically demonstrating improved convergence and generalization.  Weak Points: 1. Questionable Dropoff Strategy: The study mentions that the performance of TSLA may not be monotonic over the dropping epoch raising concerns about the reliability of the dropoff strategy for LSR. 2. Limited Comparative Analysis: While the paper compares TSLA with LSR and a baseline more indepth comparative analysis with a wider range of stateoftheart methods could provide additional insights. 3. Hyperparameter Sensitivity: The sensitivity of hyperparameters such as the smoothing parameter and dropping epoch could affect the algorithms performance stability and general applicability.  Questions and Feedback: 1. Further Algorithm Comparison: Have you considered comparing TSLA with other advanced optimization algorithms besides SGD to assess its performance across a wider spectrum of methods 2. Generalization Performance: How does the generalization performance of TSLA compare with other regularization techniques in scenarios involving complex datasets with high dimensionality and class imbalance 3. Sensitivity Analysis: Could you provide more insights into the sensitivity of TSLAs performance to hyperparameters and how robust the algorithm is under varying conditions  Review Summary: The paper presents a novel algorithm TSLA that integrates label smoothing regularization with a twostage training paradigm to enhance convergence and generalization in deep neural networks. The theoretical analysis provides a strong foundation supported by empirical results demonstrating the effectiveness of TSLA. While the dropoff strategy for LSR in TSLA may raise concerns on its reliability and hyperparameter sensitivity the overall contributions make this paper a valuable addition to the literature on label smoothing regularization in deep learning. Therefore I recommend this paper for publication given its innovative approach and significant contributions to the optimization of deep neural networks.", "in1ynkrXyMH": "Peer Review 1) Summary of the Paper: The paper proposes a novel twostage architecture for neural networks called introspective learning. The first stage involves the quick sensing of patterns in data for making initial decisions while the second stage involves reflecting on these decisions based on all possible choices to make a final decision. The authors utilize gradients of trained neural networks as a measure of reflection. The paper demonstrates the benefits of introspective networks in various applications including recognition active learning outofdistribution detection and image quality assessment. 2) Strengths and Weaknesses: Strengths:  Innovative Concept: The introduction of introspective learning as a twostage decisionmaking process is innovative and provides a new perspective on neural network architectures.  Visual Explanations: The use of posthoc visual explanations enhances the understanding of introspective features and their impact on decisionmaking.  Comprehensive Evaluation: The paper provides a detailed evaluation of introspective networks across multiple applications showcasing improved robustness and calibration.  Realworld Applications: The application of introspective learning in tasks like active learning and outofdistribution detection demonstrates practical relevance. Weaknesses:  Complexity: The proposed introspective feature extraction process has a space complexity proportional to the number of classes which may limit scalability for large datasets.  Experimental Setup: Some sections like IQA and active learning lack detailed analysis and may benefit from more indepth discussions or additional experiments.  Clarity: The paper at times uses technical language and complex concepts that might be challenging for readers not wellversed in the subject matter. Questions for the Authors: 1. Scalability: How do you plan to address the space complexity issue associated with the number of classes in the introspective feature extraction process for largescale applications 2. Comparison: How does introspective learning compare with existing techniques in terms of computational efficiency and model interpretability 3. Generalization: Have you explored the generalizability of introspective networks to other types of neural network architectures beyond the ones evaluated in the paper Feedback for the Authors: 1. Provide more clarity in explaining complex concepts ensuring accessibility for readers with varying levels of expertise. 2. Conduct additional experiments or analyses to further strengthen the conclusions drawn in specific sections like IQA and active learning. 3. Consider discussing potential realworld implications and adoption challenges of introspective learning in practical settings. 3) Review Summary: The paper introduces an intriguing concept of introspective learning for neural networks demonstrating improved robustness and calibration in various applications. While innovative and comprehensive in its evaluation there are concerns regarding complexity and scalability. Additional experiments and clearer explanations would enhance the papers impact. Overall the proposed introspective learning framework presents a promising avenue for future research and practical applications. Recommendation: On the basis of the strengths and weaknesses identified I recommend this paper for acceptance with revisions to address the highlighted areas of improvement. The novel concept of introspective learning and its demonstrated benefits make a valuable contribution to the field of neural network architectures.", "l4IHywGq6a": " Review of \"DataEfficient Generative Modeling of Molecules Using Graph Grammar\"  1) Summary: The paper introduces a novel approach for generating molecular structures using a dataefficient generative model based on graph grammar construction. The proposed method addresses the challenges posed by limited classspecific chemical datasets by automatically constructing production rules from training data. The approach aims to generate highquality and synthesizable molecules with stateoftheart results even with small datasets. The method is evaluated on specific monomer datasets and compared against existing approaches like GraphNVP VAEs MHG and STONED.  2) Strengths and Weaknesses: Strengths:  Innovative Approach: The integration of graph grammar construction with domainspecific optimization is a novel approach that addresses the limitations of DLbased models with small datasets.  Performance: The method achieved remarkable performance in generating highquality molecules even with small sample sizes outperforming existing approaches in specific metrics.  Data Efficiency: The models ability to generate meaningful production rules based on a limited number of samples demonstrates its data efficiency.  Explainability: The explainability of the model reflected in the production rules learned provides insights into the specific characteristics of generated molecules.  Applicability: The methods potential applications such as functional group extraction and its performance analysis enhance its practical utility. Weaknesses:  Evaluation Metrics: While the paper introduces new metrics like Chamfer Distance and Retro\\xe2\\x88\\x97 Score and justifies their relevance a more detailed explanation of their significance and comparison with existing metrics would enhance the clarity of the evaluation.  Detailed Algorithm Description: The section explaining the algorithm for optimizing grammar construction could be further clarified for readers unfamiliar with the specific techniques such as Monte Carlo sampling and REINFORCE.  Comparative Analysis: While the comparison with existing approaches is provided a deeper analysis of failure cases or limitations of the proposed model when compared to specific scenarios could provide more insights.  Question and Feedback to the Author: 1. Can you provide more insights or examples on how the optimized grammar construction directly impacts the synthesizability and diversity of the generated molecules 2. Considering future research directions how do you envision extending the current model to handle more complex molecular structures beyond monomers and polymers especially in challenging chemical spaces  3) Review Summary: The paper on dataefficient generative modeling using graph grammar construction presents an innovative approach to molecular structure generation addressing the challenges posed by limited chemical datasets. The models ability to learn production rules effectively from small samples and generate highquality synthesizable molecules demonstrates its data efficiency and practical utility. The competitive performance against existing methods on various metrics and the explainability provided by the learned graph grammars highlight the potential of the proposed methodology. Further improvements in algorithm clarity metric explanations and exploring wider applications could enhance the impact and relevance of this research. Overall based on the strengths and potential of the proposed model I recommend acceptance after addressing the minor weaknesses highlighted and suggestions for further exploration.", "fOsN52jn25l": " Summary: The paper introduces the Dual Lottery Ticket Hypothesis (DLTH) as a complement to the Lottery Ticket Hypothesis (LTH) in the context of sparse neural network training. DLTH posits that randomly selected subnetworks from a randomly initialized dense network can be transformed into a trainable condition and achieve comparable performance to LTH. The proposed Random Sparse Network Transformation (RST) method leverages a regularization term to extract information from masked weights and facilitate the training of the selected subnetworks. Extensive experiments on various datasets validate DLTH and RST against competitive baselines showcasing promising results.  Strengths: 1. Novel Contribution: The introduction of DLTH presents a unique perspective on sparse network training offering a valuable and more general hypothesis compared to LTH. 2. Comprehensive Experiments: The paper conducts thorough experiments on multiple datasets and compares the proposed RST method with strong baselines enhancing the credibility of the findings. 3. Method Clarity: The RST approach is welldetailed with a clear explanation of how it transforms randomly selected subnetworks into trainable conditions adding practical value to the research. 4. Comparative Analysis: The comparison with other pruning techniques like L1 and GraSP as well as the discussion on computational cost and efficiency enriches the depth of the study.  Weaknesses: 1. Theoretical Discourse: While the paper details the practical aspects well a deeper theoretical discussion on the mathematical underpinnings of DLTH and RST could provide additional insight. 2. Generalization: While the results appear promising the paper could further discuss the generalizability of the proposed methods across different network architectures and datasets. 3. Visual Aid: Including illustrative figures or diagrams to better explain the concepts of DLTH and RST may enhance the readability and comprehension of the paper.  Question and Feedback to the Author: 1. Could you elaborate on how the proposed RST method could potentially be extended or adapted to different network structures beyond the ones tested in the current experiments 2. In what scenarios do you foresee DLTH and RST being particularly beneficial or applicable and are there specific types of problems or datasets where these approaches might excel 3. Considering the iterative nature of some of the experiments how do you envision scaling the proposed methods to larger and more complex models or datasets in terms of time and computational requirements  Review Summary: The paper on Dual Lottery Ticket Hypothesis and the accompanying Random Sparse Network Transformation method presents a compelling argument for a new approach to studying sparse network training. The experimental results validate these hypotheses effectively against strong baselines. With detailed methodology and comprehensive comparisons the paper significantly contributes to the field of neural network optimization. However further theoretical exploration generalizability considerations and visual aids could enhance the overall impact of the research. Overall the paper is wellstructured and informative suggesting that it warrants publication with minor revisions to address the identified weaknesses.", "s2UpjzX82FS": " Summary: The paper proposes a novel distributed learning framework called TAViT for image processing tasks where clients learn multiple tasks with their private data in a privacypreserving manner. TAViT leverages a taskagnostic Vision Transformer (TAViT) consisting of taskspecific heads and tails on clients and a Transformer body on the server. The key idea is an alternating training strategy between taskspecific and taskagnostic learning demonstrating improved performance and privacy preservation.  Strong Points: 1. Innovative Approach: The paper addresses an important problem of multitask learning with distributed data and presents an innovative solution using a novel architecture. 2. Privacy Preservation: The proposed TAViT framework effectively preserves data privacy by allowing clients to learn specific tasks without centralized data sharing. 3. Empirical Validation: Extensive experiments demonstrate the effectiveness of TAViT in various image processing tasks outperforming existing strategies and CNNbased models. 4. Detailed Explanations: The paper provides a detailed explanation of the proposed method including the training scheme communication cost analysis and privacy preservation mechanisms. 5. Comparative Analysis: The comparisons with existing strategies CNNbased models and individual task learning provide a comprehensive evaluation of TAViTs performance.  Weak Points: 1. Generalization: Limited discussion on the generalizability of TAViT to different types of tasks beyond image processing could enhance the papers impact. 2. Theoretical Analysis: More theoretical insights into the models computational complexity and convergence properties could strengthen the papers technical depth.  Questions: 1. Have you considered the scalability of TAViT when dealing with a larger number of clients or tasks How does the computational complexity scale with the increasing number of clients 2. Could you discuss potential realworld applications where TAViT could be particularly beneficial due to its privacypreserving nature  Feedback:  Clarity and Structure: The paper is wellstructured and provides significant details about the proposed method. The authors should ensure consistency in terminology and make the paper more accessible to readers less familiar with the topic.  Experimental Validation: The empirical results are robust however the authors could include additional visualization or qualitative analysis to complement the quantitative metrics.  Review Summary: The paper introduces a novel distributed learning framework TAViT which effectively addresses multitask learning with distributed privacysensitive data in image processing tasks. The proposed approach demonstrates innovation privacy preservation and superior performance compared to existing strategies. With comprehensive experiments and detailed explanations the paper offers a significant contribution to the field. Addressing minor weaknesses and expanding on practical implications could further enhance the papers impact. I recommend acceptance after minor revisions to enhance clarity and generalizability.", "h0OYV0We3oh": "Review of \"SLATE: A Simple and Novel SlotBased Autoencoder for ZeroShot Image Generation\" Summary: The paper introduces SLATE a slotbased autoencoding architecture that combines elements of DALL\u00b7E and objectcentric representation models to achieve systematic generalization in zeroshot image generation without the need for text prompts. SLATE utilizes a Transformer decoder conditioned on slots derived from images to capture complex interactions and improve generation quality. The proposed model significantly outperforms pixelmixture decoders in reconfiguring slots for compositionbased image generation. Strengths: 1. Innovative Approach: The paper introduces a novel architecture that bridges the gap between DALL\u00b7E and objectcentric representation models addressing limitations in zeroshot image generation. 2. Integration of Transformer Decoder: The usage of a Transformer decoder with slots allows for capturing detailed interactions improving the overall image quality and systematic generalization. 3. Experimental Validation: Thorough experiments and comparisons with existing models on diverse datasets demonstrate the superior performance of SLATE in generating images from arbitrary slot combinations. 4. Ablation Studies: The inclusion of ablation studies provides valuable insights into the impact of each architectural component on the models performance and efficiency. 5. Efficient Design: The simplicity and ease of implementation of the SLATE architecture make it highly accessible and applicable to various image generation tasks. Weaknesses: 1. Theoretical Grounding: While the empirical results are promising the paper could benefit from more theoretical analysis explaining the underlying mechanisms driving the improvements in SLATE. 2. Complexity vs. Performance: The paper mentions that using multiple heads in slot attention could harm disentanglement in simple datasets. Providing more clarity on balancing complexity and model performance is essential. 3. Scalability Concerns: The paper mentions potential scalability issues when scaling up the model. Further insights on scalability and model efficiency could enhance the practicality of SLATE. Questions to the Authors: 1. Could you elaborate on how SLATE ensures global semantic consistency in zeroshot image generation especially in complex scenes with multiple objects and textures 2. How does SLATE adapt to handle semantic variability and challenges in compositional image generation across diverse datasets 3. Have you considered exploring the interpretability of the learned slot representations and their generalizability to unseen data distributions in future work Feedback to the Authors: 1. Strengthen the theoretical underpinnings by providing more indepth explanations of the computational mechanisms behind SLATEs performance. 2. Address scalability concerns by discussing potential strategies to enhance model efficiency and handle larger datasets. 3. Provide insights on model interpretability and generalization to enhance the applicability and robustness of SLATE. Review Summary: The paper presents an innovative approach SLATE for zeroshot image generation using a slotbased autoencoding architecture. The integration of a Transformer decoder and slot attention mechanism significantly improves systematic generalization and image quality. However addressing theoretical grounding scalability concerns and model interpretability could enhance the overall impact and practicality of SLATE. Overall the papers novel approach and empirical validation make it a compelling contribution to the field of objectcentric image generation. Further clarification and enhancements in certain areas could strengthen the papers impact and applicability. Based on the promising results and potential for significant contributions I recommend acceptance with minor revisions.", "xOHuV8s7Yl": " Summary: The paper introduces two interpretable neural network constructions namely Triangularlyconstructed NN (TNN) and SemiQuantized Activation NN (SQANN) for universal approximation. These models aim to address the issues of interpretability and resistance to catastrophic forgetting commonly associated with deep neural networks. TNN is designed for onedimensional input data and facilitates clear interpretability of activations while SQANN is a multidimensional universal approximator that retains key features of TNN while incorporating deeper neural network properties. Strong Points: 1. Interpretability: The paper successfully addresses the issue of interpretability in neural networks providing clear explanations of how activations are organized and utilized in the proposed models. 2. Resistance to Catastrophic Forgetting: The models display strong resistance to catastrophic forgetting which is crucial for continuous learning and adaptation to new datasets without losing accuracy on existing data. 3. Empirical Demonstrations: The inclusion of python code and experimental results with common datasets enhances the credibility and applicability of the proposed models. 4. Comprehensive Explanation: The paper thoroughly explains the concepts constructions and applications of both TNN and SQANN providing detailed insights into their theoretical foundations and practical implementations. Weak Points: 1. Limited Application Scope: TNN is specifically designed for onedimensional data limiting its applicability to more complex multidimensional datasets. 2. Complexity in SQANN Construction: The construction process of SQANN especially in handling collisions and layer destruction may be computationally inefficient and require further optimization. 3. Insufficient Generalizability Analysis: While the paper provides empirical results on test datasets with varying spread a more detailed analysis of generalizability to unseen and noisy data could strengthen the evaluation.  Question to the Authors: 1. How does the combination of TNN and SQANN improve the overall interpretability and performance compared to using each model individually 2. Are there plans to extend the applications of TNN to handle multidimensional data in future research 3. Can you provide more insights into the computational efficiency of the proposed models especially in handling largescale datasets  Feedback:  Clarity and Structure: The paper is wellstructured and organized providing clear definitions and explanations of key concepts.  Implementation and Validation: The inclusion of python code and experimental results enhances the papers credibility and facilitates practical implementation of the proposed models.  Future Directions: Consider discussing potential enhancements to the models to address weaknesses such as scalability to larger datasets and optimization of construction processes.  Review Summary: The paper presents a novel approach to interpretable neural network constructions TNN and SQANN addressing interpretability and resistance to catastrophic forgetting. The rigorous theoretical foundations empirical demonstrations and clear explanations make the paper a valuable contribution to the field of interpretable artificial intelligence. However further improvements in scalability and efficiency would enhance the practical utility of the proposed models. Overall the paper is wellstructured informative and warrants consideration for publication pending minor revisions and further validations.", "fwsdscicqUm": "1) Summary of the paper: The paper introduces a novel solution FEDFB for training fair classifiers on decentralized data. It begins with setting the context of group fairness in decisionmaking algorithms highlighting the challenges of training fair models on decentralized data. The authors analyze existing approaches like Unfederated Fair Learning (UFL) Centralized Fair Learning (CFL) and Federated Fair Learning via FEDAVG (FFL via FEDAVG). They reveal the limitations of these approaches and propose FEDFB which leverages a modified FEDAVG protocol to improve fairness on decentralized data. The paper presents theoretical analyses and empirical evaluations showing the superiority of FEDFB over existing methods in terms of accuracyfairness tradeoffs. 2) Strong points: Strengths:  Indepth theoretical framework: The paper provides a comprehensive theoretical framework for decentralized fair learning allowing a rigorous analysis of the performance tradeoffs of different approaches.  Novel contribution: The introduction of FEDFB as a solution to enhance fairness in federated learning on decentralized data is a significant contribution.  Rigorous analysis: The paper rigorously compares and analyzes existing methods like UFL CFL and FFL via FEDAVG providing insights into their limitations and necessity for federated learning. Weak points: Weaknesses:  Clarity of presentation: The paper contains dense technical content that might be challenging for readers without a deep understanding of the subject matter.  Lack of practical use cases: While the theoretical framework and analysis are strong more practical use cases or realworld applications could enhance the papers relevance and applicability.  Limited comparison with existing solutions: While the paper introduces FEDFB and demonstrates its superiority a more extensive comparison with a broader range of existing solutions could provide a more holistic view of the landscape. Questions and Feedback to the author: 1. Can you provide more insights into the practical implications of FEDFB in realworld scenarios and how it can be integrated into existing systems 2. How does FEDFB handle scenarios with high data heterogeneity and varying sensitivities in different clients Is there a risk of bias in such cases 3. Could the paper include some qualitative analysis or user studies to demonstrate the effectiveness of FEDFB in diverse domains 3) Review Summary: The paper presents a novel solution FEDFB for training fair classifiers on decentralized data addressing the limitations of existing methods like UFL CFL and FFL via FEDAVG. The theoretical framework and analysis provide valuable insights into the necessity of federated learning for improving fairness and the performance tradeoffs involved. FEDFB is introduced as a promising solution that outperforms existing approaches showing significant improvements in accuracyfairness tradeoffs. However the paper could benefit from clearer presentation and more practical use case scenarios to enhance its relevance and applicability. Overall the paper makes a valuable contribution to the field of decentralized fair learning but could further strengthen its arguments with additional empirical validations and realworld applications.", "figzpGMrdD": " Summary of the Paper: This paper investigates the interaction between pretrained language models (PLMs) and continual learning (CL) methods in the context of natural language processing tasks. The study involves comprehensive experiments comparing the continual learning performance of 5 PLMs and 4 CL approaches on 3 benchmark datasets in two incremental settings. The paper provides insights into the performance differences across PLMs and CL methods as well as detailed layerwise analyses to understand the extent of forgetting in inner layers and the impact of CL approaches. The results highlight important research questions that can guide the development of effective continual learning techniques.  Strengths of the Paper: 1. Rigorous Experimental Evaluation: The paper conducts extensive experiments covering various PLMs CL methods and benchmark datasets providing a comprehensive comparative study. 2. Novel Insights: The layerwise probing analyses shed light on the performance characteristics of PLMs in a continual learning setting uncovering the challenges and opportunities for improvement. 3. Open Research Questions: The paper highlights important research questions based on empirical findings guiding future research directions in continual learning and PLMoriented strategies. 4. Methodological Clarity: The paper clearly outlines the experimental setup methods and metrics used for evaluation enhancing the reproducibility of the study.  Weaknesses of the Paper: 1. Lack of Theoretical Analysis: While the empirical findings are detailed there is limited theoretical analysis or discussion on the underlying reasons for the observed performance differences. 2. Limited Generalization: The study focuses on NLP tasks and specific CL approaches which may limit the generalizability of the findings to other domains or approaches in continual learning.  Questions and Feedback to the Author: 1. Question: Have you considered investigating the transferability of the observed performance trends and insights to other domains beyond NLP tasks 2. Feedback: Providing a deeper theoretical discussion on why certain PLMs exhibit better robustness in continual learning settings could enhance the papers impact and theoretical contributions.  Review Summary: This paper presents a thorough comparative analysis of PLMs and CL methods for continual learning in NLP tasks offering valuable insights through extensive experiments and layerwise probing analyses. While the study is wellstructured and provides novel insights addressing theoretical explanations and exploring generalizability to other domains could further strengthen the papers contributions. Overall the paper makes a significant contribution to the understanding of continual learning performance with PLMs and sets a foundation for future research in this area. Based on the strengths observed in the study I recommend the paper for publication pending minor revisions to address the identified weaknesses.", "vr39r4Rjt3z": " Summary The paper introduces novel modifications Masked Highway Connection (MHC) and LayerWise Normalisation (LWN) as classifiers to mitigate catastrophic forgetting in neural networks that learn multiple tasks sequentially. The study focuses on internal modifications to create stable yet plastic network architectures and demonstrates the effectiveness of MHC and LWN in reducing forgetfulness. Through experiments on datasets like Permuted MNIST Cifar100 and CUB200 the paper shows that MHC and LWN outperform baseline classifiers and are compatible with various continual learning techniques aiding in achieving stateoftheart results.  Strengths 1. Innovative Approach: Introducing novel modifications MHC and LWN to address catastrophic forgetting internally is a unique and valuable contribution to the field of continual learning. 2. Comprehensive Experiments: The paper conducts experiments on various datasets and tasks providing a thorough evaluation of the proposed modifications effectiveness. 3. Compatibility with Existing Techniques: MHC and LWN are shown to be compatible with different continual learning archetypes showcasing their versatility and potential for integration with existing methods. 4. WellStructured and Clear: The paper is wellorganized with detailed explanations of the proposed modifications experiment setups and results making it easy to understand.  Weaknesses 1. Clarity on Implementation: The paper could benefit from providing more details on the implementation of MHC and LWN such as specific hyperparameters used and potential limitations of the proposed modifications. 2. Generalizability Concerns: While the experiments show promising results extending the evaluations to more diverse datasets and tasks could strengthen the generalizability of the proposed modifications.  Questions for the Authors 1. Can you provide insight into how the performance of MHC and LWN changes with varying hyperparameters or settings 2. Have you considered any potential drawbacks or limitations of implementing MHC and LWN in practical scenarios outside the experimental setup presented in the paper  Feedback The paper presents innovative modifications MHC and LWN as effective solutions to mitigate catastrophic forgetting in neural networks. The thorough experimental evaluation and compatibility with various continual learning techniques highlight the practical value of the proposed modifications. Addressing the weak points mentioned could enhance the clarity and robustness of the study. Overall the paper provides a valuable contribution to the field of continual learning and warrants publication based on its significant findings and wellstructured presentation.", "nWprF5r2spe": " Peer Review of \"WAFL: Wasserstein Distributionally Robust Federated Learning\"  1. Summary: The paper presents WAFL a novel Wasserstein distributionally robust optimization framework for addressing statistical heterogeneity in federated learning. WAFL is framed as an empirical surrogate risk minimization problem and solved using a local SGDbased algorithm. The authors demonstrate that WAFL is more general in robustness compared to related approaches and provide a robust generalization bound. Through experiments on benchmark datasets WAFL is shown to generalize better in noni.i.d. settings and outperform existing robust methods in distribution shift scenarios.  2. Strengths and Weaknesses:  Strong Points: 1. Innovative Solution: Proposing a novel Wasserstein distributionally robust optimization framework is a strong contribution to the federated learning domain. 2. Theoretical Rigor: Comprehensive theoretical framework with clear explanations of key concepts like Wasserstein distance robust optimization and empirical surrogate risk minimization. 3. Algorithm Design: Designing a local SGDbased algorithm for solving the optimization problem shows practical applicability and scalability. 4. Empirical Evaluation: Conducting extensive experiments on benchmark datasets to validate WAFLs performance and robustness against distribution shifts and noni.i.d. settings. 5. Generalization Bounds: Providing explicit generalization bounds and robustness properties enhances the theoretical understanding of WAFL.  Weak Points: 1. Clarity of Presentation: The paper is dense with technical details making it challenging for readers unfamiliar with the domain to grasp all concepts easily. 2. Lack of Comparative Analysis: While WAFL is compared to FedAvg and robust methods in experiments a more detailed comparative analysis with a wider range of existing methods could enhance the validation of WAFLs effectiveness. 3. Heterogeneity Handling: While WAFL is designed to address statistical heterogeneity a deeper discussion on the methods performance under varying degrees of heterogeneity could provide more insights.  Questions and Feedback to the Authors: 1. Scalability: How does WAFL perform in scenarios with a large number of clients or when dealing with highly heterogeneous client datasets 2. Comprehensive Comparisons: Have you considered comparing WAFL against a broader range of existing federated learning methods to showcase its competitiveness more widely 3. RealWorld Applications: Can you provide insights into potential realworld applications or scenarios where WAFL could offer significant advantages over traditional FL methods 4. Robustness Sensitivity: How sensitive is WAFLs robustness to hyperparameter choices especially regarding the Wasserstein ball radius and center location adjustments  3. Review Summary: The paper \"WAFL: Wasserstein Distributionally Robust Federated Learning\" proposes a novel approach to addressing statistical heterogeneity in federated learning through a Wasserstein distributionally robust optimization framework. With a strong theoretical foundation algorithm design and empirical validation WAFL demonstrates promising results in generalization and robustness. However the paper could benefit from enhanced clarity more comprehensive comparisons and deeper insights into scalability and realworld applications. Given the innovative nature of WAFL and its potential impact on federated learning this work showcases significant contributions to the field. Considering the thorough theoretical development methodological rigor and promising experimental results I recommend acceptance of the paper after addressing the minor concerns and providing clarification on scalability and realworld applications. Overall this paper significantly advances the field of federated learning by introducing a robust optimization framework that shows improvements in generalization and robustness in challenging noni.i.d. settings and distribution shift scenarios.  By incorporating the suggested improvements the authors can further strengthen the paper and provide valuable contributions to the research community.", "v3aeIsY_vVX": " Summary: The paper titled \"Conditional Waveform Synthesis with Chunked Autoregressive GAN\" addresses the limitations of current Generative Adversarial Network (GAN)based models in accurately preserving pitch and periodicity in audio waveform synthesis. The authors propose a new model Chunked Autoregressive GAN (CARGAN) which utilizes autoregressive conditioning with large chunks to improve pitch accuracy reduce training time and maintain subjective quality. The paper extensively discusses the importance of autoregression in modeling pitch and phase accurately demonstrating the effectiveness of CARGAN through experiments on speech data.  Strong Points: 1. Detailed Analysis: The paper provides a thorough analysis of the limitations in existing GANbased waveform synthesis models and presents a wellreasoned solution. 2. Novel Approach: The introduction of CARGAN a hybrid GAN model incorporating autoregressive conditioning with large chunks is innovative and addresses key issues in waveform synthesis. 3. Experimental Evaluation: The paper conducts a comprehensive set of experiments including synthetic cumulative sum experiments spectrogramtowaveform inversion and training speed benchmarks to validate the effectiveness of CARGAN. 4. Clarity and Reproducibility: The authors ensure reproducibility by providing opensource code and detailed explanations of their methodology facilitating research replication.  Weak Points: 1. Evaluation Metrics Correlation: The paper mentions weak correlations between DeepSpeech distances and subjective metrics. Further exploration or discussion on the limitations of the evaluation metrics could enhance the understanding of the models performance. 2. Boundary Artifacts: While addressing pitch and periodicity errors CARGAN induces occasional boundary artifacts. Future work to minimize these artifacts would improve the models overall quality. 3. Scale Effort on Human Subject Study: Since the research involved a human subject study more discussion on ethicsrelated considerations could enhance readers understanding of the studys implications.  Questions to Authors: 1. Can you elaborate on the specific strategies or techniques employed to address the occasional boundary artifacts induced by CARGAN during the waveform synthesis process 2. How do you plan to extend your research to address the need for diverse representations of accents and dialects in clean speech datasets for more inclusive voice synthesis systems 3. Could you provide insights into potential future directions for improving the feature matching loss compensation to alleviate metallic sound artifacts in the audio generated by CARGAN  Feedback: Overall the paper presents a significant contribution to the field of conditional waveform synthesis by introducing CARGAN and demonstrating its advantages over existing GANbased models. Enhancements in summarizing the addressed weaknesses exploring the implications of evaluation metrics and elaborating on ethical considerations related to human subject studies could further enrich the paper. With its meticulous experimental validation and innovative approach the paper holds promise for advancing research in audio waveform synthesis.", "gfwON7rAm4": " Summary The paper explores the concept of Markov Potential Games (MPGs) as a generalization of normalform potential games in the context of multiagent reinforcement learning (MARL). The authors investigate the convergence properties of independent policy gradient methods in MPGs demonstrating the existence of deterministic Nash policies and providing convergence guarantees for both exact gradients and finite samples.  Strengths 1. Clear Research Question: The paper addresses an important question regarding convergence guarantees for multiagent reinforcement learning in scenarios where agents have aligned incentives. 2. Theoretical Depth: The study involves a rigorous exploration of structural properties sufficient conditions and convergence results for MPGs providing a solid theoretical foundation. 3. Broad Relevance: The paper contributes to the field by extending findings from singleagent RL to complex multiagent settings demonstrating practical implications of the theoretical analysis. 4. Comprehensive Literature Review: The related works section effectively contextualizes the study within the existing research landscape highlighting the novelty and significance of the current work. 5. Experimental Validation: The inclusion of experiments on congestion games adds empirical evidence to support the theoretical findings enhancing the credibility of the proposed methodology.  Weaknesses 1. Complexity: The paper may be challenging for readers without a strong background in reinforcement learning and game theory due to its highly technical content. 2. Limitation of Theoretical Results: While the theoretical results are promising the practical implications of the proposed convergence guarantees in realworld applications could benefit from further validation and analysis.  Questions and Feedback Question to the Authors: 1. Have you considered exploring the scalability and computational complexity of the proposed multiagent reinforcement learning algorithms in realworld scenarios with a large number of agents and complex environments Feedback to the Authors: 1. To enhance the practical applicability of the research consider providing insights on potential challenges or limitations that may arise when implementing the proposed algorithms in practical MARL systems. 2. It would be beneficial to include a discussion on the interpretability and explainability of the Nash policies generated by the proposed methodology for a deeper understanding of the decisionmaking processes.  Review Summary The paper provides a thorough investigation into Markov Potential Games and their convergence properties for multiagent reinforcement learning. The strong theoretical foundation comprehensive analysis and experimental validation highlight the significance of the research. However addressing potential scalability concerns and practical challenges would further strengthen the paper. Overall the study represents a valuable contribution to the field of cooperative AI and MARL warranting consideration for publication.", "uUN0Huq-n_V": " Summary: The paper proposes an alternative approach to automatic music harmony composition by combining Deep Supervised Learning Deep Reinforcement Learning and Inverse Reinforcement Learning techniques. The model trained in three phases shows improvements over pretrained models without reinforcement learning. It incorporates learning a reward function based on humancomposed tracks using Adversarial Inverse Reinforcement Learning. Objective evaluations and subjective user studies demonstrate the effectiveness of the proposed methodology.  Strengths: 1. Innovative Approach: The integration of deep learning models with reinforcement learning techniques for music harmony composition is a novel and promising methodology. 2. Comprehensive Methodology: The paper covers a detailed explanation of the approach training phases reward function extraction tuning process and evaluation metrics used. 3. Objective Evaluation: The paper provides both objective metrics and subjective user study results to support the effectiveness of the proposed model. 4. Reproducibility: The paper commits to providing access to the code used for training the models ensuring reproducibility of the results.  Weaknesses: 1. Reward Function Complexity: The paper acknowledges the challenges in designing an appropriate reward function which can be a persistent limitation in reinforcement learning applications. 2. Limited Dataset: The paper mentions challenges due to limited training data available for AIRL which can impact the effectiveness of the learned reward function. 3. Subjective User Study: While the subjective user study is valuable it would benefit from a more detailed analysis of participant feedback to provide deeper insights.  Questions and Feedback: 1. Generalization: How well does the proposed model generalize to different music genres or styles beyond the training data 2. Scalability: How scalable is the proposed approach when working with larger datasets or more complex music compositions 3. Computational Efficiency: What are the computational requirements of the model and are there ways to optimize training processes for efficiency 4. Comparison with Existing Models: How does the performance of the proposed model compare to existing stateoftheart models in the field of music composition 5. Future Research Directions: Are there specific aspects or refinements in the methodology that could improve the models performance further in future research iterations  Review Summary: The paper presents an innovative and comprehensive approach to automatic music harmony composition showcasing improvements over traditional deep learning models through the integration of reinforcement learning techniques. While it addresses challenges in reward function design and dataset limitations the proposed model demonstrates promising results in both objective evaluations and subjective user studies. With a focus on reproducibility and further research potential the paper contributes valuable insights to the field of automatic music composition. Hence I recommend acceptance for publication pending minor revisions to address the identified weaknesses.", "fSeD40P0XTI": " Review of \"Continuous Classification of Time Series: Adaptive Model Training Policy\" 1) Summary: The paper introduces a new concept Continuous Classification of Time Series (CCTS) which aims to classify time series at every time point. The key idea is to model multiple data distributions simultaneously to address the problems of catastrophic forgetting and overfitting that arise in traditional models. The authors propose a novel Adaptive model training policy ACCTS consisting of an Adaptive multidistribution extraction policy and an Adaptive importancebased replay policy. Experimental results on realworld datasets demonstrate that ACCTS outperforms all baselines in terms of classification accuracy at every time point. 2) Key Points: Strong Points:  The paper addresses a significant and practical problem in time series classification  continuous classification at every time point.  The introduction of the ACCTS model advances the field by providing a solution that overcomes catastrophic forgetting and overfitting issues.  The inclusion of both an adaptive multidistribution extraction policy and an importancebased replay policy adds depth and robustness to the proposed solution.  Solid experimental results on realworld datasets support the effectiveness of ACCTS in achieving higher accuracy compared to baselines. Weak Points:  The paper could benefit from clearer explanations of the technical details of the proposed ACCTS model especially in terms of the reinforcement learningbased adaptive policies.  More detailed comparisons with existing approaches in terms of computational efficiency and scalability could enhance the evaluation of ACCTS.  The theoretical background and motivation for the choice of the specific datasets used in experiments could be expanded upon to provide a more comprehensive understanding of the practical implications. Questions and Feedback to the Authors: 1. Can you elaborate on the computational complexity and scalability of the ACCTS model compared to existing methods especially considering the realtime nature of continuous classification tasks 2. How generalizable do you think the proposed ACCTS model is to other domains beyond the datasets used in the experiments Are there specific characteristics of the datasets that influenced the performance of ACCTS 3. Could you provide further insights into the interpretability of the ACCTS model and how it can be applied in realworld scenarios for informed decisionmaking in timesensitive applications 3) Review Summary: The paper presents a novel solution ACCTS to address the challenges of continuous time series classification. The proposed model demonstrates significant improvements in classification accuracy and effectively tackles the issues of catastrophic forgetting and overfitting. The integration of adaptive learning policies and experimental validations on realworld datasets enhance the credibility of the proposed approach. While the paper excels in tackling a pertinent research problem and providing strong empirical evidence further clarification on the technical intricacies and broader applicability of ACCTS would strengthen its impact in the field of time series classification. Overall the paper offers a valuable contribution to the continuous classification of time series and warrants consideration for publication.  This comprehensive review outlines the strengths and weaknesses of the paper provides constructive feedback to the authors and synthesizes the key points for decisionmaking regarding the manuscript.", "vDwBW49HmO": "Summary: The paper proposes an interdomain gradient matching objective IDGM to enhance domain generalization in machine learning. The objective focuses on learning invariant features across different domains by encouraging consistent gradient directions. To address computational complexity a simpler firstorder algorithm named Fish is derived to approximate the optimization of IDGM. Experimental evaluations on the WILDS and DOMAINBED benchmarks demonstrate competitive performance and effectiveness of the proposed algorithm across various domain generalization tasks. Strengths: 1. Novel Contribution: The proposed IDGM objective and Fish algorithm provide a fresh perspective on tackling domain generalization tasks by emphasizing consistent gradient directions across domains. 2. Empirical Validation: Extensive experiments on various datasets from WILDS and DOMAINBED benchmarks demonstrate the effectiveness and competitive performance of the proposed algorithm. 3. Theoretical Analysis: The paper provides a detailed theoretical analysis explaining the workings of the IDGM objective and how the Fish algorithm approximates the optimization of IDGM. 4. Comprehensive Experiments: The experiments cover a wide range of datasets and architectures showcasing the versatility and robustness of the proposed algorithm in different domain generalization tasks. Weaknesses: 1. Complexity Limitations: The paper acknowledges the scalability issues when the number of domains is large where the proposed Fish algorithm may struggle to outperform ERM. Addressing this limitation and improving scalability could be a potential area for future research. 2. Limited RealWorld Application: While the algorithm demonstrates effectiveness in benchmarks the realworld applicability and performance in practical scenarios with more complex data distributions are not extensively discussed or tested. Questions for the Authors: 1. Have you considered exploring ways to enhance scalability of the Fish algorithm for scenarios with a large number of domains to improve performance in such challenging settings 2. How do you plan to validate the effectiveness of the proposed algorithm in realworld applications or datasets with more complex and diverse distribution shifts beyond the benchmarks used in the experiments Feedback to the Authors: 1. Provide more insights on how the proposed algorithm can be adapted or extended to address scalability challenges when dealing with a large number of domains to make it more practical and applicable in diverse scenarios. 2. Consider conducting experiments on more realworld datasets with complex distribution shifts to further validate the effectiveness and generalization capabilities of the proposed algorithm. Review Summary: The paper presents a novel approach IDGM for domain generalization in machine learning along with a computationally efficient approximation algorithm named Fish. The experimental results on benchmark datasets demonstrate the effectiveness and competitive performance of the proposed algorithm. While the paper holds theoretical rigor and showcases promising results addressing scalability concerns and extending the evaluation to realworld scenarios could enhance the practical applicability of the algorithm. Based on the strengths in novel contributions empirical validation and theoretical analysis and considering the suggested improvements I recommend acceptance of the paper with minor revisions.", "qI4542Y2s1D": "Summary: The paper presents \"Following Instructions in Language with Modular methods\" (FILM) a novel approach for embodied instruction following that overcomes the limitations of existing endtoend methods. FILM consists of modular components for language processing semantic mapping semantic search policy and deterministic policy. It achieves stateoftheart performance on the ALFRED benchmark by leveraging structured spatial components without the need for expert trajectories or lowlevel instructions. Strengths: 1. Modular Approach: FILMs modular design with structured components enables transparent reasons for failure and provides avenues for improving individual modules. 2. StateoftheArt Performance: The proposed FILM method achieves significant performance improvements (24.46) with a substantial gap from previous SOTA methods in the absence of expert trajectories or lowlevel instructions. 3. Generalizability: FILM demonstrates strong generalizability across both seen and unseen scenes highlighting its effectiveness in realworld scenarios. 4. Semantic Search Policy: The semantic search policy significantly improves the localization of small and flat objects in large scenes enhancing goal object detection. Weaknesses: 1. Perception and Language Processing Errors: The paper identifies common error modes related to perception and language processing particularly in detecting small objects and processing language instructions accurately. 2. Room for Improvement in Interaction Actions: Errors in interaction actions due to objects being out of view or far away indicate areas for enhancing agent performance in executing tasks accurately. 3. Limited Exploration of Closed Receptacles: FILM does not actively search inside closed receptacles such as cabinets leading to potential failures in localizing subgoal objects within. Questions to the Authors: 1. Can you elaborate on the potential scalability of FILM to more diverse and complex environments beyond household tasks 2. How do you envision addressing the limitations related to perception errors and language processing inaccuracies in future iterations of FILM 3. Have you considered incorporating reinforcement learning techniques to enhance the performance of the deterministic policy module in handling navigation and interaction actions dynamically Feedback: The paper presents a comprehensive and innovative approach to embodied instruction following showcasing notable advancements in modular design and performance outcomes on the ALFRED benchmark. Addressing the identified weaknesses through enhanced perception capabilities improved interaction strategies and expanded exploration mechanisms could further strengthen the applicability and robustness of FILM in realworld scenarios. Providing insights into the scalability optimization strategies and potential future developments of FILM would enhance the understanding and practical implications of the proposed methodology. Review Summary: Overall the paper demonstrates a strong foundation in modular design principles and achieves impressive performance gains in embodied instruction following without the reliance on expert trajectories or lowlevel instructions. While FILM showcases notable advancements in stateoftheart performance and generalizability addressing perception errors interaction challenges and enhancing active search capabilities could unlock further potential for robust and adaptable embodied agents. Based on the comprehensive analysis and innovative methodology presented I recommend the paper for publication with some revisions to address the identified weaknesses and considerations for future enhancements.", "vHVcB-ak3Si": " Summary: The paper delves into integral pose regression for human body and hand pose estimation comparing it with detectionbased heatmap methods. It focuses on the differences in performance and training between the two methods discussing theoretical aspects experimental findings and proposing solutions to enhance the performance of integral regression methods. Key aspects covered include inference backpropagation performance differences related work and analysis of heatmap decoding and supervision.  Strong Points: 1. Comprehensive Analysis: The paper provides a detailed comparison of integral regression and detectionbased methods offering theoretical insights experimental verification and practical implications. 2. Clear Structure: The structure of the paper is wellorganized with clear sections and logical progression of ideas. 3. Novel Contribution: The identification of heatmap activation shrinkage as a significant factor affecting integral regressions performance is a novel contribution that adds value to the existing literature. 4. Experimental Validation: The paper uses empirical experiments to support theoretical claims enhancing the credibility of the findings. 5. Theoretical Insights: The analysis of heatmap decoding supervision methods and gradients provides a deeper understanding of integral pose regression.  Weak Points: 1. Limited Scope: The analysis seems focused on comparing integral regression and detectionbased methods potentially missing insights from other approaches in human pose estimation. 2. Repetitive Content: Some sections seem repetitive especially in the discussion of theoretical concepts like decoding and supervision. 3. Lack of RealWorld Applications: While the theoretical insights are valuable more discussion on realworld applications and implications of the findings could enhance the practical relevance of the study.  Questions and Feedback to the Author: 1. Can you elaborate on the practical implications of your findings for developers and researchers working on human pose estimation applications 2. How do you envision integrating the proposed spatial supervision into existing integral regression frameworks and what challenges might be encountered in implementing this solution 3. Have you considered the potential impact of variations in dataset characteristics (e.g. diversity of poses image quality) on the performance of integral pose regression methods  Review Summary: The paper provides a thorough analysis of integral pose regression for human pose estimation highlighting the performance differences compared to detectionbased methods. The theoretical insights experimental validation and proposed solutions contribute to the understanding of integral regression methods. While the paper offers valuable contributions to the field addressing minor shortcomings such as focusing on a broader scope of methods and discussing realworld applications could enhance the overall impact and relevance of the study. Based on the comprehensive analysis and unique insights provided I recommend acceptance for publication with minor revisions to improve the clarity and practical relevance of the study.", "hl9ePdHO4_s": "1. Paper Summary: The paper challenges the common belief in the graph neural network (GNN) community that anisotropic models outperform isotropic models. The authors introduce an isotropic GNN Efficient Graph Convolution (EGC) which outperforms popular anisotropic models like GAT and PNA by using spatiallyvarying adaptive filters. EGC achieves higher model accuracy lower memory consumption and latency compared to anisotropic models. The paper provides empirical evidence across diverse benchmark datasets and discusses the implications of their findings for the efficiency and performance of GNNs. 2. Strengths and Weaknesses: Strong Points:  Innovative Approach: Introducing an isotropic model that outperforms anisotropic models challenges existing beliefs in the GNN community.  Practical Implications: EGC offers higher accuracy with reduced memory consumption and latency making it more efficient for realworld implementations.  Empirical Evidence: The paper provides rigorous evaluation across multiple datasets ablation studies and comparisons against strong baselines.  Resource Efficiency: Demonstrating lower memory consumption and latency compared to popular models like GAT and PNA is a significant strength.  Detailed Interpretation: The paper provides detailed spatial and spectral interpretations of the EGC architecture enhancing understanding. Weak Points:  Edge Feature Incorporation: The paper does not incorporate edge features in the experiments which could limit the generalizability of the results to realworld applications.  Sampling Techniques: Not utilizing sampling techniques may limit the scalability of the model for training on large graphs.  Comparisons: While comparisons against strong baselines are provided further comparisons with a wider range of models would strengthen the evaluation.  Scalability: While the efficiency benefits are highlighted scalability to extremely large graphs could be explored further. Questions and Feedback to the Authors: 1. Incorporating Edge Features: Have you considered how the EGC architecture could be adapted to handle edge features which are common in many realworld applications 2. Scalability: How do you envision the scalability of EGC to extremely large graphs with millions or billions of nodes especially in the context of sampling techniques 3. Comparative Analysis: Have you considered comparing the efficiency and performance of EGC against a broader range of GNN architectures beyond the selected baselines 3. Review Summary: The paper presents an innovative approach in introducing the Efficient Graph Convolution (EGC) architecture challenging the prevailing belief in the GNN community that anisotropic models are superior. The detailed evaluation and comparisons against strong baselines along with the resource efficiency demonstrated by EGC make a strong case for its effectiveness. The incorporation of spatial and spectral interpretations enhances the understanding of the proposed model. However potential areas for improvement include exploring edge feature incorporation scalability to larger graphs and broader comparisons with diverse GNN architectures. Overall the paper provides valuable insights and raises important questions for further research in the GNN community. Recommendation: Based on the innovative approach strong empirical evidence and practical implications presented in the paper I recommend accepting it for publication with minor revisions to address the highlighted weak points and questions for the authors to consider.", "moHCzz6D5H3": " Summary of the Paper The paper introduces a new concept of disguised subnetworks in randomly initialized neural networks extending the idea of hidden subnetworks. Disguised subnetworks are hidden in the random networks and can be unmasked with simple transformations on the remaining weights. The proposed PeekaBoo (PaB) algorithm identifies these disguised subnetworks through a twostage process: searching efficiently for a subnetwork at random initialization and then unmasking the disguise by learning to transform the remaining weights. The study focuses on sign flipping as the transformation operation. Extensive experiments with various large models and datasets demonstrate the efficacy of PaB over existing approaches.  Strengths of the Paper  Novel Concept: Introduces the innovative concept of disguised subnetworks advancing the field of sparse neural networks.  Efficient Algorithm: The PaB algorithm offers a twostage process that efficiently identifies disguised subnetworks without using latent weights or complex scoring functions.  Extensive Experiments: The paper provides comprehensive experimental results with various large models and datasets demonstrating the effectiveness of PaB.  Scalability: PaB shows scalability to larger networks and datasets providing a good tradeoff between accuracy and training efficiency.  Comparison and Analysis: Offers a thorough comparison with stateoftheart methods and indepth analysis of the results.  Weaknesses of the Paper  Theoretical Clarity: Some parts of the paper could benefit from clearer theoretical explanations to enhance understanding.  Complexity: The algorithm may be technically complex for readers not wellversed in the field requiring detailed explanations.  Reproducibility: While the code is provided detailed steps and instructions for reproducing the results could be clearer.  Questions and Feedback to the Author 1. Can you provide more insights into the theoretical underpinnings supporting the concept of disguised subnetworks and how they enhance the expressiveness of neural networks 2. How generalizable do you think the PaB algorithm is to different network architectures and datasets beyond the ones tested in the experiments 3. Have you considered any potential performance degradation or limitations that PaB might face in certain scenarios or with specific types of networks  Review Summary The paper introduces a novel concept of disguised subnetworks and presents the PaB algorithm for efficiently identifying them in randomly initialized neural networks. The extensive experiments demonstrate the effectiveness of PaB over existing methods showcasing scalability and competitive performance. While the paper presents significant strengths in innovation efficiency and thorough experimentation there is room for improvement in theoretical clarity and reproducibility. Overall considering the innovation effectiveness and scalability of the PaB algorithm I recommend acceptance after addressing the minor weaknesses identified.", "jJWK09skiNl": " Summary of the Paper The paper introduces the concept of zeroshot object detection for industrial robots to handle varying objects in a manufacturing environment without requiring retraining. It focuses on detecting daily objects in indoor scenes using YCB Video Dataset and proposes a modified YOLOv5 neural network architecture to perform generalized zeroshot detection. The methodology involves network architecture design object localization attribute prediction and objectness prediction with detailed descriptions of loss functions. The paper presents experimental results discussing dataset splitting testing outcomes and performance analysis.  Strengths of the Paper  Innovative Approach: The paper addresses the practical need for zeroshot object detection in a dynamic manufacturing setting emphasizing the importance of flexibility in robot vision systems.  Methodological Rigor: The methodology is welldetailed including network architecture design object localization techniques attribute prediction objectness prediction and loss function formulations.  Experimental Results: The testing results provide insights into the effectiveness of the proposed approach showcasing recall rates for both seen and unseen objects and discussing factors affecting detection performance.  Novelty: The paper contributes a novel approach to zeroshot object detection specifically tailored for daily objects in indoor scenes filling a gap not extensively explored in existing literature.  Weaknesses of the Paper  Limited Evaluation Metrics: While recall rates are provided for assessment more comprehensive evaluation metrics like precision F1 score and objectspecific performance analysis could enhance the completeness of the experimental results.  Data Imbalance Concern: The unbalanced number of labels in the train and test datasets could introduce biases in the evaluation impacting the interpretation of the algorithms performance.  Generalization Challenges: The paper acknowledges limitations in the algorithms performance for objects with distinct attributes highlighting the need for further research to improve generalization across diverse object characteristics.  Questions and Feedback for the Author 1. Generalization Improvement: Have you considered strategies to enhance the algorithms generalization to objects with diverse attributes beyond color and shape similarities 2. Dataset Augmentation: How does the paper address challenges related to limited training data variety and illumination conditions that affect the algorithms performance on unseen objects 3. Comparative Analysis: Could the paper benefit from a comparative study with other stateoftheart zeroshot object detection methods to highlight the advantages of your proposed approach  Review Summary The paper presents a novel framework for zeroshot object detection in a manufacturing environment demonstrating a practical solution for industrial robots to handle varying objects without frequent retraining. While the methodology is robust and the experimental results provide valuable insights addressing concerns regarding data imbalances generalization capabilities and comparison with existing approaches would strengthen the papers impact. Overall the paper contributes significantly to the field of zeroshot object detection emphasizing the importance of adaptability and flexibility in robot vision systems for highmixlowvolume production scenarios.", "wogsFPHwftY": "Summary: The paper proposes a novel architecture for deep image retrieval called FIRe which introduces Superfeatures that are constructed by an iterative attention module. These Superfeatures offer a more discriminative and less redundant alternative to traditional local features. The paper introduces a training framework based on contrastive loss directly applied to Superfeatures eliminating the discrepancy between training and testing. Experimental results on landmark retrieval benchmarks show that Superfeatures outperform stateoftheart methods while requiring less memory. Strengths: 1. Innovative Approach: The paper introduces a novel architecture that addresses the issues of redundancy and inconsistency between training and testing phases in deep image retrieval. 2. Performance: Experimental results validate that Superfeatures substantially outperform existing methods while requiring less memory indicating the effectiveness of the proposed approach. 3. Extensive Evaluation: The paper provides thorough evaluations on landmark retrieval tasks demonstrating the superiority of the proposed method over stateoftheart approaches. 4. Clear Explanations: The paper provides detailed explanations of the proposed architecture training framework loss functions and the rationale behind key design choices making it accessible for readers to grasp the concepts. 5. Comparative Analysis: Through ablations and comparisons with existing methods the paper effectively showcases the benefits and improvements of the FIRe approach providing a comprehensive assessment. Weaknesses: 1. Evaluation Datasets: It would be beneficial to include a discussion on the limitations and biases of the benchmark datasets used and how they might influence the generalizability of the results. 2. Theoretical Insights: While the practical results are impressive deeper theoretical insights into why Superfeatures outperform traditional local features could enhance the papers impact. Questions to the Author: 1. Could you elaborate on potential realworld applications of Superfeatures beyond landmark retrieval Does the method have applications in other domains or tasks 2. How robust is the proposed approach to variations in image content such as different lighting conditions perspectives or complex backgrounds 3. Have you considered scenarios where landmark images may contain multiple landmarks within the same image How does the method handle such cases 4. How sensitive is the performance of the method to variations in hyperparameters such as the number of iterations or templates in the attention module Feedback to the Author: 1. The paper would benefit from addressing the weaknesses highlighted to enhance the robustness and completeness of the research. 2. Providing additional insights into the interpretability of Superfeatures and how they capture specific image patterns could further enrich the paper. Review Summary: The paper presents a novel architecture for deep image retrieval introducing Superfeatures as a more discriminative and less redundant alternative to traditional local features. The proposed approach demonstrates superior performance on landmark retrieval benchmarks while requiring less memory. Despite some minor weaknesses the papers innovative approach comprehensive evaluations and clear explanations make it a strong contribution to the field. Therefore I recommend this paper for acceptance given its significant advancements and compelling results in deep image retrieval methodology.", "u6ybkty-bL": " Summary: The paper explores the use of recurrent deep learning methods for time series outlier detection compared to nonrecurrent methods. It investigates various algorithms on synthetic and realworld datasets including a dataset monitoring people with dementia. The paper introduces new outlier detection methods based on transformer models and selfattention mechanisms. Evaluations are conducted using F1 score and ROC curves. Results indicate that recurrent methods perform better in detecting complex temporal relationships while nonrecurrent methods excel in scenarios without these relationships.  Strong Points: 1. Comprehensive Overview: The paper provides a detailed exploration of recurrent vs. nonrecurrent methods for time series outlier detection covering various algorithms and datasets. 2. Novel Contributions: The introduction of new outlier detection methods based on transformer models and selfattention mechanisms is a significant contribution to the field. 3. Thorough Evaluations: The use of F1 score and ROC curves for evaluation provides solid metrics for comparing the performance of different algorithms on various datasets. 4. RealWorld Relevance: The application of outlier detection to healthcare monitoring datasets specifically for people with dementia adds practical relevance to the research.  Weak Points: 1. Lack of Comparative Analysis: While the paper extensively discusses recurrent and nonrecurrent methods more direct comparisons between the two approaches could enhance the clarity of results. 2. Overemphasis on Experimentation: The paper dedicates substantial content to detailing the methods and datasets potentially overshadowing the discussion of key findings and implications. 3. Need for Clarity on Model Selection: Given the range of algorithms evaluated providing a clear rationale for selecting specific models for comparison would improve reader understanding. 4. Limited Discussion of Hyperparameters: While some discussion on hyperparameters is included a deeper analysis of their impact on model performance could enhance the paper\u2019s rigor.  Question to Author: 1. Could you elaborate on the specific challenges faced during the implementation of the transformerbased outlier detection methods and how these challenges were addressed 2. How do you envision the practical deployment of the outlier detection models in realworld settings especially in the context of healthcare monitoring for individuals with dementia  Feedback to the Author: The paper presents valuable insights into the application of recurrent deep learning methods for outlier detection in time series data. Enhancing the clarity of comparative analyses providing additional insights on hyperparameter selection and discussing practical deployment scenarios would further strengthen the research.  Overall Recommendation: The paper represents a significant contribution to the field of time series outlier detection particularly in healthcare monitoring. With some enhancements in emphasizing key findings and addressing specific methodological challenges it has the potential to be a valuable resource for researchers in the domain.", "qpcG27kYK6z": " Summary of the Paper: The paper introduces a novel approach for learning 3D representations of point clouds using concentric spherical grids and convolutional neural networks. The proposed method includes intrasphere and intersphere convolutions designed for rotationally equivariant feature learning. The paper demonstrates the effectiveness of this approach in tasks such as 3D object classification and resolving the electronic structure of atomistic systems.  Strengths: 1. Innovative Approach: The proposed concentric spherical representation and the combination of intrasphere and intersphere convolutions represent an innovative approach to learning 3D representations. 2. Rotationally Equivariant Design: The emphasis on rotationally equivariant convolutions is a strong point addressing a key challenge in learning representations for arbitrary orientations. 3. Experimental Validation: The paper provides comprehensive experimental validation showcasing the effectiveness of the proposed method in 3D object classification and molecular modeling tasks. 4. Comprehensive Background and Related Work: The inclusion of a detailed background on related work and the need for rotationally equivariant models adds credibility to the research context.  Weaknesses: 1. Complexity: The paper involves intricate mathematical formulations and technical details that may pose a challenge for readers not wellversed in the specific domain. 2. Evaluation Metrics: The evaluation metrics used in the experiments could be further elaborated to provide a clearer understanding of the performance comparison with existing methods. 3. Ablation Studies: While the paper includes some ablation studies deeper analysis and discussion of these results could enhance the understanding of the models key components.  Questions: 1. Could you elaborate on how the proposed concentric spherical representation compares to existing methods in terms of computational efficiency and memory requirements 2. How does the proposed method perform when dealing with noisy or incomplete point cloud data Have there been any experiments to evaluate the robustness of the model in such scenarios 3. In the context of molecular modeling how scalable is the proposed approach when dealing with larger atomic systems or complex molecular structures  Feedback to the Author: 1. Provide more insights into the scalability of the proposed approach especially when dealing with larger datasets or more complex structures. 2. Consider expanding the discussion on potential applications beyond 3D object classification and molecular modeling to showcase the versatility of the proposed method. 3. Enhance the explanations and visual aids to make the complex technical details more accessible to a broader audience.  Review Summary: The paper introduces a novel approach for learning 3D representations of point clouds using concentric spherical representations and rotationally equivariant convolutions. The method demonstrates promising results in 3D object classification and molecular modeling tasks showcasing its effectiveness in addressing the challenge of learning representations for arbitrary orientations. While the paper presents an innovative and wellmotivated solution improvements in explaining the technical details and expanding the discussion on scalability and robustness would further enhance its impact. Considering the strengths in innovation and experimental validation with further refinements in clarity and depth of analysis I recommend acceptance of this paper for publication.", "yrD7B9N_54F": " Summary: The paper addresses the issue of imbalanced data in link prediction for graphs with multiple domains proposing an adversarial trainingbased framework to improve prediction outcomes. The framework focuses on learning domainagnostic graph embeddings using adversarial learning and enclosing subgraphs inspired by fewshot learning in computer vision. The authors conduct experiments on three benchmark datasets and compare their results with stateoftheart methods demonstrating better performance in underrepresented domains.  Strong Points: 1. Innovative Approach: The use of adversarial training for domainagnostic graph embedding is a novel and effective solution to the imbalanced data problem in link prediction. 2. Clear Description: The paper provides a detailed explanation of the proposed methodology including problem definition notations domains and link prediction specifics making it easy to understand the technical aspects. 3. Comprehensive Experimental Analysis: The comparison with baseline methods and the detailed results analysis on different datasets provide a strong basis for evaluating the proposed framework\u2019s performance and effectiveness. 4. Visualization and Interpretation: The use of tSNE plots to visualize the impact of the proposed method on graph embeddings adds value by offering insights into the distribution of embeddings across domains.  Weak Points: 1. Lack of RealWorld Application: The paper would benefit from including practical applications or case studies where the proposed framework could be implemented to solve realworld link prediction problems. 2. Simplification for NonExperts: Certain sections with technical details could be simplified for readers who may not be familiar with advanced concepts in graph neural networks and adversarial learning.  Questions to the Author: 1. How transferable do you believe the proposed framework is to other link prediction tasks beyond the datasets used in your experiments 2. Have you considered the scalability of your approach when applying it to larger graphs with diverse domains and imbalanced data distributions 3. Could you elaborate on any limitations or potential challenges in implementing your adversarial trainingbased framework in practical scenarios  Feedback to the Author: Overall the paper presents a wellthoughtout approach to address imbalanced data in graph link prediction through adversarial training. Consider providing more insights into the practical implications of your framework and discussing scalability aspects for wider adoption. Clarifying the generalizability and any limitations of your methodology would further enhance the significance of the research findings. Great work on the thorough experimental analysis and visualization explanations these aspects greatly contribute to the clarity and effectiveness of your paper.", "wqD6TfbYkrn": "1) Summary of the Paper: The paper proposes a novel Point DiffusionRefinement (PDR) paradigm for enhancing 3D point cloud completion. This paradigm comprises a Conditional Generation Network (CGNet) utilizing a denoising diffusion probabilistic model (DDPM) to generate a coarse completion based on partial observations and a ReFinement Network (RFNet) to refine the output. The dualpath architecture of both networks effectively extracts multilevel features from partially observed point clouds for guided completion and manipulates spatial locations to achieve uniform generation and sharp details. The proposed method outperforms existing stateoftheart methods in terms of point cloud completion quality and acceleration of the generation process. 2) Strong and Weak Points Questions and Feedback: Strong Points:  Novel Paradigm: The PDR paradigm introduces a unique approach combining DDPM with RFNet for improved point cloud completion showcasing better uniformity and detail preservation.  DualPath Architecture: The dualpath design of CGNet and RFNet helps in effective feature extraction and spatial manipulation leading to highquality completions.  Experimental Results: Extensive experiments demonstrate significant performance gains over stateoftheart methods both in terms of quality and efficiency. Weak Points:  Efficiency Concerns: While the proposed method accelerates the DDPM generation process a notable performance drop is observed. Further optimization for maintaining performance under acceleration could be explored.  Inference Speed: It might be beneficial to address the inefficiency in the inference phase of DDPM to boost realtime applicability. Question to the Author:  Have you considered further improving the efficiency of the DDPM for faster inference without compromising performance besides the current acceleration method Feedback to the Author:  Expanding on the efficiency improvement aspect could enhance the applicability of the method especially in realtime applications.  Providing additional insights into potential tradeoffs between speed and quality under accelerated DDPM could strengthen the methods utility. 3) Review Summary: The paper presents a novel PDR paradigm for point cloud completion that leverages DDPM and RFNet to achieve uniform and highquality completions. The dualpath architecture along with innovative modules enhances feature extraction and spatial manipulation for superior results. Despite efficiency challenges in the inference phase and acceleration methods causing performance drops the proposed method outperforms existing stateoftheart methods. Further research on efficiency improvements while maintaining performance and additional insights into inference speed enhancements would strengthen the methods applicability. Overall the paper offers significant contributions to the field of point cloud completion.", "gggnCQBT_iE": " Peer Review:  1. Summarize the Paper: The paper introduces a novel framework called the metastructural causal model (metaSCM) to address unresolved theoretical problems especially in the context of cyclic causal relationships. The framework is based on understanding causality as information transfer and introduces the concept of active mechanisms to connect data and underlying causal mechanisms. It aims to provide a new approach to modeling cyclic causal relations proposing the sufficient activated mechanisms assumption. The paper also discusses the relationship between the framework and existing hypotheses in causal inference and learning.  2. Evaluation: Strong Points:  Conceptual Innovation: The introduction of the metaSCM framework and active mechanisms concept presents a novel approach to addressing cyclic causal relationships filling a theoretical gap.  Theoretical Depth: The discussion on acyclic SCMs cyclic SCMs multiple solutions and unsolvable SCMs showcases a robust theoretical foundation.  Clear Presentation: The paper is wellstructured and easily navigable with rigorous definitions provided for key concepts and hypotheses. Weak Points:  Clarity on Implementation: While the theoretical concepts are wellexplained there could be a clearer indication of how the metaSCM framework can be practically applied in realworld scenarios.  Empirical Validation: The paper is theoretical in nature lacking empirical validation or practical case studies to demonstrate the effectiveness of the proposed metaSCM framework. Questions and Feedback to the Author: 1. Practical Application: How do you envision the metaSCM framework being applied in empirical research or realworld scenarios Are there plans for empirical validation of the framework 2. Comparison to Existing Models: How does the metaSCM framework compare to existing causal modeling frameworks in terms of computational complexity and ease of implementation 3. Generalizability: Do you believe the metaSCM framework is generalizable across different types of datasets and domains or are there specific limitations to consider  3. Review Summary: The paper presents a highly innovative and conceptually sound framework in the form of the metastructural causal model (metaSCM) to address challenges in cyclic causal relationships. The introduction of active mechanisms and the sufficient activated mechanisms assumption provide a fresh perspective on causal inference and learning. The theoretical depth and clarity of discussions on acyclic and cyclic SCMs are commendable. However the paper lacks empirical validation and clarity on practical implementation. Despite these limitations the novel approach and theoretical contributions make the paper valuable to the research community interested in causal modeling. Future work could focus on empirical validation and practical applications to enhance the impact of the proposed framework.  Overall Recommendation: Given the novelty and theoretical depth of the proposed metaSCM framework I recommend the paper for acceptance with minor revisions to address issues related to practical application and empirical validation. The theoretical contributions and clarity of discussions make this work valuable for researchers in the field of causal inference and modeling.", "rdBuE6EigGl": "Peer Review of \"A Dual Architecture for Language Modeling\" 1. Summary: The paper introduces a novel dual architecture for language modeling where a direct connection between the input and output is added bypassing the recurrent module. The study compares the performance of this dual architecture against the standard models on the Penn Treebank and WikiText2 datasets. Results demonstrate that the dual architecture consistently outperforms the standard models even with parameters tuned for the standard architecture. Experimentation with a simplified mogrifier LSTM within the dual architecture sets a new stateoftheart perplexity score for the Penn Treebank dataset. The paper concludes by highlighting the potential of the dual architecture in enhancing sequence modeling tasks beyond language processing. 2. Strengths:  Innovative Approach: The introduction of the dual architecture presents a novel and effective way to improve language modeling performance.  Comprehensive Experiments: The paper conducts thorough experiments on multiple datasets and recurrent architectures providing detailed results and comparisons.  StateoftheArt Performance: Achieving a new stateoftheart perplexity score on the Penn Treebank dataset indicates the effectiveness of the proposed dual architecture.  Theoretical Grounding: The paper explains the rationale behind the dual connection and references previous work to support the research direction. 3. Weaknesses:  Implementation Specifics: While the paper mentions not including several standard techniques used in stateoftheart models a comparison incorporating these techniques could provide a more comprehensive evaluation.  Scalability Concerns: The scalability of the proposed dual architecture to larger datasets should be addressed or discussed in future work.  Model Complexity: The paper could elaborate more on the complexity implications of incorporating the dual connection and its impact on training time and computational resources. 4. Questions and Feedback:  Generalization: How well do you believe the results obtained on moderatescale datasets would generalize to larger datasets commonly used in language modeling  Comparison Metrics: In future work would you consider incorporating additional evaluation metrics beyond perplexity to provide a more holistic assessment of model performance  Impact of Hyperparameters: How sensitive is the performance of the dual architecture to different hyperparameter settings especially with respect to scaling up to larger datasets 5. Review Summary: The paper presents an innovative dual architecture for language modeling that consistently outperforms standard models on moderatescale datasets. By setting a new stateoftheart perplexity score and highlighting the practical benefits of the dual connection the research contributes significantly to advancements in sequence modeling approaches. However further exploration especially regarding scalability and comprehensive model comparisons would strengthen the findings. Overall based on the theoretical grounding comprehensive experiments and promising results I recommend acceptance of this paper with minor revisions to address the highlighted weaknesses and questions for future investigations.", "voEpzgY8gsT": " PeerReview of \"Additive Poisson Process: Modeling HigherOrder Interaction Effects in Poisson Processes using LowerDimensional Projections\"  Abstract: The paper introduces the Additive Poisson Process (APP) a novel framework to model higherorder interaction effects of intensity functions in Poisson processes by utilizing lowerdimensional projections. The model combines techniques from information geometry and generalized additive models to tackle the challenge of sparsity in multidimensional intensity function estimation. The empirical results demonstrate the models effectiveness in estimating higherorder intensity functions with sparse observations.  Strong Points: 1. Innovative Framework: The APP introduces a novel approach to modeling higherorder interactions in Poisson processes by leveraging lowerdimensional projections. This technique addresses the limitation of sparsity in estimating multidimensional intensity functions. 2. Theoretical Background: The paper provides a comprehensive theoretical background linking the proposed model to information geometry and generalized additive models. The KolmogorovArnold representation theorem is effectively utilized to justify the models effectiveness in learning from lowerdimensional projections. 3. Empirical Results: The experimental results showcase the models superiority over existing approaches in learning higherorder interactions in both synthetic and realworld data scenarios. The models robustness to varying sample sizes is a significant advantage. 4. Optimization Method: The optimization technique utilizing the information geometric structure for parameter estimation is well explained. The use of the natural gradient for convex optimization adds credibility to the proposed models efficacy.  Weak Points: 1. Comprehensibility: The technical nature of the paper might be challenging for readers without a strong background in mathematics and statistics. Simplifying explanations or visual aids could enhance the papers accessibility to a wider audience. 2. Scalability: While the model demonstrates promising results in experiments up to fourthorder processes the scalability to higherorder processes should be discussed. Addressing potential computational challenges could enhance the models applicability in more complex scenarios. 3. Comparison with StateoftheArt: The paper does not include a detailed comparative analysis with stateoftheart models in terms of computational complexity and performance metrics other than loglikelihood. Providing a more thorough comparison could strengthen the models credibility.  Question to the Authors:  Can you elaborate on the potential challenges or limitations in scaling the APP model to higherorder interactions beyond fourthorder processes  Feedback to the Authors: The paper presents a significant advancement in modeling higherorder interactions in Poisson processes. To further enhance the papers impact it is recommended to enhance the readability for a broader audience provide a more detailed comparison with existing stateoftheart models and discuss future directions for scalability in handling higherorder interactions. Overall the Additive Poisson Process demonstrates promising results and has the potential to contribute significantly to the field of intensity function estimation in Poisson processes.  Recommendation: Based on the innovative framework solid theoretical foundation and impressive empirical results I recommend accepting this paper after addressing the minor weaknesses highlighted above. The Additive Poisson Process can significantly contribute to the field and provides a valuable advancement in modeling higherorder interactions in Poisson processes.", "t5s-hd1bqLk": "Summary of the Paper: The paper introduces a novel approach to neural network conditioning by learning intermediate layer activations based on a conditioning vector. Traditional methods of conditioning neural networks involve input concatenation or modulation with the conditioning vector leading to increased model size. In contrast the proposed method referred to as the learned activation (LA) functions achieves effective conditioning without significantly increasing model parameters making it suitable for ondevice deployment. The study specifically explores the application of the LA approach in personalized sound enhancement (PSE) and personalized automatic speech recognition (PASR) tasks showcasing competitive performance with reduced model sizes compared to traditional conditioning methods. Strengths of the Paper:  Innovative Approach: The paper introduces a novel approach to neural network conditioning by learning intermediate layer activations based on conditioning vectors which offers a resourceefficient solution for ondevice deployment.  Comprehensive Evaluation: The study systematically explores and evaluates the proposed LA approach in the context of personalized sound enhancement and personalized ASR showcasing competitive performance while reducing model sizes.  Technical Depth: The paper provides detailed explanations of the methodology including mathematical formulations and visual representations enhancing the understanding of the proposed approach.  Comparative Analysis: The paper compares the LA approach with traditional conditioning techniques highlighting its effectiveness in terms of model size reduction without compromising on performance. Weaknesses of the Paper:  Limited Generalization: The evaluation is limited to specific tasks and datasets (PSE and PASR) which may restrict the generalization of the proposed approach to other application domains.  Availability of Clean Enrolment Data: The reliance on clean enrolment data for speaker embeddings in the evaluation poses a limitation as realworld deployment scenarios may not always have access to such data. Question and Feedback to the Author: 1. Generalization: Have you considered testing the proposed LA approach on a broader range of tasks or datasets to assess its applicability across various domains 2. Robustness: How does the LA approach perform when clean enrolment data is not available considering the common challenge of data availability in realworld scenarios 3. Scalability: How scalable is the LA approach in terms of training complexity and convergence properties when applied to larger and more complex neural network architectures Review Summary: The paper presents an innovative approach to neural network conditioning through learned activation functions based on conditioning vectors. The methodology is wellexplained and the results demonstrate competitive performance with reduced model sizes compared to traditional conditioning techniques. However further testing and validation on diverse tasks and datasets are recommended. Overall the paper contributes a valuable and resourceefficient solution for ondevice deployment in sequencetosequence modeling tasks making it suitable for consideration in a broader range of application domains. Recommendation: Based on the thorough evaluation and innovative methodology presented in the paper it is recommended for acceptance with minor revisions to address the limitations identified and improve the generalizability of the findings.", "v3LXWP63qOZ": " 1) Summary: The paper introduces MODINV a method for learning minimal representations in neural networks. MODINV utilizes multiple predictors over a common representation to enhance generalization performance. By independently training predictor heads with different learning rates MODINV aims to avoid spurious correlations and achieve better generalization. The method is evaluated in both reinforcement learning and vision settings showcasing performance boosts in tasks like DeepMind Control Suite and CIFAR10STL10 datasets.  2) Strengths and Weaknesses Questions and Feedback: Strong Points:  The paper addresses the important issue of learning efficient representations for better generalization.  MODINVs approach of incorporating multiple predictors over a shared representation is innovative and conceptually sound.  The method is clearly explained making the implementation straightforward.  Experimental results demonstrate significant performance improvements in both reinforcement learning and vision tasks. Weak Points:  While MODINV shows improved generalization a more indepth analysis of the computational overhead or scalability aspects could be beneficial.  The comparison with other stateoftheart methods could be expanded to provide a more comprehensive evaluation.  Future directions for applying MODINV in more challenging scenarios or domains could enhance the papers impact. Questions:  How does MODINV handle situations with highly complex and overlapping spurious correlations Are there cases where the method might struggle to disentangle such correlations effectively  Could you elaborate on the computational efficiency of MODINV compared to other representation learning methods especially in scenarios involving largescale datasets or complex tasks Feedback:  Consider elaborating on the significance of choosing specific learning rates for the multiple predictor heads in MODINV and how these choices impact the learning process and final performance.  Providing insights into how MODINV can be extended or adapted for different types of tasks or environments could further enhance the methods applicability and versatility.  3) Review Summary: The paper introduces an intriguing method MODINV for learning minimal representations through the utilization of multiple predictors over a shared representation. The method demonstrates promising results in reinforcement learning and vision settings showcasing performance improvements in various tasks. Despite the methods simplicity and effectiveness in enhancing generalization addressing scalability comparing with a broader range of methods and exploring additional applications or extensions could enhance the relevance and impact of MODINV further. Overall the paper presents a valuable contribution to representation learning and warrants further research in optimizing and extending the MODINV method for various domains and scenarios.", "y_tIL5vki1l": " Summary of the Paper: The paper introduces LatentKeypointGAN a twostage GAN framework that enables image content control through a set of space keypoints. The model disentangles image content into spatial and appearance factors without domain knowledge or supervision signals. The key contributions include a GANbased methodology for image manipulation and keypoint detection a keypoint generator that models dependent and independent factors explicitly an empirical study comparing different editing methods in terms of perceptual quality and a new methodology for keypoint detection challenging established autoencoder techniques. The paper showcases the ability of LatentKeypointGAN to generate interpretable latent spaces and enable editing operations like moving switching and adding image parts.  Strong Points: 1. Innovative Approach: LatentKeypointGAN introduces a novel GANbased framework for image manipulation and keypoint detection addressing the challenge of controlling image content without prior domain knowledge or supervision. 2. Disentanglement of Image Factors: The model successfully disentangles images into spatial and appearance factors allowing users to manipulate images interactively and artistically. 3. Interpretability and Editability: LatentKeypointGAN provides an interpretable latent space for rearranging generated images through keypoint embeddings enabling local editing operations. 4. Novelty in Key Detection: The paper presents a novel methodology for unsupervised keypoint detection challenging existing autoencoderbased techniques showcasing the potential of LatentKeypointGAN in keypoint localization.  Weak Points: 1. Limited Generalization: The paper provides results on specific datasets and may benefit from demonstrating the models performance on a wider range of datasets to assess generalization. 2. Performance Evaluation: While the paper presents user studies and comparisons with existing methods a more detailed performance evaluation including benchmark datasets and metrics could enhance the papers empirical strength.  Questions and Feedback: 1. Can you provide insights into the scalability and generalization of LatentKeypointGAN across various image domains beyond faces 2. How does LatentKeypointGAN handle challenges related to occlusions or diverse backgrounds in images 3. Could you elaborate on the computational efficiency and training stability of LatentKeypointGAN particularly when dealing with highresolution images or complex scenes 4. Are there any plans for realworld applications or deployments of LatentKeypointGAN and if so what considerations are being made for practical implementation  Review Summary: The paper presents an innovative GANbased framework LatentKeypointGAN addressing the challenge of image content control through spatial keypoints. The models ability to disentangle image factors and provide interpretable latent spaces for image manipulation is commendable. While the paper showcases novel contributions in keypoint detection and editing quality further exploration on generalization scalability and realworld applications could strengthen the research. Overall I recommend acceptance of the paper for its technical contributions and potential impact on image generation and manipulation research.", "v-v1cpNNK_v": " Summary of the Paper: The paper introduces a novel Neural Architecture Search (NAS) algorithm called NAS at Initialization (NASI) which eliminates the need for model training during the search process by leveraging the Neural Tangent Kernel to estimate the performance of candidate architectures at initialization. NASI aims to improve search efficiency while maintaining competitive search effectiveness across datasets like CIFAR10100 and ImageNet. The algorithm is demonstrated to be label and dataagnostic under mild conditions ensuring transferability of selected architectures.  Strong Points: 1. Novel Approach: NASI proposes an innovative method to conduct NAS without the need for model training during the search process thereby significantly enhancing search efficiency. 2. Theoretical Grounding: The paper provides theoretical insights into the use of Neural Tangent Kernel for characterizing the performance of candidate architectures and formulating NAS as an optimization problem without model training. 3. Empirical Validation: The algorithm is empirically validated on different search spaces and benchmark datasets demonstrating competitive search effectiveness.  Weak Points: 1. Complexity: The theoretical derivations and mathematical formulations presented in the paper may be challenging for readers without a strong background in neural network architecture and optimization. 2. Dataagnostic Claims: While the paper claims label and dataagnostic search more empirical evidence and analysis could be provided to support these assertions. 3. Limited Comparison: The comparison with other NAS algorithms could be further expanded to include a wider range of stateoftheart methods for a more comprehensive evaluation.  Questions and Feedback to the Author: 1. Could you provide more details on the practical implementation and computational requirements of NASI in realworld scenarios 2. How does NASI perform in terms of computational efficiency compared to other NAS algorithms 3. Further exploration of the implications and limitations of the label and dataagnostic search claim could strengthen the paper.  Review Summary: The paper introduces a promising NAS algorithm NASI which eliminates the need for model training during the search process leading to improved search efficiency. The theoretical grounding and empirical validation support the effectiveness of NASI across various datasets. While the paper offers an innovative approach with strong theoretical backing further empirical evidence and comparison with existing methods could enhance the comprehensiveness of the study. Overall based on the merits of the proposed algorithm and the presented results I recommend acceptance of the paper pending minor revisions to address the weak points highlighted.", "lP11WtZwquE": " Summary of the Paper: The paper introduces a novel approach to address the issue of false negatives in discriminative pretrained language models (PrLMs). The authors highlight that existing PrLMs train on corrupted texts without distinguishing whether the predictions are true negatives or false negatives leading to inefficiency and decreased robustness. To rectify this the authors propose enhanced pretraining methods that focus on training models on true negatives by correcting harmful gradient updates resulting from false negative predictions. Experimental results on GLUE and SQuAD benchmarks demonstrate the effectiveness of the proposed methods in enhancing performance and robustness of PrLMs.  Strong Points: 1. Novel Contribution: The paper offers a unique perspective by addressing the false negative issue in discriminative PrLMs a critical aspect that has been overlooked in previous research. 2. Experimental Validation: The proposed methods are tested on wellknown benchmarks and the results indicate significant performance improvements showcasing the effectiveness of the approach. 3. Methodological Approach: The use of soft regularization and hard correction methods to counteract false negatives demonstrates a sound theoretical foundation and practical implementation to improve pretraining objectives. 4. Comparative Analysis: The thorough comparison with baselines and detailed analysis of the proposed methods against existing strategies provide a clear understanding of the effectiveness of the new approach.  Weak Points: 1. Complexity: The paper delves into advanced concepts and methodologies making it challenging for readers without a deep understanding of natural language processing and pretrained models to grasp the intricacies. 2. Reproducibility: The detailed description of the proposed methods could be more streamlined for ease of implementation and replication by other researchers.  Questions and Feedback to the Author: 1. Did you observe any specific limitations or tradeoffs in the proposed methods that were not discussed in detail in the paper 2. How would you address potential scalability issues when implementing the proposed approach on largerscale language models 3. Could you provide more insights into how the proposed methods could be extended or applied to domainspecific pretraining tasks  Review Summary: The paper presents a wellmotivated and thoroughly researched approach to mitigate the false negative issue in discriminative PrLMs. The experimental results support the effectiveness of the proposed methods in enhancing the performance and robustness of PrLMs. The novelty methodological soundness and experimental validation make the paper a strong contribution to the field of NLP research. However addressing the complexity and reproducibility concerns could further enhance the impact and practical applicability of the findings. Overall based on the strong points and results presented I recommend acceptance with minor revisions to improve clarity and ease of implementation.", "zLb9oSWy933": "1) Summary The paper presents an indepth analysis of the compute and memory requirements for computing the Neural Tangent Kernel (NTK) in finite width neural networks. It introduces two novel algorithms that significantly improve the efficiency of NTK computation by leveraging the structure of neural networks. The paper demonstrates the computational benefits of these algorithms in fullyconnected and convolutional settings providing insights on reducing the time and memory costs associated with NTK computation. 2) Strengths and Weaknesses  Question and Feedback Strong Points:  Novel Algorithms: The paper introduces two novel algorithms that leverage the structure of neural networks to significantly improve the efficiency of NTK computation. This is a major contribution to the field of deep learning.  Detailed Analysis: The paper provides a detailed analysis of the compute and memory requirements for NTK computation in finite width networks addressing a crucial practical challenge in implementing NTKbased techniques.  Practical Applications: By demonstrating the efficiency gains of the proposed algorithms in various network architectures the paper highlights the practical implications of their work in improving the computational efficiency of NTK in realworld scenarios. Weak Points:  Complexity: The paper contains extensive technical details and equations that may be challenging for readers to grasp without a strong mathematical background. Providing more intuitive explanations or examples could enhance the accessibility of the material.  Comparison: While the paper discusses the computational benefits of the proposed algorithms a more comprehensive comparison with existing methods or benchmarks could strengthen the evaluation of the techniques.  Future Directions: It would be beneficial to discuss potential future research directions or extensions of the proposed algorithms to enhance the impact and relevance of the work. Questions for the Authors: 1. How do the proposed algorithms compare in terms of efficiency and computational cost with existing methods for NTK computation 2. Can you provide insights into the scalability of the algorithms to larger networks or more complex architectures beyond the scope of the study 3. Have you encountered any specific practical limitations or challenges in implementing the proposed algorithms in realworld scenarios and if so how were they addressed 4. What are the potential implications of your research findings on the broader applications of NTKbased techniques in deep learning and machine learning research Feedback for the Authors: 1. Consider providing more intuitive explanations or illustrative examples to aid readers in understanding the technical details and complexities of the proposed algorithms. 2. Enhance the discussion section by elaborating on the significance of the findings in the context of current trends and future developments in the field of deep learning and neural network optimization. 3) Review Summary Overall the paper makes a significant contribution by introducing novel algorithms that improve the efficiency of NTK computation in finite width neural networks. The detailed analysis of compute and memory requirements along with practical demonstrations in various network architectures underscores the importance of the proposed techniques. The papers strengths lie in its innovation and practical relevance although further comparative analysis and broader implications could enhance the impact of the research. With some improvements in clarity and depth of discussion the paper has the potential to make a valuable contribution to the field of deep learning. Given the importance of the topic and the novelty of the proposed algorithms I recommend acceptance of the paper with revisions to address the weak points highlighted above for further clarification and enhancement of the research findings.", "hyuacPZQFb0": "PeerReview: 1)  Summary: The paper introduces a benchmarking evaluation suite ADATIME aimed at facilitating the systematic and fair evaluation of domain adaptation methods on time series data. The work addresses existing limitations in time series domain adaptation such as inconsistent evaluation schemes usage of labeled target data for model selection and lack of general TSUDA methods. The proposed evaluation suite standardizes preprocessing backbone neural network architectures and model selection strategies. A comprehensive set of experiments is conducted to evaluate 10 stateoftheart domain adaptation methods on four representative datasets across 20 crossdomain scenarios. The results reveal valuable insights such as the competitiveness of visual domain adaptation methods on time series data the effectiveness of model selection strategies without target labels and the influence of backbone network architectures on performance. 2) Strengths and Weaknesses: Strong Points:  Novel Contribution: The introduction of the ADATIME evaluation suite fills a critical gap in time series domain adaptation research.  Comprehensive Evaluation: The paper conducts extensive experiments across multiple datasets and scenarios providing thorough insights into the performance of domain adaptation methods.  Realistic Model Selection: The exploration of model selection strategies without target domain labels adds practical relevance to the evaluation process.  Standardization: Standardizing preprocessing and backbone architectures enhances the fairness and comparability of the evaluation results.  Insightful Findings: The paper uncovers valuable insights on the performance of visual UDA methods the impact of model selection approaches and the significance of backbone network architectures. Weak Points:  Imbalance Handling: The implications of class imbalance on model performance could be further explored and discussed in more detail.  Limited Comparison: The comparison between visual UDA methods and TSUDA methods could have been further elaborated with additional analyses or comparisons.  Model Complexity: The complexity of the backbone network architectures and their impact could be discussed in relation to computational efficiency and scalability. Questions and Feedback to the Author: 1. Could you elaborate on how ADATIME can be extended to handle more diverse and complex time series datasets beyond the ones considered in this study 2. How would the proposed model selection strategies perform under extreme data scarcity scenarios such as very few labeled samples available for target domain adaptation 3. Have you considered the computational overhead associated with the proposed model selection approaches particularly with larger datasets and more complex model architectures 3) Review Summary: The paper presents a significant contribution to the field of domain adaptation in time series data with the introduction of the ADATIME evaluation suite. The rigorous evaluation of stateoftheart domain adaptation methods across diverse datasets and scenarios provides valuable insights into the performance factors and practical considerations in time series UDA. The standardization of evaluation procedures exploration of realistic model selection strategies and comparison between visual UDA and TSUDA methods demonstrate the papers methodological rigor and relevance to the research community. Overall the work is wellstructured comprehensive and offers practical recommendations for future research directions. Considering the strengths of the paper in addressing critical challenges in time series domain adaptation and producing insightful findings I recommend acceptance for publication after addressing the minor concerns raised. Recommendation: Accept with Minor Revisions.", "tge0BZv1Ay": " 1. Summary of the Paper The paper proposes a novel method based on Deep Reinforcement Learning for automated production scheduling in semiconductor manufacturing systems. It introduces the Predictron Deep QNetwork (PDQN) which combines the Deep QNetwork (DQN) and predictron methods to develop dynamic scheduling policies through interaction with simulated stochastic manufacturing systems. The study experiments with simulated systems based on a complex Western Digital semiconductor plant and demonstrates that PDQN outperforms existing dispatching policies like Critical Ratio and FirstInFirstOut for minimizing lateness of parts.  2. Evaluation of the Paper Strong Points:  Innovative Approach: Introducing a novel method that combines DQN and predictron for scheduling in semiconductor manufacturing is innovative and potentially impactful.  Detailed Explanations: The paper thoroughly explains the methodology including the MDP modeling action space reward signal and the PDQN algorithm.  Experimental Results: The study provides comprehensive experimental results comparing PDQN with existing policies demonstrating its superiority in minimizing lateness in simulated environments.  Related Work: The paper effectively discusses and compares the proposed method with existing research in the field of Deep RL and production scheduling. Weak Points:  Validation: The paper mentions a large variance in the performance of models after training which could potentially impact the generalizability and consistency of the proposed approach.  Additional Objectives: The study discusses the possibility of integrating lateness and throughput as an objective function but does not explore this further in the experimental evaluation.  Hyperparameter Optimization: While hyperparameters are provided the paper could benefit from further discussion or experimentation on optimizing these parameters for better model performance. Question to the Authors:  Can you provide more insights on how the large variance in model performance posttraining can be addressed to ensure robust and consistent results  Have you considered exploring the integration of additional objectives such as throughput alongside lateness in the scheduling optimization process Feedback to the Authors:  Consider addressing the issue of model variance posttraining to ensure the reliability and reproducibility of results.  Further investigate the integration of additional objectives to enhance the optimization process and potentially improve overall system performance.  3. Review Summary The paper presents an innovative approach utilizing Deep Reinforcement Learning for automated production scheduling in semiconductor manufacturing systems showing improved performance over traditional dispatching policies. Despite some issues regarding model performance variance and unexplored additional objectives the study provides valuable insights and promising results in advancing scheduling algorithms in complex manufacturing environments. Additional work on ensuring model consistency and exploring multiobjective optimization could further enhance the applicability and robustness of the proposed method. Overall the paper makes a significant contribution to the field of production scheduling and Deep RL applications in manufacturing.", "mhv2gWm3sf": " Peer Review of \"fTVO: Generalized Thermodynamic Variational Objective\"  1) Summary of the Paper: The paper introduces a novel concept called f divergence Thermodynamic Variational Objective (f TVO) that extends the Thermodynamic Variational Objective (TVO) by replacing the Kullback\u2013Leibler (KL) divergence with an arbitrary differentiable f divergence. This new framework f TVO approximates the dual function of the model evidence f\\xe2\\x88\\x97(p(x)) rather than the log model evidence log p(x) in TVO. It is derived from a deformed \\xcf\\x87geometry perspective and integrates f TVO along the \\xcf\\x87path which is the deformed geodesic between the variational posterior distribution and the true posterior distribution. The authors demonstrate the effectiveness of f TVO through experiments on Variational Autoencoders (VAE) and Bayesian neural networks showing superior performance compared to existing f divergence variational inference methods.  2) Strengths and Weaknesses: Strengths:  Innovative Contribution: The introduction of f TVO as a generalization of TVO is a novel and significant contribution to the field of variational inference.  Theoretical Foundation: The paper provides a strong theoretical foundation by connecting f divergence with \\xcf\\x87geometry and demonstrating the effectiveness of f TVO through rigorous derivations.  Experimental Validation: The experiments on VAE and Bayesian neural networks provide concrete evidence of the superiority of f TVO over existing f divergence variational inference methods.  Clarity and Organization: The paper is wellstructured with clear explanations of concepts and thorough sections detailing the proposed framework. Weaknesses:  Lack of Comparative Analysis: While the experimental results show the effectiveness of f TVO a more detailed comparison with other stateoftheart methods could provide a clearer evaluation.  Optimization Strategy: The paper briefly mentions the optimization scheme of f TVO using reparameterization trick and Monte Carlo approximation but could benefit from a more detailed discussion on the computational efficiency and scalability of the proposed method.  Generalization Scope: While the paper highlights the flexibility and effectiveness of f TVO discussing the limitations or potential challenges in practical applications would enhance the papers depth. Questions and Feedback to the Authors: 1. Can you elaborate on the computational complexity and scalability of f TVO in largescale applications especially in scenarios with highdimensional data 2. How does the performance of f TVO compare against other advanced variational inference methods such as BlackBox Variational Inference (BBVI) or Importance Weighted Autoencoders (IWAE) 3. Have you considered exploring the interpretability of f TVO in capturing complex data distributions or in applications where model transparency is crucial  3) Review Summary: The paper \"fTVO: Generalized Thermodynamic Variational Objective\" presents a novel framework that extends Thermodynamic Variational Objective (TVO) with f divergence termed f TVO to improve variational inference. The paper is wellstructured providing a comprehensive theoretical basis and demonstrating the effectiveness of f TVO through experiments. The innovative contribution experimental validation and the sound theoretical foundation make this work a valuable addition to the field of variational inference. However the paper could benefit from a more extensive comparative analysis detailed discussion on optimization strategies and potential generalization challenges to further enhance its impact.  Recommendation: Based on the innovative contributions strong theoretical foundation and promising experimental results I recommend accepting this paper with minor revisions to address the weaknesses highlighted above. Further clarification on optimization strategies comparative analysis and consideration of potential challenges would strengthen the papers impact and relevance in the field.", "l8It-0lE5e7": " Summary: The paper provides theoretical insights into the implicit bias imposed by adversarial training for deep neural networks. It analyzes the behavior of both linear and nonlinear homogeneous deep neural networks when trained adversarially showing convergence properties towards maximizing the margin for adversarial examples. The paper highlights the difference in optimization objectives between adversarial training and standard training leading to improved robustness against adversarial examples.  Strong Points: 1. Theoretical Rigor: The paper provides a comprehensive theoretical analysis of adversarial training for deep neural networks offering valuable insights into its implicit bias. 2. Generalization: The study addresses both deep linear networks and more generalized nonlinear networks broadening the applicability of the findings. 3. Tradeoff Analysis: The analysis of the tradeoff between robustness and accuracy through adversarial training is a significant contribution. 4. Experimental Validation: The numerical experiments on the MNIST dataset support the theoretical claims made in the paper providing empirical evidence for the results.  Weak Points: 1. Complexity: The theoretical framework presented may be challenging for readers without a strong background in deep learning and optimization theory. 2. Accessibility: The paper assumes familiarity with terms like adversarial examples marginmaximization and KKT points which may limit the accessibility to readers from varied backgrounds. 3. Clarity: Some sections especially regarding technical proofs and mathematical formulations may benefit from more detailed explanations or examples to aid comprehension.  Questions and Feedback: 1. Can you provide further insights into the practical implications of the findings in realworld scenarios where deep neural networks are commonly deployed 2. How do the results of this study compare or complement existing research on adversarial training and its implications on network robustness 3. Have you considered exploring the computational efficiency and scalability of the proposed theoretical framework for larger and more complex neural network architectures 4. Is there additional work planned to validate the theoretical findings through more extensive experiments on diverse datasets and network configurations  Review Summary: The paper presents a thorough theoretical analysis of the implicit bias in adversarial training for deep neural networks encompassing both linear and nonlinear models. The findings regarding margin maximization for adversarial examples provide valuable insights into enhancing robustness through adversarial training. While the theoretical rigor and broad generalization are commendable the complex nature of the analysis and terminology choice may limit accessibility. Additional clarity in the presentation of technical details and potential further experiments for validation could strengthen the impact of this study. Overall the paper contributes significantly to the understanding of adversarial training dynamics and its implications on network robustness making it a valuable contribution to the field of deep learning security and optimization.", "p7LSrQ3AADp": " Summary of the Paper: The paper introduces a framework for characterizing and comparing saliency methods which are used to understand the importance of input features in machine learning models. The framework consists of nine dimensions grouped into three categories: methodology sensitivity and perceptibility. It challenges the notion of \"faithfulness\" in saliency methods and instead presents a more granular approach to evaluating these techniques. The authors illustrate the framework through case studies in radiology settings and identify opportunities for future research in evaluating saliency methods.  Strengths of the Paper: 1. Comprehensive Framework: The paper presents a thorough and wellstructured framework that breaks down saliency methods into nine dimensions providing a detailed understanding of their characteristics. 2. Practical Application: The use of a radiology setting as a case study demonstrates the practical relevance of the framework and highlights how it can guide decisionmaking in realworld scenarios. 3. Innovative Perspective: By reevaluating the traditional focus on faithfulness in saliency methods and proposing a different approach the paper offers a fresh perspective that challenges existing notions. 4. Clear Illustrative Examples: The paper provides clear and illustrative examples to help readers grasp the implications and applications of each dimension within the framework. 5. Identification of Future Research Directions: The paper identifies gaps in current research and suggests avenues for future exploration within the field of evaluating saliency methods contributing to the advancement of interpretability in machine learning.  Weaknesses of the Paper: 1. Lack of Formalization: While the framework is descriptive and intuitive it would benefit from a more formal treatment to provide a rigorous foundation for future research and development. 2. Limited Empirical Validation: The paper could benefit from more extensive empirical validation to demonstrate the effectiveness and applicability of the proposed framework in diverse settings. 3. Clarity on Implementation: The paper could provide more guidance on how researchers and practitioners can practically implement the framework in evaluating and selecting saliency methods for specific tasks.  Questions and Feedback to the Authors: 1. Could you elaborate on the process of validating the framework and how it was applied in other domains or tasks beyond the radiology setting 2. How do you envision researchers leveraging this framework to develop new saliency methods that prioritize specific attributes highlighted in the dimensions 3. Have you considered potential limitations or tradeoffs in using the framework especially in scenarios where certain dimensions conflict with the objectives of a specific application  Review Summary: The paper introduces a comprehensive and novel framework for characterizing and comparing saliency methods providing a valuable contribution to the field of machine learning interpretability. The frameworks clear structure and practical application make it a useful tool for researchers and practitioners seeking to evaluate and choose saliency methods for various tasks. While the paper is strong in its conceptualization and practical examples further validation and formalization could enhance its impact. Therefore I recommend the paper for acceptance with the potential for minor revisions to strengthen its applicability and rigor.", "gLqnSGXVJ6l": " PeerReview of \"Solving the Vehicle Routing Problem with Time Windows using Neural Networks and Reinforcement Learning\"  1. Summary: The paper proposes a framework for solving the Vehicle Routing Problem with Time Windows (VRPTW) using neural networks and reinforcement learning. The approach is based on an attention model trained in a reinforcement learning environment to predict nearoptimal solutions for different instances of the problem. The model is tested on synthetic data and outperforms existing baselines in terms of solution quality and computation time.  2. Evaluation:  Strong Points: 1. Innovativeness: The integration of neural networks and reinforcement learning for solving the VRPTW is a novel and promising approach. 2. Comprehensive Model Architecture: The detailed description of the encoderdecoder architecture provides a clear understanding of the proposed solution. 3. Training Procedure: The use of the REINFORCE algorithm for training the model in a stochastic policy gradient manner is well explained. 4. Experimental Results: The experimental results show the effectiveness of the proposed method in outperforming existing baselines in terms of solution quality and computation time.  Weak Points: 1. Clarity of Presentation: The paper contains technical jargon and complex equations that may pose a challenge for readers not wellversed in neural networks and reinforcement learning. 2. Validation: While the experimental results are promising a more indepth validation with realworld data or comparison with more diverse benchmarks could strengthen the study. 3. Scalability: The scalability of the proposed approach to larger problem instances and realtime applications should be discussed further. 4. Discussion of Limitations: It would be valuable for the authors to discuss the limitations of the proposed framework such as computational complexity and potential overfitting.  Questions and Feedback: 1. Generalization: How well does the proposed model generalize to unseen data and more complex problem instances 2. Scalability: Have you tested the scalability of the model to larger problem sizes beyond what was mentioned in the experiments 3. Sensitivity Analysis: Have you conducted sensitivity analysis on hyperparameters or model architecture variations to optimize the performance further 4. Comparative Evaluation: Could you provide a more detailed comparison with existing stateoftheart methods in solving VRPTW to showcase the uniqueness and advantages of your approach  3. Review Summary: The paper presents a novel framework for solving VRPTW using neural networks and reinforcement learning showcasing promising results in terms of solution quality and computation time. The comprehensive model architecture and detailed training procedure demonstrate the effectiveness of the proposed approach. However addressing the clarity of presentation scalability to larger problem instances and conducting more extensive validation and comparative studies would further strengthen the study. Overall the paper provides valuable insights into the application of machine learning techniques for solving combinatorial optimization problems.  Recommendation: Based on the innovative approach detailed methodology and promising experimental results I recommend accepting the paper with revisions addressing the weak points highlighted. The study has the potential to contribute significantly to the field of combinatorial optimization and machine learning.", "xMJWUKJnFSw": "Summary: The paper introduces NodePiece a novel approach for learning parameterefficient node embeddings in knowledge graphs. Drawing inspiration from subword tokenization in NLP NodePiece constructs a fixedsize entity vocabulary from anchor nodes and relation types in the graph. By tokenizing nodes into a sequence of anchor nodes anchor distances and relational context NodePiece allows for efficient node representation with reduced memory consumption and computational costs. Experimental results demonstrate that NodePiece achieves competitive performance in node classification link prediction and relation prediction tasks while using fewer parameters and retaining competitive accuracy compared to traditional shallow embedding methods. Strengths: 1. Innovative Approach: NodePiece introduces a novel strategy for learning parameterefficient node embeddings in knowledge graphs drawing inspiration from subword tokenization in NLP. 2. Efficiency: NodePiece significantly reduces memory consumption and computational costs compared to traditional shallow embedding methods making it more scalable for largescale knowledge graphs. 3. Competitive Performance: Experimental results show that NodePiece performs competitively in node classification link prediction and relation prediction tasks while using fewer parameters and retaining competitive accuracy. 4. Versatility: NodePiece demonstrates versatility across various tasks including transductive inductive outofsample link prediction and node classification tasks. 5. Reproducibility: The paper provides the source code openly available on GitHub along with detailed hyperparameters implementation details and dataset information for reproducibility. Weaknesses: 1. Metric Evaluations: While the paper provides a comprehensive evaluation of NodePieces performance more indepth analysis and comparisons with additional baseline methods could provide a better understanding of its effectiveness. 2. Ablation Studies: Further ablation studies could help in understanding the impact of different components of NodePiece on its performance in different tasks. Questions: 1. Scalability: How does the performance of NodePiece scale with the size of knowledge graphs particularly with respect to extremely large graphs such as Wikidata 2. Generalization: How does NodePiece perform in scenarios with noisy or incomplete graphs and what is its robustness to changes in graph structure Feedback: 1. Consider providing more detailed explanations on the impact of node piece size configurations on performance in different tasks. 2. It would be beneficial to explore the interpretability of node embeddings generated by NodePiece and how they capture semantic relationships in the knowledge graph. Review Summary: The paper introduces a novel approach NodePiece for learning parameterefficient node embeddings in knowledge graphs. With reduced memory consumption and computational costs NodePiece demonstrates competitive performance in various tasks showcasing its potential as a scalable and efficient representation learning method. Additional experiments thorough ablation studies and further analysis on scalability and generalization could enhance the comprehensiveness of the evaluation. The openness of the source code and detailed reproducibility information contribute to the papers credibility. Overall based on the innovative concept efficiency and promising results I recommend acceptance of the paper with minor revisions to address the suggested points.", "mmUA7_O9mjY": " Peer Review  1) Summary: The paper introduces CPDeform a framework that integrates optimal transportbased contact discovery into differentiable physics to address challenges faced by gradientbased solvers in soft body manipulation tasks. CPDeform addresses issues related to finding optimal initial contact points and contact switching during multistage tasks. The approach is evaluated on PlasticineLabM a dataset with multistage soft body manipulation tasks showcasing improved performance compared to baseline methods.  2) Strengths:  Innovative Approach: The integration of optimal transportbased contact discovery into differentiable physics is novel and addresses critical challenges in soft body manipulation.  Extensive Evaluation: The authors conduct thorough experiments on a new dataset PlasticineLabM demonstrating the effectiveness of CPDeform on a variety of multistage soft body manipulation tasks.  Clear Explanations: The paper provides clear explanations of the motivation method and experimental results making it easy to follow the logic and understand the proposed approach.  3) Weaknesses:  Evaluation on Real Robots: While the method is evaluated extensively in simulation it would strengthen the paper if there were realworld experiments on physical robots to demonstrate the practical applicability of CPDeform.  Comparison with StateoftheArt: More comparative analysis with stateoftheart methods in soft body manipulation could provide a better context for the performance of CPDeform.  Scalability: The scalability of CPDeform to more complex and realworld scenarios could be elaborated upon to understand its limitations and potential future directions.  Question to the Authors:  How does CPDeform handle uncertainties in soft body dynamics such as varying material properties or unexpected deformations  Feedback to the Authors:  Consider including real robot experiments to validate the performance of CPDeform in physical environments.  Provide a more detailed comparison with existing stateoftheart methods in soft body manipulation to showcase the superiority of CPDeform.  4) Review Summary: The paper presents a novel approach CPDeform that integrates optimal transportbased contact discovery into differentiable physics to improve soft body manipulation tasks. The experiments on multistage tasks demonstrate the effectiveness of CPDeform in guiding the solver towards solving complex manipulation tasks. The paper is wellwritten logically structured and provides clear explanations. However further evaluations on physical robots and a deeper comparison with existing methods could enhance the impact of the work. Considering the innovative approach thorough evaluation and clarity of presentation I recommend acceptance of the paper after revisions based on the feedback provided to strengthen the experimental validation and contextualization of the proposed method.", "tD7eCtaSkR": " Summary: The paper introduces a novel procedure to certify the robustness of 1Lipschitz convolutional neural networks (CNNs) without requiring the last linear layer of the network to be orthogonal. The proposed method significantly advances the stateoftheart for both standard and provable robust accuracies on CIFAR100 and CIFAR10 datasets. Additionally the paper introduces a novel Gradient Norm preserving activation function called Householder activation function as well as a certificate regularization technique to further boost the networks robustness. The experiments demonstrate substantial improvements in both standard and provable robust accuracies across different 1Lipschitz CNN architectures.  Strengths: 1. Innovative Methodological Contributions:  The introduction of a certification procedure without orthogonalizing the last linear layer results in significant improvements in both standard and provable robust accuracies.  The proposed Householder activation function and certificate regularization techniques further enhance the robustness of 1Lipschitz CNNs. 2. Thorough Experimental Validation:  Extensive experiments on CIFAR100 and CIFAR10 datasets using various 1Lipschitz CNN architectures demonstrate the effectiveness of the proposed methods.  Detailed comparison with existing methods and baselines showcases the superiority of the proposed approach. 3. Theoretical Grounding:  The paper provides theoretical underpinnings for the proposed Householder activation functions and their necessity for ensuring continuity in piecewise linear functions.  Weaknesses: 1. Complexity:  The paper contains complex theoretical definitions and proofs which may hinder accessibility for readers with limited background knowledge in the domain. 2. Reproducibility:  While the paper states that code for reproducing experiments is available the complexity of the method and lack of detailed implementation instructions may pose challenges for replication by other researchers.  Questions: 1. How does the proposed method compare with other stateoftheart approaches for adversarial robustness on image classification tasks beyond CIFAR datasets  Feedback:  The paper provides significant advancements in enhancing the robustness of 1Lipschitz CNNs through innovative techniques. Clarifying the practical implications of the proposed approach and discussing the generalizability of the findings to diverse datasets and network architectures would further strengthen the impact of the work.  Overall Assessment: The paper presents a strong contribution in improving the robustness of 1Lipschitz CNNs through novel techniques and theoretical grounding. The thorough experimental validation and comparison with existing methods establish the effectiveness of the proposed approach. Addressing the identified weaknesses and providing further insights into the practical implications and generalizability of the findings would enhance the overall impact of the work. Based on the strengths and significance of the contributions I recommend this paper for publication after addressing the minor points raised.", "zxEfpcmTDnF": "1) Paper Summary: The paper discusses the application of deep generative models specifically variational autoencoders (VAEs) in learning and controlling the latent representations of speech signals based on the sourcefilter model of speech production. The study showcases that a VAE trained on natural speech signals can learn meaningful latent subspaces corresponding to fundamental frequency and formant frequencies. By using artificially generated labeled speech data the paper presents a method to analyze and control these factors of variation within the learned subspaces. The proposed approach enables the precise and independent modification of these speech factors leading to an interpretable method for generating speech signals. 2) Strengths and Weaknesses: Strong Points:  Innovative Approach: The paper introduces a novel method to interpret and control the latent space of a VAE in the context of speech production offering a unique perspective on disentangled representation learning.  Experimentation: The study conducts comprehensive experiments demonstrating the effectiveness of the proposed method in encoding and manipulating the fundamental frequency and formant frequencies of speech signals.  Interpretability: The approach enhances the interpretability of the generative model by connecting the sourcefilter model of speech production to the latent space of a VAE.  Applications: The paper explores potential applications in speech analysis transformation and synthesis which can have significant implications in fields such as speech processing and generation. Weak Points:  Validation: While the experiments are detailed and insightful the paper could benefit from additional validation on larger and more diverse datasets to further establish the generalizability of the proposed method.  Comparative Analysis: Although the paper compares its approach with existing methods a more extensive comparison with a broader range of techniques could provide a clearer benchmark for evaluating the proposed method.  Complexity: The method involves several steps and concepts that might be challenging for readers unfamiliar with deep learning or speech processing necessitating clearer explanations in some sections. Question and Feedback to the Author: 1. Question: Could you elaborate on the scalability of the proposed method to handle larger datasets and its potential adaptations to different languages or speech styles 2. Feedback: Providing more insights into the computational efficiency of the method and addressing potential limitations in realworld applications would strengthen the papers impact and practical relevance. 3) Review Summary: The paper presents an innovative approach to learning and controlling the latent representations of fundamental frequency and formant frequencies in speech signals using VAEs. The method exhibits promising results in encoding and manipulating these critical speech factors within the learned subspaces. The experiments conducted demonstrate the efficacy of the proposed method emphasizing its potential applications in speech analysis and synthesis. Despite the strengths of the paper in introducing a novel perspective on disentangled representation learning in speech processing further validation on larger datasets and a more extensive comparative analysis would enhance the credibility and utility of the proposed approach. Recommendation: I recommend accepting the paper with minor revisions to address the weak points highlighted primarily focusing on validation and comparative analysis to strengthen the findings and implications of the study.", "uF_Wl0xSA7O": "1. Summary of the Paper: The paper presents a novel gradientbased multitask learning (MTL) approach that aims to balance training in multitask systems by aligning the independent components of the training objective. The proposed method addresses the challenge of properly balancing different tasks in MTL by dynamically constructing consistent losses and handling cases where there is a large difference in task gradients. The paper provides theoretical analysis introduces new metrics like the dominance rate and demonstrates the effectiveness of the approach on various MTL benchmarks including digit classification image classification camera relocalization and scene understanding. 2. Evaluation of Strong and Weak Points: Strong Points:  The paper introduces a novel MTL approach that addresses the challenge of imbalanced task gradients in a systematic and theoretically grounded manner.  The proposed method is scalable reduces overfitting and performs favorably compared to stateoftheart MTL methods as demonstrated in various experiments.  The paper includes a comprehensive analysis of the method along with a theoretical foundation making the proposed approach robust and wellsupported.  The inclusion of experiments on multiple benchmarks across different domains strengthens the validity and applicability of the proposed method.  The paper provides a reproducibility statement including proofs code package and plans for opensourcing the implementation on GitHub enhancing the transparency and reproducibility of the research. Weak Points:  The paper could further elaborate on the limitations and potential challenges of the proposed approach especially in scenarios where assumptions may not hold in practical applications.  More insights or discussions on how the method could be extended or modified for specific types of multitask learning problems would enhance the completeness of the paper. Questions for the Author: 1. Can you elaborate on the computational complexity implications of the proposed method for largerscale MTL problems especially regarding the number of tasks and iterations required 2. How does the proposed approach handle scenarios where the assumptions around gradients and model architectures may not hold and what impact does it have on the convergence and performance 3. Have you considered exploring realworld applications or datasets with more complex task dependencies or varying levels of correlation for further validation of the proposed approach Feedback for the Author:  Provide a more thorough discussion on the scalability and generalizability of the approach particularly when applied to diverse and complex MTL problems.  Address the potential limitations or drawbacks of the proposed method and suggest future directions for research to enhance the robustness and applicability of the approach.  Consider discussing scenarios where the proposed approach could complement or be combined with existing MTL methods or frameworks for enhanced performance. 3. Review Summary: The paper presents a novel gradientbased multitask learning approach that aims to balance training in multitask systems by aligning orthogonal components of the training objective. The proposed method is supported by theoretical analysis and empirical results showcasing its effectiveness across various MTL benchmarks. The paper covers strong points such as scalability reduced overfitting and superior performance compared to existing methods. Efforts to enhance reproducibility through the provision of proofs a code package and plans for opensourcing the implementation are commendable. However the paper would benefit from further elaboration on potential challenges and extensions of the proposed method as well as a more nuanced discussion on the applicability of the approach to diverse multitask scenarios. Addressing these points would strengthen the completeness and impact of the research. Overall the paper presents a valuable contribution to the field of multitask learning and is recommended for publication with minor revisions and enhancements.", "pLNLdHrZmcX": " Peer Review  1) Summary: The paper proposes SANE an endtoend ensemble learning method that actively enforces model specialization by training models to specialize in specific simple distributions and aggregating them based on their specialties. The method aims to improve ensemble performance by guiding base models to focus on subregions in the latent space representing the data distribution composition. Extensive experiments on tabular and image datasets show superior performance over stateoftheart ensemble methods.  2) Strengths and Weaknesses: Strong Points:  The paper addresses an important issue of explicit model specialization in ensemble learning which is not adequately covered in existing methods.  The proposed method is well described and the motivation behind enforcing specialization is clearly articulated.  The experimental results demonstrate the effectiveness of the proposed method in improving ensemble performance across various datasets.  The inclusion of ablation studies adds depth to the analysis and provides insights into the impact of different components in the model. Weak Points:  The paper could benefit from a more detailed discussion on the scalability and computational efficiency of the proposed method especially in larger datasets or complex realworld scenarios.  While the paper discusses the effectiveness of the proposed method compared to existing techniques a more indepth discussion on the limitations and potential failure modes of the proposed method could provide a more comprehensive view.  The experimental setup and hyperparameters used need to be presented more clearly for reproducibility purposes. Questions and Feedback to the Author: 1. Can you provide more insights into how the proposed method would scale to larger datasets or more complex data distributions and how it might perform in scenarios with highdimensional data 2. Could you elaborate on any potential limitations or failure modes of the proposed method that have not been explicitly discussed in the paper 3. It would be beneficial to include a more detailed discussion on the computational efficiency and training time required for the proposed method especially in comparison to existing ensemble methods.  3) Review Summary: The paper presents a novel approach SANE aiming to enhance ensemble learning by actively enforcing model specialization. The method effectively addresses the issue of explicit specialization which is often overlooked in ensemble methods. The experimental results demonstrate superior performance over stateoftheart methods particularly in tabular and image datasets. The inclusion of ablation studies further strengthens the analysis highlighting the importance of different components in the proposed method. However the paper could benefit from a more comprehensive discussion on scalability potential limitations and computational efficiency. Overall the proposed method shows promise in improving ensemble learning performance and should be further explored and validated in diverse realworld scenarios. In conclusion I recommend this paper for publication after addressing the above feedback and enhancing the discussion on scalability limitations and computational efficiency of the method.  End of Review ", "fwJWhOxuzV9": " Summary of the Paper The paper introduces Pretrained Decision Transformer (PDT) a new algorithm for semisupervised offline reinforcement learning (RL). The focus is on leveraging large rewardfree datasets for pretraining RL agents and then finetuning them on small rewardannotated datasets for specific tasks. PDT extends the Decision Transformer (DT) model masking reward tokens during pretraining and predicting rewardtogo during finetuning. The approach is demonstrated on tasks from the D4RL benchmark showing promising results in limited reward annotations scenarios.  Strong Points: 1. Innovative Approach: The paper addresses an important gap in RL by introducing a novel semisupervised offline learning paradigm. 2. Technical Contributions: The introduction of PDT and the extension of DT for offline RL showcase significant technical advancements. 3. Applicability: The demonstrated effectiveness of PDT on tasks from the D4RL benchmark indicates its potential for practical applications. 4. Experimental Validation: The paper includes comprehensive experiments and ablation studies to provide a thorough evaluation of the proposed approach.  Weak Points: 1. Clarity in Methodology: The paper can benefit from better explaining the technical details of the PDT algorithm and how it compares to existing RL methods. 2. Evaluation Metrics: While the paper discusses results on various datasets a more indepth discussion on the performance metrics and benchmarks utilized would strengthen the evaluation. 3. Ablation Studies: The ablation studies mentioned can be elaborated further to provide deeper insights into the impact of various components of the PDT algorithm.  Question to the Author:  How does the performance of PDT compare with stateoftheart RL algorithms in scenarios with abundant reward annotations  Can you elaborate on the scalability and computational efficiency of PDT when applied to largescale RL problems with diverse reward functions  Feedback to the Author:  Consider enhancing the clarity of the method description and results presentation for better understanding by readers.  Further explore the implications of PDT in realworld applications beyond the D4RL benchmark to showcase its versatility and generalizability.  Review Summary: The paper presents a valuable contribution to the field of RL by introducing PDT a novel algorithm for semisupervised offline RL. While the technical advancements and experimental results are promising the paper could benefit from improved clarity in explaining the methodology and a more comprehensive evaluation discussion. Overall the innovative approach and demonstrated efficacy of PDT in lowdata scenarios support a positive recommendation for publication pending addressing the mentioned suggestions.", "psh0oeMSBiF": "Summary: The paper introduces a new framework called COPA to certify the robustness of offline reinforcement learning (RL) against poisoning attacks focusing on certification criteria such as perstate action stability and cumulative reward bound. The framework includes policy partition and aggregation protocols as well as certification methods for different aggregation protocols. The paper conducts thorough experimental evaluations on different RL environments and algorithms to demonstrate the effectiveness of COPA. Strengths: 1. Innovative Framework: The paper introduces a novel framework COPA that addresses the important issue of certifying the robustness of offline RL against poisoning attacks. 2. Theoretical and Practical Contributions: The paper makes significant theoretical contributions by proposing certification criteria and methods as well as practical contributions by conducting experiments on different RL environments with various algorithms. 3. Comprehensive Evaluation: The experimental evaluations cover different aspects of the proposed framework including perstate action stability and cumulative reward bound using different RL algorithms and environments. 4. Thorough Explanation: The paper provides detailed explanations of the proposed framework including partition and aggregation protocols certification criteria and the certification process. The appendix includes additional details for interested readers. 5. Novel Aggregation Protocols: The introduction of different aggregation protocols such as PARL TPARL and DPARL enhances the diversity of the proposed framework. Weaknesses: 1. Complexity in Certification Methods: The certification methods for different aggregation protocols may have high time complexity or lack theoretical tightness as indicated in the paper. This complexity may limit the scalability of the framework. 2. Limited Practical Implementation Details: While the paper discusses theoretical aspects in detail more insights into practical implementation challenges and considerations could provide a more holistic view of the framework. 3. Manuscript Length: The extensive theoretical and technical details while beneficial for comprehensiveness may make the paper challenging to digest for readers not deeply familiar with the subject matter. Questions and Feedback for the Authors: 1. Can you provide insights into how the proposed certification methods could be practically implemented and scaled to larger datasets or more complex RL environments 2. How does the framework handle cases where the poisoning attacks are more sophisticated or targeted beyond the discussed scenarios 3. Considering the NPcompleteness and theoretical challenges in certification are there ongoing efforts to simplify the methods or find more efficient solutions for realworld applications 4. How do you envision the adoption of COPA in industry applications of RL especially in highstakes domains where robustness is critical such as autonomous driving or healthcare Review Summary: The paper presents a novel framework COPA for certifying the robustness of offline RL against poisoning attacks addressing critical challenges in the field. With innovative certification criteria thorough experimental evaluations and detailed theoretical contributions the paper demonstrates the effectiveness of COPA in enhancing the certified robustness of RL policies. While facing complexity and scalability challenges in certification methods the papers comprehensive approach and theoretical insights make it a valuable contribution to the research community. Considering the strength of the proposed framework and the authors contributions I recommend acceptance after addressing the minor weaknesses and providing more practical insights.", "rzvOQrnclO0": " 1) Summary of the Paper: The paper focuses on addressing the issue of model accuracy in modelbased reinforcement learning through a twomodelbased learning approach. The authors begin by analyzing the impact of model gradient errors on policy optimization convergence rates and propose a solution based on theoretical results. They introduce the directional derivative projection policy optimization (DDPPO) algorithm as a practical implementation of their proposed method. The paper presents empirical results demonstrating the effectiveness of their approach in improving sample efficiency in benchmark continuous control tasks.  2) Evaluation of the Paper:  Strong Points:  Theoretical Analysis: The paper provides a detailed theoretical analysis of how model errors specifically gradient errors impact policy optimization algorithms in modelbased reinforcement learning.  Proposed Method: The twomodelbased learning approach along with the DDPPO algorithm provides a novel and practical solution to mitigate the effects of model errors in reinforcement learning.  Empirical Results: Empirical results showcase the effectiveness of the proposed algorithm demonstrating better sample efficiency compared to stateoftheart methods on benchmark tasks.  Experimental Design: The paper conducts experiments on both simple and benchmark continuous control environments offering thorough evaluations of their proposed method.  Weak Points:  Implementation Details: While the paper provides a highlevel overview of their proposed algorithm more details on the implementation specifics hyperparameters and algorithm convergence would benefit readers.  Comparative Analysis: Although the paper compares its method to existing modelbased and modelfree algorithms a more indepth comparative analysis with additional stateoftheart methods could enrich the discussion.  Reproducibility: Providing more detailed information on the experiments including hyperparameters training procedures and runtime specifics would enhance reproducibility for the research community.  Questions to the Authors: 1. Could you provide more insights into the hyperparameters used in your experiments such as learning rates batch sizes and other key parameters 2. How scalable is your proposed method to more complex environments or different types of reinforcement learning tasks 3. Have you considered conducting experiments on largerscale environments to further evaluate the scalability and generalizability of your approach  Feedback to the Authors:  Considering the significant impact of model errors in reinforcement learning the paper presents a promising approach to address this challenge. Enhancing the reproducibility and providing more detailed implementation specifics would strengthen the papers contribution to the research community.  3) Review Summary: The paper offers valuable theoretical insights into the impact of model gradient errors on policy optimization algorithms in modelbased reinforcement learning. The proposed twomodelbased learning approach implemented through the DDPPO algorithm demonstrates significant improvements in sample efficiency on benchmark continuous control tasks. While the paper presents a strong contribution with promising empirical results further elaboration on implementation details comparative analyses and reproducibility aspects would enhance the papers overall impact. Therefore considering the innovative approach theoretical depth and empirical success I recommend the paper for acceptance after minor revisions to address these points.", "qEGBB9YB31": "Peer review of the paper: \"A ParameterSpace Perspective on Neural Network Errorneous Decisions\" 1) Summary of the Paper The paper introduces a novel approach to saliency maps by highlighting network parameters rather than input features responsible for erroneous decisions in neural networks. The method involves identifying salient parameters through gradient analysis and shows that turning off these parameters leads to improved predictions. The paper details experiments validating the effectiveness of the parameter saliency maps and demonstrates the semantic similarity of samples with malfunctioning parameters. Additionally the paper introduces an inputspace saliency counterpart for visualizing how image features affect network components providing a methodology for understanding network failures. The paper includes thorough evaluation case studies and additional experiments on various datasets and architectures. 2) Strengths and Weaknesses: Strong Points:  Novel Approach: Introducing parameter saliency as an alternative to conventional saliency maps is innovative and offers a new perspective on explaining network decisions.  Experimental Validation: The paper provides thorough experimental validation of the proposed method on different datasets and architectures demonstrating the effectiveness of the approach.  Case Studies: The inclusion of detailed case studies adds depth to the findings and helps in understanding how the method works in practice.  Additional Experiments: The supplementary experiments and comparisons add value to the study showcasing the robustness and applicability of the proposed method. Weak Points:  Complexity of Method: The method and experiments described in the paper might be challenging for readers without an advanced understanding of neural networks and gradientbased techniques.  Limited RealWorld Application: The paper mainly focuses on image classification tasks limiting the generalizability of the method to other domains.  Lack of Comparative Analysis: While the paper introduces a novel method additional comparison with existing saliency techniques would enhance the evaluation and validation of the proposed approach. Questions and Feedback to the Author: 1. Clarification on Generalizability: Can the proposed parametersaliency method be applied to other domains beyond image classification such as natural language processing tasks or reinforcement learning problems 2. Scalability: How scalable is the method in terms of computational resources and processing time especially when dealing with larger datasets or more complex network architectures 3. Comparative Analysis: Have you considered comparing the proposed method with existing interpretability techniques like Lime or SHAP Including such comparisons could further establish the superiority of your approach. 3) Review Summary: The paper presents an innovative parametersaliency approach for understanding neural network errors which is wellsupported by thorough experimental validation and case studies. The method shows promise in providing valuable insights into misunderstood model behaviors. However clarification on generalizability scalability and comparative analysis with existing techniques would strengthen the papers impact and applicability. Overall based on the novelty of the method the comprehensive experimental validation and the potential impact on interpretability I recommend acceptance after addressing the suggested points for improvement.", "krI-ahhgN2": " Summary of the Paper: The paper introduces a novel supervised contrastive learning framework termed SelfContrastive (SelfCon) Learning which selfcontrasts within multiple outputs from different levels of a multiexit network. It eliminates the need for additional augmented samples resolving concerns related to multiviewed batch approaches. The authors demonstrate that SelfCon learning leads to an increase in the lower bound of labelconditional mutual information enhancing classification performance compared to crossentropy and Supervised Contrastive (SupCon) learning. The success of SelfCon learning is attributed to the regularization effect associated with singleview and subnetwork features.  Strong Points: 1. Innovative Approach: The concept of SelfContrastive learning is innovative and addresses key issues associated with traditional contrastive learning methods. 2. Theoretical Grounding: The paper provides theoretical justifications for the effectiveness of SelfCon loss in maximizing labelconditional mutual information. 3. Experimental Results: Extensive experiments conducted on various benchmarks support the superiority of SelfCon learning over existing methods like crossentropy and SupCon. 4. Efficiency and Generalization: The comparison between singleview and multiview approaches demonstrates the efficiency and improved generalization of SelfCon learning.  Weak Points: 1. Unsupervised Learning Applicability: The paper briefly touches on the unsupervised learning applicability of SelfCon but does not delve deep into this aspect. Further exploration could enhance the papers scope. 2. Complexity: The model architecture with multiple exit paths and subnetworks may increase complexity which could be a concern in practical applications. 3. Visualization: While the paper explains the methodology comprehensively additional visual aids such as diagrams could further enhance understanding especially in the complex network structures involved.  Questions to the Author: 1. How does the proposed SelfCon approach compare with stateoftheart unsupervised contrastive learning methods in terms of representation learning quality 2. Can the additional complexity introduced by the subnetwork in SelfCon potentially hinder scalability and deployment in realworld applications  Feedback to the Author: 1. Considering the comprehensive experimental analysis presented a comparison with more recent and diverse datasets could strengthen the robustness of the findings. 2. Providing visual aids such as network architecture diagrams or feature visualization plots could enhance the readers understanding of the proposed SelfCon framework.  Review Summary: The paper presents a novel supervised contrastive learning framework SelfCon which eliminates the need for augmented samples and outperforms existing methods in terms of classification accuracy and generalization. The theoretical justifications and extensive experimental results support the effectiveness of SelfCon learning. Addressing potential scalability concerns and further exploring unsupervised learning applicability would enhance the papers impact. Overall the innovative approach and strong empirical results make the paper worthy of publication pending minor updates and additional clarity on key aspects.", "oj2yn1Q4Ett": " Review of \"A Novel Framework for CommunicationEfficient Distributed Learning with Overparameterized Models\" 1. Summary: The paper introduces a novel framework for communicationefficient distributed learning with overparameterized models focusing on kernel learning problems. It proposes a multiagent kernel approximation technique that allows distributed estimation of the full kernel function without direct data or parameter exchange. The framework is compared to existing algorithms and shows reduced communication complexity for training models on UCI benchmarking datasets. The proposed algorithms achieve minimax optimal generalization performance with minimal communication. 2. Strengths:  Innovative Framework: The proposed framework deviates from classical consensusbased approaches and demonstrates advancements in communicationefficient distributed learning for overparameterized models.  Theoretical Rigor: The paper provides rigorous analyses on optimization and generalization performance clearly presenting the benefits of the proposed algorithms.  Communication Efficiency: The comparison with stateoftheart algorithms showcases the significant reduction in communication complexity achieved by the proposed algorithms.  Clear Algorithm Description: The algorithms are clearly outlined in the paper aiding in understanding the methodology adopted. 3. Weaknesses:  Practical Implementation: While the theoretical aspects are compelling the practical implementation and scalability of the proposed algorithms need more discussion.  Generalization Beyond Kernel Learning: The generalization of the proposed framework beyond kernel learning problems is not explored. Addressing potential extensions would strengthen the paper.  Evaluation Metrics: More detailed discussion on additional evaluation metrics or realworld use cases would enhance the practical relevance of the proposed framework. 4. Questions  Feedback:  Scalability: How does the proposed framework scale with larger datasets or more complex models beyond kernel learning problems  Comparative Analysis: Can you further elaborate on the specific performance advantages of the proposed algorithms over existing approaches in practical scenarios  Privacy Concerns: Given the focus on data privacy how does the proposed framework address potential privacy vulnerabilities in a distributed setting  Review Summary: The paper introduces a novel framework for communicationefficient distributed learning with overparameterized models offering theoretical advancements and reduced communication complexity. The strength lies in the innovative approach theoretical analyses and improved communication efficiency. However further discussion on practical scalability generalization beyond kernel learning and privacy considerations would enhance the papers impact. Overall with solid theoretical foundations and promising results the paper presents a valuable contribution to the field of distributed learning. Therefore I recommend acceptance after addressing the outlined feedback for improved clarity and practical insights.", "oVE1z8NlNe": "Summary: The paper conducts an indepth empirical study on federated selfsupervised learning (FedSSL) to address the nonindependently and identically distributed (nonIID) data problem of decentralized data. The authors propose a generalized framework for FedSSL and introduce a new method Federated Divergenceaware Exponential Moving Average update (FedEMA) to improve performance on nonIID data. Through extensive experiments the authors demonstrate that FedEMA outperforms existing methods by 34 on linear evaluation. Key findings include insights on the necessity of predictors the effectiveness of EMA and the importance of retaining local knowledge for better performance. Strong Points: 1. Clear Problem Statement: The paper effectively articulates the problem of learning representations from decentralized nonIID data and proposes a novel method to address the challenge. 2. Methodological Rigor: Extensive experiments ablation studies and comparisons with existing methods are conducted showcasing the thoroughness of the research. 3. Contributions: The paper makes significant contributions with the introduction of FedSSL framework insights on fundamental components and the proposal of FedEMA which results in notable performance improvements. 4. Insightful Analysis: The paper provides detailed insights on the impact of various components in FedSSL such as predictors EMA and the importance of retaining local knowledge enhancing the understanding of the proposed methods. Weak Points: 1. Complexity of Concepts: The paper presents advanced concepts such as federated learning selfsupervised learning and nonIID data which may be challenging for readers unfamiliar with these topics. 2. Clarity of Presentation: The dense technical information in the paper might be overwhelming for readers not deeply versed in the field potentially hindering comprehension. 3. Evaluation Metrics: While the paper demonstrates performance improvements with FedEMA a discussion on the computational complexity and scalability of the proposed method could enhance the completeness of the findings. Questions and Feedback to the Authors: 1. Can you provide more insights on the scalability and computational efficiency of FedEMA especially when dealing with largescale datasets and client populations 2. How do you envision implementing FedEMA in realworld applications where decentralized unlabeled data are prevalent What challenges do you foresee in practical implementations Review Summary: The paper presents a comprehensive exploration of FedSSL introducing a novel framework and algorithm (FedEMA) to enhance selfsupervised learning in decentralized nonIID data settings. The extensive experiments insightful analysis and performance improvements achieved with FedEMA demonstrate the strong contributions of the research. While the papers complexity and presentation clarity can be improved the significant findings and advancements in the field warrant publication. Therefore based on the methodological rigor innovative contributions and promising results I recommend accepting this paper for publication with minor revisions to improve clarity for a wider audience.", "ygGMP1zkiD1": "Summary: The paper proposes a debiasing method for pretrained text encoders to reduce social stereotypes encoded in the attention layer focusing on mitigating bias across different social groups. The approach redistributes attention scores to ensure equal attention to all social classes in input sentences minimizing semantic damage through knowledge distillation. The evaluation involves likelihood and inferencebased assessments to measure bias intensity and effectiveness in reducing stereotypes across gender race religion age and sexual orientation. Strengths: 1. Innovative Approach: The paper tackles an important issue of bias in modern text encoders by targeting the attention mechanism a novel and insightful approach. 2. Comprehensive Coverage: Addressing multiple bias types (gender race religion age sexual orientation) sets the study apart providing a holistic perspective on debiasing in NLP. 3. Experimental Rigor: The paper demonstrates the effectiveness of the proposed AttD method through rigorous evaluations across likelihoodbased inferencebased and representationbased benchmarks showcasing an indepth analysis of bias reduction. 4. Comparative Analysis: Conducting comparisons with existing methods like SentD and Kaneko  Bollegala helps contextualize the performance of AttD and highlights its superior outcomes across various bias types. Weaknesses: 1. Limited Clarity in Evaluation Metrics: While the likelihood and inferencebased evaluations provide insightful results the paper lacks clarity on the exact algorithms used for these assessments which could hinder reproducibility and thorough understanding. 2. Lack of Scalability Discussion: The paper focuses on debiasing specific predefined categories potentially limiting scalability to more complex or unknown bias types warranting discussion on the applicability beyond the identified social groups. 3. Assumption on Semantic Preservation: The methods reliance on knowledge distillation to preserve semantics may introduce limitations in scenarios where certain words inherently carry group associations that should not be equalized. Questions and Feedback: 1. Clarification on Evaluation Metrics: Could the paper provide detailed descriptions of the likelihood and inferencebased evaluation methodologies to ensure transparency and reproducibility 2. Scalability Considerations: How adaptable is the proposed method to handle diverse and evolving bias categories beyond the ones studied in this work and how might it address potential intersectionality in biases 3. Semantic Preservation Concerns: Considering challenging cases like inherently biased words how does AttD manage such cases to avoid unintentionally equalizing attention on sensitive terms Review Summary: The paper introduces a novel method AttD for debiasing text encoders by focusing on attention distribution across social groups. With a comprehensive evaluation showcasing superior performance over existing methods the study significantly contributes to advancing fairness in NLP models. Addressing multiple bias types and showcasing effectiveness through meticulous assessment the paper stands out for its innovative approach and rigorous experimental validation recommending its adoption in mitigating biases effectively across various social constructs.", "nO5caZwFwYu": " Summary of the Paper: The paper proposes an extension to active search a generic search strategy that adjusts the weights of a trained model at test time to improve the likelihood of generating highquality solutions in subsequent iterations. The proposed Efficient Active Search (EAS) reduces the time and memoryintensive nature of adjusting all model parameters by selectively updating only a subset of parameters during the search process. Three different implementations of EAS are evaluated on three combinatorial optimization problems  the Traveling Salesperson Problem (TSP) Capacitated Vehicle Routing Problem (CVRP) and Job Shop Scheduling Problem (JSSP). The paper demonstrates that EAS approaches outperform stateoftheart MLbased methods including heuristic solvers and significantly improve model generalization performance.  Strengths of the Paper: 1. Innovative Approach: The paper introduces an efficient modification to active search that significantly reduces runtime without compromising solution quality showcasing the importance of selective parameter updates in enhancing search procedures. 2. Comprehensive Evaluation: The proposed EAS implementations are thoroughly evaluated on multiple combinatorial optimization problems demonstrating superior performance over existing methods and showcasing their ability to generalize effectively to larger instances. 3. Clear Methodology: The paper provides a detailed description of the EAS strategies implemented including EASEmb EASLay and EASTab allowing for clear understanding and reproducibility of the proposed methods. 4. Comparison to StateoftheArt Techniques: The paper compares the EAS approaches against a variety of stateoftheart methods on challenging optimization tasks highlighting the competitive performance of the proposed strategies. 5. Ablation Study: The ablation study on the impact of the imitation learning loss provides insights into the effectiveness of the proposed approach in comparison to traditional active search methods.  Weaknesses of the Paper: 1. Resource Limitation: The paper focuses on GPUbased evaluations which may limit the practical applicability for researchers with only CPU resources or those working in resourceconstrained environments. 2. Complexity: The paper introduces multiple EAS implementations which while effective may increase the complexity for readers and implementation in realworld scenarios.  Questions for the Authors: 1. Can you provide more insights into the hyperparameter tuning process for the EAS strategies particularly how you determined optimal values for \u03bb \u03c3 and \u03b1 2. How do you think the proposed EAS approaches would perform on more complex realworld optimization problems with intricate constraints  Feedback for the Authors:  The paper is wellstructured and presents a novel approach to enhancing active search strategies for combinatorial optimization problems.  Providing a section on practical implications and potential realworld applications would enhance the relevance and impact of the proposed EAS implementations.  Consider discussing any potential limitations or challenges in implementing the EAS strategies in realworld optimization scenarios to provide a more balanced evaluation.  Review Summary: The paper introduces an innovative Efficient Active Search (EAS) approach to enhance active search strategies for combinatorial optimization problems. The proposed EAS implementations demonstrate superior performance outperforming stateoftheart methods and showcasing effective generalization capabilities. Despite some limitations in resourceintensive evaluations and complexity the clear methodology and comprehensive evaluations make the paper a valuable contribution to the field. Therefore I recommend acceptance after addressing minor concerns and potentially expanding on the practical implications of the proposed techniques.", "nL2lDlsrZU": "1) Summary of the Paper: The paper introduces SAINT (SelfAttention and INtersample attention Transformer) a specialized deep learning architecture tailored for tabular data. SAINT leverages selfattention and intersample attention mechanisms to provide insights into features from multiple samples in a tabular dataset. The paper also introduces a novel contrastive selfsupervised pretraining method along with an enhanced embedding approach for continuous features. Experimental results demonstrate that SAINT outperforms existing deep learning methods and even competes with traditional treebased algorithms like XGBoost CatBoost and LightGBM across various tabular datasets. 2) Strong Points:  Innovations: The introduction of intersample attention contrastive selfsupervised pretraining and an approach for embedding continuous features are notable innovations and contribute to improving model performance on tabular datasets.  Performance: Results show that SAINT consistently outperforms other deep learning methods and competes closely with established treebased algorithms across diverse tabular datasets in regression binary classification and multiclass classification tasks.  Interpretability: The paper discusses the interpretability of SAINT through attention visualization offering insights into how the model makes decisions and focuses on specific features and data points within a batch.  Experimental Evaluation: The extensive experimental evaluation including supervised and semisupervised settings across 30 benchmark datasets provides a comprehensive analysis of SAINTs performance and robustness to data corruptions noise and missing values.  Clarity: The paper is wellstructured and clearly explains the motivation methodology architecture and experimental results enabling both theoretical understanding and practical implementation of SAINT. Weak Points:  Comparison Metrics: While the paper provides information on the comprehensive experimental evaluation it would be beneficial to include more detailed comparisons with existing stateoftheart models in the field to provide a broader benchmark for SAINTs performance.  Scalability: The paper discusses the tradeoff between performance and computational complexity in SAINTi and SAINTs variants. Further discussion on the scalability of the proposed method especially for largescale datasets would enhance the papers applicability in realworld scenarios.  Implementation Details: Including implementation details such as code repositories pretrained models and hyperparameters used in the experiments would facilitate reproducibility and adoption of SAINT in practical applications. Questions and Feedback: 1. Can you provide more insights into how SAINTs performance scales with increasing dataset sizes and complexities How does the computational efficiency of SAINT compare with other deep learning and treebased methods on very large datasets 2. Could you elaborate on the interpretability of attention mechanisms in SAINT and how it can aid domain experts in understanding model decisions and feature importance 3. Have you considered extending the experimental evaluation to include scenarios where data distributions vary significantly to assess SAINTs robustness and generalization capabilities across heterogeneous datasets 3) Review Summary: The paper presents a comprehensive study on SAINT a specialized deep learning architecture designed for tabular data analysis. With innovations like intersample attention contrastive selfsupervised pretraining and enhanced continuous feature embedding SAINT exhibits competitive performance with traditional treebased algorithms and outperforms existing deep learning methods in regression binary classification and multiclass classification tasks across varied tabular datasets. The paper is wellstructured and presents a clear narrative on the architecture methodology and experimental results. However further exploration into scalability comparison metrics and interpretability aspects could enhance the impact and applicability of SAINT in practical scenarios. Overall the paper provides valuable contributions to the field of deep learning for tabular data analysis and is recommended for publication after addressing the mentioned suggestions for improvement.", "qiMXBIf4NfB": " Summary of the Paper: The paper presents a theoretical analysis of iterative selftraining on nonlinear neural networks focusing on the benefits of leveraging unlabeled data in improving learning convergence and generalization ability. It establishes the convergence and generalization improvements with the increasing number of unlabeled samples showcasing results for onehiddenlayer neural networks. The work addresses the challenges of characterizing selftraining in the semisupervised paradigm and provides analytical insights into its benefits compared to supervised learning.  Strong Points: 1. Groundbreaking Theoretical Analysis: This paper stands out for being the first to provide a theoretical study of iterative selftraining on nonlinear neural networks offering valuable insights into the impact of unlabeled data. 2. Quantitative Justification: The paper quantitatively justifies the generalization improvement brought by unlabeled data providing concrete evidence of the theoretical claims through empirical experiments on shallow and deep neural networks. 3. Clear Contributions: The contributions of the paper are welldefined and structured focusing on providing analytical justifications for the effectiveness of iterative selftraining in improving learning outcomes. 4. Detailed Technical Content: The theoretical results are supported by rigorous mathematical formulations aiding readers in understanding the complex analyses involved in characterizing selftraining algorithms.  Weak Points: 1. Limited Empirical Validation: While the paper mentions experiments conducted on neural networks more detailed results and analyses could enhance the validation of the theoretical claims and provide stronger support for the proposed methods. 2. Complexity of Content: The technical nature of the paper might make it challenging for nonexperts or readers not wellversed in neural network theory to grasp the finer details of the analyses.  Questions for the Author: 1. Can you provide more details on the experimental setup particularly the datasets used training parameters and performance metrics measured in your empirical evaluations 2. How do the results of your theoretical analysis align with existing empirical studies on semisupervised learning methods utilizing unlabeled data  Feedback for the Authors: 1. Consider expanding the empirical section to provide more detailed insights into how the theoretical findings translate into practical improvements in realworld scenarios. 2. Clarify any assumptions made in the theoretical analysis that might influence the generalizability of the results to broader neural network architectures or semisupervised learning tasks.  Review Summary: Overall this paper presents a significant theoretical advancement in the understanding of unlabeled data utilization in semisupervised learning via iterative selftraining on neural networks. The rigorous theoretical analysis supported by empirical experiments provides compelling evidence of the benefits of leveraging unlabeled data. However enhancing the empirical validation and addressing the complexity of the content could further strengthen the impact and accessibility of the research. Based on the thorough theoretical contributions and the promising empirical results I recommend the paper for publication pending minor revisions to address the weak points outlined above.", "xs-tJn58XKv": " Summary of the Paper: The paper proposes a method called Transfer OF Unstable features (TOFU) to improve the robustness of machine learning models in the transfer learning setting by explicitly informing the target classifier about unstable features from the source data. The authors derive a representation that encodes unstable features using source environments and cluster examples based on this representation. They apply group Distributionally Robust Optimization (DRO) to minimize the worstcase risk over these clusters. The method is evaluated on text and image classifications demonstrating its ability to maintain robustness in both synthetic and realworld environments.  Strengths of the Paper: 1. Innovative Approach: The paper introduces a novel method (TOFU) to address the challenge of bias by leveraging related tasks to improve the robustness of the target model. 2. Theoretical Foundation: The paper provides a strong theoretical foundation by connecting the prediction results of classifiers to unstable features and demonstrating how to learn feature representations that encode these latent unstable features. 3. Effective Experimental Evaluation: The authors conduct thorough experiments on both synthetic and realworld environments to validate the effectiveness of TOFU compared to several baselines. The results consistently show significant performance gains. 4. Clear and Detailed Explanations: The paper is wellstructured with clear explanations of the problem formulation method steps experimental setups and results. The process of inferring unstable features and learning stable correlations is welldescribed. 5. Visualization and Interpretation: The paper provides visualizations to help understand how TOFU identifies the unstable features and discusses the interpretability of the generated clusters in relation to hidden biases.  Weaknesses of the Paper: 1. Limited Complexity Consideration: The paper focuses on identifying and handling unstable features in a transfer learning setting which may limit its applicability to more complex scenarios involving multiple types of bias in a single task. 2. Generalization and Scalability: While the method shows promising results the generalizability of TOFU to diverse datasets and tasks needs further exploration. Scalability issues with larger datasets and more complex models may also need to be addressed.  Questions for the Author: 1. How does the proposed method perform when dealing with multiple types of bias in a single task Are there any plans to extend the approach to handle such scenarios 2. Have you considered the scalability of TOFU to larger datasets and more complex models Are there any limitations or challenges in applying the method to realworld datasets with higher dimensions 3. How sensitive is the method to hyperparameters and did you conduct sensitivity analysis to optimize the hyperparameter settings 4. Can you provide insights into how the method may be extended to address biases in other domains beyond text and image classifications  Feedback to the Author: The paper presents an innovative method for addressing bias in machine learning models using related tasks and transfer learning. The theoretical foundation experimental evaluation and visualizations provided enhance the clarity and effectiveness of the presentation. However further exploration of the methods scalability generalizability and capability to handle multiple types of bias may strengthen the impact and applicability of the approach.  Review Summary: The paper introduces a novel method TOFU for improving machine learning model robustness by leveraging related tasks and transfer learning to identify and address unstable features. While the paper demonstrates strong theoretical underpinnings and effective experimental results considerations for scalability generalizability and handling multiple types of bias could enhance the methods applicability across diverse datasets and tasks. Overall the paper presents a valuable contribution to the field of bias reduction in machine learning models.", "pQ02Y-onvZA": " Summary: The paper proposes \u03b42exploration an exploration strategy that extends Sample Average Uncertainty (SAU) from bandits to the general sequential Reinforcement Learning (RL) setting. \u03b42exploration does not rely on maintaining model posterior distributions making it computationally efficient. The authors empirically study \u03b42exploration in tabular and Deep Qlearning scenarios demonstrating its practical advantage and adaptability to complex reward models in modern RL. The proposed algorithm is evaluated on Qlearning in the cliffwalking task and Deep Q Network (DQN) in Atari 2600 games.  Strengths: 1. Innovative Approach: The extension of SAU to general RL settings is a novel and potentially impactful contribution to the field of reinforcement learning. 2. Efficiency and Flexibility: \u03b42exploration is highlighted for its efficiency simplicity of implementation and wide adaptability in various RL scenarios showcasing its potential as a competitive baseline exploration strategy. 3. Comprehensive Analysis: The paper provides a thorough analysis of the proposed exploration strategy including theoretical foundations algorithm details and empirical evaluations on both tabular and Deep Qlearning tasks. 4. Empirical Performance: Empirical results demonstrate the superior performance of the proposed \u03b42exploration strategies over conventional greedy action selection in both tabular Qlearning and DQN tasks.  Weaknesses: 1. Evaluation Scope: The evaluation primarily compares the proposed method against greedy strategy. Additional comparisons with more diverse stateoftheart exploration techniques could provide further insights into the effectiveness of \u03b42exploration. 2. Theoretical Discussion: While the paper offers a practical demonstration and empirical validations a deeper theoretical discussion on the mathematical underpinnings and convergence properties of \u03b42exploration would enhance the clarity and rigor of the proposed method.  Questions and Feedback to the Author: 1. Clarification on Evaluation Metrics: Could the authors provide more details on the specific evaluation metrics used in the empirical studies How were the performance comparisons quantified 2. Generalizability: How transferable do you believe the proposed \u03b42exploration strategies are to other RL algorithms beyond Qlearning and DQN Have you considered potential challenges or limitations in applying these strategies to different RL frameworks  Review Summary: The paper introduces \u03b42exploration as an efficient exploration strategy for general sequential RL demonstrating its practical advantage and superior performance through empirical evaluations. The innovative approach along with the comprehensive analysis and promising empirical results make a strong case for the effectiveness and adaptability of \u03b42exploration in modern RL scenarios. However further exploration of diverse evaluation metrics and theoretical discussions would enhance the robustness and depth of the proposed method. Overall I recommend acceptance of the paper with minor revisions to address the identified weaknesses and queries.", "sNuFKTMktcY": " Summary: The paper introduces a novel approach Hierarchical Exploration with Stable Subgoal representation learning (HESS) to address challenges faced by existing Goalconditioned Hierarchical Reinforcement Learning (GCHRL) methods in longhorizon tasks with sparse external rewards. HESS incorporates a stable subgoal representation learning regularization measures of novelty and potential for subgoals and an active hierarchical exploration strategy to improve highlevel policy learning. Experimental results demonstrate that HESS significantly outperforms stateoftheart baselines in challenging continuous control tasks with sparse rewards.  Strengths: 1. Innovative Approach: The paper introduces a novel method HESS which addresses key challenges in GCHRL and significantly outperforms existing baselines in challenging continuous control tasks showcasing its effectiveness. 2. Comprehensive Experimental Evaluation: The paper provides a detailed and thorough experimental evaluation on a suite of challenging sparsereward environments comparing HESS against stateoftheart baselines and demonstrating its superior performance. 3. Clear Explanations: The paper provides detailed explanations of the proposed method including the algorithms measures for subgoals and the ablation studies enhancing the readers understanding of the approach. 4. Effective Visualization: The visual representation of state embeddings and trajectories provides clarity on the subgoal representation learning process and highlights the impact of stability regularization on representation stability. 5. Insightful Related Work Comparison: The paper provides a comprehensive comparison with related work effectively highlighting the novelty and advantages of the proposed approach.  Weaknesses: 1. Theoretical Background Clarity: The paper assumes familiarity with topics such as Markov Decision Processes (MDPs) and reinforcement learning terminology which could be a barrier for readers not wellversed in the field. 2. Algorithm Complexity: The paper introduces several novel components and algorithms which might be challenging to understand without prior knowledge in the field especially for readers new to hierarchical reinforcement learning. 3. Hyperparameter Sensitivity: While the ablation tests demonstrated robustness to hyperparameter selection further insights into the sensitivity of key hyperparameters could enhance understanding and reproducibility. 4. Discussion on Failure Cases: While the results showcase the superiority of HESS over baselines a discussion on failure cases or challenges faced during the experiments could provide a more balanced view of the approach.  Questions and Feedback: 1. Question: Can you provide more insights into the generalizability of HESS across different environments or tasks Have you tested the approach on a wider range of scenarios to evaluate its scalability and adaptability 2. Question: The paper mentions the proposed approach significantly outperforms baselines. Can you elaborate on the computational efficiency of HESS compared to existing methods How does the training time and sample efficiency of HESS compare to other approaches 3. Feedback: Consider providing additional details on the computational resources required for training HESS such as GPU usage training time and scalability to larger datasets or environments. 4. Feedback: It would be beneficial to include a discussion on potential limitations or scenarios where HESS may not perform optimally. Identifying failure cases or challenges could provide valuable insights for future improvements.  Review Summary: The paper introduces a novel Hierarchical Exploration with Stable Subgoal representation learning (HESS) approach addressing limitations of existing GCHRL methods in longhorizon tasks with sparse rewards. With strong experimental results demonstrating superior performance over stateoftheart baselines comprehensive explanation of the proposed approach and insightful comparisons with related work the paper showcases the effectiveness and innovation of HESS. The inclusion of ablation studies visualization aids and proposed exploration strategy enhances the credibility and applicability of the approach. However improving accessibility to readers with varied expertise levels discussing algorithm complexity and providing insights on scalability and limitations could further strengthen the papers impact and practical relevance. Overall based on the significant performance improvement demonstrated I recommend the acceptance of this paper with minor revisions to enhance clarity and completeness.", "fgcIb5gd99r": " Peer Review  1. Summary: The paper introduces a novel multiscale fusion self attention model designed to address the issue of extracting phrase level information which is often overlooked by traditional self attention mechanisms. The model combines a multiscale attention module and a dynamic sparse module to extract phrase level information at different scales and construct a sparse attention matrix respectively. Experimental results demonstrate the effectiveness of the proposed model in relation extraction tasks and on the GLUE dataset. The paper also explores the impact of different kernel sizes on model performance.  2. Strengths:  Innovative Contributions: The paper presents a novel approach to tackle the limitation of existing self attention mechanisms in extracting phrase level information effectively.  Experimental Validation: The model is extensively evaluated on multiple benchmark datasets showcasing superior performance compared to baseline models.  Detailed Methodology: The paper provides a comprehensive explanation of the proposed model detailing the multiscale attention module dynamic sparse module and the experimental setup.  Comparison with Related Work: The paper effectively discusses and compares its approach with existing literature showcasing the novelty and superiority of the proposed model.  3. Weaknesses:  Clarity of Explanations: While the paper provides detailed explanations of the model architecture some sections could be further simplified for better understanding.  Generalizability: The paper focuses on specific NLP tasks and it could benefit from discussing the generalizability of the proposed model to other domains.  Evaluation Metrics: It would be beneficial to provide a more indepth discussion on the specific evaluation metrics used and how the proposed model outperforms baseline models.  Questions and Feedback: 1. Clarification: Could the paper provide more insights into how the proposed sparsity strategy helps in better feature selection for constructing the attention matrix 2. Generalizability: How do you envision extending this model beyond NLP tasks such as to other domains like image recognition or speech processing 3. Parameter Sensitivity: Have you conducted experiments to analyze the sensitivity of model performance to hyperparameters such as learning rate and kernel size variations  Review Summary: The paper presents a novel multiscale fusion self attention model that effectively extracts phrase level information showcasing superiority in relation extraction tasks and on the GLUE dataset. Despite its strengths in innovation and detailed methodology improvements can be made in clarity generalizability and further analysis of evaluation metrics. Overall the paper offers a valuable contribution to the field of self attention mechanisms. Therefore I recommend acceptance pending minor revisions to address the weaknesses highlighted.", "s51gCxF70pq": " Review of \"kStep Latent: Improving Sample Efficiency of RL Agents in HighDimensional State Spaces\"  Summary: The paper introduces kStep Latent (KSL) a novel representation learning method for deep reinforcement learning agents operating in highdimensional state spaces. KSL enforces temporal consistency of representations through a selfsupervised auxiliary task enabling agents to recurrently predict actionconditioned representations multiple steps into the future. KSL significantly enhances sample efficiency and asymptotic performance in the PlaNet benchmark suite showcasing stateoftheart results compared to other methods.  Strengths: 1. Innovative Approach: The introduction of KSL addresses a critical bottleneck in representation learning for RL agents demonstrating improved performance metrics. 2. Comprehensive Analysis: The paper conducts thorough analyses of learned encoders and latent representations providing significant insights into the effectiveness of KSL. 3. StateoftheArt Results: KSL outperforms existing methods on various performance checkpoints showcasing its efficacy in enhancing sample efficiency and generalization.  Weaknesses: 1. Increased Wall Clock Time: While KSL achieves superior results it comes at the cost of longer computation time. Exploring strategies to reduce this burden could enhance the practical applicability of KSL. 2. Lack of Comparative Analysis: While the paper compares KSL with selected baselines it could benefit from a more extensive comparison with a wider range of stateoftheart methods to provide a comprehensive evaluation.  Questions for the Authors: 1. Scalability Concerns: How does the performance of KSL scale with the complexity of state spaces and tasks Have scalability tests been conducted 2. Hyperparameter Sensitivity: Did the authors observe sensitivity of KSLs performance to hyperparameter choices How robust is KSL to variations in parameters 3. RealWorld Applicability: How transferable is KSL to realworld scenarios outside benchmark suites Have there been any experiments or considerations regarding deploying KSL in practical RL applications  Feedback: Overall the paper presents a compelling contribution with significant implications for improving the efficiency of RL agents in highdimensional state spaces. Strengthening the computational efficiency and conducting more expansive comparative evaluations would enhance the impact and robustness of KSL. The detailed analyses provided in the paper are commendable showcasing a deep understanding of the proposed methods implications. Considering the questions and feedback provided could further enrich the discussion and evaluation of KSLs capabilities.  Recommendation: Based on the innovative approach comprehensive analysis and superior results demonstrated by KSL I recommend accepting this paper for publication. The significant enhancements in sample efficiency and generalization make KSL a noteworthy addition to the field of deep reinforcement learning. Addressing the scalability hyperparameter sensitivity and realworld applicability aspects as suggested can further strengthen the paper and its contributions.", "lKrchawH4sB": " Summary: The paper introduces a novel normalization technique called Heterologous Normalization (HN) to address the limitations of Batch Normalization (BN) when dealing with small batch sizes. HN computes the mean and standard deviation from different pixel sets to leverage various normalization methods. Experimental results showcase that HN outperforms or achieves comparable performance to existing normalization methods such as IN LN GN and SN across different datasets. Additionally the paper conducts indepth analyses and comparisons to demonstrate the effectiveness and superiority of HN in handling small batch sizes.  Strengths: 1. Innovative Approach: The paper proposes a novel normalization technique Heterologous Normalization to effectively address the challenges related to small batch sizes showcasing the innovation in the field. 2. Experimental Validation: The paper provides comprehensive experimental results on various datasets demonstrating the superior performance of HN compared to existing normalization methods like BN IN LN GN and SN across different batch sizes. 3. Indepth Analysis: The paper conducts detailed analyses to explain the effectiveness of HN such as the impact of different channel groups on normalization performance and the comparison with Switchable Normalization (SN). 4. Clear Presentation: The paper follows a structured format making it easy to understand the problem statement proposed solution experimental setup and analysis of results.  Weaknesses: 1. Lack of Comparison: Although the paper extensively evaluates HN against various normalization methods a comparison with other recent and stateoftheart normalization techniques in the field could further strengthen the study. 2. Theoretical Explanation: While the empirical results are robust further theoretical analysis explaining the underlying principles of why HN performs better especially in scenarios with small batch sizes would enhance the papers academic value.  Question to the Authors:  Have you considered exploring the adaptability of HN to different network architectures beyond the ones evaluated in the paper  Feedback to the Authors:  Consider addressing the weaknesses by providing a comparative analysis with other contemporary normalization techniques and incorporating theoretical insights explaining the underlying mechanisms behind the effectiveness of HN in small batch size scenarios.  Review Summary: The paper introduces a novel normalization technique Heterologous Normalization (HN) and demonstrates its effectiveness in addressing the challenges associated with small batch sizes. The experimental results convincingly showcase the superior performance of HN compared to existing methods. While the paper is commendable for its innovative approach and thorough experimental evaluation addressing the noted weaknesses through further comparative analyses and theoretical insights would further enhance the papers academic rigor. Consequently I recommend this paper for publication as it contributes significantly to the field of deep learning and normalization techniques.", "qQuzhbU3Gto": " Summary of the Paper: The paper introduces a novel edgeindependent graph generative model that aims to capture heterophily structures in realworld graphs. The model is expressive producing nonnegative embeddings that assist in interpreting link predictions in terms of communities. The authors provide theoretical results demonstrating the models ability to represent graphs effectively. Experimental results validate the models performance on tasks like multilabel clustering and link prediction.  Strong Points: 1. Novel Approach: Introducing the first edgeindependent graph generative model to capture heterophily is a significant contribution. 2. Expressiveness: The models ability to exactly reconstruct a graph using a minimal number of clusters and capture heterophily and homophily is impressive. 3. Interpretability: The emphasis on producing nonnegative embeddings for interpretability enhances the models utility in realworld applications. 4. Experimental Validation: The experiments demonstrate the models effectiveness across various tasks enhancing its credibility.  Weak Points: 1. Complexity: The models complexity might pose challenges in practical implementation and scalability. 2. Comparative Analysis: The paper lacks a detailed comparison with existing stateoftheart methods for graph generative models which could enhance the evaluation. 3. Theoretical Explanations: The theoretical explanations while comprehensive might be challenging for readers not wellversed in the field.  Questions and Feedback to the Authors: 1. Can the proposed model efficiently scale to large graphs without compromising performance 2. How does the model compare in terms of computational efficiency compared to existing methods 3. Have you considered the potential limitations or assumptions of the model in capturing highly dynamic or evolving graph structures  Review Summary: The paper presents a novel edgeindependent graph generative model that effectively captures heterophily in realworld graphs. The models theoretical underpinnings are robust and experimental results validate its efficacy in various application tasks. While the models complexity and feasibility for largescale applications need further investigation the paper makes significant contributions to the field of graph modeling. Overall I recommend acceptance after addressing the minor concerns about scalability and comparative analysis.", "oJGDYQFKL3i": " Summary: The paper proposes an Object Dynamics Distillation Network (ODDN) for distilling explicit object dynamic representations from raw video input. The network includes a relation module to model objectpair interactions. The approach is evaluated on video events reasoning and video prediction tasks demonstrating superior performance compared to previous methods. The paper highlights the significance of object dynamics for video understanding and scene decomposition.  Strengths: 1. Innovative Approach: The paper addresses an important gap in existing research by focusing on distilling object dynamics from videos which is crucial for tasks like video understanding and reasoning. 2. Performance: Results show that ODDN outperforms previous methods in tasks of video understanding and reasoning future frame prediction and scene decomposition quality. 3. Comprehensive Evaluation: The paper provides a thorough evaluation of ODDN on different downstream tasks showcasing its effectiveness in various scenarios. 4. Comparison with Baselines: The comparisons with baseline models like IODINE and PROVIDE demonstrate the superiority of ODDN in capturing object dynamics and interactions.  Weaknesses: 1. Error Accumulation in Prediction: The paper acknowledges that prediction quality decreases over time due to error accumulation which could be a limitation in capturing longterm dynamics accurately. 2. Complexity: The model involves intricate components such as dynamic distillation and relation modules which may lead to challenges in implementation and deployment. 3. Training Time: Training the model on multiple GPUs for two days per model might hinder scalability for practical applications. 4. Ablation Studies: While the paper provides ablation experiments on representations more detailed ablation studies on other components like the relation module could further enhance the understanding of the models performance.  Questions and Feedback: 1. Generalization: How well does ODDN generalize to realworld scenarios beyond synthetic datasets like CLEVRER 2. Interpretability: Can the model provide insights into how explicit object dynamics influence its decisionmaking process in tasks like video prediction 3. Scalability: Are there any limitations or challenges in scaling up the model for larger datasets or realworld applications 4. Impact of Hyperparameters: How sensitive is the models performance to hyperparameters like latent dimensionality learning rate and batch size  Review Summary: The paper presents a novel Object Dynamics Distillation Network (ODDN) for distilling object dynamics from videos demonstrating superior performance in video understanding reasoning prediction and scene decomposition tasks compared to existing methods. While the approach showcases innovation and promising results addressing limitations in error accumulation complexity and scalability could further enhance its practical applicability. Overall the paper contributes significantly to the field of video understanding and scene decomposition and I recommend it for publication pending minor revisions and more detailed ablation studies.", "gjNcH0hj0LM": " 1. Summary of the Paper: The paper introduces a novel label propagation framework Temporal Coherencebased Label Propagation (TCLP) for timeseries active learning. TCLP exploits temporal coherence in timeseries data to propagate labels efficiently. It estimates the extent of segments for label propagation through a quadratic plateau model fitted to classifier model outputs.  2. Strong Points: a. Novel Framework: TCLP introduces a new approach for label propagation in timeseries active learning addressing the challenge of missing labels in timeseries data efficiently. b. Experimental Validation: Extensive experiments demonstrate the effectiveness of TCLP showing up to a 7.1 times improvement in classification accuracy when only 0.8 of data points are queried for labels. c. Theoretical Analysis: The paper provides a theoretical analysis supporting the effectiveness of the plateaubased segment estimation method in TCLP compared to thresholdbased approaches. d. SparsityAware Techniques: The incorporation of temperature scaling and plateau width regularization to handle label sparsity makes TCLP robust and effective in diverse datasets.  3. Weak Points: a. Theoretical Rationale Clarification: Providing a more detailed explanation of the theoretical underpinnings of the plateaubased segment estimation method could enhance the clarity and understanding of the proposed approach. b. Expanding Comparison: While the paper compares TCLP with existing label propagation methods extending the comparison to additional stateoftheart approaches could further demonstrate its superiority.  Question and Feedback to the Author:  Can you elaborate on the generalizability of TCLP to different types of timeseries datasets beyond the ones tested in your experiments  How does the computational complexity of TCLP compare to other label propagation methods and how does it affect scalability  Consider discussing potential realworld applications and practical implications of TCLP in the paper to enhance its relevance to a broader audience.  4. Review Summary: The paper presents a significant contribution to the field of timeseries active learning with the introduction of TCLP a novel label propagation framework. It demonstrates strong empirical results and provides theoretical justification for its approach. The incorporation of sparsityaware techniques further enhances the robustness of TCLP. However additional clarifications and expansions in theoretical rationale and comparisons could strengthen the paper. Overall based on its originality effectiveness and experimental results I recommend this paper for publication in its current form.", "vh-0sUt8HlG": "Summary of the Paper: The paper introduces MobileViT a lightweight vision transformer designed for mobile devices. The authors aim to combine the strengths of Convolutional Neural Networks (CNNs) and Transformers to create a lowlatency network suitable for mobile vision tasks. They propose the MobileViT block which effectively encodes local and global features in an input tensor. The network is evaluated on tasks including image classification object detection and semantic segmentation demonstrating superior performance compared to existing models across different datasets. Strengths: 1. Innovative Approach: The concept of combining CNNs and Transformers to create a lightweight network for mobile vision tasks is commendable. 2. Performance: MobileViT outperforms existing CNNs and ViTs on various vision tasks across different datasets showcasing its effectiveness and generalization capability. 3. Model Architecture: The MobileViT block creatively represents the local and global features in an input tensor allowing for effective learning of representations with fewer parameters. 4. Experimental Results: The extensive experimental results provide a comprehensive evaluation of MobileViTs performance on various tasks offering robust evidence of its superiority over stateoftheart models. Weaknesses: 1. Visualization: The paper lacks visual aids such as diagrams or graphical representations to explain the MobileViT architecture further which could have enhanced the clarity of the presentation. 2. Theoretical Analysis: While the paper discusses practical implications and results extensively a deeper theoretical analysis of why the proposed architecture works effectively could have added more depth to the paper. Questions and Feedback: 1. Clarification: Can you provide a more detailed explanation of how the MobileViT block combines local and global features in the input tensor 2. Comparison: How does the efficiency and performance of MobileViT compare with other stateoftheart lightweight CNNs when considering factors beyond accuracy such as energy consumption 3. Scalability: Have you considered how MobileViT might scale in more complex tasks or when processing larger datasets beyond the ones evaluated in the study Review Summary: The paper presents a novel approach in the realm of mobile vision tasks by introducing MobileViT a lightweight vision transformer. The innovative combination of CNNs and Transformers in the MobileViT architecture demonstrates superior performance across various tasks and datasets showcasing its potential for realworld applications. Extensive experimental results provide strong evidence in support of the MobileViT models effectiveness and generalization capability. While there are areas for improvement such as additional visual aids and deeper theoretical analysis overall the paper stands out for its innovative approach robust experimental validation and potential impact in the field of mobile vision tasks. Therefore I recommend accepting this paper for publication.", "jXKKDEi5vJt": " PeerReview of \"Byzantine Robust Optimization in Heterogeneous Federated Learning\"  1) Summary: The paper discusses the challenges posed by heterogeneity in distributed or federated learning specifically focusing on Byzantine robust optimization. It addresses the limitations of current aggregation rules in scenarios with nonidentical data distribution among workers. The paper introduces a mimic attack exploiting data heterogeneity and proposes a bucketing scheme to adapt existing robust aggregation algorithms to heterogeneous datasets. The proposed approach is theoretically and experimentally validated demonstrating improved performance against various attacks in noniid settings.  2) Strengths and Weaknesses:  Strong Points:  Novelty: The paper addresses an important and less explored aspect of Byzantine robust optimization in heterogeneous federated learning settings.  Proposed Solutions: Introducing new attacks bucketing scheme and combining robust aggregation with worker momentum display innovative approaches to tackle the challenges arising from data heterogeneity.  Comprehensive Validation: The theoretical proofs and experimental results provide a thorough validation of the proposed methods demonstrating their effectiveness under different attack scenarios.  Weak Points:  Complexity: The technical depth and mathematical rigor of the paper may make it challenging for readers with limited background in the topic.  Lack of Practical Demonstration: While the theoretical and algorithmic advancements are significant a practical demonstration or realworld application of the proposed methods could strengthen the paper.  Questions and Feedback: 1. Clarification: Could you provide more context on how the proposed bucketing scheme practically addresses the challenges of heterogeneity in distributed learning 2. Scalability: How do you foresee the scalability of the proposed methods in largescale federated learning systems with a higher number of workers and more complex data distributions 3. Comparison: How does the proposed approach compare to existing solutions in terms of computational efficiency and convergence rates particularly in scenarios with varying degrees of data heterogeneity  3) Review Summary: The paper introduces novel attacks and a bucketing scheme to address challenges related to data heterogeneity in Byzantine robust optimization for federated learning. The theoretical analyses and experimental validations illustrate the effectiveness of the proposed methods offering a comprehensive solution to the problem. While there is a strong theoretical foundation supporting the paper\u2019s claims practical demonstrations and scalability considerations would enhance its applicability in realworld settings. Overall the paper makes significant contributions to the field of federated learning security and Byzantine robust optimization under heterogeneous data distributions.  Recommendation: Based on the innovative methodologies presented the thorough validation and the significance of addressing heterogeneity in distributed learning I recommend the paper for publication pending minor revisions to clarify practical implications and scalability considerations. The paper provides valuable contributions to the field and addresses an important gap in current research on Byzantine robust optimization.", "pETy-HVvGtt": " 1) Summary of the Paper: The paper proposes a new framework for analyzing disentanglement in multivariate representations by introducing a disentanglement metric based on Partial Information Decomposition. The framework aims to capture interactions among multiple variables to distinguish between redundancy and synergy in representation entanglement. The paper presents a detailed analysis of learned representations particularly focusing on variational autoencoders (VAEs) to evaluate different models disentanglement scores and characteristics. The contributions include the development of the PIDbased disentanglement analysis framework the derivation of tractable bounds for partial information terms and the detailed analyses of learned representations.  2) Strong Points:  Strengths:  Innovative Framework: The use of Partial Information Decomposition for analyzing disentanglement involving interactions among multiple variables is a novel and valuable contribution.  Clear Problem Definition: The paper effectively defines the problem of disentanglement in multivariate representations and the limitations of current metrics.  Methodological Rigor: Detailed explanation of the proposed framework the derivations of bounds for partial information terms and the methodology for assessing disentanglement through entanglement attacks demonstrate the technical rigor of the paper.  Empirical Evaluation: The empirical analysis on annotated datasets using different VAE models provides practical insights into the performance of the proposed disentanglement metric.  Comparative Analysis: The comparison with existing disentanglement metrics and the observation of disagreements between metrics in certain cases adds depth to the evaluation.  Weak Points:  Complexity of Content: The high technicality and complexity of the content may pose challenges for readers not familiar with information theory and disentanglement concepts.  Clarity of Definitions: Some terms particularly related to information theory concepts and metrics could be further clarified or defined explicitly for better understanding.  Generalizability: The paper focuses on specific types of datasets and models which may limit the generalizability of the findings to broader contexts.  Questions and Feedback: 1. Clarity on Unique Information: Could you provide a more intuitive explanation or example to help readers understand the concept of unique information in disentanglement analysis 2. Extension to RealWorld Data: How do you envision the application of the proposed framework and metric to realworld datasets beyond the controlled datasets used in the experiments 3. Potential Biases: Are there potential biases introduced by the choice of datasets or VAE models used in the empirical evaluation and how might these biases impact the generalizability of the results  3) Review Summary: The paper presents a comprehensive framework for analyzing disentanglement in multivariate representations introducing a novel disentanglement metric based on Partial Information Decomposition. The strengths lie in the innovative approach methodological rigor and empirical evaluation on annotated datasets using VAE models. While the paper excels in technical depth and empirical analysis clarity in definitions and potential biases in experimental setups could be further addressed. Overall the contributions and findings of the paper support a positive recommendation for publication after addressing the minor concerns raised.", "i1ogYhs0ByT": " Review of \"Transformer with a Mixture of Gaussian Keys\"  1. Summary: The paper proposes a novel transformer architecture called Transformer with a Mixture of Gaussian Keys (TransformerMGK) to reduce the redundancy in attention heads of transformers. The TransformerMGK replaces redundant heads with mixtures of keys following a Gaussian mixture model. The paper demonstrates the effectiveness of TransformerMGK in various tasks including language modeling and long input sequence tasks.  2. Strengths and Weaknesses:  Strong Points:  Novel Idea: The concept of using a mixture of Gaussian keys to reduce redundancy in transformer heads is innovative and has the potential to improve efficiency.  Empirical Validation: The paper provides empirical evidence of the advantages of TransformerMGK in various practical applications demonstrating its effectiveness.  Efficiency Improvement: The experimental results show that TransformerMGK achieves comparable or better performance with fewer attention heads leading to improved computational efficiency.  Detailed Experiments: The paper provides extensive experimental analysis across various benchmarks to validate the proposed TransformerMGK.  Weak Points:  Detailed Explanation: While the paper delves into the technical details of the TransformerMGK approach some parts of the explanation can be complex for readers unfamiliar with transformer architecture.  Questions and Feedback:  Clarity on Implementation: Can you provide more details on the practical implementation of TransformerMGK particularly on how the mixture of Gaussian keys is integrated into the existing transformer architecture  Comparison Metrics: It would be helpful to include more detailed comparisons with existing methods that address redundancy in attention heads to showcase the uniqueness and superiority of TransformerMGK.  Scalability: How does TransformerMGK perform as the scale of the model increases particularly in scenarios with very large datasets or complex tasks  Runtime Overhead: Have you measured the inference time overhead of TransformerMGK compared to traditional transformers especially in scenarios where realtime processing is crucial  3. Review Summary: The paper introduces a novel concept of TransformerMGK to address the redundancy issues in attention heads within transformers. The empirical results demonstrate the effectiveness of TransformerMGK in improving efficiency and maintaining performance across various tasks. The extensive experimentation and empirical validation in different scenarios provide strong evidence of the benefits of adopting TransformerMGK. The innovative approach and the demonstrated advantages make a compelling case for further exploration and application of TransformerMGK in realworld scenarios.  Recommendation: Based on the novel concept extensive experimental validation and the potential to enhance efficiency in transformers I recommend acceptance of this paper with the suggestion to further clarify the implementation details and provide additional comparisons to strengthen the impact of the proposed TransformerMGK approach.", "mz7Bkl2Pz6": " Summary of the Paper: The paper addresses the limitations in the theory of stochastic gradient descent (SGD) by making it more applicable to realworld nonconvex optimization problems with complex noise models. It introduces new assumptions regarding local Ho\\xcc\\x88lder continuity and general noise models establishing the global convergence behavior of SGD despite the nonconvexity and noise complexity. The paper proves that SGDs iterates converge to a stationary point or diverge under nearly arbitrary nonconvexity and noise models. It also demonstrates that the objective function cannot diverge even if the iterates diverge ensuring stability in applying SGD to stochastic optimization problems.  Strong Points: 1. Novel Contributions: The paper introduces new assumptions and analysis strategies that improve the applicability and effectiveness of SGD in nonconvex optimization problems with complex noise models. 2. Theoretical Advances: It addresses the gap in existing SGD theory by proving global convergence and stability under more realistic assumptions enhancing the understanding of SGD behavior. 3. Detailed Analysis: The paper provides a thorough mathematical analysis clearly defining assumptions strategies and proving theorems to support the main results. 4. Practical Implications: The results have practical implications for machine learning optimization problems enabling confidence in using SGD and facilitating downstream analyses.  Weak Points: 1. Complexity: The mathematical derivations and technical terminology may pose challenges for readers not wellversed in optimization theory and stochastic processes. 2. Assumption Realism: While the paper introduces more realistic assumptions additional discussions on the practical implications and limitations of these assumptions could enhance the papers impact. 3. Further Explanations: Some proofs and results could benefit from more detailed explanations to aid comprehension and showcase the significance of the findings to a wider audience.  Questions for the Authors: 1. Can you elaborate on how the new assumptions regarding local Ho\\xcc\\x88lder continuity and noise models address the limitations of existing SGD theory and improve its practical applicability 2. How do the proposed analysis strategies the pseudoglobal strategy and the local strategy contribute to the advancements in understanding SGD behavior in nonconvex optimization problems 3. Do you have plans to validate the theoretical findings of this work through empirical studies or realworld applications to demonstrate the impact of the proposed strategies in practical scenarios  Feedback to the Authors: 1. Provide additional explanations and contexts in the paper to make the complex mathematical derivations and theoretical contributions more accessible to a broader audience. 2. Consider discussing the implications of the new assumptions and analysis strategies on practical machine learning applications to emphasize the significance and relevance of the theoretical advancements.  Review Summary: The paper presents significant advancements in the theory of stochastic gradient descent by introducing new assumptions and analysis strategies to enhance its applicability in nonconvex optimization problems. The novel contributions detailed mathematical analysis and practical implications of the results demonstrate the papers strong potential. Addressing the weak points by providing more explanations and implications for practical applications could further strengthen the paper\u2019s impact. Based on the theoretical contributions and potential practical implications I recommend this paper for publication pending minor revisions for clarity and broader accessibility.", "fyLvrx9M9YP": " Summary The paper presents a novel framework focusing on disentanglement of syntactic roles in sentences without supervision. It introduces an AttentionDriven Variational Autoencoder (ADVAE) model and an evaluation protocol to measure disentanglement of syntactic roles. The paper shows that the ADVAE successfully disentangles syntactic roles better than standard Sequence VAEs and Transformer VAEs. The experiments on the SNLI dataset demonstrate the ability to control realizations of syntactic roles separately during content generation without supervision.  Strong Points  Innovative Approach: The paper introduces a novel architecture ADVAE for unsupervised disentanglement of syntactic roles offering a new perspective in NLP.  Evaluation Protocol: The paper presents a comprehensive evaluation protocol utilizing both encoder attention values and decoder perturbations to measure disentanglement of syntactic roles effectively.  Empirical Results: Through experiments the paper demonstrates the effectiveness of the proposed ADVAE model in separating and controlling syntactic roles outperforming standard VAE models.  Weak Points  Generalization: The experiments are conducted on a specific dataset (SNLI) limiting the generalizability of the findings. More diverse datasets should be considered for robustness.  Incomplete Evaluation Metrics: While the evaluation protocol is comprehensive it lacks comparison with human performance or downstream NLP tasks to validate the practical impact of the disentanglement.  Simplicity of Sentence Structures: The study is limited to relatively simple sentence structures and scalability to more complex linguistic phenomena such as in UGC remains unaddressed.  Questions 1. Is there any consideration given to the potential impact of using different datasets with more varied sentence structures on the effectiveness of the ADVAE model 2. Can the disentanglement metrics used in the evaluation protocol be further validated against human judgements or downstream NLP tasks to establish practical significance 3. How does the proposed ADVAE model handle nuances or ambiguities in realworld language data beyond syntactic roles and what steps are planned to enhance its capabilities in handling complex linguistic phenomena  Feedback 1. Dataset Diversification: Consider testing the ADVAE model on datasets with more varied sentence structures to assess its robustness and generalization capabilities. 2. Validation: Expand the evaluation to include a qualitative human evaluation or NLP tasks to validate how disentanglement of syntactic roles impacts realworld applications. 3. Enhanced Modeling: Address the scalability to complex linguistic phenomena beyond syntactic roles and explore strategies to handle nuances and ambiguities in natural language data effectively. Overall the paper presents a promising direction in advancing unsupervised disentanglement of syntactic roles in NLP but future work should focus on improving robustness and practical relevance in diverse linguistic contexts.", "yRYtnKAZqxU": " Summary: The paper investigates the effectiveness of unsupervised learning approaches specifically Graph Contrastive Learning (GCL) and reconstructionbased methods for graph representation learning. It explores the impact of dataset properties and augmentation strategies on the success of these approaches. The authors present theoretical analyses and empirical evaluations to provide insights into when each approach performs well. They introduce a synthetic data generation process to control the style vs. content ratio in dataset samples and compare the performance of different approaches in various scenarios.  Strengths: 1. Theoretical Analysis: The paper provides a comprehensive theoretical analysis to understand the conditions under which Graph Contrastive Learning is expected to be effective. 2. Empirical Evaluations: Extensive empirical evaluations are conducted to compare the performance of reconstructionbased approaches and Graph Contrastive Learning under different dataset conditions. 3. Benchmark Setup: The introduction of a synthetic benchmark setup to control the style vs. content ratio in samples is a valuable contribution for evaluating graph representation learning methods. 4. Insights: The paper offers valuable insights into the performance of unsupervised learning methods for graph representation learning and highlights the effectiveness of untrained GNNs as a competitive baseline.  Weaknesses: 1. Limited Use of Real Datasets: The focus on synthetic data raises questions regarding the generalizability of the findings to realworld graph datasets. 2. Extension to Real Applications: The paper could benefit from discussing practical implications and applications of the findings providing more context for potential realworld use cases. 3. Unexplored Components: While the paper covers a breadth of topics related to graph representation learning further exploration of specific augmentation strategies or comparisons with stateoftheart methods could enhance its relevance.  Question and Feedback to the Authors: 1. Could the findings from synthetic data generation be validated on realworld graph datasets to assess the practical implications of the proposed approach 2. How do the results of this study compare with stateoftheart methods in graph representation learning on real datasets 3. Have you considered exploring more advanced augmentation strategies or incorporating domainspecific knowledge to further enhance the performance of unsupervised learning approaches for graph representation  Review Summary: The paper presents a thorough investigation into the performance of Graph Contrastive Learning and reconstructionbased methods for unsupervised graph representation learning. The theoretical analyses and empirical evaluations shed light on the conditions under which each approach excels. The introduction of a synthetic benchmark to control the style vs. content ratio in samples is a notable contribution. The study provides valuable insights but further validation on real datasets and exploration of advanced augmentation strategies could strengthen the papers impact. Overall the paper contributes to the understanding of unsupervised graph representation learning methods and offers a solid framework for analysis in this field.", "k-sNDIPY-1T": " Summary of the Paper: The paper investigates the feasibility of generating lower complexity blackbox models for the nervous system of C. elegans using datadriven techniques. Specifically the study focuses on comparing the performance of recurrent neural network architectures such as LSTMs and GRUs in capturing the system dynamics. The experiments are based on a highfidelity model of the C. elegans nervous system and aim to create reduced models that can replicate the systems response to various stimuli with low error.  Strong Points: 1. Comprehensive Background: The paper provides a detailed background on the challenges in studying complex neuronal systems and the motivation behind utilizing simpler organisms like C. elegans for modeling. 2. Methodological Rigor: The study outlines a clear methodology for generating reduced order models using neural networks and presents a systematic evaluation of different recurrent neural network architectures. 3. Detailed Experiments: The experiments are wellstructured comparing the performance of RNNs LSTMs and GRUs along with varying sizes of the recurrent layers providing thorough analysis and insights. 4. Clear Communication: The paper effectively communicates complex concepts algorithms and results in a clear and understandable manner making it accessible to readers with varying levels of expertise.  Weak Points: 1. Validation: While the paper mentions validating the model against specific scenarios a more detailed discussion on validation methods and results could strengthen the credibility of the findings. 2. Model Complexity: The focus on reducing model complexity is essential however a discussion on potential limitations or tradeoffs in simplifying the model could enhance the paper.  Question to the Authors:  Can you elaborate on the criteria used to select the optimal model size for the GRU units in Experiment 2 How did you determine that a hidden size of 4 units was optimal for low RMSE  Feedback to the Authors:  It would be beneficial to include a discussion on the implications of your studys findings for computational neuroscience and further research directions in this area.  Consider expanding on the practical applications of the datadriven models developed in this study and how they can be utilized in realworld contexts.  Review Summary: The paper presents a wellstructured and methodologically rigorous study on generating reduced complexity models for the C. elegans nervous system using recurrent neural networks. The detailed experiments and comparisons provide valuable insights into utilizing datadriven techniques for capturing system dynamics. While the paper effectively communicates its findings and methodology addressing validation methods and discussing the implications of the studys results would enhance its overall impact. Overall based on the strength of the methodology thorough analysis and clear presentation I recommend this paper for publication pending minor revisions and additional considerations as outlined in the feedback.", "u2GZOiUTbt": " Summary: The paper introduces an asymmetric affinity score called Task Affinity Score (TAS) to quantify the complexity of leveraging knowledge from one task to train another. The TAS is based on the Fisher Information matrix and is used for proposing a novel fewshot learning algorithm. This approach identifies relevant training data labels for the test data and finetunes a fewshot model using these relevant data. The paper presents theoretical analyses the methodology and experimental results on various datasets showcasing improved classification accuracy compared to stateoftheart methods.  Strengths: 1. Innovative Approach: The introduction of the TAS for task similarity identification in fewshot learning is novel and adds value to the research domain. 2. Theoretical Foundation: The paper provides solid theoretical background including the definition of TAS Fisher Information matrix and tasks involved in the study. 3. Comprehensive Methodology: The proposed fewshot learning algorithm is welldetailed spanning from the affinity score computation to episodic finetuning providing a stepbystep walkthrough of the approach. 4. Empirical Results: Demonstrating the efficacy of the proposed approach on various benchmarks including miniImageNet and tieredImageNet showcases its potential for practical applications.  Weaknesses: 1. Complexity of TAS: The methodology for calculating TAS using Fisher Information matrices may pose computational challenges especially when scalability to larger datasets is considered. 2. Empirical Evaluation: While the paper presents positive results further comparisons with more stateoftheart methods and diverse datasets could strengthen the empirical validation. 3. Implementation Details: More insights into implementation details hyperparameter tuning and potential limitations of the proposed approach could enhance the research presentation.  Questions to Authors: 1. Could you elaborate on the computational complexity of calculating TAS for large datasets and how it impacts the scalability of the proposed approach 2. How does the TAS approach compare to other methods in terms of generalization to unseen data and robustness to noise or label variations  Feedback: The paper presents a promising approach in utilizing TAS for enhancing fewshot learning. I recommend further clarifications on the computational aspects comprehensive comparisons and potential limitations to strengthen the research contribution. Recommendations for future work could include exploring realworld applications and potential extensions of TAS to different learning scenarios. The paper\u2019s wellstructured presentation theoretical foundation and empirical results support its significance in the fewshot learning domain. With minor revisions and further insights the paper could make a valuable contribution to the field.", "xiXOrugVHs": " PeerReview of the Paper on Text Summarization  Summary: The paper proposes a novel approach to improve text summarization models by introducing a hierarchical latent structure for documents and synergizing bottomup inference with topdown inference. The proposed framework named the topdown transformer utilizes local selfattention for bottomup inference and toplevel selfattention for topdown correction aiming to improve token representation inference for better summarization performance. The experimental results demonstrate the effectiveness of the framework on summarizing diverse datasets including scientific documents short news articles TV show scripts and even entire books.  Strengths: 1. Innovative Framework: The paper introduces a novel hierarchical approach that combines bottomup and topdown inference to improve text summarization models showcasing a unique perspective in the field. 2. Extensive Evaluation: The proposed framework is rigorously evaluated on various datasets including scientific documents short news articles challenging TV show scripts and lengthy book summaries demonstrating its effectiveness across different domains and document lengths. 3. Comparison with Existing Models: The paper provides comprehensive comparisons with stateoftheart summarization models efficient transformers and even a GPT3based model showcasing the superiority of the proposed topdown transformer. 4. Efficiency Consideration: Addressing the computational complexity issue of transformers on long documents the paper implements local attention for bottomup inference reducing computational costs without compromising summarization performance. 5. Empirical Results: The experimental results clearly demonstrate the competitive performance of the topdown transformer on a diverse range of datasets highlighting its potential for practical applications in realworld summarization tasks.  Weaknesses: 1. Additional Details Needed: The paper could benefit from providing more details on the experimental settings hyperparameters tuning and training process for completeness and reproducibility. 2. Comparison Metrics: While the evaluation is based on ROUGE scores it would be beneficial to include qualitative evaluations or user studies to assess the summarization quality from a human perspective. 3. Model Complexity: Since the proposed framework utilizes a multilevel approach there might be concerns regarding the complexity and interpretability of the model which could be addressed in the discussion.  Questions: 1. Can you provide more insights into the interpretability of the token representations inferred using the bottomup and topdown inference in the proposed framework 2. How does the window size parameter in the local selfattention impact the models performance on different document lengths and complexities  Feedback to the Author: 1. Consider including a thorough discussion on the interpretability and explainability of the proposed framework especially regarding how the hierarchical structure impacts the final summarization output. 2. Enhance the paper by providing illustrations or visual representations demonstrating the workflow of the topdown transformer for better understanding by readers.  Review Summary: The paper presents an innovative topdown transformer framework for text summarization addressing the challenges of efficient inference and longrange dependency modeling. With extensive experimental evaluations across diverse datasets the proposed framework demonstrates competitive or superior summarization performance compared to existing models. While the paper showcases strong empirical results and a principled approach further details on model complexity and interpretability aspects could be beneficial. Overall the proposed framework shows promise for improving summarization models and warrants further exploration and development in the field.  Recommendation: Based on the innovative approach comprehensive evaluation and competitive performance demonstrated in the experiments I recommend accepting this paper for publication with minor revisions to address the highlighted weak points and additional context for clarity.", "j97zf-nLhC": "Summary: The paper investigates the impact of network architectures on learning algorithms ability to utilize semantic relationships between features of actions and observations in zeroshot coordination (ZSC). It introduces a novel environment called the \"hintguess\" game for studying coordination where agents must coordinate to guess a target card based on hints. The study compares the performance of agents using different neural network architectures including Deep QLearning (DQN) Recurrent Deep QLearning (RDQN) and attention mechanisms specifically Attention for Observation Attention with Action as Output and Attention with Action as Input. The results show that attentionbased architectures outperform traditional methods in exploiting shared action and observation features for coordination. Strengths: 1. Innovative Approach: The paper introduces a novel environment the \"hintguess\" game to study coordination providing a unique setting for evaluating ZSC strategies. 2. Comprehensive Analysis: The study compares different neural network architectures and provides a detailed breakdown of their performance and behaviors in various scenarios shedding light on the mechanisms driving successful coordination. 3. HumanCompatible Policies: The attentionbased architectures demonstrate the ability to generate humancompatible policies showing promising results in zeroshot coordination scenarios with high similarity to human behavior. 4. Bridging Cognitive Science and Machine Learning: The paper connects concepts from cognitive science such as mutual exclusivity and implicature to machine learning approaches offering a multidisciplinary perspective on coordination strategies. Weaknesses: 1. Complexity of Presentation: Some parts of the paper particularly the technical details of the network architectures and experiments might be challenging for nonexperts to follow. Simplifying the language or providing visual aids could enhance understanding. 2. Limited Evaluation: While the paper extensively evaluates the network architectures in the proposed game environment expanding the study to more diverse scenarios or benchmark games could strengthen the generalizability of the findings. 3. Lack of RealWorld Applications: While the study design is rigorous the practical implications of the results in realworld applications or broader multiagent scenarios could be further discussed to enhance the papers impact. Questions and Feedback: 1. How did the design of the \"hintguess\" game capture the complexity of coordination challenges encountered in realworld scenarios 2. Could the paper explore potential limitations of the attentionbased architectures in more complex environments or with larger agent populations 3. How can the findings from this study be extended to address coordination challenges in different domains beyond the specific \"hintguess\" game Review Summary: The paper presents a rigorous investigation into the impact of network architectures on zeroshot coordination introducing a novel coordination environment for evaluation. The comparison of various architectures showcases the superiority of attentionbased models in exploiting shared features for effective coordination. The studys emphasis on humancompatible policies and its integration of cognitive science concepts enrich the research landscape. Despite some complexities in presentation and potential for broader realworld applications the papers innovative approach comprehensive analysis and bridging of interdisciplinary concepts warrant a positive recommendation for publication.", "tyrJsbKAe6": "(a) (model (1)) (2 ( (1 ( ( (1 ( (1) (1 ( ( (1 ( (1 ( (1 ( (1 ( (1 ( (F and (F ( (M ( (M ( (M ( (M ( (M) ( (M ( (M ( (M ( (M ( (M ( (M ( (M ( (M) ( (M ( (M ( (M ( (M ( ( (M ( ( (M ( (Phi ( (Phi ( (Phi ( (Phi ( ( (I) (M ( (I) (M ( (I) (M ( (I) (M ( ( (I ( ( (I ( ( (I ( ( (I ( ( (I ( ( (I ( ( (I ( ( (II ( ( (II ( ( (II ( ( (II ( ( (III ( ( (IV ( (IV) (IV) (IV) (M ( (IV) (I (III (II) (II) (IV) (II) (I ( (I ( (I (  (I ( (I ( (I ( (I ( (I ( (I ( (I ( (I ( (I ( (I ( (I ( (I ( (I so (I so (I (I (I (I (I (I (I (I (I (I ( (I (I (I) (I (I (I) (I) (I (I (I (I (I (I (I) (I (I) (I) (I (I (I) (I (I) (I (I ( (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I (I) (I) (I) (I) (I) ( (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (", "siCt4xZn5Ve": " Summary: The paper presents a novel framework to analyze the implicit bias of Stochastic Gradient Descent (SGD) for overparameterized models in deep learning. By adapting ideas from previous works the paper introduces a general framework that characterizes the regularization effect of SGD around manifolds of local minimizers. The analysis is extended to a global level providing insights into the regularization effects of SGD with different noise covariances. Furthermore the paper demonstrates applications of the framework showing that label noise SGD can escape the kernel regime improving sample complexity for learning sparse linear models.  Strengths: 1. Novel Framework: The paper introduces a novel framework for analyzing the implicit bias of SGD providing valuable insights into the regularization effects in deep learning. 2. Global Analysis: By extending the analysis to a global level the paper offers a comprehensive characterization of the regularization effect of SGD around local minimizers enhancing the understanding of the implicit bias. 3. Applications and Results: The paper provides concrete applications of the framework demonstrating improved sample complexity for learning sparse linear models showcasing the practical implications of the developed theory.  Weaknesses: 1. Complexity: The paper delves into highly technical aspects of stochastic gradients and implicit biases which may make it less accessible to a broader audience. 2. Assumptions: The analysis appears to rely on several technical assumptions and prerequisites potentially limiting the general applicability of the framework. 3. Clarity: Some sections with advanced mathematical derivations may be challenging to follow for readers without a deep understanding of the underlying concepts.  Questions: 1. Can you provide insight into the practical implications of the improved sample complexity demonstrated in the applications section How significant is the improvement in realworld scenarios 2. How robust is the framework to variations in model architectures or training datasets Are there limitations to the scalability of the analysis to more complex deep learning models  Feedback: The paper presents a comprehensive framework for analyzing the implicit bias of SGD in deep learning contributing valuable insights into the regularization effects. However ensuring clarity and accessibility for a wide audience will be crucial for broader dissemination and practical application of the developed framework. Additional discussion on the practical implications and robustness of the framework can further enhance the impact and relevance of the research findings. Further clarification on the assumptions made and their implications could also strengthen the paper.", "zHZ1mvMUMW8": " Summary: The paper introduces a novel approach called \"Succinct Compression\" for enabling faster Deep Neural Network (DNN) inference with nearoptimal compression. The method leverages Succinct Data Structures to support fast queries without decompression. It involves three stages: formulating DNN models in Elementwise or Blockwise manner compressing using Succinct Data Structures and specialized execution pipelines for inference. Experimental results demonstrate significant speedup and nearoptimal compression synergistic with Pruning and Quantization.  Strengths: 1. Innovative Approach: The concept of Succinct Compression leveraging Succinct Data Structures for efficient inference is novel and promising. 2. Comprehensive Evaluation: The experimental results showcase the superiority of the proposed method demonstrating significant speedup and nearoptimal compression compared to Huffman Coding. 3. Synergy with Existing Techniques: The paper highlights the synergistic relationship with existing compression schemes like Pruning and Quantization. 4. Clear Explanations: The paper provides detailed explanations of the methods stages formulations and implications making it easy to understand.  Weaknesses: 1. Limited Comparison: While the comparison with Huffman Coding is insightful further comparisons with other stateoftheart methods could enhance the papers robustness. 2. Complexity: The method involves intricate concepts such as Succinct Data Structures and specialized pipelines which may pose implementation challenges for practitioners. 3. Evaluation Setup: More details on the experimental setup including hardware and software specifics could enhance reproducibility and validity of the results.  Questions: 1. How does the \"Succinct Compression\" method compare with other advanced compression techniques like Tensor Decompositions or Tensor Train Formats 2. Have you considered exploring the scalability of your method to larger and more complex neural networks 3. Can you elaborate on the practical feasibility and computational overhead of implementing the \"Succinct Compression\" method in realworld scenarios  Feedback:  Provide additional comparisons with stateoftheart methods to further demonstrate the superiority of the proposed approach.  Discuss potential limitations or challenges in implementing the method in practical applications for a more comprehensive evaluation.  Enhance the clarity of the experimental setup and methodology to ensure reproducibility and reliability of the results.  Review Summary: The paper presents an innovative \"Succinct Compression\" method for efficient DNN inference with nearoptimal compression. While the approach shows promising results in terms of speedup and compression further comparisons and discussions on practical implementation aspects would strengthen the paper. Overall the paper is wellstructured and provides valuable insights into enhancing DNN efficiency. Further clarity on implementation implications and broader comparisons would improve the papers impact and applicability in the field.", "kDF4Owotj5j": " Summary: The paper presents novel advancements in the field of recurrent neural networks for tackling logical extrapolation problems. The authors propose a recall architecture that retains the input problem instance during multiple iterations preventing information loss or corruption. Additionally they introduce a new training routine that incentivizes networks to make incremental improvements toward a solution without learning iterationspecific behaviors. These innovations aim to alleviate the overthinking problem observed in existing recurrent systems enabling deep thinking systems to handle complex reasoning tasks with extreme logical extrapolation on benchmark problems like computing prefix sums mazesolving and chess puzzles.  Strengths: 1. Innovative Approach: The paper introduces novel architectural modifications and training routines to address the overthinking problem in recurrent neural networks showcasing the potential for deep thinking systems to handle complex logical reasoning tasks. 2. Comprehensive Experiments: The authors provide detailed experiments across benchmark tasks demonstrating significant improvements in performance with the proposed recall architecture and progressive training approach. 3. Ablation Studies: The inclusion of ablation studies helps in understanding the individual contributions of each proposed modification reinforcing the importance of both recall architecture and incremental training in enhancing logical extrapolation capabilities. 4. Indepth Analysis: The paper discusses the impact of the modifications on model convergence sensitivity to input and feature manipulation and the ability of the networks to converge to a fixed point enriching the understanding of the proposed techniques.  Weaknesses: 1. Complex Terminology: The paper delves deep into technical details potentially making it challenging for readers unfamiliar with the intricacies of recurrent neural networks and logical extrapolation to grasp the content fully. 2. Clarity in Experiment Results: While the experiments show promising results the visualization and presentation of data could be enhanced for better clarity and interpretation especially in comparing different models and their performance. 3. Reproducibility: While the paper mentions that the code is publicly available more emphasis could be placed on reproducibility including detailed documentation and instructions to facilitate replication of the experiments.  Question: 1. Generalization: How do you envision the proposed recall architecture and training routine impacting the generalization capabilities of deep thinking systems across a wider range of logical reasoning tasks beyond the benchmark problems considered in this study  Feedback: The paper presents a significant contribution to the field of logical extrapolation using recurrent neural networks. Enhancing the clarity and userfriendliness of the technical content improving the visualization of experimental outcomes and emphasizing reproducibility will further strengthen the impact and accessibility of the research. Consider highlighting the applicability of the proposed techniques in realworld scenarios and discussing potential avenues for future research to broaden the scope of the work. Overall the study offers valuable insights into advancing the capabilities of deep thinking systems for handling complex reasoning tasks.  Recommendation: Based on the innovative approach comprehensive experiments and indepth analysis presented in the paper I recommend acceptance after minor revisions to address the identified weaknesses and enhance the clarity and reproducibility aspects of the research. The contributions made by the proposed recall architecture and training routine for improving logical extrapolation capabilities are substantial and warrant publication in the target venue.", "zeGpMIt6Pfq": "Peer Review 1) Summary of the Paper: The paper introduces an innovative approach to image classification using a locally connected spiking neural network (SNN) called BioLCNet. The proposed network is trained using spiketimingdependent plasticity (STDP) and its rewardmodulated variant (RSTDP) learning rules. The authors emphasize the biological plausibility of their approach compared to convolutional neural networks (CNNs). The network architecture consists of a ratecoded input layer a locally connected hidden layer and a decoding output layer with a spike populationbased voting scheme. The study evaluates the classification accuracy on the MNIST dataset and the effectiveness of the rewarding system using experiments. 2) Strengths:  The paper addresses the issue of biological plausibility by proposing a locally connected SNN with reinforcement learning.  The use of STDP and RSTDP for training the network is novel and aligns with biological principles.  The experimental validation on the MNIST dataset and the classical conditioning experiment add credibility to the proposed approach.  The comparison with CNNs and the discussion of past research provide relevant context. Weaknesses:  The paper lacks a quantitative comparison of the proposed BioLCNet with existing stateoftheart image classification models.  The hyperparameter settings and their impact on classification performance could be further elaborated.  The training methodology could be more detailed including convergence analysis and computational efficiency.  The discussion on potential limitations or challenges of implementing the proposed approach in realworld applications is minimal. Questions and Feedback to the Author: 1. Can you provide insights into the computational efficiency of BioLCNet compared to traditional CNNs in terms of training time and memory requirements 2. How does the network perform when exposed to noisy input data or adversarial attacks Have you conducted any robustness tests beyond the MNIST dataset 3. Could the proposed model be extended or adapted for more complex datasets like CIFAR10 or ImageNet If so what modifications would be necessary 4. In future work do you plan to investigate the scalability of BioLCNet to larger datasets and the potential for transfer learning applications 3) Review Summary: The paper presents an intriguing methodology by introducing BioLCNet a locally connected spiking neural network trained using STDP and RSTDP for image classification. The approach focuses on biological plausibility and complements existing CNNbased solutions. While the experimental results on the MNIST dataset are promising the paper could benefit from a more detailed evaluation and comparison with current stateoftheart models. Additionally addressing computational efficiency robustness and scalability concerns could strengthen the proposed approach. Overall the paper presents a unique concept that warrants further exploration and development in future research endeavors. Recommendation:\\ Based on the innovative approach and experimental validation provided in the paper I recommend acceptance with minor revisions to address the mentioned weaknesses and questions. A more thorough discussion on scalability robustness and practical applicability would enhance the overall contribution of the work.", "nxcABL7jbQh": " Summary of the Paper: The paper introduces a novel approach for boundary detection by reinterpreting boundaries as 1D surfaces leading to a vector transform function that eliminates class imbalance issues and allows for the prediction of crisp boundaries without the need for postprocessing steps. The proposed method is theoretically justified and evaluated on multiple datasets showing superior performance over existing approaches in boundary detection tasks.  Strengths of the Paper: 1. Innovative Approach: The reinterpretation of boundaries as 1D surfaces and the introduction of the vector transform function represent a unique and innovative approach to address class imbalance and achieve crisp boundary detection. 2. Theoretical Justification: The paper provides thorough theoretical justifications for the proposed vector transform representation which increases the credibility of the method. 3. Comprehensive Evaluation: Extensive evaluations are conducted on various datasets showcasing the performance of the method using surface distance metrics and standard evaluation metrics for boundary detection tasks. 4. Comparison with Existing Methods: The paper includes a comparative analysis with existing boundary representation methods demonstrating the superiority of the proposed Vector Transform in terms of performance and stability. 5. Clarity and Structure: The paper is wellstructured with clear explanations of the problem formulation methodology and results. Figures and equations are used effectively to convey complex concepts.  Weaknesses of the Paper: 1. Limited Dataset Evaluation: While the method is evaluated on multiple datasets the performance on the BSDS500 dataset is noted to have limitations due to inconsistent annotations. Further evaluation on a larger dataset with clearer boundary definitions could strengthen the findings. 2. Threshold Sensitivity: The paper highlights the dependence of traditional evaluation metrics on the relative thickness of ground truth and prediction indicating some limitations in evaluating boundaries with varying thickness.  Questions and Feedback to the Author: 1. How does the proposed Vector Transform method perform when applied to realworld applications or integrated into larger deep networks for tasks like object detection or semantic segmentation 2. Can the Vector Transform method be further optimized or adapted for scenarios with extremely complex or ambiguous boundaries such as in medical imaging or outdoor scene analysis  Review Summary: The paper presents a novel approach to boundary detection using a vector transform representation that addresses class imbalance issues and enables the prediction of crisp boundaries without postprocessing steps. The methods theoretical foundation comprehensive evaluation and comparison with existing techniques highlight its effectiveness and superiority in boundary detection tasks. Overall the paper is wellstructured with clear explanations innovative concepts and strong empirical results supporting a positive recommendation for publication.", "oxxUMeFwEHd": "1) Summary of the Paper: The paper introduces TOGL a novel layer that incorporates topological information into graph neural networks (GNNs) using persistent homology. TOGL is designed to enhance the expressivity of existing GNN architectures by allowing them to capture topological structures like cycles that are typically overlooked by traditional messagepassing GNNs. The authors demonstrate that augmenting GNNs with TOGL leads to improved predictive performance for graph and node classification tasks on both synthetic and realworld data sets. 2) Strengths and Weaknesses Questions and Feedback: Strong Points:  Novel Contribution: The introduction of TOGL as a topological layer is a significant contribution that improves the performance of GNNs in capturing topological features.  Theoretical Grounding: The paper provides a strong theoretical foundation explaining concepts such as persistent homology and how it enhances the expressivity of GNNs.  Empirical Validation: The experimental results demonstrate the effectiveness of TOGL in improving predictive performance across various data sets and architectures. Weak Points:  Complexity: The paper covers a considerable amount of technical detail which may be overwhelming for readers unfamiliar with persistent homology and graph neural networks.  Experimental Setup: It might be beneficial to provide more insights into the hyperparameters and settings chosen for the experiments to help readers better understand the impact of these choices on the results.  Comparison to Existing Methods: While the paper compares TOGL with various GNN architectures more direct comparisons to other topologyaware algorithms could enhance the evaluation of TOGLs performance. Questions and Feedback to the Authors:  Could you provide additional insights into the choice of specific topological filtration functions used in TOGL and how different choices impact performance  Did you consider exploring the scalability of TOGL to larger data sets or domains with more complex topological structures  Have you conducted any analysis on the interpretability of TOGLenhanced GNNs to understand how they leverage topological information for classification tasks 3) Review Summary: The paper presents an innovative layer TOGL which integrates topological information into GNNs using persistent homology. The authors provide a robust theoretical background validate their approach with compelling experimental results and show that TOGL significantly improves predictive performance on graph and node classification tasks. However the paper could benefit from a clearer explanation of the experimental setup and a more extensive comparison to existing topologyaware algorithms. Overall based on the novelty of the approach strong theoretical basis and promising empirical results I recommend acceptance of the paper with minor revisions to address the identified weaknesses.", "vSix3HPYKSU": " Summary The paper introduces a novel approach to solving partial differential equations (PDEs) using neural message passing aiming to build an endtoend neural solver that satisfies various structural user and implementational requirements of PDE problems. The method is validated on fluidlike flow problems in 1D and 2D showcasing fast stable and accurate performance compared to stateoftheart numerical solvers.  Strengths 1. Innovative Methodology: The paper introduces a novel approach by combining neural message passing with autoregressive methods to build a neural solver for PDEs addressing various requirements of PDE problems. 2. Comprehensive Evaluation: The method is evaluated on various fluidlike flow problems in 1D and 2D demonstrating superior performance in terms of speed and accuracy compared to existing numerical solvers. 3. Theoretical Foundation: The paper provides a strong theoretical foundation by connecting the neural message passing methods with classical solvers like finite differences finite volumes and WENO schemes.  Weaknesses 1. Complexity: The paper introduces several advanced concepts such as the pushforward trick and temporal bundling which might be challenging for readers not wellversed in the field of PDEs and neural networks. 2. Lack of Detailed Explanations: Some critical components of the methodology such as the network architecture and training framework could benefit from more detailed explanations to enhance understanding. 3. Evaluation Scenarios: The paper could benefit from including more diverse evaluation scenarios to provide a broader perspective on the performance and generalizability of the proposed method.  Questions 1. Can you elaborate on how the proposed neural message passing solver performs in scenarios with highly dynamic or chaotic behavior in the fluid flow problems 2. How does the computational efficiency of the proposed neural solver compare to traditional numerical solvers when solving PDEs with complex geometries or irregular boundaries 3. Have you considered conducting sensitivity analyses to assess the robustness of the neural solver to variations in input parameters or model configurations  Feedback 1. Provide more detailed explanations of the network architecture and training framework to make the methodology more accessible to readers from diverse backgrounds. 2. Consider expanding the evaluation section to include more diverse scenarios providing a comprehensive analysis of the proposed methods performance and generalizability. 3. Include discussions on the robustness of the neural solver to variations in input parameters and scenarios with complex dynamics to strengthen the validation of the method.  Overall Review The paper presents an innovative approach to solving PDEs using neural message passing demonstrating superior performance in fluidlike flow problems compared to stateoftheart numerical solvers. While the methodology is theoretically sound and the evaluation is comprehensive the complexity of the concepts introduced and the need for more detailed explanations pose challenges for readers. Addressing these aspects could enhance the clarity and accessibility of the paper and further strengthen the robustness and applicability of the proposed neural solver. Therefore with some refinements in presentation and the inclusion of additional evaluation scenarios I recommend the paper for acceptance after addressing the mentioned points.", "scSheedMzl": " Summary of the Paper: The paper introduces a new method called Locally INvariant EXplanations (LINEX) inspired by the invariant risk minimization principle. LINEX aims to provide stable unidirectional and faithful explanations for blackbox models at a local level. The method is compared against popular methods such as LIME MeLIME and MAPLE on various datasets including tabular image and text data. LINEX is shown to outperform or perform comparably to the competitors particularly in stability and unidirectionality metrics.  Strengths of the Paper:  Innovative Approach: The paper introduces a novel explanation method inspired by the invariant risk minimization principle showcasing originality in the field.  Comprehensive Analysis: The paper explores various datasets and evaluations to provide a holistic understanding of the proposed methods performance.  Clear Presentation: The methodology and results are wellorganized with detailed explanations and clear presentation of findings through tables and figures.  Theoretical Underpinnings: The paper provides sound theoretical explanations and justifications for the proposed method enhancing its credibility.  Weaknesses of the Paper:  Limited RealWorld Application Discussion: The paper lacks a discussion on realworld applications or case studies where the proposed method could be practically applied.  Complex Terminology: Some parts of the paper contain technical and complex terms that may make it challenging for readers unfamiliar with the field to grasp the concepts easily.  Comparisons: While the comparisons with existing methods are detailed a deeper analysis or discussion on scenarios where each method excels or fails could provide additional insights.  Questions and Feedback to the Authors: 1. Can you provide further insights on the computational efficiency of LINEX in comparison to other methods especially in deployment scenarios with large datasets 2. How does the LINEX method handle interpretability in scenarios where explanations may conflict with ethical considerations or bias issues 3. Could you elaborate more on the generalization of LINEX to different types of models beyond the ones considered in the paper  Review Summary: The paper demonstrates a novel method LINEX for local model explanations inspired by the invariant risk minimization principle. The method shows promising results in stability and unidirectionality metrics compared to existing methods like LIME MeLIME and MAPLE. The paper is wellstructured provides a clear understanding of the method and offers valuable contributions to the explainability of blackbox models. Considering the innovative approach comprehensive analysis and theoretical foundations provided in the paper I recommend its acceptance with minor revisions to address the weaknesses highlighted.", "kO-wQWwqnO": "PeerReview 1) Summary The paper proposes a novel image enhancement model L2BGAN for translating low light images to bright images without paired supervision. The model uses geometric and lighting consistency along with contextual loss to achieve competitive results. The paper includes an extensive experimentation on benchmark datasets realtime driving datasets and image understanding tasks using DarkFace and ExDark datasets. 2) Strengths and Weaknesses Strong points:  Novel approach: The paper introduces an innovative method for low light image enhancement without paired supervision.  Comprehensive experimentation: The paper conducts a thorough analysis on benchmark datasets and realworld scenarios to demonstrate the effectiveness of the proposed model.  Multiple contributions: The use of geometric and lighting consistency contextual loss and multiscale discriminators show holistic improvements over existing techniques. Weak points:  Clarity in presentation: The paper can benefit from improved clarity in presenting the methodology and results especially in visuals provided.  Lack of comparison metrics: While the paper compares the proposed model with existing techniques it could benefit from additional quantitative metrics to support the visual results.  Scalability concerns: The paper does not explicitly discuss the scalability of the proposed model with larger datasets or more complex image scenarios. Question and Feedback to the Author: 1. Can you provide more insights on the scalability and computational efficiency of the proposed model especially when dealing with larger datasets 2. How does the proposed model handle varying degrees of noise and image quality for low light images in comparison to paired supervision methods 3) Review Summary The paper presents a compelling approach for low light image enhancement using an unpaired GANbased method showcasing significant contributions through geometric and lighting consistency contextual loss and multiscale discriminators. While the experimental results are promising the paper could benefit from improved clarity in presentation and additional quantitative metrics for comparison. Overall based on the novelty of the approach and the demonstrated results I recommend this paper for acceptance after addressing the aforementioned points.", "xRK8xgFuiu": " Summary The paper introduces a novel algorithm Causal Discovery via Cholesky Factorization (CDCF) for recovering the directed acyclic graph (DAG) structure from observed data. The algorithm is based on Cholesky factorization of the covarianceprecision matrix and has time complexity O(p2n  p3) where p is the number of nodes and n is the number of samples. The algorithm aims to recover the DAG structure with high accuracy and efficiency and outperforms existing methods in terms of both time and sample complexities. Experimental results on synthetic data proteins data sets and knowledge base data set showcase the effectiveness and efficiency of the proposed algorithm.  Strengths 1. Efficiency and Performance: The paper introduces a novel algorithm with superior time and sample complexities compared to existing methods showcasing efficient and highperformance DAG structure recovery. 2. Theoretical Guarantees: The paper provides theoretical analysis and guarantees for the exact recovery of the DAG structure under proper assumptions adding confidence to the algorithms efficacy. 3. Experimental Validation: Extensive experiments on synthetic data proteins data sets and knowledge base data demonstrate the algorithms efficiency stateoftheart performance and practical utility. 4. Novel Algorithm: CDCF presents a fresh approach to topology order search algorithms merging two phases into one for improved efficiency and accuracy in recovering DAG structures.  Weaknesses 1. Complex Presentation: The paper is highly technical and may be challenging for readers not familiar with the domain. Simplification of key concepts and clearer explanations could enhance readability. 2. Limited Comparison: The paper does not include results from all baseline methods tested making it difficult to fully assess how CDCF compares to all existing techniques. 3. Appendix Reliance: Several important details such as additional experiments and results are provided in the appendix. Incorporating some of this information into the main text could improve the papers comprehensiveness.  Questions to the Author 1. Can you elaborate on the practical implications of your algorithm for realworld applications such as in biological research or economics 2. How does the algorithm handle noisy or sparse data and what considerations were made to ensure robust performance in such scenarios 3. Are there any plans to extend the CDCF algorithm to handle nonlinear structural equation models in the future  Feedback to the Author Overall the paper presents a significant contribution to the field of causal inference and DAG structure recovery. To enhance the paper consider simplifying technical aspects for broader readership providing a more comprehensive comparison with other methods and integrating key findings from the appendix into the main text for better continuity. Additionally addressing practical implications and potential future directions could add depth to the discussion.", "gfUPGPMxB7E": "Summary of the Paper: The paper introduces two novel algorithms Unsupervised Data Sharing (UDS) and Conservative Unsupervised Data Sharing (CUDS) aiming to improve multitask offline reinforcement learning without requiring reward relabeling for every task pair. UDS involves relabeling data from other tasks with a constant reward label and using a valueaware strategy to determine which transitions to include for the new task. On the other hand CUDS refines the datasharing strategy by selectively filtering out irrelevant transitions. The empirical evaluation showcases that both UDS and CUDS outperform vanilla multitask offline RL and comparative approaches that either learn the reward function explicitly or implicitly. Strong Points: 1. Innovative Approach: Proposes novel algorithms (UDS and CUDS) that address the challenge of multitask offline RL without requiring reward relabeling for each task pair offering a fresh perspective in the field. 2. Theoretical Grounding: Provides theoretical justification for the effectiveness of UDS and CUDS offering insights into how these methods perform under different structural assumptions on the dataset. 3. Empirical Validation: Conducts a comprehensive empirical evaluation across various domains demonstrating the superior performance of UDS and further improvement with CUDS over alternative methods. 4. Comparative Analysis: Compares the proposed methods with existing stateoftheart approaches showcasing the competitiveness and effectiveness of UDS and CUDS in multitask offline RL scenarios. Weak Points: 1. Domain Specificity: The study is limited to MDPs with binary rewards and the impact of UDS and CUDS in MDPs with continuous rewards is not explored. Adding experiments with continuous rewards could enhance the generalizability of the findings. 2. Implementation Details: While the paper outlines the algorithms highlevel workings more detailed insights into the exact implementation steps hyperparameter tuning and computational complexity could benefit readers looking to replicate the experiments. 3. Theoretical Expansion: Expanding on the theoretical analysis and providing deeper insights into how UDS and CUDS fare in scenarios beyond the binaryreward assumption would strengthen the papers contribution. Question to Authors: 1. What are the key factors that influenced the decision to use binary rewards in the experimental evaluation and how might the findings differ in scenarios with continuous rewards 2. Can you elaborate on the potential challenges or limitations of UDS and CUDS when applied to more complex and largerscale multitask offline RL scenarios 3. Is there ongoing work to extend the proposed algorithms to address data sharing without reward relabeling in different reinforcement learning settings or domains beyond the ones evaluated in this study Feedback to Authors: 1. Provide more detailed information on the implementation specifics of UDS and CUDS to enhance reproducibility and facilitate a deeper understanding of the algorithmic workflow. 2. Consider expanding the theoretical discussion to address potential variations in performance when applied to MDPs with continuous rewards thereby broadening the applicability and robustness of the proposed methods. 3. Highlight the practical use cases or realworld applications where UDS and CUDS could offer significant improvements in performance and efficiency further reinforcing the relevance of the proposed algorithms in diverse environments. Review Summary: The paper presents UDS and CUDS as innovative solutions for multitask offline reinforcement learning demonstrating superior performance over existing methods in various responsive domains. While the manuscript showcases robust theoretical grounding and empirical validation addressing limitations regarding domain specificity and expanding theoretical discussions could enhance the papers impact and applicability. Recommending acceptance with minor revisions to strengthen the theoretical depth and provide additional insights for broader applicability.", "vyn49BUAkoD": " Summary: The paper focuses on addressing the biasvariance tradeoff in metamodeling with active learning using Gaussian Processes (GPs). It introduces two new acquisition functions  Bayesian QuerybyCommittee (BQBC) and Query by Mixture Gaussian Processes (QBMGP) to optimize the biasvariance tradeoff through the hyperparameters. The authors demonstrate through experiments on seven simulators that BQBC outperforms benchmark functions while QBMGP is the most robust with the best accuracy and fewest iterations. The paper highlights the importance of incorporating the biasvariance tradeoff into acquisition functions to mitigate unnecessary data labeling costs.  Strengths: 1. Novel Contribution: The paper introduces two new acquisition functions tailored for active learning with GPs to address the biasvariance tradeoff filling a gap in current research. 2. Experimental Validation: The empirical evaluation across multiple simulators provides thorough evidence of the effectiveness of the proposed acquisition functions compared to benchmark methods. 3. Comprehensive Background: The paper provides a detailed background on GPs active learning and related work setting a strong foundation for the proposed methods. 4. Clear Explanations: The methodology algorithms and experimental setups are explained cohesively making it easy to understand the proposed approach.  Weaknesses: 1. Limited Comparison: The paper focuses on comparing the proposed acquisition functions with a few benchmark methods. A more extensive comparison with a wider range of methods could enhance the comprehensiveness of the study.  Questions and Feedback to the Authors: 1. Scope of Generalizability: How generalizable are the proposed acquisition functions to a wider range of simulators or realworld problems Have you tested these functions on a broader set of diverse datasets 2. Computational Efficiency: Considering the complexity of MCMC sampling for hyperparameters estimation do the proposed acquisition functions significantly impact computational efficiency compared to traditional methods 3. Scalability: How scalable are the proposed functions to highdimensional input spaces or large datasets Have you explored their performance under such scenarios  Review Summary: The paper presents a valuable contribution to the field by introducing two new acquisition functions tailored for active learning with GPs addressing the biasvariance tradeoff in metamodeling. The comprehensive experimental evaluation demonstrates the superiority of these functions over benchmark methods across various simulators. The clarity of presentation coupled with the logical flow of the paper strengthens its impact. Recommendations for expanding the scope of comparison and exploring scalability could further enhance the robustness and applicability of the proposed methods. Overall based on the sound methodology empirical evidence and significant contributions I recommend acceptance after addressing the minor concerns outlined above.", "gaYko_Y2_l": " 1) Summary of the Paper: The paper introduces the concept of Weakly Supervised Graph Clustering (WSGC) which aims to cluster nodes in a graph using global graph labels with the help of node features and structures. It proposes a new model named Gaussian Mixture Graph Convolutional Network (GMGCN) that combines Gaussian mixture model (GMM) and graph convolutional network (GCN) to address the WSGC problem. The paper conducts experiments on synthetic S3DIS and PHEME datasets to validate the effectiveness of GMGCN in weakly supervised graph clustering tasks.  2) Evaluation of the Paper: Strong Points:  Novelty: The paper addresses a new problem of Weakly Supervised Graph Clustering proposing a unique approach using GMGCN that integrates GMM and GCN.  Comprehensive Experimentation: Extensive experiments are conducted on synthetic and realworld datasets validating the proposed GMGCNs effectiveness over various baseline methods.  Detailed Analysis: The paper provides indepth exploration into different settings of WSGC and ablation studies showcasing the importance of each component of the proposed GMGCN. Weak Points:  Lack of Comparison: The paper lacks a comparison with stateoftheart graph clustering methods making it challenging to gauge the performance improvement with other advanced techniques.  Clarity of Explanation: Certain parts of the paper can be further elaborated especially in explaining the intricacies of the proposed model and experimental setups.  Reproducibility: The paper could benefit from more detailed descriptions of the experimental setup and hyperparameters for reproducibility purposes. Questions and Feedback to the Author:  How does the proposed GMGCN model compare with the latest advancements in graph clustering methods  Can the paper provide additional insights into the scalability and efficiency of GMGCN when applied to larger graphs  Considering the rapid advancements in graph learning are there plans to optimize GMGCN for deployment in realworld applications  3) Review Summary: The paper presents a novel approach to Weakly Supervised Graph Clustering using GMGCN showcasing remarkable improvements over baseline methods in various experiments. The detailed analysis and extensive experimentation highlight the potential significance of GMGCN in addressing the WSGC problem effectively. However clarifying certain aspects and enhancing the comparative analysis with recent methods could strengthen the overall contribution of the work. Based on the novelty comprehensive experimentation and the substantial improvements demonstrated I recommend the acceptance of this paper with minor revisions to enhance clarity and", "yztpblfGkZ-": " Summary of the Paper: The paper introduces a novel graph convolution operator called BankGCN aimed at extending the capabilities of existing message passing graph convolutional networks (MPGCNs). BankGCN utilizes an adaptive filter bank to process graph data in the frequency domain enabling it to effectively capture diverse spectral characteristics of graph signals. By decomposing multichannel signals into subspaces and sharing adaptive filters BankGCN offers a compact architecture that can adaptively capture various spectral components of graph data for target applications. Extensive experiments showcase that BankGCN outperforms existing MPGCNs and spectral methods on a variety of benchmark graph datasets in terms of performance and generalization.  Strengths of the Paper: 1. Innovation: The paper introduces a novel approach BankGCN that extends existing graph convolutional networks to efficiently represent graph data in terms of frequency filtering. 2. Performance: Extensive experiments and comparisons show that BankGCN achieves excellent performance on various benchmark graph datasets outperforming existing methods in terms of classification accuracy and generalization. 3. Adaptive Filter Bank: The use of an adaptive filter bank allows BankGCN to effectively handle diverse spectral characteristics of graph data offering a more robust and flexible representation. 4. Compact Architecture: The paper highlights that BankGCN provides a compact architecture compared to existing spectral methods avoiding overfitting issues while maintaining high performance.  Weaknesses of the Paper: 1. Complexity: The paper delves into technical details and mathematical formulations which may be challenging for readers unfamiliar with graph convolutional networks and signal processing. 2. Limited Practical Examples: While the theoretical framework is welldeveloped the paper could benefit from more practical examples or case studies to demonstrate the realworld applicability of BankGCN.  Questions for the Authors: 1. How does the proposed BankGCN model scale with larger and more complex graph datasets in terms of computational complexity and training time 2. Can you provide insights on potential limitations or challenges that might arise when applying BankGCN to realworld graph data scenarios outside of the benchmarks used in the experiments  Feedback for the Authors:  The paper could be enhanced by providing visual representations or case studies that demonstrate the application of BankGCN to realworld graph data.  Consider highlighting the practical implications and potential use cases of BankGCN in diverse domains to make the research more accessible to a wider audience.  It would be beneficial to include discussions on the interpretability of the learned spectral characteristics by BankGCN and how they can guide decisionmaking in various applications.  Review Summary: The paper introduces a novel graph convolution operator BankGCN which addresses the limitations of existing methods in capturing diverse spectral characteristics of graph data. It presents a wellthoughtout architecture with adaptive filter banks and subspace decomposition leading to superior performance on benchmark graph datasets. While the paper is technically strong and provides valuable insights into spectral processing in graph data enhancing it with practical examples and addressing potential scalability challenges will further strengthen its impact. Overall I recommend accepting this paper for publication with minor revisions to improve readability and practical applicability.", "gc8zLQWf2k": "1) Summary of the paper: The paper delves into the impact of memorization on adversarial trained deep neural networks (DNNs). It observes that while memorizing atypical samples can improve clean accuracy on test atypical samples it does not enhance robustness. Additionally the paper finds that memorizing atypical samples can harm the performance on typical samples. Based on these observations the paper proposes a novel algorithm called Benign Adversarial Training (BAT) to mitigate the negative effects of memorizing harmful atypical samples while enhancing performance on useful atypical samples. 2) Strong and Weak Points Questions and Feedback: Strong Points:  The paper addresses a relevant and significant topic related to the impact of memorization on adversarial training in DNNs.  Empirical evidence and theoretical analysis are provided to support the observations and proposed solution.  The proposed BAT algorithm shows promising results in terms of clean accuracy and adversarial robustness tradeoff.  The paper provides detailed explanations and discussions on its methodology findings and implications. Weak Points:  The paper is quite technical and may be challenging to follow for readers unfamiliar with the subject matter.  It would be beneficial to include a more detailed discussion on the practical implications of the research findings.  Additional insights on the scalability and efficiency of the proposed algorithm in realworld applications would strengthen the paper. Questions and Feedback to the Author:  How does the proposed BAT algorithm compare with other stateoftheart methods in terms of computational efficiency  Could you provide more insights into the practical applications and realworld implications of the research findings  Have you considered conducting experiments on additional datasets or scenarios to further validate the effectiveness of the BAT algorithm  Clarify the selection criteria for hyperparameters \u03b1 and \u03b2 in BAT and discuss any sensitivity analyses performed. 3) Review Summary: The paper presents a wellstructured study on the impact of memorization in adversarial trained DNNs and proposes a novel algorithm Benign Adversarial Training to address the disadvantages of memorizing harmful atypical samples. The research findings are supported by empirical experiments and theoretical analyses showcasing the effectiveness of the BAT algorithm in achieving a better tradeoff between clean accuracy and adversarial robustness. While the paper is comprehensive and insightful further enhancements such as practical implications comparisons with alternative methods and scalability considerations would strengthen its contribution to the field. Overall the paper offers valuable insights into improving the performance of adversarial trained DNNs through effective memorization strategies. Recommendation: Given the significance and contributions of the research I recommend this paper for publication with minor revisions to address the weak points and incorporate the feedback provided.", "qTHBE7E9iej": " Summarize the Paper The paper introduces an approach to learn hierarchical latent mixture skills from offline data allowing for the transfer and adaptation of motor skills to different tasks and scenarios. The method captures both discrete and continuous variations in skill levels enabling reusable and adaptable behaviors. The effectiveness of the proposed approach is demonstrated in manipulation tasks showcasing improved sample efficiency and performance in various transfer scenarios.  Strong Points  Innovative Methodology: The proposed Hierarchical Latent Mixtures of Skills (HeLMS) approach combines discrete and continuous elements in a threelevel hierarchy to capture varied behaviors effectively.  Transferability: The model demonstrates successful transfer and adaptation of skills to new tasks unseen objects and different policy modalities improving sample efficiency and performance.  Interpretability: The learned skills are distinct intuitive and reusable indicating the ability of the model to capture meaningful behavioral patterns.  Comprehensive Evaluation: The paper includes extensive experiments ablations and analysis to validate the proposed methods effectiveness and understand its underlying principles.  Weak Points  Complexity: The complexity of the proposed model may limit its practical applicability in realworld scenarios that require simpler solutions.  Generalization: While the model performs well in most scenarios there are instances (e.g. object set 2) where it struggles indicating potential challenges in generalizing skills to all types of tasks.  Additional Experiments: While the paper covers a wide range of experiments further investigations into specific aspects such as temporal consistency finetuning lowerlevel behaviors and encouraging directed exploration could provide deeper insights.  Questions and Feedback to the Author 1. Complexity vs. Practicality: How does the complexity of the proposed approach impact its scalability and practical deployment in realworld robotic systems that require simpler and more interpretable solutions 2. Generalization Challenges: Can you elaborate on the specific challenges faced when generalizing skills to different object sets especially in scenarios where the models performance is not as strong 3. Temporal Consistency: In what ways can the model enhance the emphasis on temporal consistency to further improve transferability and adaptation of behaviors across tasks  Review Summary The paper presents a novel approach HeLMS for learning hierarchical latent mixture skills from offline data demonstrating effective transfer and adaptation to new tasks and scenarios. The methodology shows promise in improving sample efficiency and performance in manipulation tasks. Despite the complexity of the model and some challenges in generalization the comprehensive evaluation and interpretability of the learned skills contribute significantly to its strengths. Additional experiments focused on mitigating generalization challenges and enhancing temporal consistency could further strengthen the proposed approachs applicability and effectiveness. Overall the paper offers a valuable contribution to the field of robotic learning and skill transfer.", "xm6YD62D1Ub": "PeerReview Summary: The paper introduces VICReg a method for selfsupervised image representation learning that aims to prevent the collapse problem while achieving results comparable to stateoftheart methods on downstream tasks. VICReg achieves this by applying two regularization terms variance and covariance separately to the embedding vectors produced by the encoder branches. The method does not require weight sharing normalization techniques or contrastive samples making it more flexible for different architectures and tasks. Experimental results demonstrate the effectiveness of VICReg on various image recognition tasks. Strengths: 1. Novel Approach: VICReg introduces a unique methodology for selfsupervised learning that explicitly addresses the issue of collapse and stabilizes training without the need for common techniques like weight sharing or contrastive samples. 2. Flexibility: Unlike other methods VICReg does not impose strict constraints on architecture weight sharing or normalization making it more adaptable to various scenarios including multimodal signals. 3. Performance: The experimental results show that VICReg achieves competitive results on downstream tasks demonstrating its efficacy in learning highquality representations. Weaknesses: 1. Lack of Theoretical Analysis: The paper lacks a detailed theoretical analysis of how VICReg performs compared to other methods. Providing insights into why and how the proposed method works could strengthen the paper. 2. Empirical Evaluation: While the experimental results demonstrate the effectiveness of VICReg a more indepth analysis of different scenarios and comparisons with more existing methods would enhance the papers credibility. Question to the Author: 1. Can you provide more insights into the tradeoffs between the variance and covariance regularization terms in VICReg How do these terms impact the learning process and the quality of representations Feedback to the Author: 1. Consider including a more comprehensive discussion on the limitations of VICReg and potential areas for future research to further enhance the applicability and performance of the method. Review Summary: Overall the paper presents a novel approach VICReg for selfsupervised image representation learning that effectively addresses the collapse problem without the need for common techniques. The methods flexibility and competitive performance on downstream tasks make it a promising contribution to the field. However a more thorough theoretical analysis and extensive empirical evaluation would enhance the papers impact and credibility. Thus I recommend further elaboration on the theoretical principles behind VICReg and conducting more indepth comparisons with existing methods to reinforce the papers findings.", "fuYtttFI-By": " Review of \"Neural Networks for 3D Snapshot Microscopy Reconstruction\"   1) Summary of the Paper: The paper presents a novel neural network architecture FourierNet designed for decoding information from multiple depths in a 3D snapshot image captured through advanced microscopy techniques. The study focuses on fast volumetric imaging using nonlocal optical encodings and computational reconstruction for applications in biological imaging. The research showcases the efficiency of FourierNet in engineering and reconstructing optical encoders for 3D snapshot microscopy outperforming existing stateoftheart methods.   2) Strengths and Weaknesses: Strong Points:  Innovative Approach: The introduction of the FourierNet architecture based on global kernel Fourier convolutional neural networks is a significant contribution to the field of computational microscopy.  Performance: The paper demonstrates superior reconstruction and optimization results compared to existing UNetbased architectures showcasing the effectiveness of the proposed FourierNet architecture.  Application Relevance: Addressing the need for fast highquality volumetric imaging in biological applications especially in neural activity studies demonstrates the practical importance of the research. Weak Points:  Lack of Experimental Validation: While simulation results are provided the absence of experimental validation on physical systems limits the practical applicability of the proposed method.  Complexity: The sheer number of parameters involved in the optical encoder (over 106) while necessary could potentially make implementation and optimization challenging in realworld scenarios.  Training Times: The time required for training the proposed networks especially for largescale experiments could be a limiting factor for practical adoption. Question and Feedback: 1. Question: Have you considered experimentally validating the efficiency and accuracy of the proposed FourierNet architecture on real microscopy systems with physical samples to demonstrate its practical feasibility Feedback: While simulation results are promising integrating some level of experimental validation can strengthen the credibility of the proposed method and enhance its potential for realworld applications. 2. Question: How do you plan to address the challenge posed by the computational burden of simulating and optimizing nonlocal encoders especially considering the need for multiGPU parallelization Feedback: Providing insights into potential strategies or optimizations to mitigate the computational challenges associated with largescale simulations could enhance the practicality of the proposed approach.   3) Review Summary: The paper introduces a novel neural network architecture FourierNet for decoding nonlocal optical encodings in 3D snapshot microscopy with enhanced efficiency and performance compared to existing methods. While the research demonstrates promising results through simulations the lack of experimental validation and potential challenges concerning computational complexity and training times raise concerns about realworld applicability. Addressing these aspects through experimental verification and discussions on optimization strategies for computational efficiency can further enhance the significance and practical utility of the proposed FourierNet architecture. Overall the paper presents a valuable contribution to the field of computational microscopy but would benefit from additional insights and experimentation to reinforce the applicability and robustness of the proposed methodology.  This review provides a critical assessment of the strengths weaknesses and potential areas for improvement in the paper offering constructive feedback to enhance the researchs impact and significance.", "wNsNT56zDkG": "Peer Review: 1. Summary: The paper focuses on analyzing the generalization of adversarial training through adversarial Rademacher complexity for deep neural networks. Current analysis of adversarial Rademacher complexity is extended to deep neural networks providing upper bounds for robust generalization gap. The paper presents theoretical analyses and empirical experiments using VGG networks on CIFAR10 dataset. 2. Strengths:  Theoretical Rigor: The paper offers a thorough analysis of adversarial Rademacher complexity for deep neural networks and provides clear theoretical insights into the reasons why adversarial training does not generalize well.  Novelty: The study extends the existing bounds to deep neural networks addressing a gap in the literature and providing valuable insights into the challenges of adversarial training.  Empirical Experiments: The experiments conducted on VGG networks on CIFAR10 dataset support the theoretical findings enhancing the credibility of the research.  Clear Explanations: The paper provides detailed explanations of concepts theorems and experiments making it accessible for readers to grasp the complex subject matter. 3. Weaknesses:  Incomplete Discussion: The paper could benefit from a more detailed discussion on the implications of the findings for practical applications and further research directions.  Clarity in Results Presentation: While the experiments are valuable the presentation of results could be enhanced by providing more detailed visualizations or tables to aid in understanding the comparisons made. Question to Author: 1. How do you envision the practical applications of your research findings in enhancing the robustness of deep neural networks against adversarial attacks in realworld scenarios 2. Considering the significant difference in weight norms between standard and adversarial training do you anticipate any implications for model interpretability or computational efficiency in deploying adversariallytrained models Feedback to Author: 1. Strengthen the Discussion Section: Expand on the practical implications of the research findings for realworld applications and provide insights into potential avenues for further research in this area. 2. Improve Visualization of Results: Enhance the presentation of experimental results through detailed visualizations or tables to facilitate better understanding and interpretation by the readers. Review Summary: The paper provides valuable insights into the generalization issues of adversarial training for deep neural networks through the lens of adversarial Rademacher complexity. The theoretical analysis and empirical experiments conducted on VGG networks demonstrate the impact of weight norms on the performance of adversariallytrained models. Strengthening the discussion section and improving result visualization will enhance the overall quality of the research. Given the papers theoretical rigor and novel contributions I recommend acceptance with minor revisions to address the identified weaknesses.", "viWF5cyz6i": " Paper Summary: The paper proposes a novel fast principal component analysis (PCA) algorithm that operates based on a userprescribed tolerance rather than a fixed rank. The algorithm aims to provide an approximate truncated singular value decomposition (TSVD) that satisfies certain properties. The approach is compared with existing fast TSVD methods showcasing its utility and efficiency across various matrices in applications like image data computational fluid dynamics video data and kernel matrices.  Strengths: 1. Innovative Approach: The proposed tolerancebased PCA algorithm is a novel contribution that addresses a key challenge in TSVD algorithms offering potential benefits over traditional methods. 2. Theoretical Justification: The paper provides detailed mathematical derivations and proofs to support the algorithms properties ensuring reliability and accuracy. 3. Experimental Validation: The algorithm is rigorously tested on various test matrices demonstrating its efficacy in accurately approximating the TSVD while maintaining computational efficiency. 4. Comparative Analysis: Comparative evaluation against existing methods like TSVD and randQB EI provides a clear understanding of the algorithms performance and advantages. 5. Comprehensive Explanations: The paper offers detailed explanations of the algorithms derivations and experimental setups making it accessible to readers with varying levels of expertise in the field.  Weaknesses: 1. Technical Complexity: The paper is highly technical which may make it challenging for readers without a deep understanding of singular value decompositions and related concepts to grasp the content. 2. Validation Scope: While the experimental results are informative expanding the comparisons to additional stateoftheart algorithms or datasets could further enhance the comprehensiveness of the evaluation. 3. Clarity of Implementation Details: The paper could benefit from additional clarity in explaining the practical implementation of the proposed algorithm making it easier for readers to replicate and understand the methodology.  Questions and Feedback to the Author: 1. Scalability Concern: How does the proposed algorithm perform in terms of scalability when dealing with extremely large matrices with high dimensionality 2. Evaluation Metrics: Are there any additional evaluation metrics or experiments planned to further validate the algorithms performance and compare it with other cuttingedge methodologies 3. Open Source Implementation: Will the authors provide the implementation code or a reference to a repository for researchers and practitioners to use and validate the algorithm in their work 4. RealWorld Applications: Could the authors provide more insights into potential realworld applications where this algorithm could be particularly beneficial or showcase its practical advantages  Review Summary: The paper presents a wellstructured and comprehensive approach to addressing the challenge of fast PCA algorithms based on a toleranceprescribed method. With a strong theoretical foundation rigorous experimentation and clear explanations the proposed algorithm demonstrates promising results in approximating TSVD efficiently. While improvements in clarity and broader validation could enhance the papers impact the innovative approach and thorough analysis make it a valuable contribution to the field of data science and machine learning. I would recommend acceptance with minor revisions to address the outlined weaknesses and queries.", "giBFoa-uS12": "Summary: The paper introduces InfoPG an informationtheoretic approach for MultiAgent Reinforcement Learning (MARL) that maximizes Mutual Information (MI) among agents by reformulating an agents policy to be conditional on the policies of its neighboring teammates. The paper builds on decisionmaking under bounded rationality and cognitive hierarchy theory proposing a decentralized cooperative MARL framework where agents communicate with their neighbors to achieve consensus in actions and maximize shared utility. InfoPG outperforms existing baselines in learning emergent collaborative behaviors and sets the stateoftheart in decentralized cooperative MARL tasks demonstrating higher sample efficiency and larger cumulative rewards in complex multiagent domains. Furthermore the paper introduces an Adv. InfoPG variant that modulates MI depending on agents cooperativity demonstrating robust performance in scenarios with fraudulent agents. Strengths: 1. Innovative Approach: The paper presents a novel informationtheoretic framework InfoPG for maximizing MI among cooperating agents contributing to the advancement of cooperative MARL research. 2. Theoretical Foundation: Drawing on cognitive hierarchy theory the paper establishes a solid theoretical foundation for decisionmaking under bounded rationality enhancing the credibility and depth of the proposed approach. 3. Empirical Validation: Thorough empirical evaluations across diverse cooperative MARL domains demonstrate the effectiveness of InfoPG and Adv. InfoPG showcasing their superior performance compared to existing baselines. 4. Resilience to Fraudulent Agents: The paper addresses the challenge of handling fraudulent agents in cooperative scenarios by introducing Adv. InfoPG which showcases robust performance in BGP simulations. Weaknesses: 1. Complexity: The paper may be challenging to follow for readers unfamiliar with MARL and informationtheoretic concepts potentially limiting the accessibility of the findings. 2. Limited Generalization: The experiments are conducted on a specific set of cooperative MARL domains restricting the generalizability of the proposed InfoPG framework to a broader range of scenarios. 3. Assumptions and Simplifications: The uniform priors assumption and strict nonnegative reward condition may oversimplify realworld scenarios potentially limiting the applicability of the proposed approach in more complex environments. 4. Evaluation Metrics: While the paper provides detailed experimental results a more comprehensive comparison with additional stateoftheart methods or diverse scenarios could further strengthen the empirical validation. Questions to the Authors: 1. How would the InfoPG framework adapt to scenarios with dynamic communication graphs or varying levels of agent cooperation 2. Could the proposed approach be extended to handle largerscale multiagent systems with diverse behavioral patterns and hierarchical reasoning depths 3. How sensitive is the performance of InfoPG to hyperparameter choices and initialization configurations especially in scenarios with highdimensional action spaces or complex environment dynamics 4. Have you considered exploring the application of InfoPG in realworld multiagent systems or collaborative robotic tasks to assess its feasibility and scalability outside simulated environments Feedback: Overall the paper presents a wellstructured and insightful contribution to the field of cooperative MARL highlighting the importance of information sharing and rational decisionmaking mechanisms in enhancing team cognition and coordination. Enhancing the clarity of theoretical explanations and addressing potential limitations in generalizability and complexity could further strengthen the impact and applicability of the proposed InfoPG framework. Additionally considering diverse scenarios and evaluating the approach in practical settings could provide valuable insights into the scalability and realworld utility of the proposed method.", "iGffRQ9jQpQ": " Summary of the Paper The paper introduces a novel semisupervised learning framework called Selfinterested Coalitional Learning (SCL). The framework aims to address the challenges faced by traditional semisupervised learning methods that heavily rely on labeled data and suffer from error accumulation. SCL incorporates a companion task of discriminating label observability which allows for better utilization of information from both labeled and unlabeled data. Through theoretical deduction and empirical evaluations on classification and regression tasks the paper demonstrates that SCL consistently enhances the performance of semisupervised algorithms.  Strong Points 1. Novel Contribution: The paper introduces a fresh perspective with the SCL framework addressing the challenges of overreliance on labeled data and error accumulation in semisupervised learning. 2. Comprehensive Evaluation: The paper provides a thorough evaluation of the proposed framework on various tasks showcasing its consistent enhancement in performance compared to traditional semisupervised methods. 3. Theoretical Foundation: The theoretical deduction of the frameworks connection to loss reweighting on noisy labels adds depth to the proposed approach. 4. Algorithmic Framework: The practical algorithmic framework of SCL is welldescribed providing clarity on its implementation. 5. Interpretability: The interpretability of discriminator outputs offers additional insights that can be beneficial for realworld applications with noisy data.  Weak Points 1. Clarity: While the paper is detailed certain parts especially in the theoretical derivations and algorithms may be challenging to understand for readers without a strong background in machine learning. 2. Experimental Setup: While the empirical evaluations are comprehensive a more detailed comparison with existing stateoftheart methods could enhance the papers credibility. 3. Implementation Challenges: The practical implementation of SCL in a wide range of algorithms with limited changes could raise questions about the complexity and adaptability of the approach in different scenarios.  Questions for the Author 1. Can you provide more insights into the practical implementation challenges of incorporating SCL into various semisupervised algorithms especially in scenarios beyond the ones presented in the paper 2. How does the proposed loss reweighting mechanism address the issue of error accumulation in pseudolabeling and are there any specific scenarios where it may not be as effective 3. Have you considered any realworld applications or datasets with extreme noise or label errors to test the robustness and adaptability of SCL in more challenging settings  Feedback to the Author The paper presents a compelling and innovative approach with the Selfinterested Coalitional Learning framework for semisupervised learning. Strengthening the clarity and understanding of complex theoretical derivations and algorithms as well as providing additional insights from comparisons with stateoftheart methods would further enhance the papers impact and credibility. Additionally exploring the practical implications and realworld applications of SCL in diverse and noisy data settings could provide further validation of its effectiveness. Overall the paper shows promise in advancing the field of semisupervised learning with a unique and practical approach.", "l5aSHXi8jG5": " 1. Summary of the Paper The paper investigates the targeted transferability of optimization attacks against Automatic Speech Recognition systems (ASRs). It explores why existing optimization attacks successful in other domains fail to exhibit transferability in ASRs. Through exhaustive experiments and ablation studies the paper identifies six factors impacting transferability in ASRs: input type Mel Frequency Cepstral Coefficient (MFCC) Recurrent Neural Network (RNN) output type vocabulary size and sequence size. The study reveals that these factors not only hinder transferability but are also essential for the accurate functioning of ASRs.  2. Strengths and Weaknesses  Strong Points:  Comprehensive Analysis: The paper provides a thorough examination of the factors influencing the transferability of optimization attacks in ASRs filling a crucial gap in research.  Novel Findings: Uncovering previously unknown factors that limit transferability in ASRs adds significant value to the field of adversarial attacks on ASRs.  Methodological Rigor: The experimental design including ablation studies and control experiments demonstrates a robust approach to investigating the problem.  Clarity and Structure: The paper is wellorganized and effectively communicates complex concepts making it accessible to a wide audience.  Weak Points:  Limited Realworld Application: The study focuses on simplified ASR models which may not fully represent the complexity of realworld ASRs potentially limiting the generalizability of the findings.  Lack of Comparative Analysis: While the paper effectively explores the impact of each factor on transferability more comparisons with existing approaches or related works could enhance the depth of the analysis.  Future Directions: The paper mentions potential future research directions but could benefit from a more explicit discussion on how to bridge the identified gaps and translate findings into practical solutions.  Questions and Feedback to the Author: 1. Generalization Concerns: How do you plan to validate the findings from simplified ASR models in realworld complex ASRs to ensure the transferability of results 2. Potential Experiments: Have you considered conducting experiments with a wider variety of optimization attacks or exploring the impact of combining multiple factors simultaneously on transferability 3. Application Relevance: How do you envision translating the identified factors into practical strategies for enhancing ASR security against adversarial attacks in realworld applications  3. Review Summary The paper presents a wellstructured and comprehensive study on the factors that hinder targeted transferability of optimization attacks in ASRs. By identifying six critical factors impacting transferability the paper offers valuable insights into the limitations of optimization attacks in the audio domain. The rigorous experimental approach and novel findings contribute significantly to the research field. Recommendations for potential future research directions and practical implications enhance the papers overall impact. Given the papers thorough analysis novel findings and methodological rigor I recommend acceptance for publication with minor revisions addressing the potential to bridge the gap between simplified ASR models and realworld applications enhancing the experimental comparisons and elaborating on practical implications for mitigating adversarial attacks in ASRs.", "wfZGut6e09": " Summary The paper introduces a policy gradient method for MultiObjective Reinforcement Learning under unknown linear preferences. It enforces Pareto stationarity a firstorder condition for Pareto optimality to design a simple policy gradient algorithm that approximates the Pareto front and infers unknown preferences. The key innovation is the introduction of Pareto Policy Adaptation (PPA) a loss function that adapts the policy to any preference distribution using implicit differentiation. The method is evaluated through experimentation on reinforcement learning tasks.  Strengths 1. Innovative Approach: The paper introduces a novel method that bridges the gap between single and multiple policy MultiObjective Reinforcement Learning methods. 2. Theoretical Foundations: The paper provides a strong theoretical foundation by introducing concepts like Pareto stationarity and implicit differentiation in adapting the policy. 3. Experimental Evaluation: The paper includes a comprehensive experimental evaluation across various domains comparing against stateoftheart baselines demonstrating the effectiveness of the proposed method.  Weaknesses 1. Complexity in Backpropagation: Backpropagating through the numerical solvers solutions may introduce inefficiency in training for practical problems. 2. Limited analytical solutions: The optimal solution is analytically known only for M  2 which may limit the broader applicability of the method to higherdimensional preferences. 3. Control over preferences: Lack of direct control over identified preferences may limit adaptability to specific problem scenarios.  Questions to the Author 1. How does the implicit differentiation approach impact training efficiency compared to traditional methods in practice 2. Are there considerations for extending the methods applicability to cases where analytical solutions for preferences are not available for M  2 3. How does the PPA loss function handle scenarios where preferences are drastically different from those inferred by the method  Feedback The paper presents a novel policy gradient method for MultiObjective Reinforcement Learning under unknown linear preferences. The theoretical foundations innovative approach and comprehensive experimental evaluation showcase the methods strengths. However addressing the complexity in backpropagation and enhancing control over preferences for practical adaptability would further strengthen the method. Overall the paper provides valuable contributions to the field of MultiObjective Reinforcement Learning and is recommended for publication.", "s03AQxehtd_": "1) Summary of the Paper: The paper introduces ProtoRes a novel neural architecture designed to reconstruct full human poses from sparse and variable user inputs in the context of pose authoring and game development. The proposed model combines residual connections with a prototype encoding scheme achieving better accuracy and computational efficiency compared to existing solutions. ProtoRes leverages highquality motion capture data to develop a learnable model for skeleton Inverse Kinematics (IK) mapping desired joint configurations to predicted skeleton internal parameters. The paper also introduces two new datasets for the static human pose modeling problem and provides a detailed analysis of the models architecture training methodology and experimental results across various evaluation setups. 2) Strengths and Weaknesses: Strong Points:  Innovative Approach: The paper introduces a novel neural architecture (ProtoRes) to reconstruct full human poses from sparse user inputs addressing an important problem in animation and game development.  Superior Performance: ProtoRes outperforms existing ML baselines and nonlearnable IK models in terms of accuracy and computational efficiency as demonstrated through comprehensive evaluation setups.  User Interface Development: The integration of ProtoRes into the Unity game engine and the emphasis on userfacing tools indicate practical applicability and userfriendly implementation.  Detailed Methodology: The paper provides indepth explanations of the models architecture training methodology loss functions and ablation studies enhancing clarity and reproducibility. Weak Points:  Lack of Temporal Consistency: The paper acknowledges the limitation of focusing on discrete poses which may hinder capturing temporal dynamics in motion sequences.  Approximate Satisfaction of Constraints: While the model reconstructs poses from sparse inputs effectively there may be limitations in satisfying constraints precisely especially for exotic poses.  Evaluation Setup Comparison: Direct comparison between the random effector evaluation and the 5point benchmark could be challenging due to different evaluation contexts. Questions and Feedback to the Author:  Could you elaborate on how ProtoRes handles the temporal dimension in pose reconstruction tasks especially in scenarios requiring smooth transitions between poses  How does ProtoRes perform when confronted with poses significantly deviating from the distribution of training data and are there strategies to address this challenge  Have you considered incorporating explicit constraints or heuristics into ProtoRes to enhance the models ability to generate poses with specific characteristics or qualities  Could you discuss potential realworld applications beyond game development where ProtoRes could be beneficial given its capabilities in reconstructing complex human poses from sparse inputs 3) Review Summary: The paper presents a significant contribution in the field of pose modeling and animation by introducing ProtoRes a learnable neural architecture for reconstructing full human poses from sparse and variable user inputs. The detailed analysis experimental results and comparison with existing ML baselines and nonlearnable IK models showcase the superior performance of ProtoRes in terms of accuracy and computational efficiency. The integration of userfacing tools in Unity further strengthens the practical relevance of the proposed model in game development and animation. While the paper provides a robust methodology and comprehensive evaluation there are areas for improvement such as addressing temporal consistency handling constraints more precisely and exploring applications outside of game development. Overall based on the strengths of the paper in terms of innovation performance and practical implications I recommend proceeding with minor revisions to address the identified limitations and provide further insights into the models capabilities and potential applications.", "j8J97VgdmsT": " Summary: The paper introduces FLAMEinNeRF a neural rendering method for controllable portrait video synthesis. It addresses the challenge of modeling dynamic and controllable objects specifically focusing on portrait videos with explicit control over facial expressions. The method utilizes a neural radiance field combined with a 3D Morphable Model to enable expression control and novel view synthesis. The paper demonstrates the effectiveness of FLAMEinNeRF through qualitative and quantitative evaluations.  Strengths: 1. Innovative Approach: Introducing a novel method for controllable portrait video synthesis is a significant contribution to the field. 2. Effective Problem Statement: Clearly defining the problem of lacking controllable human head models for novel view synthesis sets the context well. 3. Technical Depth: The paper demonstrates a strong technical understanding with detailed explanations of the proposed method and its components. 4. Experimental Validation: The thorough evaluation including comparisons with baseline methods ensures the validity and effectiveness of FLAMEinNeRF. 5. Practical Applications: Highlighting the potential impact of the method in entertainment education and humancomputer interaction adds value to the research.  Weaknesses: 1. Limitation on Large Head Movements: The methods inability to model large head movements is a significant limitation that needs to be addressed for broader applicability. 2. Dependency on Subjects Stillness: Requiring the subject in the portrait video to stay relatively still restricts the methods use in dynamic scenarios. 3. Control Over Head Pose: Lack of control over head pose poses a limitation that could be addressed in future work.  Question to the Author: 1. How do you plan to enhance FLAMEinNeRF to handle large head movements for improved flexibility in portrait video synthesis 2. Can you provide insights into potential extensions of the method to allow for more dynamic scenes and diverse facial expressions  Feedback: The paper presents a compelling method with clear contributions and technical details. Enhancing the flexibility to model large head movements and improving the control over head pose could significantly enhance the methods utility. Additionally discussing potential directions for addressing these limitations in future work would enrich the paper and further contribute to the field of neural rendering and portrait video synthesis.  Recommendation: Based on the innovative approach comprehensive evaluation and potential impact I recommend accepting the paper with minor revisions to address the limitations highlighted and provide insights into future research directions. The methods significance and contribution to the field of controllable portrait video synthesis make it a valuable addition to the academic community.", "shpkpVXzo3h": " Peer Review:  Summary: The paper presents a novel approach to developing 8bit optimizers that maintain the performance of 32bit optimizers particularly focusing on reducing memory footprint in largescale neural network models. The proposed approach combines blockwise dynamic quantization dynamic quantization and a stable embedding layer to achieve high precision quantization while significantly decreasing memory usage. The study demonstrates the effectiveness of the 8bit optimizers across various tasks showcasing their potential as a dropin replacement for 32bit optimizers without the need for hyperparameter adjustments.  Strengths: 1. Innovative Approach: The paper introduces a novel concept of 8bit optimizers and effectively demonstrates their ability to maintain performance levels while reducing memory requirements. 2. Comprehensive Analysis: The study covers a wide range of tasks reflecting the versatility and applicability of the proposed 8bit optimizers on various benchmarks and datasets. 3. Detailed Methodology: The authors provide a thorough explanation of the methods employed including blockwise dynamic quantization dynamic quantization and stable embedding layer enabling readers to understand the technical aspects of the work. 4. Replication Results: The results are supported by replication studies enhancing the credibility of the findings and ensuring the robustness of the proposed 8bit optimizers.  Weaknesses: 1. Complexity: The highly technical nature of the paper may pose challenges for readers unfamiliar with optimization methods and quantization techniques. Simplification or additional explanation in certain sections could improve accessibility. 2. Evaluation Metrics: While the paper presents impressive results in reducing memory footprint a deeper insight into runtime performance improvements and computational efficiency could enhance the comprehensiveness of the evaluation.  Questions  Feedback to the Authors: 1. Could the paper provide more insights into the comparative computational efficiency of 8bit optimizers versus 32bit in terms of runtime performance 2. How do the proposed 8bit optimizers compare with existing stateoftheart optimization methods such as AdaGrad particularly in terms of stability and convergence speed 3. Consider discussing potential limitations or challenges faced during the implementation and deployment of 8bit optimizers to provide a more holistic view of the practical implications of the approach.  Review Summary: Overall the paper presents a significant contribution to the field by introducing 8bit optimizers that maintain the performance levels of 32bit optimizers while reducing memory overhead. The innovative combination of blockwise dynamic quantization dynamic quantization and a stable embedding layer demonstrates promising results across various tasks. However further exploration of runtime performance comparison with other optimization methods and discussion on practical challenges could enhance the impact of the study. Based on the comprehensive analysis and robust methodology presented I recommend acceptance of the paper with minor revisions to address the outlined weaknesses and questions for further clarification.", "o6dG7nVYDS": " Review of \"Understanding Domain Generalization: A LearningTheoretic Perspective\"  Summary: The paper addresses the domain generalization (DG) problem setting in machine learning where models are trained on multiple known data distributions to generalize well on unseen distributions. The authors present a novel learningtheoretic generalization bound for DG linking performance to the models Rademacher complexity. They propose that existing methods efficacy is determined by a tradeoff between empirical risk and model complexity affecting their performance variability. The paper also suggests that regularized empirical risk minimization with leaveonedomainout crossvalidation may achieve domain generalization effectively. Empirical results on the DomainBed benchmark support these claims.  Strong Points:  Theoretical Analysis: The paper presents a rigorous theoretical analysis linking domain generalization performance to model complexity providing valuable insights into the tradeoff between empirical risk and complexity.  Empirical Validation: The empirical validation on the DomainBed benchmark reinforces the theoretical findings and sheds light on the impact of model complexity on domain generalization performance.  Practical Recommendations: The paper offers practical recommendations on model selection strategies and regularized optimization for optimal domain generalization performance.  Methodological Clarity: The papers methodology is welldefined and the experiments conducted on linear models and neural networks provide a comprehensive understanding of the proposed concepts.  Weak Points:  Complexity Control in Deep Learning: The paper acknowledges the challenge of accurately controlling model complexity in deep learning models due to various factors like optimization parameters batch size and weight initialization. It might be beneficial to provide further insights on addressing these challenges.  Comparative Analysis: While the paper compares different methods and provides insights into their performance variability a more indepth comparative analysis of the approaches and their effectiveness in achieving domain generalization could enhance the studys impact.  Questions and Feedback to the Author: 1. Could you elaborate on specific strategies or approaches that could be utilized to effectively control model complexity in deep learning models considering the various confounding factors involved in the training process 2. Given the complexity of hyperparameter tuning and model selection in neural methods do you have any suggestions or insights on potential advancements or methods that could improve the accuracy and reliability of hyperparameter tuning for domain generalization 3. Have you considered expanding the empirical analysis to include a more extensive comparison of different stateoftheart DG methods to provide a more detailed understanding of their effectiveness in achieving domain generalization  Review Summary: The paper \"Understanding Domain Generalization: A LearningTheoretic Perspective\" offers a novel learningtheoretic perspective on the domain generalization problem linking performance to model complexity and providing insights into methods for achieving effective domain generalization. The combination of theoretical analysis empirical validation and practical recommendations strengthens the papers contribution to the domain generalization research landscape. While the study provides valuable insights addressing complexities in deep learning model control and extending the comparative analysis could enhance the comprehensiveness and impact of the research. Overall the papers theoretical foundations empirical evidence and practical implications make it a valuable contribution to the field of domain generalization.  Recommendation: Based on the robust theoretical analysis empirical validation and practical recommendations provided in the paper I recommend accepting this manuscript for publication in its current form. The studys contributions to understanding domain generalization and its implications for model complexity control and performance optimization offer significant value to the machine learning community.", "kK3DlGuusi": "1) Summarize the paper: The paper introduces a novel weight compression method that stores weight tensors as sparse quantized matrix factors. The method unifies weight SVD vector quantization and sparse PCA to achieve stateoftheart tradeoffs between accuracy and model size. The approach involves onthefly computation of weight tensors during inference by iteratively solving a quantized sparse PCA problem. 2) Strengths and Weaknesses Questions and Feedback: Strong Points:  Comprehensive Introduction: The paper provides a thorough introduction to the challenges in deploying neural networks and the existing methods for weight compression.  Innovative Methodology: The novel approach of storing weights as sparse quantized matrix factors is innovative and potentially impactful in achieving high compression ratios with minimal loss of accuracy.  Experimental Results: The experimental results demonstrate the effectiveness of the proposed method showcasing competitive performance compared to stateoftheart compression techniques.  Detailed Experiments: The paper provides a detailed experimental setup ablation studies and comparisons to baseline methods offering a comprehensive evaluation of the proposed approach. Weak Points:  Sparse PCA Complexity: While the proposed method addresses the quantized sparse PCA problem the complexity and computational demands of this approach could be a potential drawback especially in largescale applications.  Evaluation Metrics: The paper could benefit from a more detailed discussion on evaluation metrics used for comparing the proposed method with baseline techniques.  Limited Generalization: The paper focuses on specific architecture setups (e.g. ResNet18 MobileNetV2) for ImageNet classification. The generalization of the proposed method to diverse architectures and datasets should be discussed in more detail. Questions to the Author: 1. How does the proposed method scale in terms of computational efficiency and memory usage with increasing model complexity or larger datasets beyond ImageNet 2. Can you provide insights into the robustness of the method under different hyperparameter settings and variations in compression ratios 3. How does the proposed approach handle transfer learning scenarios and finetuning on different tasks or datasets Feedback to the Author:  Enhance the discussion on the computational complexity and scalability of the proposed method to make it more applicable to a wide range of neural network architectures and datasets.  Consider including a broader range of evaluation metrics to provide a more comprehensive understanding of the methods performance across various scenarios.  Provide insights into the adaptability of the method to different neural network architectures tasks and datasets to highlight its versatility and generalizability. 3) Review Summary: The paper introduces an innovative weight compression method based on quantized sparse PCA that unifies existing weight compression techniques. While the methodology shows promise in achieving high compression ratios without significant loss of accuracy there are concerns regarding the computational complexity and generalizability of the approach. The experimental results are promising but further investigation into scalability robustness under different hyperparameters and adaptability to diverse architectures and datasets would strengthen the papers contributions. Overall the paper presents a unique approach to weight compression but additional analysis and discussion are needed to enhance its practical applicability and impact in realworld settings. Recommendation: The paper shows promise in terms of introducing a novel weight compression method with competitive performance compared to existing techniques. However further experiments discussions on scalability generalization and robustness are required to validate the effectiveness and applicability of the proposed method. With additional insights and enhancements addressing the mentioned points the paper has the potential to make a significant contribution to the field of neural network compression. Therefore with revisions addressing the highlighted aspects I recommend the paper for acceptance after the necessary improvements.", "iMqTLyfwnOO": " Summary: The paper introduces a novel distance metric called the Augmented Sliced Wasserstein Distance (ASWD) to enhance computational efficiency and accuracy when comparing distributions in largescale machine learning problems. The ASWD constructs hypersurfaces using neural networks for flexible nonlinear slicing of data distributions optimizing hypersurfaces efficiently through gradient ascent. The paper demonstrates that the ASWD outperforms other Wasserstein variants in both synthetic and realworld problems through numerical experiments.  Strengths: 1. Innovative Approach: The introduction of the ASWD utilizing neural networks for hypersurface mapping is a novel and promising approach to improve the efficiency and accuracy of comparing distributions. 2. Theoretical Foundation: The paper provides a strong theoretical foundation proving the ASWD is a valid metric and offering insights into its optimization. 3. Comprehensive Experiments: Extensive numerical experiments and comparisons with stateoftheart methods demonstrate the superior performance of the ASWD in various machine learning tasks. 4. Clarity and Organization: The paper is wellstructured with clear explanations of concepts methods and experimental results.  Weaknesses: 1. Limited Impact Comparison: The paper could benefit from a deeper analysis or discussion on the practical implications and realworld applications of the proposed ASWD compared to existing methods. 2. Complexity of Method: The methods complexity especially in terms of injective neural networks and optimization might pose challenges for practical implementation and scalability in largerscale applications. 3. Abstraction Level: The highlevel nature of the paper may require additional details or examples to facilitate understanding for readers less familiar with the subject matter. 4. Resource Requirements: The computational requirements and scalability of the proposed ASWD method are not fully addressed which could be crucial for realworld deployment.  Questions and Feedback to the Author: 1. Can you provide further insights into the computational complexity of implementing the ASWD in largescale machine learning problems especially considering the optimization of hypersurfaces through gradient ascent 2. How do you envision the practical adoption of the ASWD in realworld applications and what are the potential challenges or limitations that might arise 3. Could you discuss the robustness of the ASWD in handling diverse datasets and distributions particularly in scenarios with high dimensionality or complex data structures 4. It would be beneficial to elaborate on the interpretability of the ASWD results and the implications for data understanding and decisionmaking processes.  Review Summary: The paper introduces an innovative approach the Augmented Sliced Wasserstein Distance (ASWD) which shows significant improvements in computational efficiency and accuracy for comparing distributions in machine learning problems. While the theoretical foundation and numerical experiments support the effectiveness of ASWD practical considerations around implementation complexity resource requirements and realworld adoption need further exploration. Overall the paper presents a valuable contribution to the field of machine learning but would benefit from more detailed insights into the methods practical implications and challenges.", "rbPg0zkHGi": " Summary: The paper introduces a novel method called SingleTraining MultiInference for uncertainty estimation in active learning using noise stability. By perturbing model parameters with noise the proposed method measures the deviation of the models output from the original observation providing a metric for selecting uncertain data samples for annotation. The method is compared with stateoftheart active learning baselines and demonstrates superior performance in image classification and semantic segmentation tasks.  Strengths: 1. Innovative Methodology: The paper introduces a novel approach using noise stability for uncertainty estimation which eliminates the need for complex pipelines and auxiliary models thus making it more practical and efficient. 2. Theoretical Understanding: The paper provides a solid theoretical foundation by connecting noise stability with variance reduction providing insightful explanations for the proposed methods effectiveness. 3. Extensive Experiments: The method is rigorously evaluated on multiple datasets for image classification semantic segmentation and 3D cryoET subtomogram classification showcasing significant improvements over existing baselines. 4. TaskAgnostic Nature: The proposed method is highlighted as taskagnostic indicating its versatility and applicability to various tasks without specialized modifications.  Weaknesses: 1. Assumptions and Limitations: The paper extensively discusses theoretical aspects and experimental results but lacks a detailed discussion on the limitations of the proposed method or the scenarios in which it might not perform optimally. 2. Algorithmic Details: While the algorithm for active learning with noise stability is explained additional insights into computational complexity and scalability could enhance the clarity for readers looking to implement the method. 3. Comparative Analysis: While the experimental results demonstrate the superiority of the proposed method over baseline algorithms further comparisons with more recent stateoftheart methods could enhance the comprehensiveness of the evaluation.  Questions and Feedback: 1. Validation and Robustness: Have sensitivity analyses been conducted to evaluate the robustness of the method to hyperparameter choices beyond the specific datasets used in the experiments 2. Scalability: How does the proposed method scale with larger datasets in terms of computational efficiency and performance Have scalability studies been conducted 3. Generalization: How well does the method generalize to different types of data and tasks not covered in the current experiments Are there plans to validate its applicability in other domains 4. Comparison with Recent Advances: Given the rapid advancements in active learning methods how does the proposed method compare with cuttingedge algorithms beyond those considered in this paper  Review Summary: The paper presents a novel method for uncertainty estimation in active learning using noise stability showcasing notable improvements over existing baselines in image classification and semantic segmentation tasks. The theoretical foundation is robust and extensive experiments support the methods efficacy. However further discussions on limitations scalability and comparisons with recent advances could enhance the papers impact and applicability. Overall based on the innovative methodology theoretical insights and strong experimental results I recommend this paper for publication pending minor revisions addressing the weak points highlighted.", "q9zIvzRaU94": "Summary: The paper proposes a probabilistic deep learning approach called StateDependent Causal Inference (SDCI) for causal discovery in scenarios with nonstationary behavior over time. The method aims to uncover causeeffect relationships in scenes where dynamics change with the underlying state variable and it is evaluated in two synthetic scenarios showing high accuracy in recovering causal dependencies even in cases with hidden states. Strengths: 1. Innovation: The paper introduces a novel approach SDCI for causal discovery in nonstationary scenarios which fills a gap in the literature by considering dynamics conditioned on underlying state variables. 2. Comprehensive Evaluation: The method is thoroughly evaluated in two synthetic scenarios showcasing its capability in accurately identifying causal dependencies in nonstationary settings even with hidden states. 3. Detailed Explanations: The paper provides detailed explanations of the problem formulation implementation details and experimental setups allowing for a clear understanding of the proposed method. 4. Comparative Analysis: The paper compares the proposed SDCI method with existing approaches like ACD highlighting the superiority of SDCI in handling nonstationary dynamics. Weaknesses: 1. Complexity: The methods implementation is intricate involving probabilistic models graph neural networks and latent variable modeling which may pose challenges for replication and practical application. 2. Limited RealWorld Validation: While the method is tested on synthetic scenarios realworld validation is lacking which may limit the generalizability of the results and practical utility of the proposed approach. Questions to Authors: 1. Can you elaborate on the scalability of the proposed SDCI method for larger datasets or more complex realworld scenarios 2. How does the SDCI method perform in scenarios with noisy data or incomplete observations which are common challenges in realworld applications Feedback to Authors: 1. Include realworld datasets or more diverse and challenging scenarios to validate the proposed methods effectiveness and applicability in practical settings. 2. Provide insights on computational efficiency and scalability especially when applied to largerscale datasets or realtime inference tasks. Review Summary: The paper presents a novel StateDependent Causal Inference (SDCI) method for causal discovery in nonstationary scenarios showcasing promising results in two synthetic scenarios. The work is welldocumented with detailed explanations and thorough evaluations. While the method shows advancements in handling nonstationary dynamics further validation on realworld datasets and addressing practical issues like scalability and noise resilience will enhance the impact and applicability of the proposed approach. Consideration of the provided feedback and questions will strengthen the paper and contribute to advancing the field of causal discovery in dynamic systems.", "mF5tmqUfdsw": " Summary: The paper introduces the ZerothOrder ActorCritic algorithm (ZOAC) which unifies evolutionbased zerothorder optimization methods and policy gradientbased firstorder methods into an onpolicy actorcritic architecture. ZOAC combines the advantages of both approaches allowing for wider adaptability to policy parameterization robustnessseeking property statedependent exploration and high sample efficiency. The algorithm conducts rollouts collection with timestepwise perturbation in parameter space alternating between firstorder policy evaluation (PEV) and zerothorder policy improvement (PIM) in each iteration. Experimental results demonstrate that ZOAC outperforms zerothorder and firstorder baseline algorithms across challenging continuous control benchmarks.  Strengths: 1. Innovation: ZOAC presents a novel approach that integrates the strengths of zerothorder and firstorder methods in reinforcement learning. 2. Comprehensive Evaluation: The paper provides a thorough analysis of ZOAC through extensive experiments across various continuous control benchmarks showcasing its superiority over baseline algorithms. 3. Robustness Analysis: The evaluation of learned policies under observation and parameter noise demonstrates the robustness of ZOAC. 4. Ablation Studies: The ablation studies on key components and hyperparameters help in understanding the effectiveness of each part of the algorithm. 5. Clarity of Presentation: The paper is wellstructured providing a detailed description of the algorithm experimental methodology and results.  Weaknesses: 1. Complexity: The algorithm involves multiple components and techniques which might make it challenging for implementation and understanding for readers unfamiliar with the topic. 2. Notation and Terminology: The paper includes a complex notation system that might be hard to follow for readers without a strong background in reinforcement learning and optimization.  Question to the Author:  Have you considered exploring the applicability of ZOAC in tasks with more complex reward structures or environments to further assess its robustness and generalization capabilities  Feedback to the Author:  Adding more insights into the robustness of ZOAC by exploring tasks with different characteristics or noise levels could enhance the papers significance.  Simplifying the notation and providing more intuitive explanations of the concepts could improve the readability and accessibility of the paper to a wider audience. This paper presents a promising algorithm in the field of reinforcement learning showing significant improvements over existing methods. Given the strengths and positive experimental results I recommend acceptance after addressing the minor weaknesses mentioned above.", "hcQHRHKfN_": "Summary: The paper presents a novel algorithm called RewardSwitching Policy Optimization (RSPO) for discovering diverse strategies in Reinforcement Learning (RL) environments. RSPO switches between extrinsic and intrinsic rewards during policy optimization to encourage the learning policy to converge towards novel locally optimal solutions. The algorithm is tested on various domains including singleagent navigation tasks MuJoCo control multiagent staghunt games and the StarCraft II MultiAgent Challenge showing superior performance compared to existing methods. Strengths: 1. Innovative Methodology: RSPO introduces a unique approach by switching between extrinsic and intrinsic rewards to encourage exploration and discover diverse strategies. 2. Empirical Validation: The paper provides thorough experiments across a variety of challenging RL domains demonstrating the effectiveness and superiority of RSPO over existing baselines. 3. Clear Presentation: The paper is wellorganized and structured with detailed explanations of the algorithm and experimental results. Weaknesses: 1. Theoretical Justifications: While the empirical performance is strong the paper lacks indepth theoretical analysis and justification of the algorithm. 2. Limited Generalizability: The paper focuses on specific RL domains limiting the generalizability of the proposed RSPO algorithm to broader contexts. 3. Comparison Metrics: Some parts of the comparison with baseline methods lack quantitative metrics making it challenging to fully assess the algorithms performance. Questions and Feedback: 1. Generalizability: How does the RSPO algorithm perform in more traditional RL settings or in scenarios with different complexity levels 2. Sample Efficiency: Can you provide insights into the sample efficiency of RSPO compared to existing methods especially in scenarios with a large state or action space 3. Theoretical Analysis: Could the paper benefit from a more detailed theoretical analysis to further understand the underlying principles and implications of RSPO Review Summary: The paper presents a novel algorithm RSPO for discovering diverse strategies in RL environments with promising results in a variety of challenging domains. While the empirical validation is strong the paper would benefit from further theoretical analysis and a broader exploration of the algorithms performance in different scenarios to enhance its impact in the field of RL. Overall the innovative approach and superior performance demonstrated in the experiments support a positive recommendation for publication provided the suggested enhancements are addressed.", "vfsRB5MImo9": " Summary: The paper introduces a novel Continual Learning (CL) problem called Continual Knowledge Learning (CKL) to address the challenge of renewing the internal world knowledge of Large Language Models (LMs) in response to the changing world. The paper formulates a benchmark and metric to quantify the retention of timeinvariant world knowledge the update of outdated knowledge and the acquisition of new knowledge. Various CKL methods are explored including regularization rehearsal and parameterexpansion methods and an indepth experimental analysis is conducted using the T5 model highlighting the challenges and findings of CKL.  Strengths: 1. Innovative Problem Formulation: CKL addresses a critical issue of maintaining the internal world knowledge of LMs in face of changing information presenting a novel and significant problem formulation. 2. Thorough Experimental Analysis: The paper conducts extensive experiments comparing various CKL methods and providing insights into their performance on different tasks which adds depth and credibility to the findings. 3. Detailed Methodology and Metric Development: The paper introduces a new metric FUAR and detailed datasets to evaluate the performance of CKL methods enhancing the reproducibility of the study. 4. Clear Presentation of Results: The results are presented in tables and graphs facilitating easy comprehension and comparison of the performance of different methods in various CKL scenarios.  Weaknesses: 1. Complexity of Content: The paper contains a high level of technical content which may be challenging for readers unfamiliar with continual learning and language modeling. 2. Limited Discussion on Future Directions: While the paper presents interesting results there could be more emphasis on discussing future research directions and practical implications for the development of everchanging LMs.  Questions  Feedback: 1. Clarity in Definitions: Could you provide more clarification on the categorization of world knowledge into timeinvariant outdated and new knowledge to help readers grasp the distinctions 2. Generalizability of Findings: How transferable are the CKL methods proposed in this study to other LM architectures or external datasets for continual pretraining 3. Efficiency Concerns: Given the memory inefficiency of parameterexpansion methods have you explored potential solutions to optimize memory usage while retaining performance in future CKL studies  Review Summary: The paper presents a novel and important problem formulation in Continual Knowledge Learning for LMs. It exhibits thorough experimentation and methodological rigor providing valuable insights into the challenges and potential solutions for maintaining evolving world knowledge in LMs. The findings underscore the effectiveness of parameterexpansion methods while highlighting memory inefficiency concerns. Overall the paper makes a substantial contribution to the field of continual learning and warrants consideration for publication with minor revisions to enhance clarity and future research directions.", "qynwf18DgXM": " Summary: The paper proposes the use of linearly nearly Euclidean metrics to assist in manifold microsurgery under the RicciDeTurck flow. It proves dynamical stability and convergence of such metrics and explores the gradient flow on these metrics based on information geometry and mirror descent. Experimental results demonstrate the effectiveness of linearly nearly Euclidean metrics in optimizing solutions on a manifold particularly in the context of training neural networks.  Strengths: 1. Innovative Approach: The paper introduces a novel concept of linearly nearly Euclidean metrics to address manifold surgery showing originality in the field. 2. Thorough Analysis: The paper provides comprehensive theoretical proofs and analyses to support the proposed methodology enhancing the credibility of the findings. 3. Experimental Validation: The experimentation on CIFAR datasets with ResNet models adds practical relevance to the theoretical framework presented in the paper. 4. Clear Structure: The paper is wellorganized with clear section divisions and detailed explanations making it easier for readers to follow the complex concepts.  Weaknesses: 1. Technical Complexity: The paper is highly technical which may pose a challenge for readers not wellversed in the field of manifold surgery and Ricci flow. 2. Lack of Comparison: While the experimental results are promising benchmarking the performance of the proposed linearly nearly Euclidean metrics against existing methods could further strengthen the paper. 3. Ambiguity in Definitions: Some definitions and notations could be better clarified to avoid potential confusion particularly for readers new to the topic.  Questions to the Author: 1. Can you elaborate on the practical implications of using the linearly nearly Euclidean metrics in realworld applications beyond theoretical analysis 2. How does the convergence rate of the metrics under the proposed approach compare to other optimization methods on manifolds 3. Have you considered the scalability of the proposed method to higherdimensional datasets or more complex neural network architectures  Feedback to the Author: 1. Consider providing a more intuitive explanation or visual aids to help readers grasp the concept of linearly nearly Euclidean metrics and its evolution under the RicciDeTurck flow. 2. Including a comparative analysis with existing stateoftheart methods could enhance the papers impact and relevance. 3. Addressing the clarity of definitions and ensuring consistency in notation throughout the paper would improve readability and understanding for a wider audience. Overall the paper presents a promising approach with potential implications in manifold surgery and neural network training. Further refinement and comparative analysis could enhance the contributions of the work.", "izvwgBic9q": " Peer Review  1. Summary: The paper proposes an unsupervised learning method UPFWI to solve the FullWaveform Inversion (FWI) problem in geophysics. FWI aims to estimate subsurface velocity maps from seismic measurements. The approach integrates convolutional neural networks (CNN) and partial differential equations (PDE) by combining a differentiable forward modeling operator with a CNN network. The paper introduces a new largescale dataset OpenFWI for benchmarking purposes. Experimental results show that the proposed unsupervised approach performs comparably to supervised methods and outperforms them with more seismic data.  2. Strong Points:  Innovative Approach: Combining CNN with PDE for unsupervised learning is novel and has potential applications beyond FWI.  Largescale Dataset: Introduction of OpenFWI dataset adds value to the research community by providing a challenging benchmark.  Effective Results: Demonstrated superior performance compared to supervised methods with larger amounts of unlabeled data.  Perceptual Loss Utilization: Highlighting the importance of perceptual loss in preserving coherence of reconstructed waveforms.  Weak Points:  Complexity of Method: The integration of CNN and PDE in an unsupervised loop may raise concerns about scalability and computational complexity.  Limited Generalization: The paper focuses on geophysics applications limiting the generalizability to other domains.  Evaluation Metrics: While the metrics used are standard additional analysis on the interpretability and robustness of the results could be beneficial.  Questions for the Authors: 1. How does the computational complexity of UPFWI scale with dataset size Have you considered any strategies for optimization 2. Can UPFWI be applied to realworld seismic datasets that may have noise or uncertainties 3. How sensitive is the method to hyperparameters particularly in the loss function Have you explored hyperparameter tuning extensively  Feedback:  It would be beneficial to further discuss the implications of UPFWI beyond geophysics and explore potential limitations in realworld applications.  Providing insights into model interpretability and robustness especially in the presence of noise and uncertainties can enhance the papers impact.  3. Review Summary: The paper presents an innovative unsupervised approach UPFWI for solving the FWI problem in geophysics. By integrating CNN and PDE the method demonstrates promising results on a largescale dataset. The utilization of perceptual loss and the superior performance compared to supervised methods are strong arguments for the proposed approach. However considerations about scalability generalizability and robustness should be further addressed to strengthen the papers impact. Overall the research makes a valuable contribution to the field of geophysics and machine learning but more indepth analysis and discussion are needed to fully assess the methods potential.", "wRODLDHaAiW": " Summary of the Paper: The paper introduces iLQRVAE a novel controlbased approach to variational inference in nonlinear dynamical systems. The method is capable of simultaneously learning latent dynamics initial conditions and external inputs by utilizing the iterative linear quadratic regulator algorithm (iLQR) in the recognition model. The paper demonstrates the effectiveness of iLQRVAE on synthetic systems as well as on neural and behavioral recordings in nonhuman primates performing reaching tasks. The method shows highquality kinematic reconstructions from neural data and offers insights into motor preparation dynamics.  Strengths and Weaknesses: Strengths: 1. Innovation: The paper introduces iLQRVAE a novel approach to learning inputdriven latent dynamics from data which leverages the powerful iLQR algorithm in recognition modeling. 2. Performance: The method exhibits stateoftheart performance on various synthetic and real datasets particularly in decoding hand kinematics from neural data in primates. 3. Flexibility: iLQRVAEs ability to model complex nonlinear dynamics including sparse input inference and learning ground truth dynamics illustrates its flexibility and broad applicability. 4. Robust Inference: The method shows robust inference capabilities even on datasets with missing data or heterogeneous trial lengths. 5. Clarity: The paper provides detailed explanations and thorough experimental results making it accessible to readers from various backgrounds in systems neuroscience and computational neuroscience. Weaknesses: 1. Complexity: The use of iLQR for nonlinear control inference introduces computational complexity potentially limiting scalability to larger datasets. 2. Identifiability Issues: The degenerate nature of decoupling ongoing inputs from dynamics may pose challenges in model interpretation and parameter identifiability especially in scenarios with low data or mismatched prior distributions. 3. Potential Localization: The focus on neuroscience applications might limit the generalizability of iLQRVAE to other domains highlighting the need for broader experimental validations.  Questions and Feedback to the Authors: 1. Can you elaborate on how iLQRVAE addresses identifiability issues and potential local minima during inference especially in the context of complex nonlinear systems 2. How do you envision overcoming the computational challenges associated with using iLQR as a controller for largescale datasets or realtime applications in neural decoding tasks 3. Have you considered incorporating additional prior information or constraints to enhance the interpretability and robustness of iLQRVAE especially in scenarios with limited data availability or heterogeneous input distributions  Review Summary: The paper presents a novel method iLQRVAE for learning inputdriven latent dynamics in nonlinear systems with applications in neuroscience. The method demonstrates stateoftheart performance on synthetic and real datasets showcasing robust inference capabilities and highquality kinematic reconstructions from neural data. While the approach is innovative and flexible addressing challenges related to identifiability and computational complexity will be crucial for broader adoption in complex system identification tasks. Overall the paper contributes significant advancements in understanding neural dynamics and behavior through a controlbased variational framework. Recommended for acceptance pending clarity on addressing computational challenges and identifiability concerns.", "jZQOWas0Lo3": " Peer Review:  1. Summary: The paper presents a novel method called \"source fiction\" for semisupervised optimal transportbased domain adaptation leveraging the connection between adversarial attacks and cycle monotone maps. By utilizing adversarial examples perturbed to resemble source samples the method aims to improve the performance of optimal transport in semisupervised domain adaptation. Experimental evaluations are conducted on various datasets to demonstrate the effectiveness of the proposed approach.  2. Evaluation:  Strong Points:  Innovative Approach: The paper introduces a unique method source fiction that leverages adversarial attacks to improve semisupervised domain adaptation using optimal transport.  Theoretical Grounding: The paper provides a solid theoretical foundation by exploring the connection between adversarial attacks cycle monotonicity and optimal transport in a wellstructured manner.  Experimental Validation: The experiments conducted on diverse datasets demonstrate the effectiveness of the proposed method and showcase improvements in performance compared to traditional approaches.  Ablation Study: The ablation study provides insights into the impact of different parameters enhancing the understanding of the methods behavior under varying conditions.  Weak Points:  Lack of Comparison: While the method is innovative there is a lack of comparison with existing stateoftheart approaches in semisupervised domain adaptation using optimal transport or adversarial attacks.  Algorithm Complexity: The paper briefly introduces the source fiction algorithm but more detailed information on the computational complexity and scalability would enhance the understanding of its practical implications.  Experimental Analysis: The evaluation results are promising but additional analysis on the limitations or failure cases of the method could provide a more comprehensive assessment.  Questions and Feedback: 1. Comparison with Existing Methods: Can you provide a comparison with other stateoftheart methods in semisupervised domain adaptation using optimal transport or adversarial attacks to better understand the relative performance of the proposed approach 2. Scalability and Computational Complexity: How does the complexity of the source fiction algorithm impact its scalability to larger datasets or realworld applications Are there any practical limitations to consider 3. Limitations and Failure Cases: Have you investigated any scenarios where the source fiction method may fail to improve adaptation performance significantly Understanding the limitations can help in determining the methods applicability in different contexts.  3. Review Summary: The paper presents an innovative method source fiction for semisupervised domain adaptation using optimal transport and adversarial attacks. The theoretical grounding is solid and the experimental results demonstrate the methods effectiveness. However a more detailed comparison with existing methods thorough analysis of algorithm complexity and exploration of limitations could further strengthen the paper. Overall the research contributes to advancing the field of domain adaptation and showcases potential future directions for utilizing adversarial attacks in optimization frameworks.  Recommendation: Based on the innovative approach theoretical soundness and promising experimental results I recommend this paper for acceptance after addressing the suggestions for improvement and providing additional insights into the comparison with existing methods and algorithm complexity. The proposed method shows potential for enhancing domain adaptation techniques and warrants further exploration in future research.", "q1QmAqT_4Zh": " Peer Review for Paper on Offline Reinforcement Learning with Koopman Latent Representations Summary: The paper introduces an innovative approach to address the limitations of offline reinforcement learning by leveraging Koopman latent representations. The key idea is to infer symmetries of the systems dynamics which are then utilized to guide a novel data augmentation framework. The paper provides theoretical insights on symmetries relevant for control systems and empirically evaluates the proposed method on benchmark offline reinforcement learning tasks such as D4RL Metaworld and Robosuite. The results indicate that the proposed Koopman Forward Qlearning algorithm outperforms stateoftheart methods in improving Qlearning algorithms. Strengths: 1. Innovative Approach: The paper introduces a novel data augmentation framework based on Koopman latent representations which offers a unique perspective to address the limitations encountered in offline reinforcement learning. 2. Theoretical Foundation: The paper provides comprehensive theoretical results on symmetries of dynamical control systems expanding the understanding of utilizing Koopman theory in reinforcement learning. 3. Empirical Evaluation: The empirical evaluation on benchmark tasks demonstrates the effectiveness of the proposed method in consistently improving the stateoftheart for Qlearning algorithms showcasing its practical relevance. 4. Detailed Methodology: The paper provides a detailed description of the proposed Koopman Forward Qlearning algorithm as well as the theoretical underpinnings and theoretical limitations. 5. Comparison with Existing Works: The paper effectively relates its approach to existing works in reinforcement learning highlighting the novelty and significance of the proposed method. Weaknesses: 1. Theoretical Limitations: The paper mentions limitations related to assuming differentiable state transitions and the Bilinearisation Ansatz for the Koopman operator which may raise concerns about the generalizability of the proposed framework to all environments. 2. Global Conditions Inference: While the paper discusses local symmetries the challenges associated with inferring global conditions are not adequately addressed. 3. Algorithm Variants: The discussion around the KFC and KFC variants could be more detailed especially in terms of their computational complexities and respective performance differences. Question and Feedback to the Author: 1. Can you provide more insights into the limitations of assuming differentiable state transitions and the constraints of the Bilinearisation Ansatz for the Koopman operator How do these assumptions impact the applicability of your framework to various environments 2. Considering the KFC and KFC variants could you elaborate on the decision criteria for selecting one over the other in specific scenarios What factors influence the choice between the two variants 3. How do you plan to address the challenges associated with inferring global conditions for symmetry shifts given the focus on local symmetries in your current work Review Summary: The paper presents an innovative and theoretically grounded approach to offline reinforcement learning through the use of Koopman latent representations to infer symmetries for guiding data augmentation strategies. While the method shows promising results in empirical evaluations there are concerns related to theoretical limitations and the inference of global conditions for symmetry shifts. Despite these limitations the paper contributes a significant advancement in the field of reinforcement learning and data augmentation techniques. Overall the paper is wellstructured provides substantial insights and offers a valuable addition to the existing literature. Recommendation: Based on the strengths of the proposal innovative approach theoretical foundation and promising empirical results I recommend accepting this paper with minor revisions to address the mentioned weaknesses related to theoretical limitations algorithm variants and global conditions inference. Addressing these concerns will further strengthen the papers contribution to the field.", "k9bx1EfHI_-": "Summary: The paper addresses the challenges in automated seizure detection and classification using EEG data by introducing a novel approach that combines graph neural networks (GNN) with selfsupervised pretraining. The study proposes two EEG graph structures to represent spatiotemporal dependencies in EEG signals and introduces a quantitative model interpretability approach to localize seizures within EEGs. The approach is evaluated on a large public dataset demonstrating superior performance in seizure detection and classification compared to existing methods. Additionally the study shows significant improvements in the classification of rare seizure types and precise localization of seizures within EEGs. Strengths: 1. Innovative Approach: The use of graph neural networks and selfsupervised pretraining is a novel and effective approach to address the challenges in automated seizure detection and classification. 2. Performance: The results show superior performance in seizure detection and classification outperforming previous methods on a large public dataset. 3. Model Interpretability: The study introduces a quantitative approach to assess the models ability to localize seizures providing valuable insights for clinicians. 4. Detailed Experiments: The paper provides thorough experimentation including comparisons with baselines model ablation studies and analyses on different aspects of the proposed method. Weaknesses: 1. Dataset Limitation: The study uses only one dataset for evaluation which may limit the generalizability of the proposed method across different datasets with varying characteristics. 2. Lack of Clinical Validation: While the method shows promising results the paper lacks clinical validation to demonstrate the realworld impact and usability of the proposed approach. 3. Occlusionbased Analysis: The paper relies heavily on occlusionbased techniques for interpretability and it might benefit from exploring additional approaches for model interpretation. Questions to the Authors: 1. Did you consider conducting experiments on multiple datasets to evaluate the generalizability of your proposed method 2. Have you engaged with clinicians to validate the practical utility of your approach in realworld clinical settings 3. How would your method perform on EEG data with noise or artifacts which are common in clinical EEG recordings Feedback to the Authors: 1. Consider expanding the dataset evaluation to include multiple datasets to ensure the generalizability of your method. 2. Think about partnering with clinicians for practical validation and feedback to enhance the clinical relevance of your approach. 3. Explore additional interpretability techniques beyond occlusionbased analysis to provide a more comprehensive understanding of model decisions. Review Summary: Overall the paper presents a highly innovative approach combining graph neural networks and selfsupervised pretraining for automated seizure detection and classification from EEG data. The study demonstrates superior performance especially in rare seizure types and provides valuable insights into localizing seizures within EEG signals. Addressing the weaknesses through additional dataset evaluations clinical validation and diverse interpretability approaches would further enhance the significance and applicability of the proposed method in clinical settings.Based on the compelling results and thorough experimentation in the paper I recommend the publication of this study with minor revisions to address the limitations discussed.", "kj8TBnJ0SXh": " Summary: The paper introduces FaceDet3D a method that generates geometric facial details consistent with a desired target expression from a single image. This method consists of a detail hallucination network to infer facial details and a rendering network to photorealistically render the facial details based on the geometry. The authors address the limitations of existing methods in capturing fine facial details and demonstrate the effectiveness of their approach through extensive evaluation and comparisons with other methods.  Strong Points: 1. Innovative Approach: The paper introduces a novel method that addresses the challenge of capturing fine facial details and renders them realistically based on expressions. 2. Comprehensive Evaluation: The authors thoroughly evaluate their method with detailed results comparisons and ablation studies showcasing the effectiveness of FaceDet3D. 3. Use of Weak Supervision: Training the model with weak supervision and without groundtruth 3D data highlights the practicality and applicability of the proposed approach. 4. Technical Details: The paper provides indepth technical explanations detailed methodologies and clear illustrations aiding in understanding the proposed method.  Weak Points: 1. Limited Dataset: The reliance on a single dataset (FFHQ) for training the model may limit the generalizability and robustness of the proposed method. 2. Complexity: The complexity of the method and training process especially with multiple losses and ablation studies may make it challenging for broader adoption and implementation. 3. Occlusions Handling: The methods limitation in handling occlusions like glasses or makeup should be addressed for practical usability in realworld scenarios.  Question and Feedback: 1. Dataset Diversity: How do you plan to address the limitation of dataset diversity especially in training data to ensure the model generalizes well to a wide range of facial variations and expressions 2. Practical Implementation: Considering the complexity of the method do you foresee any challenges in implementing and deploying FaceDet3D in realworld applications 3. Future Research: What additional enhancements or avenues for future research do you envision to address the identified limitations and further improve the performance of your method  Review Summary: The paper presents a novel method FaceDet3D for generating plausible geometric facial details based on expressions from single images. While demonstrating promising results and addressing existing limitations the methods complexity and potential issues such as dataset diversity and handling occlusions need to be further addressed. Overall the paper is wellstructured technically detailed and provides valuable insights into advancing facial expression editing and rendering techniques. Further research and improvements could enhance the practicality and applicability of FaceDet3D in realworld scenarios.", "jxTRL-VOoQo": " Summary of the Paper: The paper investigates the limitations of current architecture designs in deep Graph Neural Networks (GNNs) through a systematic experimental evaluation. The study focuses on two essential questions: 1) the factors hindering the performance of deep GNNs and 2) designing effective deep GNNs to outperform existing models. The paper introduces Deep Graph MultiLayer Perceptron (DGMLP) a novel approach addressing the identified limitations. Experimental results demonstrate that DGMLP achieves high accuracy flexibility scalability and efficiency surpassing stateoftheart node classification performance on various datasets.  Strengths of the Paper: 1. Comprehensive Experimental Analysis: The paper provides a thorough empirical investigation to identify the key reasons for the limitations of deep GNNs. 2. Novel Model Proposal: Introducing the DGMLP model based on the findings from the analysis is a significant contribution to the field. 3. Clear Problem Statement and Research Questions: The paper clearly defines the research questions and provides tangible answers based on experimentation. 4. Guidelines for Designing Deep GNNs: The guidelines proposed by the authors provide valuable insights for designing deep GNNs with large propagation and transformation depths. 5. High Predictive Accuracy: Demonstrating superior predictive accuracy compared to stateoftheart models across multiple datasets highlights the effectiveness of DGMLP.  Weaknesses of the Paper: 1. Lack of Comparison with Recently Introduced Models: While the paper compares DGMLP with existing baselines including the comparison with recently introduced models could provide a more comprehensive assessment. 2. Theoretical Insight: While the experimental results are detailed more theoretical insights and derivations could strengthen the papers claims and contribute to a deeper understanding of the proposed model.  Questions and Feedback to the Authors: 1. Clarification on Model Complexity: How does the proposed DGMLP model compare in terms of computational complexity and model interpretability with existing deep GNN approaches 2. Scalability Concerns: Have you considered conducting experiments on larger and more diverse datasets to assess the scalability of DGMLP further 3. Deployment and Realworld Application: How feasible is the deployment of DGMLP in practical applications or realworld scenarios Have you tested the model in any real industrial environments beyond the provided dataset  Review Summary: The paper provides a detailed analysis of the limitations of deep GNNs and introduces a novel model DGMLP to address these issues. With comprehensive experimental validation and superior performance across various datasets the paper makes a valuable contribution to the field of graph mining and neural networks. Despite some limitations the clear articulation of research questions and the proposed guidelines for designing deep GNNs make this study a significant and impactful piece of research. Therefore based on the strengths observed in the paper I recommend its acceptance for publication after addressing the minor weaknesses highlighted.", "zhynF6JnC4q": " Summary The paper introduces a unified framework called Adaptive Qlearning that effectively combines offline and online reinforcement learning (RL) by using adaptive update strategies. The framework addresses the challenges of poor performance in offline RL due to dataset quality and the difficulties in evaluating policies solely from offline data. By distinguishing between online and offline datasets and utilizing a novel replay buffer the proposed method achieves expert performance with limited online exploration steps even with lowquality datasets. Extensive experiments on MuJoCo tasks validate the effectiveness of the proposed approach.  Strengths 1. Innovative Framework: The Adaptive Qlearning framework introduces a novel approach to mix offline and online RL addressing key challenges in the field. 2. Experimental Validation: The extensive experiments provide strong empirical evidence of the frameworks effectiveness demonstrating improved performance compared to baselines. 3. Comprehensive Evaluation: The paper considers various aspects including different dataset qualities ablation studies and hyperparameter analysis providing a thorough evaluation of the proposed method.  Weaknesses 1. Impact of Dataset Quality: The performance of the method is significantly affected by the quality of the offline dataset which may limit its applicability in scenarios with poor datasets. 2. Hyperparameters Sensitivity: While the method is robust to certain hyperparameters the influences of others like the sampling probability from the online buffer could impact performance and require careful tuning.  Questions to the Author 1. How does the framework handle catastrophic forgetting or destabilization during the learning process especially when switching between online and offline data  Feedback to the Author 1. Consider discussing potential strategies to mitigate the impact of dataset quality on performance such as data preprocessing or augmentation techniques. 2. Provide insights on how the framework could be applied to scenarios with dynamically changing datasets or nonstationary environments.  Review Summary The paper presents an innovative Adaptive Qlearning framework for combining offline and online RL effectively. It addresses key challenges in the field and demonstrates superior performance through extensive experiments. While the method shows promise its sensitivity to dataset quality and hyperparameters may require further exploration. Overall the paper offers a valuable contribution to the RL research landscape and warrants consideration for publication after addressing the identified weaknesses.", "qZNw8Ao_BIC": " Peer Review  1. Summary of the Paper: The paper investigates the robustness of vision transformers (ViTs) by analyzing their sensitivity to patchbased transformations. It demonstrates that ViTs heavily rely on nonrobust features surviving patchbased transformations which do not necessarily correspond to semantic classes recognizable by humans. The study aims to reduce ViTs dependence on nonrobust features to enhance outofdistribution performance. The proposed approach involves using patchbased negative augmentation to regularize training away from nonrobust features showing consistent improvements in ViTs robustness across ImageNetbased benchmarks.  2. Strengths and Weaknesses:  Strong Points:  The paper addresses an important topic by exploring ViTs reliance on nonrobust features and its impact on outofdistribution performance.  The methodology is wellstructured and clearly explained from the analysis of patchbased transformations to the proposal of negative augmentation.  Extensive experiments are conducted on ImageNetbased benchmarks to validate the effectiveness of the proposed approach.  The studys contributions in understanding nonrobust features in ViTs and proposing negative augmented views as a regularization technique are significant.  Weak Points:  While the methodology is rigorous there could be more detailed explanations on the implementation of negative augmentation and its comparison with existing robustness techniques.  The paper could benefit from more visualizations to aid in understanding patchbased transformations and their effects on ViTs performance.  In the Related Work section more comparisons with existing methodologies and approaches could provide further context for the significance of the study.  Questions and Feedback to the Author: 1. How does the proposed negative augmentation technique compare with stateoftheart robustness methods in terms of computational efficiency and performance 2. Could the authors elaborate more on the tradeoffs between relying on robust features versus nonrobust features in the context of ViT training and performance 3. Have you considered exploring the generalizability of the proposed negative augmentation technique on other tasks beyond image classification  3. Review Summary: The paper provides a comprehensive analysis of ViTs reliance on nonrobust features and proposes an innovative approach using patchbased negative augmentation to enhance ViTs robustness. The methodology is sound and the experiments validate the effectiveness in improving outofdistribution performance. The findings underscore the importance of mitigating model reliance on nonrobust features for better generalization. Therefore based on the strong theoretical framework thorough experimentation and significant contributions to improving ViTs robustness I highly recommend this paper for publication.  Overall the paper presents a novel perspective on enhancing the robustness of vision transformers and makes valuable contributions to the field. Your thorough analysis and experimental results strengthen the credibility of the study and position it as a valuable addition to robustness research in deep learning. Well done on a comprehensive piece of work.", "g6UqpVislvH": " Stage 1: Summary of the Paper The paper introduces a method for applying positional encoding on nonEuclidean manifolds to enable learning highfrequency functions. It proposes using orthonormal bases on the manifold moving away from the typical sinusoidal encoding used for Euclidean spaces. The paper demonstrates the method on various tasks such as learning panoramas on spheres probability distributions on the rotation manifold neural radiance fields on the product of cube and sphere and light fields represented as the product of spheres. The study focuses on the generalization of the widely adopted \"Fourier Features\" technique to nonEuclidean settings.  Stage 2: Evaluation of the Paper  Strong Points: 1. Innovative Approach: The papers proposal to use orthonormal bases for positional encoding on nonEuclidean manifolds is innovative and addresses a crucial aspect of highfrequency function learning. 2. Theoretical Foundation: The use of the neural tangent kernel (NTK) theory to guide the design of positional encoding and ensuring shiftinvariance on manifolds demonstrates a strong theoretical foundation. 3. Comprehensive Experiments: The paper conducts experiments on various tasks and manifolds showcasing the effectiveness of the proposed methodology. Results demonstrate significant improvements over existing approaches.  Weak Points: 1. Complexity: The paper introduces complex mathematical concepts and techniques which may be challenging for readers without a strong background in the field.  Questions and Feedback to Authors: 1. Clarity on Methodology: Can the authors explain any potential limitations or constraints of using orthonormal bases over sinusoidal bases for positional encoding on nonEuclidean manifolds 2. Scalability: How scalable is the proposed method for large datasets or highdimensional manifolds Are there any computational constraints that need to be considered  Stage 3: Review Summary The paper presents a novel approach to apply positional encoding for learning highfrequency functions on nonEuclidean manifolds shifting from sinusoidals to orthonormal bases. The use of NTK theory and ensuring shiftinvariance on manifolds offer a new perspective on this research area. The comprehensive experiments validate the methods effectiveness across different tasks and manifolds. While the paper introduces significant advancements ensuring clarity around the methodology and addressing scalability concerns will enhance its impact. Overall based on the strong theoretical foundation and promising experimental results I recommend this paper for publication with minor revisions to address the mentioned points.  End of Review.", "lL3lnMbR4WU": " Review of \"ViLD: Vision and Language Knowledge Distillation for OpenVocabulary Object Detection\"  Summary: The paper presents a novel method called ViLD (Vision and Language knowledge Distillation) for advancing openvocabulary object detection. The paper addresses the challenge of training data availability by proposing a method that distills knowledge from a pretrained openvocabulary image classification model into a twostage detector. ViLD achieves impressive performance on the LVIS dataset outperforming the supervised counterparts and stateoftheart models on various metrics. The paper provides detailed experiments comparisons and analyses to support the effectiveness of the proposed method.  Strong Points: 1. Innovative Methodology: ViLD proposes a novel approach for openvocabulary object detection by leveraging vision and language knowledge distillation which is wellmotivated and addresses a key challenge in the field. 2. Strong Performance: The results presented in the paper demonstrate the effectiveness of ViLD as it outperforms supervised models and achieves stateoftheart performance on various datasets. 3. Comprehensive Experiments: The paper includes detailed experimental setups evaluation on multiple datasets comparison with existing methods and analysis of results providing a comprehensive assessment of the proposed method. 4. OpenSource Code: The authors have made the code and demo available on a public GitHub repository enhancing the reproducibility and accessibility of their work.  Weak Points: 1. Inference Speed: The paper acknowledges that the method is slow for inference which could be a limitation in realworld applications. Suggesting strategies or discussing potential solutions to address this issue would be beneficial. 2. Model Ensembling Complexity: The ensembling approaches discussed in the paper add complexity and may not be practical for realtime applications. Providing insights on simplifying the ensembling process could enhance the methods feasibility. 3. Transferability: While ViLD shows promising transfer performance to other datasets discussing limitations or challenges that may arise when transferring to significantly different datasets could provide a more comprehensive view.  Questions and Feedback: 1. Can you provide more insights into how ViLD can be made more efficient for realtime applications especially in terms of inference speed 2. Have you explored any tradeoffs or limitations when transferring the ViLD model to other datasets Are there specific challenges encountered in the transfer process that need to be addressed 3. Considering the model ensembling approaches discussed do you have insights on how to simplify the ensembling process without compromising performance significantly  Review Summary: The paper presents a novel and effective method ViLD for openvocabulary object detection by leveraging knowledge distillation from pretrained openvocabulary image classifiers. The detailed experiments comparisons and results demonstrate the superiority of ViLD over existing methods establishing it as a promising approach in the field. While the paper excels in methodology performance and experimental rigor addressing concerns related to inference speed and model ensembling complexity could further enhance the practical applicability of the method. Overall considering the strong points and the potential for improvement I recommend this paper for acceptance with minor revisions to address the raised concerns.", "rw1mZl_ss3L": " Review of \"Concurrent Adversarial Learning for LargeBatch Training of Neural Networks\"  1) Summary: The paper introduces Concurrent Adversarial Learning (ConAdv) a novel method to increase batch size in largebatch training without relying on data augmentation. Adversarial learning is leveraged to smooth decision boundaries and address performance degradation associated with larger batch sizes in training deep neural networks. By decoupling sequential gradient computations using stale parameters ConAdv aims to parallelize computations and achieve similar efficiency as standard optimizers. Experimental results demonstrate that ConAdv successfully scales the batch size of ResNet50 training on ImageNet to 96K while maintaining high accuracy.  2) Strengths and Weaknesses:  Strengths:  Innovative Solution: Introducing ConAdv as a novel approach to address the challenges of largebatch training is commendable.  Experimental Findings: The experimental results convincingly show the effectiveness of ConAdv in scaling batch sizes while maintaining accuracy.  Theoretical Analysis: Theoretical analysis and convergence results help in understanding the behavior and efficiency of ConAdv.  Clear Presentation: The paper is wellstructured clearly presenting the motivation method results and implications of the proposed ConAdv.  Weaknesses:  Comparative Analysis: While the paper compares ConAdv to data augmentation and DisAdv a more extensive comparative analysis against other stateoftheart approaches could strengthen the evaluation.  Theoretical Assumptions: The paper relies on specific assumptions for convergence analysis which may limit the generalizability of the results.  Scalability Evaluation: While the scalability of ConAdv is discussed a more detailed analysis of its performance with a larger number of processors could provide additional insights.  Question and Feedback to the Author: 1. Adversarial Perturbation Analysis: Could the authors provide insights into the sensitivity of ConAdv to the choice of adversarial perturbation values How does this impact the tradeoff between performance and perturbation intensity 2. Scalability Concerns: How does ConAdv perform with a significantly larger number of processors beyond the limits tested in the experiments Are there scalability concerns or benefits observed at extreme scales  3) Review Summary: The paper presents an innovative and promising approach ConAdv for largebatch training using adversarial learning. The experimental results demonstrate the efficacy of ConAdv in scaling batch size on ImageNet without compromising accuracy. The theoretical analysis and convergence results further support the robustness of ConAdv. While the paper is wellstructured and presents clear findings addressing scalability concerns and providing a detailed analysis of adversarial perturbation sensitivity could enhance the completeness of the study.  Recommendation: Based on the innovative approach convincing experimental results and theoretical analysis presented in the paper I recommend accepting this paper for publication with minor revisions to address the highlighted weaknesses and provide additional insights on scalability and sensitivity to adversarial perturbations.", "qiukmqxQF6": "Summary: The paper introduces Latent Temporal Flows (LatTeFlows) a method for probabilistic multivariate timeseries analysis tailored for high dimensional systems with temporal dynamics driven by variations in a lowerdimensional discriminative subspace. LatTeFlows learns autoencoder mappings to a latent space and the temporal distribution of lowerdimensional embeddings of input sequences enhancing forecasting accuracy while reducing computational complexity. The method significantly outperforms stateoftheart on multistep forecasting benchmarks and is applied to sensorsignal forecasting using wearable devices data. Strengths: 1. Novel Methodology: The paper introduces a novel approach LatTeFlows which effectively models crossseries dependencies across time while scaling to large dimensions and enables faster sequence generation. 2. Performance: Extensive experiments demonstrate the stateoftheart performance and computational tractability of LatTeFlows on various datasets showcasing its superiority over existing methods. 3. Interpretability: The interpretability of the latent representations learned by LatTeFlows is demonstrated showcasing its ability to capture meaningful latent trends. 4. Practical Application: The application of LatTeFlows to a challenging sensorsignal forecasting task from wearable devices data adds realworld relevance to the research. Weaknesses: 1. Complexity: The paper delves into technical details that may be challenging for readers unfamiliar with the specific domain or advanced statistical methods. 2. Data Accessibility: While extensive experimentation is shown on public datasets details on the Apple Heart and Movement Study data are discussed but access to the actual data is not provided limiting reproducibility for this specific application. 3. Evaluation Metrics: The paper focuses on metrics like CRPS and NMSE for evaluation which may not be commonly understood by all readers. Providing more explanation or comparative metrics could enhance understanding. Question to the Author: 1. How does the LatTeFlows method generalize to datasets with different characteristics or varying levels of complexity 2. Can you elaborate on the computational requirements and scalability of LatTeFlows particularly when applied to larger datasets 3. How does the method handle potential challenges such as missing data or outliers in the timeseries Feedback to the Author: 1. Consider providing more clarity and illustrative examples to make the technical aspects more accessible to a broader audience. 2. Enhance reproducibility by sharing code implementation or providing a detailed description of the models and hyperparameters utilized in the experiments. 3. Address the limitations of using only specific evaluation metrics by discussing potential alternative measures for performance assessment. Review Summary: The paper presents a wellstructured and innovative methodology Latent Temporal Flows (LatTeFlows) for probabilistic multivariate timeseries analysis. The strengths lie in the novel approach demonstrated performance superiority and practical application in sensorsignal forecasting. While technical complexity and data accessibility are minor concerns the papers overall quality and contribution make it suitable for publication pending minor revisions to enhance understanding and reproducibility.", "x4tkHYGpTdq": "Summarize the Paper: The paper introduces a framework called Dually SparsityEmbedded Efficient Tuning (DSEE) to address the resource and parameterefficiency challenges faced during finetuning gigantic pretrained language models. The key focus of DSEE is to leverage sparsity in weight updates and final model weights to achieve both parameter efficiency during finetuning and resource efficiency during inference. The authors propose methods for unstructured and structured sparsity generation using magnitudebased pruning and `1 sparse regularization on pretrained language models BERT GPT2 and DeBERTa. Extensive experiments demonstrate that DSEE achieves impressive efficiency gains while maintaining competitive downstream transfer performance across various datasets. Strong Points: 1. Innovative Framework: The proposed DSEE framework is innovative in simultaneously optimizing parameter efficiency during finetuning and resource efficiency during inference by leveraging sparsity in weight updates and final model weights. 2. Comprehensive Experiments: The paper conducts extensive experiments on various pretrained language models and diverse datasets to demonstrate the effectiveness of the proposed framework showcasing impressive parameter and computation efficiency gains. 3. Effective Results: Results show that DSEE outperforms other baseline methods in terms of parameter efficiency resource efficiency and downstream task performance. It achieves substantial reductions in the number of trainable parameters and computational cost while maintaining competitive performance. Weak Points: 1. Complexity: The paper introduces multiple methods and algorithms within the DSEE framework which might be difficult for readers to grasp without a detailed understanding of sparsity weight updates and finetuning techniques. Questions to the Authors: 1. Could you elaborate on how the performance of the proposed DSEE framework compares to other stateoftheart methods on similar tasks and models 2. How does the proposed structured sparsity compare to unstructured sparsity in terms of computational efficiency and downstream performance across different datasets Feedback: The paper provides a comprehensive and thorough exploration into addressing the challenges of finetuning large pretrained language models. The DSEE framework shows significant promise in enhancing both parameter and resource efficiency while maintaining competitive performance. The detailed experiments and analyses solidify the effectiveness of the proposed methods. For future work it would be beneficial to consider comparisons with other recent efficient finetuning methods and explore applications in diverse domains beyond NLP. Review Summary: Overall the paper presents a wellstructured and insightful study on addressing resource and parameterefficiency challenges in finetuning pretrained language models through the innovative DSEE framework. With strong experimental validation and clear explanations of the proposed methods the paper convincingly demonstrates the effectiveness of DSEE in optimizing model efficiency while preserving performance quality. Therefore I recommend this paper for publication given its significant contributions to the field of natural language processing and model optimization.", "qPQRIj_Y_EW": " Summary: The paper proposes a machine learning method to address the order fulfillment problem in realtime for online retailing. The method formulates the problem as a tripartite graph and utilizes an edgefeatureembedded graph attention mechanism to learn the best assignment policy. The model is sizeinvariant and able to handle unseen instances during training. Experiments demonstrate that the proposed model outperforms baseline heuristic methods in terms of optimality and inference time.  Strong Points: 1. Innovative Approach: The paper introduces a novel machine learning method to tackle the order fulfillment problem offering a fresh perspective in solving combinatorial optimization challenges. 2. EdgeFeatureEmbedded Graph Attention: Incorporating highdimensional edge features and heterogeneous information into the attention mechanism is a strong point as it enhances the models ability to address complex realworld scenarios. 3. SizeInvariance and Generalization: The models capability to handle problem instances of varying scales including unseen cases during training showcases its robustness and generalization potential. 4. Significant Performance Improvement: The experimental results indicate a substantial performance improvement over baseline methods highlighting the efficacy of the proposed approach in achieving optimality and realtime decisionmaking.  Weak Points: 1. Complexity of Model: The complexity of the proposed machine learning model especially with the edgefeatureembedded graph attention mechanism may raise concerns regarding interpretability and implementation feasibility. 2. Limited Hyperparameter Exploration: The paper mentions a lack of an extensive hyperparameter search which could impact the models performance optimization and reproducibility. 3. Comparison to StateoftheArt: While the proposed method outperforms baseline approaches a direct comparison with stateoftheart ML models in combinatorial optimization could provide more insights into its competitiveness.  Questions to the Author: 1. How does the proposed method handle potential challenges such as overfitting especially when considering the sizeinvariance and generalization to unseen instances 2. Could the authors provide more insight into the computational efficiency of the model during training and inference beyond the comparison to traditional methods 3. Are there any limitations or potential failure modes identified during the experimentation that warrant discussion or further investigation 4. Have you considered realworld deployment challenges and how the proposed model could be integrated into existing supply chain systems seamlessly  Feedback: The paper presents an innovative machine learning approach to the order fulfillment problem showcasing significant performance improvements over heuristic methods. Further insights into the models interpretability scalability and potential integration into realworld scenarios would enhance the papers impact. Consider addressing the weak points highlighted to strengthen the overall quality of the research.  Review Summary: Overall the paper demonstrates strong potential in the realm of supply chain management optimization through the proposed machine learning approach. The focus on edgefeatureembedded graph attention and sizeinvariance sets this work apart showing promise for addressing complex realtime decisionmaking challenges. Addressing questions raised and providing further context and discussions on the weak points could enhance the papers contribution to the field. Based on the innovative methodology and promising experimental results I recommend further development and publication of this research.", "vIC-xLFuM6": " PeerReview of the Paper: \"Value Approximation Using Fourier Feature Networks in OffPolicy Deep Reinforcement Learning\"   1) Summary of the Paper: The paper addresses the issue of spectral bias in value approximation using deep neural networks in offpolicy deep reinforcement learning. The authors propose a novel approach called Fourier Feature Networks (FFN) to overcome this bias and improve performance in continuous control domains. By introducing a composite neural tangent kernel based on Fourier features the FFN achieves stateoftheart performance with faster convergence and better stability in offpolicy learning. The paper presents theoretical analysis empirical results on toy problems and empirical scaling to complex control tasks in the DeepMind Control Suite.  2) Strengths and Weaknesses: Strong Points:  Theoretical Foundation: The paper is wellgrounded in deep learning theory and reinforcement learning concepts.  Novel Approach: The introduction of Fourier Feature Networks as a solution to spectral bias in value approximation is innovative and shows promising results.  Empirical Evidence: Extensive empirical evaluations across different domains demonstrate the effectiveness of the proposed FFN approach.  Comprehensive Analysis: The paper includes detailed experiment setups analyses and visualizations that help in understanding the proposed methodology. Weak Points:  Clarity of Exposition: The paper while thorough could benefit from improved clarity in the writing to enhance readability for a broader audience.  Comparative Analysis: More comparative analysis with existing approaches or baselines could enhance the validation of the proposed method.  Reproducibility: While implementation details are provided more emphasis on opensourcing the code and making it readily available could enhance reproducibility.  Questions and Feedback to the Author: 1. Could you provide a more detailed comparison with existing stateoftheart methods in deep reinforcement learning for value approximation to highlight the unique advantages of Fourier Feature Networks 2. How does the proposed method perform under varying hyperparameters or different types of control tasks to assess its generalizability and robustness 3. Have you considered releasing the codebase opensource to encourage reproducibility and facilitate further research in this area  3) Review Summary: The paper presents a novel approach Fourier Feature Networks to address spectral bias in deep neural networks for value approximation in offpolicy reinforcement learning. The theoretical foundation empirical evaluations and scaling experiments demonstrate the efficacy of the proposed method. While the paper is comprehensive and thorough improvements in clarity more extensive comparative analysis and a focus on reproducibility could enhance its impact in the research community. Overall based on the innovative approach strong theoretical underpinnings and promising empirical results I recommend acceptance of the paper for publication after addressing the minor weaknesses highlighted above.  This feedback intends to provide constructive criticism and guidance to the authors to strengthen their work and enhance its impact in the field of deep reinforcement learning.", "rqcLsG8Kme9": " Summary: The paper proposes a new method called \"rQdia\" that regularizes Qvalue distributions with augmented images in pixelbased deep reinforcement learning. The main idea is to equalize Qvalues across statistical distributions of actions and states to improve sample efficiency and final performance in deep reinforcement learning tasks. By enhancing representation learning with image augmentation techniques rQdia aims to bridge the gap between pixelbased approaches and models that learn from state embeddings.  Strong Points: 1. Innovative Approach: The concept of regularizing Qvalue distributions with augmented images is novel and offers a unique perspective on improving deep reinforcement learning. 2. Performance Improvement: The method shows significant enhancements in sample efficiency and final performance over existing techniques like DrQ and base DER as demonstrated in experiments in both MuJoCo and Atari environments. 3. Thorough Experiments and Statistical Analysis: The paper carefully evaluates the proposed method through extensive experiments statistical analysis and comparisons with stateoftheart algorithms in different settings providing a comprehensive assessment of its effectiveness. 4. Reproducibility: The authors commit to releasing the code opensource providing transparency and facilitating future research reproducibility.  Weak Points: 1. Limited Generalization: The paper acknowledges limitations related to the assumption of specific augmentation invariance which may restrict the applicability of rQdia in environments where such invariances are not relevant. 2. Ethical Considerations: While the paper briefly touches on the potential ethical implications of reinforcement learning advancements a more indepth discussion on the ethical considerations of deploying advanced RL models in the real world could be beneficial. 3. Further Comparative Analysis: While the paper compares rQdia with existing methods a more detailed analysis comparing its performance with a wider range of algorithms and in various scenarios could provide a more comprehensive evaluation.  Questions and Feedback: 1. Question: How does the proposed rQdia method compare with stateoftheart techniques in scenarios with complex visual transformations or highly dynamic environments 2. Feedback: It would strengthen the paper to discuss potential avenues for mitigating the limitations mentioned such as exploring adaptive augmentation strategies or investigating the transferability of rQdia to diverse environments.  Review Summary: The paper presents a novel approach rQdia for regularizing Qvalue distributions in deep reinforcement learning. While the method demonstrates promising improvements in sample efficiency and final performance further investigations into its generalizability and potential enhancements to address its limitations could strengthen the overall impact. With strong experimental results thorough statistical analysis and a commitment to reproducibility through opensource code release rQdia shows potential as a valuable addition to the field of deep reinforcement learning. Recommended for publication pending minor revisions and additional clarity on comparative analyses and ethical implications.", "qaxhBG1UUaS": "Peer Review 1) Summary: The paper introduces GPTCritic an offline reinforcement learning method for training taskoriented dialogue agents using a pretrained language model. GPTCritic finetunes the GPT2 model through behavior cloning of criticguided selfgenerated sentences addressing the challenge of diverging from human language in standard RL methods. The performance of GPTCritic is evaluated on taskoriented dialogue benchmarks showing significant improvement over stateoftheart methods. 2) Strengths:  Innovative Approach: The idea of revising unsuccessful dialogues into successful ones rather than dropping them sets the paper apart from existing offline RL methods.  Clear Problem Statement: The paper effectively addresses the challenge of RL in dialogue systems and provides a clear explanation of the proposed solution.  Comprehensive Evaluation: The paper thoroughly evaluates GPTCritic using various metrics including datasetbased automatic evaluation simulatorbased evaluation and human evaluation providing a holistic view of its performance.  Theoretical Justification: The paper provides theoretical proofs to support the policy improvement and the methodology used in GPTCritic.  Comparisons with Baselines: The comparison with baseline algorithms including other offline RL methods and pretrained LMbased agents strengthens the credibility of the proposed approach. Weaknesses:  Complexity: The paper involves advanced concepts in RL and NLP which may make it challenging for readers not wellversed in these areas to understand.  Ethical Considerations: The paper lacks a discussion on ethical implications such as potential biases in the generated responses or the use of AI in dialogue systems.  Theoretical Explanation: While the paper provides theoretical justifications a more detailed explanation of the theoretical background and its practical implications could enhance clarity.  Insufficient Discussion on Limitations: The paper could benefit from a more detailed discussion on the limitations of GPTCritic and areas for future research. Question to the Authors:  Have you considered potential biases or ethical implications in using GPTCritic for dialogue systems especially in realworld applications where bias in generated responses could have significant consequences Feedback to the Authors:  Provide more insights into the theoretical foundations behind your method and how they support the practical implementation of GPTCritic.  Address the complexity of the paper by including more explanatory examples or simplifying technical concepts for a wider readership.  Discuss the limitations of GPTCritic and potential avenues for future research more explicitly in the paper. 3) Review Summary: The paper presents an innovative approach GPTCritic for training taskoriented dialogue agents using offline RL. With a robust evaluation methodology and theoretical justifications the paper convincingly demonstrates the superiority of GPTCritic over existing approaches. While the paper could benefit from addressing complexity ethical considerations and discussing limitations more explicitly its contribution to advancing the field of taskoriented dialogue systems is substantial. Overall based on the strengths and potential impact of the proposed method I recommend accepting this paper for publication pending minor revisions and improvements as suggested in the feedback.  This review provides a comprehensive evaluation of the paper highlighting its strengths weaknesses and suggestions for improvement. The provided feedback aims to guide the authors in enhancing the clarity and completeness of their work for potential publication.", "gULyf2IVll0": " Summary of the Paper The paper presents an empirical study analyzing the internal properties of Deep Neural Networks (DNNs) that affect model robustness under adversarial attacks. The authors introduce the concept of the Populated Region Set (PRS) to represent the internal properties of DNNs practically. They conduct systematic experiments to validate the hypothesis that a low PRS ratio is related to the robustness of DNNs against adversarial attacks.  Strengths of the Paper: 1. Innovative Concept: The introduction of the Populated Region Set (PRS) as a novel concept to analyze the relationship between internal properties of DNNs and robustness against adversarial attacks is a significant contribution to the field. 2. Comprehensive Experimental Analysis: The authors provide extensive experimental results from various structures of DNNs and datasets to support their hypothesis. The results demonstrate a consistent relationship between the PRS ratio and model robustness. 3. Detailed Explanations: The paper does a good job of explaining the concepts and methodologies used in the study. The definitions of Decision Boundary (DB) Decision Region (DR) and the formulation of the PRS are welldescribed. 4. Relevance and Novelty: The paper addresses an important issue in evaluating DNNs beyond traditional metrics such as generalization performance and introduces a new perspective based on the geometrical properties of DNNs.  Weaknesses of the Paper: 1. Complexity of Concepts: The paper introduces several new concepts and definitions which might be challenging for readers who are not wellversed in geometric analysis within DNNs. It could benefit from additional simplification or clearer explanations. 2. Lack of Comparison: While the paper extensively discusses the proposed concept and its significance it lacks direct comparisons with existing methods or approaches in assessing DNN robustness.  Questions and Feedback to the Authors: 1. Clarification on Key Metrics: Can the authors provide further insight into how the PRS ratio is calculated and its specific significance in determining model robustness 2. Comparative Analysis: Have the authors considered comparing their approach with other existing methods for evaluating DNN robustness to demonstrate the effectiveness and superiority of the PRS concept 3. Potential Applications: How do the findings and implications of this study impact realworld applications of DNNs particularly in scenarios where robustness against adversarial attacks is crucial  Review Summary: The presented paper offers a valuable contribution to the field by introducing the PRS concept to analyze the internal properties of DNNs concerning robustness against adversarial attacks. The comprehensive experimental analysis supports the hypothesis that a low PRS ratio correlates with higher model robustness. Despite the need for further clarifications on certain concepts and comparisons with existing methods the innovative nature of the study and its potential impact on evaluating DNNs warrant publication after addressing the mentioned concerns.", "z8j0bPU4DIw": "Summary: The paper delves into investigating the performance of Scalable Evolution Strategies (SES) in a Hierarchical Reinforcement Learning (HRL) context. While SES has demonstrated impressive scalability and results in nonhierarchical contexts its application in conjunction with HRL methods has been unexplored. The novel approach introduced in this work named Scalable Hierarchical Evolution Strategies (SHES) merges SES with HRL particularly focusing on challenging tasks like simultaneous robot locomotion and navigation. The study reveals that SES seamlessly fits into the hierarchical framework without requiring modifications showcasing competitive performance with gradientbased HRL methods. Key sections cover policy hierarchy primitive reward formulation goal encoding and controller architecture within the SHES framework. Experimental evaluation on two difficult environments Ant Gather and Ant Maze demonstrates promising stateoftheart performance alongside insights into sample efficiency and scalability issues. Strengths: 1. Innovative Approach: The integration of SES into HRL through SHES presents an innovative solution for tackling challenging RL tasks showcasing the methods adaptability. 2. Detailed Methodologies: The paper provides comprehensive explanations of policy hierarchy primitive reward mechanisms goal encoding and controller design enhancing understanding and reproducibility. 3. Experimental Rigor: Conducting experiments on challenging environments like Ant Gather and Ant Maze adds credibility to the findings demonstrating the methods performance in realworld scenarios. Weaknesses: 1. Sample Efficiency: The study acknowledges the lower sample efficiency of SES compared to gradientbased methods which might limit its applicability in scenarios where data collection is expensive or timeconsuming. 2. Comparison: While the performance of SHES is demonstrated against existing HRL methods further analysis on the tradeoffs between performance and sample efficiency could provide deeper insights into its practical utility. Questions: 1. Have you considered conducting additional experiments with varied hyperparameters or settings to explore the tradeoffs between sample efficiency and task performance further 2. How does SHES compare to existing HRL methods in terms of complexity and ease of implementation Feedback: 1. Enhance the discussion on the implications of the observed sample efficiency issues and explore potential strategies to mitigate these challenges. 2. Provide more insights into the computational requirements and practical considerations when deploying SHES in realworld applications. Review Summary: The paper introduces a novel and promising approach SHES that merges SES with HRL to tackle challenging tasks. While demonstrating stateoftheart performance the study acknowledges sample efficiency issues but highlights the methods robustness and scalability. Overall the rigorous methodologies experimental evaluation and innovative integration of SES in HRL make this work valuable and recommendable for researchers exploring advanced RL techniques. Further investigations into sample efficiency improvements and practical deployment considerations could enhance the papers impact.", "jT1EwXu-4hj": " Review of \"Optimizing Recommendation as Domain Transportation: A Principled Solution\" Summary: The paper introduces a novel approach transportationconstraint risk minimization to optimize recommendations by considering the interventional nature of recommendation in the context of causal inference and missing data problems. They propose a principled solution that involves domain transportation which aims to transport patterns learned in the source domain to the intervention domain. The paper presents theoretical results a twoplayer minimax game formulation and experimental results that demonstrate the advantages of their approach compared to existing methods. Realworld online testing with a major ecommerce platform further confirms the effectiveness of their solution. Strengths: 1. Original Contribution: The paper addresses a critical aspect of recommendation systems by introducing a novel framework that considers domain transportation providing a fresh perspective in the field. 2. Theoretical Foundation: The rigorous theoretical analysis including consistency generalization and excessive risk bounds for the proposed objective adds a strong foundation to the work. 3. Experimental Validation: The extensive realdata and semisynthetic experiments along with online testing with a realworld IR system provide comprehensive validation of the proposed approach. 4. Comparative Analysis: Comparative analysis with existing solutions including benchmark datasets and various baselines demonstrates the superior performance of the proposed approach. 5. Algorithm and Implementation: The proposed GDA algorithm and its convergence properties along with detailed experimentation setups enhance the reproducibility and applicability of the solution. Weaknesses: 1. Complexity: The approach introduced is intricate due to the consideration of domain transportation which might make it less accessible to practitioners without a strong foundation in causal inference and domain adaptation. 2. Evaluation Metrics: While the paper mentions Hit10 NDCG10 and relevance score a deeper analysis of additional evaluation metrics or user experiences could provide a more holistic understanding of the algorithms performance. Question and Feedback to the Author:  Could you elaborate on how the proposed approach could be adapted or extended to handle more dynamic or evolving user preferences in realtime recommendation scenarios  The experimental section highlights the superior performance of your approach. Could you provide more insights into the interpretability of the model or the weight distribution learned and how it influences the recommendation decisions  Review Summary: The paper presents a novel approach transportationconstraint risk minimization to optimize recommendations in IR systems by incorporating domain transportation. The theoretical foundation experimental validation and comparative analysis demonstrate the effectiveness of the proposed approach. While the complexity of the solution could be a barrier for some practitioners the original contribution and strong theoretical underpinnings make it a valuable addition to the field. Addressing minor points like expanding on realtime recommendation adaptation and providing insights into model interpretability would enhance the paper further. Overall I recommend acceptance with minor revisions to address the weak points and provide additional insights for practical application.", "nK7eZEURiJ4": " Summary The paper discusses the theoretical advantages of distributional reinforcement learning (RL) over expectationbased RL. It interprets distributional RL as entropyregularized maximum likelihood estimation and establishes connections with maximum entropy RL. The paper also sheds light on stabilitypromoting distributional loss optimization acceleration and proposes a new Sinkhorn distributional RL algorithm that interpolates between Wasserstein distance and maximum mean discrepancy (MMD). Experiments on Atari games show competitive performance of the algorithm compared to existing stateoftheart distributional RL algorithms.  Strengths 1. Theoretical Insight: The paper provides a deep theoretical understanding of distributional RL and its advantages over expectationbased RL through various perspectives such as regularization optimization and acceleration. 2. Novel Algorithm: Introducing the Sinkhorn distributional RL algorithm that interpolates between Wasserstein distance and MMD is a significant contribution. 3. Empirical Performance: The experiments demonstrate the competitive performance of the proposed algorithm on a suite of Atari games. 4. Comprehensive Analysis: The paper thoroughly covers aspects such as regularization effects neural Zfitted iteration connection with maximum entropy RL stable optimization guaranteed generalization and acceleration effects.  Weaknesses 1. Clarity of Exposition: Certain sections can be challenging to follow particularly for readers without an extensive background in reinforcement learning theory. More detailed explanations or examples could enhance clarity. 2. Algorithm Implementation: While the theoretical foundations are wellcovered additional details on the practical implementation of the proposed Sinkhorn distributional RL algorithm could be beneficial.  Questions 1. How does the proposed algorithm scale in terms of computational complexity with an increasing number of actions or states 2. Can the Sinkhorn distributional RL algorithm be extended to other domains beyond Atari games and if so what adjustments might be necessary  Feedback  Consider providing more intuitive examples or illustrations to aid in understanding complex theoretical concepts.  Enhance the discussion on potential realworld applications and implications of the proposed algorithm beyond Atari games.  Clarify the transferability and robustness of the algorithm across different environments for a broader impact.  Review Summary The paper offers a significant contribution by providing a thorough theoretical analysis of distributional RL and proposing a novel Sinkhorn distributional RL algorithm. The empirical results on Atari games demonstrate competitive performance supporting the theoretical framework presented. Despite some complexity in the exposition the paper provides valuable insights and lays a strong foundation for advancing distributional RL algorithms. Further clarity in explanation and practical implementation details could enhance the papers impact and accessibility. I recommend acceptance after addressing the feedback provided.", "nBU_u6DLvoK": " PeerReview of \"UniFormer: Efficient and Effective Spatiotemporal Video Understanding\"  1. Summary: The paper proposes a novel Unified transFormer (UniFormer) that integrates 3D convolution and spatiotemporal selfattention in a transformer format to learn rich and multiscale spatiotemporal semantics from highdimensional videos effectively. UniFormer achieves a balance between efficiency and accuracy by employing a unique MultiHead Relation Aggregator (MHRA) that handles local redundancy and global dependency separately in shallow and deep layers. Extensive experiments on popular video benchmarks show that UniFormer outperforms stateoftheart methods with fewer computations and achieves new stateoftheart performances on SomethingSomething V1 and V2 datasets.  2. Strong Points:  Strong Points: 1. Innovative Integration of 3D Convolution and SelfAttention: The paper proposes a novel approach to merge the strengths of 3D convolution and vision transformers offering a unique solution to address redundancy and dependency in video understanding. 2. Efficient Design: The novel MHRA architecture that distinguishes between local and global token relations shows promise in achieving a balance between computation cost and accuracy. 3. Extensive Experimental Validation: The paper provides thorough experimental results on various video benchmarks demonstrating the superiority of UniFormer over existing methods in terms of accuracy and efficiency.  Weak Points: 1. Clarity in Presentation: The paper might benefit from improved clarity in presenting the technical details especially in explaining the novelty and uniqueness of UniFormers design. 2. Comparison to Baselines: While comparisons to stateoftheart methods are provided more indepth comparisons with baseline models and detailed ablation studies could enhance the understanding of UniFormers effectiveness.  Questions and Feedback: 1. Clarification on Relation Aggregator: Can you provide more insights into the mechanism of the Relation Aggregator especially how it balances local and global token relations in different layers 2. Comparison with Existing Approaches: How does UniFormer compare with existing methods that attempt to combine 3D convolution and transformers for video understanding 3. Scalability and Generalization: How scalable is UniFormer to other video datasets and realworld applications beyond the benchmark datasets used in this study  3. Review Summary: The paper presents a novel and innovative approach UniFormer for efficient and effective spatiotemporal video understanding by integrating 3D convolution and selfattention in a transformer format. The method demonstrates superior performance on video benchmarks while maintaining a good balance between accuracy and computation cost. Although the paper lacks some clarity in presentation and detailed comparisons with baseline models the experimental results and unique design of UniFormer support the recommendation for acceptance. Overall UniFormer shows promise in advancing the field of video understanding with its innovative architecture and performance benefits.", "kcadk-DShNO": " PeerReview Draft: 1) Summarize the Paper: The paper proposes a novel unified nonadversarial framework for unsupervised dataset alignment. It aims to generalize previous flowbased approaches by minimizing an upper bound on the JensenShannon Divergence (JSD). The framework leverages invertible models to align multiple distributions and introduces regularization via transportation cost. The authors provide comprehensive theoretical analyses including upper bound proofs and loss function derivations. Empirical results on simulated and realworld datasets demonstrate the effectiveness of the proposed approach compared to prior methods. 2) Strengths and Weaknesses of the Paper: Strong Points:  Theoretical Rigor: The paper provides a thorough theoretical foundation proving equivalence bounding strategies and the relationship to prior works which adds significant value to the research domain.  Unified Framework: The proposed nonadversarial approach unifies previous flowbased methods and offers a cooperative optimization perspective potentially resolving challenges associated with adversarial learning.  Empirical Demonstrations: The empirical results on both simulated and realworld datasets validate the effectiveness of the proposed framework highlighting its superiority over existing methods. Weak Points:  Complexity: The paper entails intricate mathematical derivations and technical details that might pose challenges for readers without an advanced understanding of the topic. Simplifying complex concepts could improve accessibility.  Evaluation Metrics: While the paper introduces a novel alignment metric (AUB) the comparison with existing evaluation metrics like FID is limited. Including a more indepth analysis and comparison of evaluation methods could enhance the papers comprehensiveness.  Generalization: Although the paper claims to provide a domainagnostic metric for dataset alignment further exploration and validation across different data types and domains would strengthen the frameworks applicability and generalizability. Questions: 1. Can you elaborate on the scalability of the proposed framework for aligning multiple domains beyond two or three distributions How does the performance scale with an increasing number of distributions 2. In the experiment section only flowbased models are compared. Have you considered including comparisons with other approaches such as adversarial learning methods or classical alignment techniques to provide a more comprehensive evaluation Feedback to the Author: 1. Consider enhancing the papers accessibility by providing intuitive examples or visual aids to help readers grasp complex theoretical concepts more easily. 2. Incorporate a more extensive discussion on the scalability of the framework to align multiple domains and broaden the comparison scope with diverse alignment methods to strengthen the empirical evaluation. 3. Clarify the limitations of the proposed approach and potential challenges in realworld applications to provide a more nuanced perspective on the frameworks practical implications. 3) Review Summary: The paper presents a novel nonadversarial framework for unsupervised dataset alignment offering a unified perspective on flowbased methods and introducing a cooperative optimization approach. The theoretical analyses establish a robust foundation for the framework while empirical results demonstrate its superior performance compared to prior approaches. The strong theoretical rigor and empirical validation contribute significantly to the research domain. However addressing issues related to complexity evaluation metrics and generalization could further enhance the papers impact and applicability. Overall the paper provides valuable insights into nonadversarial dataset alignment and opens avenues for future research in the field. Considering the papers substantial contributions theoretical depth and empirical validation I recommend acceptance pending minor revisions to address the identified weaknesses and clarify certain aspects for a more comprehensive and impactful contribution.", "qRDQi3ocgR3": " Peer Review Draft:  1) Summary: The paper explores the phenomenon of shortcut biases in deep neural networks (DNNs) through an experimental setup named WCSTML where multiple cues are equally valid for solving a task. The study reveals that despite equal opportunities DNNs demonstrate a preference for certain cues over others due to their simplicity (Kolmogorov complexity). The authors experimentally verify this phenomenon on synthetic and face datasets highlighting implications for generalization and fairness in DNN models.  2) Strengths and Weaknesses:  Strong Points: 1. Research Contribution: The paper contributes a novel experimental setup WCSTML shedding light on the nature of shortcut biases in DNNs. 2. Theoretical Insights: The exploration of Kolmogorov complexity in understanding cue preferences provides valuable theoretical insights. 3. Comprehensive Analysis: The study employs diverse measures like unbiased accuracies parameter space analysis and complexity estimates to provide a holistic understanding of shortcut learning phenomenon.  Weak Points: 1. Lack of Solution Proposal: The paper focuses on deepening the understanding of shortcut biases without offering concrete solutions to mitigate the observed issues. 2. Validation: While the findings are backed by rigorous experiments the paper could benefit from external validation or application on realworld datasets to enhance the practical relevance. 3. Ethics Discussion: The ethical implications of using specific cues like ethnicity could be further elaborated along with suggestions on how to address potential biases.  Question and Feedback to the Authors: 1. Further Validation: How do you plan to further validate the observed preferences and biases in realworld scenarios or complex datasets beyond the synthetic and UTKFace datasets 2. Practical Implications: Considering the ethical concerns highlighted are there any specific strategies or interventions that can be proposed to address shortcut biases in DNN models 3. Generalization Challenges: How can DNNs be trained to overcome the simplicity bias and improve generalization across distribution shifts particularly in scenarios where simple cues may not be generalizable  3) Review Summary: The paper presents a compelling exploration of shortcut biases in DNNs through the WCSTML setup revealing the propensity of models to prefer simple cues despite equal opportunities. The theoretical underpinning with Kolmogorov complexity and thorough analyses provide valuable insights into the mechanisms driving shortcut learning. However the paper lacks concrete solutions to address the identified biases and could aim to enhance the practical relevance by extending the study to more diverse datasets. Overall the theoretical contributions and experimental rigor make this paper a significant contribution to understanding and addressing bias in machine learning models.  Recommendation: The paper demonstrates strong theoretical underpinnings and experimental rigor in studying shortcut biases in DNNs. However to enhance the practical relevance the authors are encouraged to propose potential solutions to mitigate shortcut biases in DNN models. With minor improvements in validating the findings on diverse datasets and providing practical interventions this paper has the potential to make a significant impact on the field of machine learning. Therefore I recommend acceptance with revisions.", "qSV5CuSaK_a": "Summary of the Paper: The paper explores the vulnerability of visual object tracking (VOT) models to backdoor attacks caused by using thirdparty resources for training. It introduces a fewshot backdoor attack (FSBA) that implants hidden backdoors into VOT models by altering the training process. The attack optimizes two losses alternately: a feature loss in the hidden feature space and the standard tracking loss. The paper demonstrates that once the backdoor is implanted using FSBA the model can fail to track specific objects when the trigger appears in only one or a few frames. The attack is evaluated in digital and physicalworld scenarios showing significant performance degradation in stateoftheart VOT trackers. Strong Points: 1. Innovative Approach: The paper introduces a novel fewshot backdoor attack specifically designed for VOT models addressing a gap in existing literature. 2. Thorough Analysis: The study thoroughly examines the attacks effectiveness across different scenarios datasets and defense mechanisms providing comprehensive insights. 3. Empirical Validation: The proposed FSBA is empirically validated to significantly degrade the tracking performance of advanced VOT models highlighting the severity of the vulnerability. Weak Points: 1. Limited Experimental Exploration: While the paper covers a variety of scenarios and datasets further exploration on a wider range of VOT models and datasets could strengthen the study. 2. Clarity and Presentation: Certain sections could be more clearly explained to enhance understanding particularly for readers less familiar with VOT and backdoor attacks. Questions and Feedback to the Author: 1. Can you elaborate on the potential realworld implications of VOT models being vulnerable to such backdoor attacks especially in critical applications like autonomous driving and surveillance systems 2. How does the proposed fewshot backdoor attack compare to existing backdoor techniques in terms of efficiency and stealthiness in the context of VOT models Review Summary: This paper presents a significant contribution by unveiling the vulnerability of VOT models to backdoor attacks and introducing a novel fewshot backdoor attack method. The thorough experimental analysis and validation indicate the effectiveness and potential threat posed by the proposed attack. Despite some areas for improvement the paper is wellstructured and the findings are clearly presented. Hence I recommend acceptance of this paper based on its innovative approach rigorous analysis and important insights into the security risks in VOT models.", "rOGm97YR22N": "Peer Review: 1) Summary:  The paper introduces a novel recurrent neural network architecture named MixedMemoryRNNs (mmRNNs) designed specifically for modeling irregularlysampled time series with longterm dependencies. The paper addresses the vanishing and exploding gradient issue faced by standard RNNs and ODERNNs by proposing a structure involving a memory compartment and continuoustime dynamical flow within an RNN. Through theoretical proofs and empirical experiments the paper demonstrates the effectiveness of mmRNNs in outperforming existing RNN architectures on various synthetic and realworld timeseries tasks. 2) Strengths and Weaknesses:  Strong Points:  The paper addresses a critical issue of vanishing and exploding gradients in RNNs offering a solution with theoretical proofs.  The introduction of mmRNNs tackles the problem effectively and outperforms other RNN models in experimental evaluations.  The explanations and proofs provided are clear logical and sound demonstrating a solid understanding of the problem and proposed solution.  Experimental results on various datasets demonstrate the superiority of mmRNNs in handling irregularly sampled time series with longterm dependencies.  The comparison with existing RNN architectures and detailed experimental evaluation provide a comprehensive view of the proposed approachs performance. Weak Points:  The transition from theoretical proofs to empirical evaluations could be more streamlined for easier understanding by readers not familiar with the technical details.  Justification for the choice of specific hyperparameters and the RNN architecture could be more detailed to help readers grasp the rationale behind design decisions.  It would be beneficial to discuss the scalability and computational efficiency of mmRNNs especially in largescale applications or realtime systems.  Clarifications on how mmRNNs handle challenges like overfitting generalization to unseen data and model interpretability would enhance the completeness of the paper. Questions and Feedback to the Author: 1. Can you provide more insights into the scalability and computational efficiency of mmRNNs especially in comparison to other recurrent architectures in terms of handling larger datasets or realtime applications 2. How does mmRNN address potential issues like overfitting generalization to unseen data and model interpretability Are there specific considerations or techniques used to mitigate these challenges 3. Could you elaborate more on the rationale behind the choice of specific hyperparameters and the architecture design of mmRNNs How do these choices contribute to the models performance 4. In the context of practical implementation are there any specific considerations or tradeoffs associated with deploying mmRNNs in various realworld applications 5. Given the promising results shown in the experiments can you discuss potential future directions or extensions of this work to further enhance mmRNNs\u2019 capabilities or overcome existing limitations 3) Review Summary:  The paper introduces a novel MixedMemoryRNNs (mmRNNs) architecture to address the vanishing and exploding gradient issues in recurrent neural networks for modeling irregularlysampled time series. The theoretical proofs and empirical experiments conducted provide strong evidence of mmRNNs\u2019 effectiveness in outperforming existing RNN architectures on various timeseries tasks. The comprehensive review concludes that the paper makes a significant contribution by proposing a practical solution and demonstrating its superior performance. Addressing minor issues such as transition clarification scalability considerations and overfitting mitigation could enhance the overall quality and impact of the paper. Based on the significant contributions and the soundness of the proposed method I recommend this paper be accepted for publication pending minor revisions and clarifications as noted in the feedback above.  This highquality peer review covers the summary strengths weaknesses questions and feedback to the author for the submitted paper effectively. Please let me know if you need further assistance or more details on any specific section.", "j30wC0JM39Q": " 1) Summary of the Paper This paper explores the inherent structure within embedding spaces focusing on word and graph embeddings trained on various datasets using popular algorithms. The authors investigate the properties observed in these embedding spaces such as frequency concentration and clustering velocities. They introduce generative models for embedding spaces compare them with real datadriven embeddings and propose statistical metrics to measure the spatial characteristics of embeddings.  2) Evaluation of Strong and Weak Points Questions and Feedback  Strong Points: 1. Comprehensive Exploration: The paper delves into the characteristics and properties of embedding spaces using a variety of models and statistical metrics. 2. Relevant Research Gap Addressed: By focusing on natural processes generating knowledge or concepts based on embedding structures the paper tackles an important research question. 3. Novel Metrics Development: The introduction of metrics like Frequency Concentration and Clustering Velocity to study embedding spaces provides unique insights into the spatial properties of embeddings.  Weak Points: 1. Complexity: The paper might be challenging for readers unfamiliar with embedding spaces and statistical methods potentially limiting its accessibility and understanding. 2. Lack of Evaluation of RealWorld Applications: While the models are tested against real embeddings the paper could benefit from discussing potential realworld applications or implications of the findings.  Questions: 1. How do the proposed generative models compare in terms of computational efficiency and scalability especially when dealing with largescale datasets 2. Can the identified spatial patterns and properties in embedding spaces be used to enhance existing machine learning applications such as information retrieval or recommendation systems  Feedback: 1. Provide a more detailed discussion on the practical implications and potential applications of the research findings to help bridge the gap between theoretical exploration and realworld utilization. 2. Consider simplifying complex concepts and methodology explanations possibly through visual aids or examples to improve the clarity and accessibility of the paper for a wider audience.  3) Review Summary Given the papers comprehensive exploration of embedding spaces development of novel metrics and addressing a relevant research gap the overall work showcases significant contributions to understanding the natural processes that generate knowledge or concepts. However to enhance the impact and accessibility of the research addressing the weak points highlighted offering more realworld applications insight and streamlining complexity for readability and broader understanding could further strengthen the paper. Based on the strong points and potential improvements suggested I recommend acceptance after addressing the outlined suggestions to enhance the papers overall quality and impact.", "jNB6vfl_680": " Summary of the Paper: The paper introduces a simple yet effective pruning technique called Global Magnitude Pruning (GP) for neural networks. It highlights that GP when combined with a Minimum Threshold (MT) method achieves stateoftheart (SOTA) performance on various neural network architectures including ResNet50 and MobileNetV1 on ImageNet. The MT method ensures that no layer is overpruned by preserving a fixed number of weights in each layer postpruning. The paper conducts experiments on multiple datasets and architectures demonstrating the superior performance of GP with MT compared to other pruning methods.  Strong Points: 1. Simplicity: The paper emphasizes the simplicity and ease of implementation of GP with MT making it userfriendly and generalizable across different architectures and datasets. 2. StateoftheArt Performance: The experiments showcase that the proposed GP with MT method outperforms other SOTA pruning algorithms on various architectures and datasets. 3. Insights and Analysis: The paper provides detailed insights into the impact of GP and MT on different architectures along with analysis on the working dynamics of the pruning techniques. 4. Importance of Minimum Threshold: The introduction of the MT technique to prevent overpruning in certain architectures is a significant contribution adding to the effectiveness of the pruning strategy.  Weak Points: 1. FLOPs Overhead: One limitation highlighted is the potential of higher FLOPs compared to some baseline methods which could impact the computational efficiency.  Questions and Feedback to the Author: 1. Scalability: Have you tested the scalability of GP with MT on larger and more complex neural network architectures beyond ResNet50 and MobileNetV1 2. Tradeoff Analysis: Could you provide more insights into the tradeoff between accuracy and pruning rate when using GP with MT across different datasets and architectures 3. Comparison with Finetuning: How does the oneshot pruning approach of GP with MT compare with iterative methods in terms of convergence speed and model performance  Review Summary: The paper presents a valuable exploration of Global Magnitude Pruning (GP) with the addition of Minimum Threshold (MT) as a stateoftheart technique for neural network pruning. The simplicity and effectiveness of GP with MT along with the insights provided on pruning behavior and network architectures make it a strong contribution to the field. The experiments demonstrating superior performance on various architectures and datasets support the recommendation for further research and application of GP with MT. However addressing the potential FLOPs overhead and expanding the analysis to diverse neural network architectures could enhance the impact of the proposed method.  Recommendation: Based on the comprehensive analysis and experimental results presented in the paper I recommend the acceptance of this work for publication.", "kQ2SOflIOVC": " Summary of the Paper: The paper introduces a study on fewshot learning in histology images an area that has been less explored compared to natural images. The authors propose a method that combines contrastive learning (CL) with latent augmentation (LA) to enable labelefficient learning and enhance generalizability in histology images. They conduct experiments on three crossdomain tasks to simulate real clinic scenarios and evaluate the performance of their proposed method against traditional supervised learning approaches.  Strengths of the Paper: 1. Innovative Contribution: The paper addresses the gap in research by applying fewshot learning to histology images and introduces a novel method combining CL with LA. 2. Experimental Setup: The authors design thorough experiments across different tasks and datasets to validate the effectiveness of their proposed method. 3. Empirical Results: The results demonstrate the superiority of the proposed CL method over traditional supervised learning approaches indicating strong performance gains in generalization. 4. Comprehensive Coverage: The paper provides detailed explanations analyses and ablation studies to explore different aspects of their method enhancing understanding and ensuring robustness.  Weaknesses of the Paper: 1. Complexity: The paper delves into technical details and introduces advanced concepts like contrastive learning and latent augmentation which might be challenging for readers not familiar with these topics. 2. Visualization: While the paper offers some visualization results further illustrations or diagrams could enhance the clarity of explanation especially in complex concepts like latent augmentation. 3. Reproducibility: While the authors provide the code for their method more details on hyperparameters and implementation specifics could improve reproducibility for interested researchers.  Questions to the Authors: 1. Can you provide more insights into the selection of specific hyperparameters in your experiments such as the number of prototypes in the base dictionary and the choice of augmentation times 2. How do you envision the practical application of your proposed method in realworld histological image analysis scenarios considering factors like computational efficiency and deployment feasibility 3. Have you considered exploring the transferability of your approach to other medical imaging domains beyond histology images and what challenges or adaptations do you foresee in such scenarios  Feedback to the Authors: 1. Strengthen the paper by providing clearer visualizations or illustrative examples to aid in understanding complex concepts. 2. Consider expanding sections on practical implications and discussing potential extensions or applications of your method to broader medical imaging contexts. 3. Enhance reproducibility by offering more detailed descriptions of hyperparameters and implementation steps to facilitate other researchers in replicating your experiments.  Review Summary: Overall the paper presents a valuable contribution to the field by introducing a novel approach to fewshot learning in histology images. The methods effectiveness is supported by strong experimental results and detailed analyses. However improvements in clarity reproducibility and practical insights could further enhance the impact and applicability of the research. Therefore I recommend acceptance with minor revisions to address the weaknesses and provide additional clarity in presentation.", "xnYACQquaGV": "1) Summary: The paper introduces a new neural contextual bandit algorithm NeuralLinUCB which leverages a deep neural network for feature transformation and combines this with upper confidence bound (UCB) exploration in the last layer. The algorithm aims to strike a balance between computational efficiency and expressiveness by decoupling deep representation learning and shallow exploration. The theoretical analysis shows that the proposed algorithm achieves a sublinear regret matching linear contextual bandits in terms of regret behavior. 2) Strengths: Strong Points:  The paper provides a clear motivation for the algorithm by addressing limitations of existing contextual bandit models and algorithms.  The proposed algorithm NeuralLinUCB showcases a novel approach that combines deep representation learning with shallow exploration demonstrating the benefits of computational efficiency and strong expressiveness.  The theoretical analysis assumptions and results are wellstructured and help in understanding the algorithms performance and behavior.  The empirical evaluations on realworld datasets provide concrete evidence of the algorithms superiority over traditional linear bandits and existing neural bandit methods.  The comparison with other algorithms like LinUCB NeuralUCB and NeuralTS adds depth to the analysis and showcases the advantages of NeuralLinUCB.  The paper is wellwritten and systematically organized enhancing readability and comprehension of the complex topic. Weak Points: Weak Points:  The paper could benefit from a more detailed discussion on the scalability and generalizability of the proposed algorithm to diverse bandit settings or environments.  Further elaboration on the practical implications of the regret analysis results and how they can guide algorithm design or application could enhance the papers relevance.  Including additional experimental results or comparing the algorithm with more diverse baseline methods could strengthen the empirical evaluation section. Questions and Feedback to the Author:  How does the proposed algorithm handle nonlinear reward structures that may not align with the assumptions made in the analysis  Could you provide insights into the scalability of the algorithm to larger datasets or more complex bandit problems  How do the assumptions imposed in the theoretical analysis translate to realworld applications and what are the potential challenges in meeting those assumptions 3) Review Summary: The paper presents an innovative approach to neural contextual bandits with the NeuralLinUCB algorithm combining deep representation learning and shallow exploration for improved performance. The theoretical analysis supports the algorithms effectiveness in achieving sublinear regret. Empirical evaluations on realworld datasets validate the algorithms superiority over traditional linear bandits and existing neural bandit methods. By addressing limitations of current models and algorithms the paper offers a significant contribution to the contextual bandit literature. Suggestions for enhancing scalability practical implications and further validation would strengthen the paper. Overall the wellstructured presentation clear motivation and strong experimental results support a positive recommendation for publication.", "hxitw01k_Ql": " Summary of the Paper: The paper investigates the tradeoff between memory architecture and optimal solutions in partially observable Markov decision processes (POMDPs) particularly focusing on a twohypothesis testing problem analogous to the twoarm bandit problem. The study compares two memory structures: random access memory (RAM) where all transitions between memory states are allowed and Memento memory which provides limited access to the agents past actions and rewards. The main results indicate that while RAM allows for flexible memory arrangements Memento memory surprisingly delivers excellent policies due to the efficient encoding of actions. The policies are optimized using gradient descent algorithms for performance evaluation under various conditions.  Strengths: 1. Novelty and Relevance: The paper addresses an important problem in reinforcement learning by exploring memory architecture constraints for optimal policy solutions in POMDPs providing valuable insights for practical applications. 2. Theoretical Contribution: The study offers theoretical analysis and conjectures supported by formal proofs enriching the understanding of optimal policy design in POMDPs. 3. Empirical Validation: Empirical evaluations are conducted to validate the theoretical findings enhancing the robustness and applicability of the results. 4. Clear Presentation: The paper is wellstructured and articulately presents the research question methodology results and implications making it accessible to a broad audience of researchers.  Weaknesses: 1. Lack of Generalizability: The study primarily focuses on a specific twohypothesis testing problem which may limit the generalizability of the findings to broader POMDP scenarios. 2. Complexity and Technicality: The paper contains intricate mathematical derivations and proofs which could make it challenging for readers without a strong background in reinforcement learning and combinatorics to fully grasp the content. 3. Limited Discussion on Application: Although the theoretical and empirical results are comprehensive more discussion on potential realworld applications or implications of the findings could enhance the practical relevance of the study.  Questions to the Authors: 1. Generalization: Have you considered how the findings regarding memory architecture constraints and optimal policies in POMDPs could be extended to more complex environments or diverse applications in reinforcement learning 2. Scalability: How scalable are the proposed memory architecture solutions to larger and more intricate POMDP tasks and have you explored scalability issues through experiments or simulations 3. Practical Implementation: What practical challenges or considerations would researchers need to address when implementing the proposed memory architecture optimizations in realworld reinforcement learning systems  Feedback to the Authors: 1. Clarify Practical Relevance: Enhance the discussion on the practical implications of the research findings highlighting potential applications and benefits in realworld reinforcement learning scenarios. 2. Simplify Technical Explanations: Consider providing simpler explanations or visual aids to assist readers in understanding the complex mathematical derivations and proofs presented in the paper. 3. Broaden Scope: Discuss avenues for further research or extensions to address the generalizability of the findings and explore applications in different POMDP settings for a broader impact.  Overall Impression: The manuscript provides a valuable contribution to the understanding of memory architecture impact on policy optimization in POMDPs. The combination of theoretical analysis empirical evaluations and formal proofs strengthens the credibility of the findings. However enhancing the discussion on practical implications and simplifying technical aspects could further improve the accessibility and relevance of the research. Based on the strengths in theoretical depth and empirical validation I recommend acceptance with minor revisions focusing on clarity and applicability.", "gijKplIZ2Y-": "Summary: The paper proposes Mistill an approach that leverages Machine Learning (ML) to automatically learn distributed protocols for Traffic Engineering (TE) in data center networks. Mistill distills forwarding behavior from exemplar forwarding decisions into a Neural Network obviating the need for manual protocol design. The paper demonstrates Mistills efficacy in learning and generalizing three TE policies along with analyzing the learned protocols performance in simulations on synthetic and realworld traffic patterns. Strengths: 1. Innovative Approach: The paper introduces an innovative approach using ML to automate the design of distributed protocols for TE in data center networks. 2. Comprehensive Evaluation: The paper extensively evaluates Mistills performance on various traffic patterns and compares it with existing TE solutions. 3. Generalization: Mistill demonstrates a remarkable ability to generalize to unseen traffic patterns showing the robustness of the learned protocols. 4. Readability and Structure: The paper is wellstructured with clear sections and explanations making it easily comprehensible. 5. Reproducibility: The authors ensure reproducibility by making the source code and trained models available enhancing transparency and research integrity. Weaknesses: 1. Limited Practical Deployment Details: While the paper hints at the potential deployment of Mistill on hardware more indepth discussion on practical deployment challenges and considerations would be beneficial. 2. Further Practical Validation: While the simulation results are promising practical validation of Mistill on real hardware in data center environments could enhance the papers practical relevance. 3. Comparative Analysis: Providing a more detailed comparative analysis with existing MLbased networking approaches and evaluation against additional stateoftheart TE schemes could strengthen the validation. Questions to Authors: 1. How do you envision overcoming potential scalability challenges when deploying Mistill in largescale data center networks 2. Have you considered the computational overhead on network devices when deploying Mistill especially in scenarios with high traffic volume 3. How do you plan to address potential security implications when implementing learned protocols on real hardware Feedback: 1. Enhance the discussion on practical implementation challenges and scalability considerations for deploying Mistill in realworld data center networks. 2. Consider extending the comparative analysis with other MLbased networking approaches and stateoftheart TE schemes to provide a comprehensive evaluation context. 3. Discuss potential security implications and measures to address them when deploying learned protocols on network devices. Review Summary: Overall the paper presents a novel and promising approach in leveraging ML for automated protocol design in data center networks. With comprehensive evaluation and reproducibility measures Mistill demonstrates robustness and generalization capabilities. Strengthening practical deployment aspects extending comparative analysis and addressing security concerns would further enhance the papers contributions. Therefore I recommend acceptance with minor revisions to address the highlighted aspects.", "g2LCQwG7Of": " 1. Summary: The paper introduces a novel probabilistic model Flexible Probabilistic Hierarchy (FPH) for hierarchical clustering on graphs. The model utilizes continuous relaxation of treebased hierarchies and draws connections to absorbing Markov chains to compute relevant quantities efficiently. The authors demonstrate the effectiveness of FPH through extensive experiments on 11 realworld graphs including a large graph with 2.3M nodes where FPH consistently outperforms traditional and recent baselines on quality metrics such as Dasgupta cost and TreeSampling Divergence (TSD). Additionally FPH performs well on link prediction tasks despite not being specifically trained for this purpose.  2. Strong Points:  Originality: The paper introduces a novel probabilistic model for hierarchical clustering on graphs providing a fresh perspective on addressing the problem.  Theoretical Analysis: The authors provide a comprehensive theoretical analysis of the model drawing connections to absorbing Markov chains and deriving efficient closedform expressions for relevant quantities.  Extensive Experiments: The experiments conducted on various realworld graphs including a very large graph with 2.3M nodes demonstrate the effectiveness of the proposed model over traditional and recent baselines.  Scalability and Efficiency: The paper addresses scalability concerns by proposing efficient batching schemes and vectorized computations for computations improving performance even on large graphs.  Versatility: The model performs well not only in hierarchical clustering but also in link prediction tasks showcasing its versatility and generalizability.  Weak Points:  Computational Complexity: The paper could benefit from a more detailed discussion on the computational complexity of the model particularly in terms of resources required for training and inference on large graphs.  Comparison with StateoftheArt: While the paper compares the proposed model with traditional and recent baselines a direct comparison with stateoftheart deep learning models for hierarchical clustering could provide additional context.  Interpretability: The authors could delve deeper into the interpretability of the hierarchies learned by FPH especially in the context of realworld applications.  Question and Feedback to the Authors: 1. Could you provide insights into how the models performance scales with the size of the graphs How does the computational complexity evolve with increasing graph sizes 2. How would the proposed model handle noisy or sparse graph data and how robust is it to such scenarios 3. It would be interesting to see a more indepth discussion on the interpretability of the learned hierarchies and potential applications beyond clustering and link prediction tasks.  3. Review Summary: The paper presents a novel probabilistic model Flexible Probabilistic Hierarchy (FPH) for hierarchical clustering on graphs showcasing its effectiveness through extensive experiments on various realworld datasets. The theoretical foundation scalability efficiency and versatility of the model make it a valuable contribution to the field of unsupervised learning. While the paper could benefit from additional discussions on computational complexity interpretability and comparisons with stateoftheart models the overall quality and significance of the work warrant its acceptance for publication.", "nZeVKeeFYf9": " Summary: The paper introduces LowRank Adaptation (LoRA) as an efficient strategy for adapting large pretrained language models to specific downstream tasks. LoRA involves injecting trainable rank decomposition matrices into different layers of Transformer architectures reducing the number of trainable parameters while maintaining model quality. The approach is compared with traditional finetuning methods on various language models like RoBERTa DeBERTa GPT2 and GPT3 showing promising results in terms of parameter efficiency training speed and model performance.  Strengths: 1. Innovative Approach: LoRA presents a novel solution to the challenge of adapting large language models efficiently by introducing lowrank decomposition matrices. 2. Empirical Validation: The paper provides comprehensive empirical evaluation across various benchmark tasks and demonstrates the effectiveness of LoRA compared to finetuning. 3. Efficiency: LoRA offers substantial benefits in terms of reducing trainable parameters GPU memory usage training speed and operational efficiency.  Weaknesses: 1. Limited Experiments: While the empirical results are promising the paper could have included a broader range of tasks and models for a more comprehensive evaluation. 2. Complexity: The technical details and mathematical formulations might be challenging for readers not familiar with the domain potentially limiting the accessibility of the paper. 3. Detailed Comparison: The comparison with existing efficient adaptation methods could have been more detailed to provide a clearer understanding of the advantages of LoRA.  Questions and Feedback to the Authors: 1. Have you considered exploring the scalability of LoRA to even larger language models or across different types of neural networks beyond Transformers 2. How does LoRA perform in scenarios with limited training data or under different training conditions compared to traditional finetuning approaches 3. It would be insightful to understand the tradeoffs and limitations of LoRA compared to other adaptation strategies in more detail. Can you provide a deeper analysis in future works  Review Summary: The paper presents an innovative and efficient approach in the form of LowRank Adaptation (LoRA) for adapting largescale pretrained language models to downstream tasks. The empirical evaluation showcases the effectiveness of LoRA in reducing parameters memory requirements and training time while maintaining model quality. The methodology is welldetailed and the results demonstrate the potential impact of LoRA in overcoming challenges associated with full finetuning. Overall the paper offers valuable insights and significant contributions to the field of natural language processing and model adaptation. Hence I recommend acceptance with minor revisions to address the outlined weaknesses and incorporate the suggested questions for further exploration.", "kxARp2zoqAk": " Summary: The paper introduces InfoTS a novel approach for adaptive data augmentation selection in contrastive representation learning for time series data. It addresses the challenge of finding suitable augmentations for time series data which lack recognizable patterns by proposing criteria based on information theory. InfoTS employs a metalearning mechanism to automatically select optimal augmentations and achieves competitive performance in forecasting and classification tasks compared to leading baselines. The method is theoretically grounded showing effectiveness in selecting augmentations with high fidelity and variety essential for representation learning.  Strengths: 1. Theoretical Grounding: The paper provides a solid theoretical foundation by deriving criteria for selecting suitable data augmentations based on information theory. This provides a clear rationale for the proposed approach. 2. Innovative Approach: The adaptation of metalearning for selecting augmentations in contrastive learning for time series data is innovative and addresses a crucial gap in the domain. 3. Empirical Validation: The paper conducts comprehensive experiments on various datasets showcasing the effectiveness and competitiveness of the proposed approach over existing baselines in both forecasting and classification tasks. 4. Evaluation of Criteria: The empirical verification of the proposed criteria on the CricketX dataset demonstrates a positive relationship between the criteria score and classification accuracy affirming the effectiveness of the approach.  Weaknesses: 1. Evaluation Metrics: While the paper extensively evaluates the proposed method on various datasets additional analysis on the interpretability of representations learned by InfoTS would provide deeper insights into the effectiveness of the approach. 2. Comparison to StateoftheArt: Although the paper compares InfoTS with leading baselines a more detailed analysis of how InfoTS outperforms these baselines in terms of computational efficiency scalability or interpretability would enrich the comparison. 3. Dataset Diversity: While the paper demonstrates success on various datasets further exploration across a wider range of time series datasets from different domains would strengthen the generalizability claims of InfoTS.  Questions and Feedback to the Author: 1. The paper highlights the adaptive selection of augmentations as a key component of the proposed method. How does the complexity of the dataset influence the selection process and how does InfoTS adapt to datasets with varying complexities 2. Could the authors provide insights into how InfoTS can potentially be extended to handle more challenging time series data tasks beyond forecasting and classification Are there specific domain applications where InfoTS might exhibit exceptional performance  Review Summary: The paper proposes an innovative approach InfoTS for adaptive data augmentation selection in contrastive representation learning for time series data. The theoretical foundation empirical validation and effectiveness in addressing the augmentation selection challenge make InfoTS a notable contribution to the field. With comprehensive experiments demonstrating competitive performance and the empirical verification of the proposed criteria the paper showcases the robustness and utility of the approach. Addressing minor weaknesses related to interpretability scalability and dataset diversity would further enhance the impact of the work. Overall based on the strong theoretical underpinning and impressive empirical results I recommend acceptance of the paper after addressing the identified points.", "xVGrCe5fCXY": "Peer Review 1) Summary: The paper introduces the Denoising Diffusion Gamma Model (DDGM) as an alternative to the conventional Gaussian noise distribution for generative diffusion models. The study investigates the effectiveness of utilizing Gamma noise distribution in improving image and speech generation tasks. The authors provide a detailed theoretical framework for the DDGM demonstrating its advantages over Gaussian noise models. Experimental results showcase the superior performance of the proposed DDGM in both image and speech generation tasks compared to baseline models. 2) Strengths:  The paper presents a wellstructured and comprehensive analysis of the proposed Denoising Diffusion Gamma Model (DDGM) providing clear explanations of the theoretical underpinnings and practical implications of using Gamma noise distributions.  The experimental evaluations conducted on both image and speech generation tasks demonstrate the effectiveness of the DDGM in improving the quality of generated outputs.  The inclusion of comparisons with existing models and comprehensive discussions on the implications of introducing nonGaussian noise distributions in diffusion models enhance the novelty and relevance of the study.  The reproducibility statement and available code for experiments contribute to the transparency and reproducibility of the research findings. Weaknesses:  While the paper extensively describes the theoretical foundations of the DDGM and provides empirical results to support its effectiveness there could be a more indepth analysis of the computational efficiency and scalability of the proposed model compared to traditional Gaussianbased diffusion models.  The paper could benefit from further discussing potential limitations or challenges in implementing the DDGM such as hyperparameter tuning for the Gamma noise distribution and computational overhead associated with more complex noise models. Questions  Feedback: 1. What are the computational implications of implementing the Denoising Diffusion Gamma Model (DDGM) in terms of training time memory usage and inference speed compared to Gaussianbased diffusion models 2. Could you elaborate on any challenges or limitations encountered in the experimental setup or implementation of the DDGM in realworld scenarios particularly in scaling up for large datasets or complex tasks 3. How sensitive is the performance of the DDGM to variations in the hyperparameters particularly the initial scale parameter (\u03b80) Have you explored methods to optimize or automate the hyperparameter tuning process for the Gamma noise distribution 3) Review Summary: Overall the paper presents a novel and promising approach to improving generative diffusion models through the utilization of Gamma noise distributions in the Denoising Diffusion Gamma Model (DDGM). The theoretical background and experimental results demonstrate the effectiveness of the proposed model in enhancing image and speech generation tasks. The comprehensive analysis reproducibility statement and comparisons with existing models contribute to the robustness and significance of the study. However further exploration of the computational implications and potential challenges of implementing the DDGM could strengthen the papers contributions to the field of generative models. Recommendation: I recommend acceptance of the paper with minor revisions to address the abovementioned weaknesses and provide additional insights into the computational efficiency and practical considerations of implementing the Denoising Diffusion Gamma Model.", "qhAeZjs7dCL": "PeerReview 1. Summary: The paper explores the idea of learning visual representations from generative models without access to their training data. It investigates different representation learning methods from samples output by an offtheshelf image generator. The study compares learning from generative samples with learning directly from real data focusing on contrastive methods for representation learning. 2. Strong Points:  Innovative Approach: The paper addresses a novel and interesting research question of learning visual representations solely from generative models without access to original training data.  Thorough Analysis: The study is comprehensive in analyzing various representation learning methods specifically focusing on contrastive learning from generative samples using latent space transformations.  Experimental Rigor: The experiments are welldesigned evaluating representations learned from unconditional generative models like BigBiGAN and StyleGAN2 on transfer tasks in ImageNet settings.  Clear Method Presentation: The paper effectively presents the contrastive learning framework and describes the methods for sampling positive and negative pairs from generative samples with latent and pixel transformations.  Valuable Insights: The findings show that representations learned from generative models can potentially outperform those learned directly from real data emphasizing the relevance of generative models in future visual representation learning. Weak Points:  Data Samples and Quality: The paper could benefit from a more detailed explanation of the quality and diversity of the generated samples used for representation learning.  Limitations of Approach: It would be helpful to include a discussion on the limitations of learning from generative models alone compared to real data and potential challenges in scaling up such approaches.  Experimental Comparison: The paper could provide a more direct comparison with existing methods and benchmarks in the field to showcase the effectiveness of the proposed approach. Questions and Feedback to the Author: 1. Can you elaborate on the diversity and quality of samples generated by the unconditional generative models used in the experiments such as BigBiGAN and StyleGAN2 2. How do the representations learned from generative samples compare to stateoftheart methods in the field of visual representation learning 3. Have you considered testing your approach on a wider range of datasets to validate the generalizability of the findings across different domains 3. Review Summary: The paper presents an innovative approach to visual representation learning focusing on contrastive methods from generative models. The study is wellstructured and thoroughly investigates the effectiveness of learning from generative samples. While the paper demonstrates promising results and valuable insights into leveraging generative models for representation learning further discussion on data quality limitations and broader experimental comparisons could enhance the significance of the research. Overall considering the novelty of the research question experimental rigor and potential implications for future visual representation learning I recommend its acceptance subject to minor revisions enhancing the discussion and elaborating on certain aspects for improved clarity. Recommendation: Accept with minor revisions.", "xo_5lb5ond": "1) Summary: The paper introduces a novel pruning method called LongEstchAiN (LEAN) for reducing the computational cost of applying convolutional neural networks (CNNs). LEAN pruning aims to consider the interdependence between consecutive operations in CNNs by representing them as a graph and extracting the longest paths based on operator norms. Experimental results on imagetoimage tasks show that LEAN pruning can reduce the number of convolutional filters while maintaining similar accuracy compared to existing approaches. 2) Strengths and Weaknesses: Strong Points:  Novel Approach: Introduces a novel pruning method that considers the interdependence of operations in CNNs which can lead to more effective pruning.  Experimental Validation: Conducted experiments on various datasets and architectures to demonstrate the efficacy of LEAN pruning.  Performance Improvement: Shows that LEAN pruning can achieve significant reduction in the number of convolutional filters without compromising accuracy. Weak Points:  Unclear Evaluation Metrics: The paper lacks a detailed discussion on evaluation metrics used in experiments making it hard to assess the significance of the reported results.  Comparison with Baselines: While the paper compares LEAN with other pruning methods a more extensive comparison with additional stateoftheart techniques could bolster the evaluation.  Theoretical Basis: The paper lacks a substantial theoretical discussion on the rationale behind choosing specific operator norms and graph algorithms for LEAN pruning. Questions and Feedback to the Author: 1. Evaluation Metrics: Can you provide more insights into the choice of evaluation metrics in experiments and how the reported results demonstrate the superiority of LEAN pruning 2. Theoretical Rationale: Could you elaborate further on the theoretical underpinnings behind selecting specific operator norms and graph algorithms for LEAN pruning to enhance the understanding of the method 3. Generalization: How generalizable is LEAN pruning across different types of CNN architectures and datasets Have you considered investigating the performance of LEAN on more diverse tasks 3) Review Summary: The paper presents an innovative approach LEAN pruning aimed at addressing the issue of interdependence between consecutive operations in CNNs for effective model pruning. The experimental results demonstrate that LEAN pruning can achieve substantial reduction in the number of convolutional filters while maintaining good accuracy. The paper would benefit from a clearer explanation of evaluation metrics a more detailed comparison with other stateoftheart methods and a deeper theoretical discussion on the methods foundation. Overall the paper makes a valuable contribution to the field of neural network pruning providing a new perspective on improving computational efficiency in CNNs. Recommendation: With its novel approach and promising experimental results I recommend accepting this paper for publication after addressing the mentioned points for clarification and improvement.", "nioAdKCEdXB": " 1) Summary of the Paper: The paper introduces a novel computational framework for likelihood training of SB models based on ForwardBackward Stochastic Differential Equations Theory. This framework aims to connect the optimization principle of Schr\u00f6dinger Bridge (SB) models with modern generative training used in Scorebased Generative Models (SGM). The authors show that this new approach results in a training algorithm called SBFBSDE that achieves comparable results in generating realistic images on datasets like MNIST CelebA and CIFAR10.  2) Strong Points: a) Theoretical Contribution:  The paper presents a significant theoretical contribution by connecting SB models with training techniques from SGM providing a new perspective on generative modeling.  The use of ForwardBackward Stochastic Differential Equations Theory to bridge the gap between SB and SGM is innovative and promising. b) Empirical Validation:  The empirical validation on various datasets demonstrates the effectiveness of the proposed SBFBSDE method in generating highfidelity images with comparable performance to existing stateoftheart models. c) Experimental Results:  The results in terms of negative loglikelihood (NLL) and Fr\u00e9chet Inception Distance (FID) on CIFAR10 showcase the practical efficacy of the proposed SBFBSDE method.  The ablation analysis showing the positive impact of incorporating the Langevin corrector on sample quality provides valuable insights. d) Clarity and Structure:  The paper is wellstructured and the methodology is explained in a clear and concise manner making it accessible to readers with relevant background knowledge.  The inclusion of detailed explanations figures and empirical results enhances the understanding of the proposed approach and its implications.  Weak Points: a) Limited Comparison:  The paper lacks a comprehensive comparison with a wider range of existing stateoftheart generative models particularly in terms of various evaluation metrics on other benchmark datasets.  The quantitative comparisons on MNIST and CelebA datasets are not presented due to the absence of widely reported scores which could limit the generalizability of the findings. b) Scalability Concerns:  It would be beneficial to discuss the scalability of the proposed approach especially in terms of computational complexity and memory requirements particularly when handling highdimensional image data.  Questions and Feedback to the Author: 1. How does the proposed SBFBSDE method compare in terms of computation time and memory usage compared to existing stateoftheart generative models 2. Can you provide more insights into the scalability of the proposed method especially when handling larger datasets beyond MNIST CelebA and CIFAR10 3. Have you considered testing the SBFBSDE method on other benchmarks to assess its generalizability across a wider range of datasets and tasks  3) Review Summary: The paper introduces a novel computational framework SBFBSDE for likelihood training of SB models establishing a connection between SB and modern generative training techniques. The proposed method demonstrates promising results in generating realistic images on various datasets. The theoretical foundations empirical validation and clear presentation make the paper a valuable contribution to the field of generative modeling. However to strengthen the findings further scalability analysis broader benchmarking and discussions on practical implications could enhance the impact and applicability of the proposed approach. Overall based on the strong theoretical basis empirical validation and clarity of presentation I recommend this paper for acceptance with minor revisions to address the identified weak points.", "ySQH0oDyp7": " Summary: The paper introduces a novel approach called QDROP for posttraining quantization (PTQ) to improve the efficiency of neural networks without requiring extensive retraining. The study explores the incorporation of activation quantization into PTQ reconstruction and demonstrates its benefits for achieving high accuracy with extremely lowbit settings. Through empirical and theoretical analyses QDROP is proposed as a simple yet effective method that randomly drops the quantization of activations during PTQ leading to improved model flatness and accuracy. Extensive experiments on various tasks such as image classification object detection text classification and question answering showcase the superiority of QDROP over existing PTQ methods.  Strong Points: 1. Novel Approach: The paper introduces a novel method QDROP which pushes the limits of PTQ to extremely lowbit settings and achieves stateoftheart results. 2. Theoretical Framework: The study establishes a solid theoretical framework to understand the interaction between activation quantization and weight tuning highlighting the importance of model flatness for accuracy. 3. Experimental Validation: Extensive experiments on various tasks demonstrate the effectiveness of QDROP in improving accuracy for different neural network architectures and tasks. 4. Comprehensive Evaluation: The paper conducts a thorough evaluation including ablation studies comparison with existing methods robustness analysis and discussion on the impact of calibration data size.  Weak Points: 1. Clarity: The paper is dense with technical details which may be challenging for readers not familiar with the field to grasp. 2. Experimental Setup: The paper could provide more details on experimental setup and hyperparameters used for better reproducibility.  Questions for the Authors: 1. Can you provide more insights into how the dropping probability for activation quantization was selected and what factors influenced this choice 2. Have you considered the implications of QDROP on the computational overhead during inference particularly for edge devices with limited resources  Feedback: The paper presents a significant contribution to the field of PTQ by proposing a novel method QDROP that demonstrates superior results in extremely lowbit settings. The theoretical framework thorough evaluation and indepth analysis provide strong support for the effectiveness of QDROP. However addressing the clarity of the technical details and providing more insights into experimental setups would enhance the papers accessibility and reproducibility. Overall based on the strong experimental results and theoretical foundations I recommend accepting this paper for publication.", "i--G7mhB19P": "PeerReview: 1) Summarize the paper: The paper investigates the behavior of natural gradient descent (NGD) in deep linear models through theoretical analyses and experimental validations. The key focus is to understand the inductive biases of NGD in comparison to Euclidean gradient descent (EGD) under various parametrizations. The experiments delve into scenarios like separable classification and deep matrix factorization to demonstrate specific behaviors observed in NGD. 2) Strong and Weak Points: Strong Points:  The paper provides a thorough theoretical foundation and clear explanations of the concepts being investigated.  The methodology is wellstructured allowing for a systematic analysis of parametrization invariance and its implications on optimization trajectories.  The experiments are meticulously designed showcasing the practical implications of the theoretical findings.  The results are wellsupported and contribute significant insights to the understanding of NGD in deep linear models. Weak Points:  The paper could benefit from more explicit connections between the theoretical analyses and the experimental results to provide a cohesive narrative.  While the counterexample and proofs provided add rigor they may be challenging for readers less familiar with the subject matter.  Further elaboration on the practical implications of the findings could enhance the applicability of the research to realworld scenarios. Questions and Feedback to the Author: 1. Could you elaborate more on how the theoretical implications drawn from the analyses align with the observed results in the experiments 2. It would be beneficial to discuss the broader implications of the work beyond the scope of deep linear models. How could the insights gained from this research impact other areas of machine learning and optimization 3. Considering the complexity of the theoretical derivations have you explored any simplifications or alternative explanations to make the findings more accessible to a wider audience of researchers and practitioners 3) Review Summary: The paper presents a comprehensive investigation into the behavior of natural gradient descent in deep linear models. By combining rigorous theoretical analyses with wellexecuted experiments the research sheds light on the inductive biases and parametrization invariance of NGD. While the paper excels in its theoretical depth and experimental validation further efforts to bridge the gap between theory and practice as well as to enhance the accessibility of the findings could strengthen its impact and relevance in the machine learning community. Recommendation: Highquality research with minor revisions suggested for improved cohesion and clarity.", "srtIXtySfT4": " Paper Summary The paper introduces Neural Parameter Allocation Search (NPAS) to address the issue of memory constraints in training neural networks. It proposes Shapeshifter Networks (SSNs) that automatically learn parameter sharing strategies to fit any parameter budget. The approach covers lowbudget and highbudget regimes allowing for compact networks or increased capacity without raising inference FLOPs. The paper demonstrates the effectiveness of NPAS and SSNs across various tasks and networks showcasing improvements in performance and memory efficiency.  Strengths 1. Innovative Approach: NPAS tackles the critical issue of memory constraints in neural network training using SSNs enabling efficient parameter sharing without architectural modifications. 2. Comprehensive Evaluation: The paper thoroughly evaluates the proposed approach across diverse tasks and networks showcasing its effectiveness in reducing memory requirements and enhancing performance. 3. Broad Applicability: The proposed method is demonstrated to be applicable to various tasks such as image classification imagesentence retrieval phrase grounding and question answering showcasing its versatility. 4. Clear Presentation: The paper provides detailed explanations illustrations and comparisons to related work enhancing the clarity of the proposed approach.  Weaknesses 1. Complexity: The method involves various components such as parameter mapping weight generation and layer grouping which may introduce complexity and require careful attention to implementation details. 2. Training Overhead: While the approach is shown to be effective the preliminary training step for learning layer to parameter mappings may result in an increased training time which could be a drawback in some scenarios. 3. Limited Comparison: More extensive comparisons with other stateoftheart methods could further strengthen the evaluation and provide a deeper understanding of the proposed approachs performance in relation to existing techniques.  Questions and Feedback to the Author 1. Scalability: How does the proposed approach scale with larger and more complex neural network architectures Have you considered the impact of scaling on training time and memory requirements 2. Realworld Deployment: Have you explored the practical feasibility and computational efficiency of implementing NPAS and SSNs in production settings with larger datasets and computational resources 3. Robustness: How robust is the proposed method to variations in hyperparameters and different types of tasks beyond the ones tested in the paper  Review Summary The paper presents a novel and effective solution NPAS with SSNs to address memory constraints in training neural networks showcasing promising results across multiple tasks. While the approach is innovative comprehensive and exhibits strong performance improvements there are complexities and potential scalability considerations that need further exploration. Overall based on the strengths of the proposed approach and the promising results I recommend acceptance after addressing the aforementioned weaknesses and questions through further analysis and discussion.", "qw674L9PfQE": "1) Summary: The paper introduces a new contrastive learning method called \"Contrastive Leave One Out Boost\" (CLOOB) which combines the InfoLOOB objective with modern Hopfield networks. It addresses limitations of the InfoNCE lower bound and the InfoLOOB upper bound for mutual information estimation in contrastive learning. CLOOB leverages retrieved embeddings from modern Hopfield networks to improve stability and reinforce covariance structures in data representations. Experimental results on Conceptual Captions and YFCC datasets demonstrate that CLOOB consistently outperforms the CLIP model in zeroshot transfer learning tasks across various architectures and datasets. 2) Strengths and Weaknesses: Strong Points:  Novel Contribution: Introducing CLOOB as a new contrastive learning method that combines InfoLOOB with modern Hopfield networks to enhance mutual information estimation and zeroshot transfer learning performance.  Theoretical Justification: The paper provides theoretical properties of the InfoLOOB bound and objective explaining the advantages over InfoNCE in capturing high mutual information structures.  Experimental Results: Comprehensive experiments on Conceptual Captions and YFCC datasets show that CLOOB consistently outperforms CLIP in zeroshot transfer learning tasks across various datasets and architectures.  Ablation Studies: Conducting ablation studies to analyze the impact of different components (InfoLOOB objective modern Hopfield networks) on the performance improvement. Weak Points:  Complexity: The paper involves technical and mathematical details that might be challenging for readers unfamiliar with concepts such as mutual information and modern Hopfield networks.  Clarity of Experiments: Some experimental details such as hyperparameter selection and learning schedule variations could be explained more clearly to help readers understand the methodology better.  Visualization: Visual aids or examples could enhance the understanding of the proposed approach especially for readers looking for more intuitive explanations. Questions and Feedback to the author: 1. Clarifications: Could you provide more detailed explanations or examples to help readers understand the concepts of modern Hopfield networks and how they are integrated into the CLOOB method 2. Scalability: Have you evaluated the scalability of the CLOOB method with larger datasets or more complex architectures to assess its performance in highdimensional spaces or with more diverse data types 3. Generalization: How does the proposed CLOOB method perform in scenarios with limited training data or in domains outside of imagetext applications 3) Review Summary: The paper presents a novel contrastive learning method CLOOB that combines the InfoLOOB objective with modern Hopfield networks to address limitations of existing mutual information estimation methods in contrastive learning. The theoretical justifications and comprehensive experiments on Conceptual Captions and YFCC datasets demonstrate that CLOOB outperforms the CLIP model in zeroshot transfer learning tasks across different architectures and datasets. The ablation studies provide further insights into the effectiveness of the individual components in improving performance. Overall the paper makes a significant contribution to enhancing mutual information estimation and zeroshot transfer learning in selfsupervised learning tasks. Thus based on the innovative approach theoretical foundations and strong experimental results I recommend the publication of this paper after addressing minor issues related to clarity and presentation.", "w-CPUXXrAj": " Summary of the Paper: The paper investigates the limitations of multimodal variational autoencoders (VAEs) in generating weaklysupervised data efficiently. It discusses the gap in generative quality between multimodal and unimodal VAEs and explores the fundamental limitation that affects mixturebased multimodal VAEs due to modality subsampling. The research uncovers that the subsampling of modalities enforces an upper bound on the multimodal ELBO restricting the models from achieving tight approximations of the joint distribution. Empirical evidence through synthetic and real data showcases the generative quality gap and the tradeoffs between different multimodal VAE variants.  Strengths of the Paper: 1. Theoretical Analysis: The paper provides a thorough theoretical analysis of the limitations of mixturebased multimodal VAEs backed by formal proofs and definitions. 2. Empirical Validation: Empirical experiments on different datasets validate the theoretical findings demonstrating the generative quality gap between unimodal and multimodal VAEs. 3. Relevance: The study addresses an important aspect of weaklysupervised data modeling and implications for realworld applications contributing to the understanding of the limitations of existing approaches for handling multimodal data.  Weak Points of the Paper: 1. Complexity: The content can be quite technical making it challenging for readers without a deep understanding of VAEs and information theory to grasp the finer details. 2. Clarity and Accessibility: The paper could benefit from simplifying the language and providing more intuitive explanations of complex concepts for easier comprehension by a wider audience. 3. Limited Comparison: While the paper discusses the limitations of existing multimodal VAEs it could benefit from a more extensive comparison with alternative generative models to provide a broader perspective.  Questions for the Authors: 1. How do you foresee addressing the identified limitations of mixturebased multimodal VAEs in future research 2. Have you explored potential alternative approaches or modifications to VAEs that could overcome the constraints imposed by modality subsampling 3. How do you plan to enhance the generative coherence of multimodal VAEs on more complex datasets given the challenges identified in your study  Feedback for the Authors: 1. Consider incorporating more illustrative examples or figures to aid in understanding the theoretical concepts presented. 2. Simplify the language where possible to make the content more accessible to a broader audience. 3. Expand on potential future research directions or practical applications that could address the limitations highlighted in the study.  Review Summary: The paper delves into the limitations of mixturebased multimodal VAEs in generative modeling of weaklysupervised data highlighting the impact of modality subsampling on generative quality. The theoretical analyses and empirical experiments provide valuable insights into the challenges faced by existing multimodal VAE approaches. While the study is comprehensive and informative clarity enhancements and broader comparisons with alternative models could further strengthen its impact. Addressing the identified limitations and exploring novel strategies to improve generative coherence on complex datasets can pave the way for more effective multimodal generative models in the future.", "zf_Ll3HZWgy": " Review Draft:  Summary: The paper investigates the benefits of using CLIP as a visual encoder for VisionandLanguage (VL) models. It explores two scenarios: 1) direct taskspecific finetuning using CLIP as the visual encoder (CLIPViL) and 2) integrating CLIP with VL pretraining (CLIPViLp). The experiments cover tasks like Visual Question Answering Image Captioning and VisionandLanguage Navigation showing significant improvements over existing visual encoders and setting new stateoftheart results on various VL tasks.  Strong Points: 1. Innovative Approach: Using CLIP as a visual encoder presents a novel approach showcasing significant performance improvement in various VL tasks compared to traditional visual encoders. 2. Empirical Evidence: The paper provides comprehensive experimental results demonstrating the superiority of CLIP in both direct taskspecific finetuning and VL pretraining scenarios offering detailed analysis and comparisons. 3. StateoftheArt Results: The paper establishes new stateoftheart results on tasks like Visual Question Answering Image Captioning and VisionandLanguage Navigation indicating the effectiveness of CLIP as a visual encoder.  Weak Points: 1. Complexity of Experiments: The paper may benefit from more detailed explanations on the experimental setups hyperparameters and model configurations used in the study to aid reproducibility and understanding. 2. ZeroShot Performance: While the paper discusses the limitations of CLIP in zeroshot setups for certain tasks it would be valuable to further explore strategies to enhance CLIPs performance in complex reasoning tasks. 3. Visualization Interpretation: The qualitative analysis regarding the visualization and comparison between CLIPResNet and CLIPViT could be expanded to provide more insights into the differences observed.  Question to the Author: 1. Reproducibility: Could you provide more details on the code availability and suggest guidelines for researchers looking to reproduce your results 2. Future Directions: Considering the observed limitations in CLIPs performance what future research directions do you recommend to enhance its capabilities in more complex VL tasks 3. Comparative Analysis: Can you elaborate on the computational efficiency and scalability aspects of using CLIP as a visual encoder compared to traditional methods  Review Summary: The paper presents a thoughtprovoking study on leveraging CLIP as a visual encoder for VL tasks showcasing remarkable performance improvements over traditional visual encoders. The empirical evidence stateoftheart results and insightful analyses support the effectiveness of CLIP across diverse VL tasks. While the study demonstrates the potential of CLIP addressing experiment complexity enhancing zeroshot performance and providing further visualizationbased interpretations can enrich the paper. Overall the innovative approach and compelling results make this paper a valuable contribution to the field of VisionandLanguage research. I recommend acceptance with minor revisions to address the weak points mentioned above and enhance the clarity and reproducibility of the study.", "gKLAAfiytI": " Summary of the Paper: The paper introduces the Equivariant SelfSupervised Learning (ESSL) framework as a generalization of stateoftheart selfsupervised learning methods. Unlike traditional approaches that solely focus on invariance ESSL encourages equivariance to specific transformations while maintaining invariance to others. The paper demonstrates the effectiveness of ESSL on improving the semantic quality of representations in various computer vision benchmarks and regression tasks in photonics science.  Strengths: 1. Innovative Approach: Introducing the concept of equivariance in selfsupervised learning is a novel and innovative approach that extends beyond traditional invariancebased methods. 2. Empirical Validation: The paper provides detailed experimental results on popular datasets like CIFAR10 ImageNet and regression problems in photonics science demonstrating the superiority of ESSL over traditional SSL methods. 3. Comprehensive Explanations: The paper provides detailed explanations of the proposed framework experimental setups and results making it easy for readers to understand the concepts and findings. 4. Reproducibility: Mention of opensourcing the code and datasets enhances the reproducibility of the experiments allowing for further research and validation by the scientific community.  Weaknesses: 1. Complexity: The paper delves into intricate mathematical concepts and transformations which might be challenging for readers not wellversed in the subject matter. 2. Limited Computational Resources: Limited computational resources are acknowledged which could have potentially affected the scope and scale of the experiments conducted. 3. Lack of RealWorld Use Cases: While the paper demonstrates efficacy in computer vision tasks more realworld use cases outside of traditional computer vision applications could enhance the broader applicability of ESSL.  Questions: 1. How does the proposed ESSL framework compare in terms of computational efficiency and scalability in comparison to traditional SSL methods 2. Have you considered the potential impact of data bias or imbalance on the efficacy of ESSL especially in realworld applications with diverse datasets 3. Are there plans to explore applications of ESSL in other domains beyond computer vision and photonics science such as natural language processing or reinforcement learning  Feedback to the Author:  The paper provides valuable insights into developing more robust and semantically meaningful representations using the ESSL framework.  Clarifying the practical implications and potential limitations of ESSL in realworld scenarios would further strengthen the applicability of the proposed method.  Providing more detailed discussions on addressing data biases and generalizability to other domains could enhance the impact of the research findings.  Review Summary: The paper presents a novel Equivariant SelfSupervised Learning (ESSL) framework that extends traditional SSL methods to encourage equivariance to specific transformations. While the paper demonstrates the superiority of ESSL in improving semantic representations in computer vision and science applications addressing complexity scalability and realworld use cases could further enhance the impact of the research. Overall the innovative approach and comprehensive experimental validations make the paper a valuable contribution to the field of machine learning and selfsupervised learning.", "swrMQttr6wN": "Peer Review Draft  Summary: The paper introduces a novel framework called Learning to Map (L2M) for object goal navigation in unseen environments. The key idea is to actively learn to generate semantic maps outside the field of view of the agent and leverage uncertainty over semantic classes in unobserved areas to make decisions on longterm goals. The proposed approach outperforms competitive baselines in the objectgoal navigation task on the Matterport3D dataset. Strengths: 1. Innovative Approach: The paper introduces a novel framework that actively learns semantic priors in indoor scenes providing a fresh perspective on goaloriented navigation in unseen environments. 2. Active Learning Strategy: The incorporation of an active training strategy to finetune the semantic map predictor enhances the models ability to focus on informative data leading to improved performance. 3. Effective Goal Selection: The utilization of uncertaintybased objectives for goal selection showcases a thoughtful approach to balancing exploration and exploitation ultimately resulting in superior navigation performance. 4. Thorough Experimental Evaluation: The paper presents a comprehensive set of experiments comparing the proposed method with baselines shedding light on the efficacy and advantages of the L2M framework. 5. Detailed Explanations and Methodology: The paper provides detailed descriptions of the proposed framework active training strategies goal selection policies and an insightful analysis of results allowing for a clear understanding of the methodology. Weaknesses: 1. Limited Generalization: The method is trained on specific home structures in North America and Europe potentially leading to biases in different regions or in varied environments limiting its generalizability. 2. Loss of 3D Spatial Context: The approach ground projects 3D information to a 2D map representation leading to potential difficulty in capturing complex 3D relationships such as objects stacked on top of each other. Questions and Feedback to the Author: 1. Question: How do you plan to address the limitation of bias in training data from specific regions when deploying the model in diverse environments to ensure unbiased performance 2. Feedback: It would be valuable to explore methods to incorporate or reconstruct 3D spatial context in the semantic mapping to enhance the systems understanding of complex object relationships and improve performance on certain objects like cushions. Review Summary: Overall the paper presents a novel and effective framework for object goal navigation in unseen environments. The active learning strategy and uncertaintydriven goal selection demonstrate significant improvements over competitive baselines. The experimental evaluation is thorough and provides strong evidence of the proposed methods efficacy. However addressing limitations in generalizability and 3D spatial context modeling would further enhance the impact and applicability of the framework. This paper is a valuable contribution to the field of robotic navigation and merits publication with minor revisions to address the identified weaknesses and questions raised.  This review provides a structured evaluation of the strengths weaknesses questions and feedback for the author ensuring a comprehensive assessment of the paper. Feel free to incorporate this draft or make modifications as needed for submission.", "pMQwKL1yctf": " 1) Summary of the Paper: The paper introduces Time Control (TC) a novel language model that implicitly plans via a latent stochastic process to address issues related to incoherent and meandering long text generation by modern language models. TC learns a representation that maps text dynamics to stochastic processes allowing it to generate text that is consistent with a latent plan. The paper compares TC to domainspecific methods and GPT2 finetuning across various text domains showing improvements in text infilling discourse coherence and long text generation settings.  2) Evaluation of the Paper:  Strong Points: 1. Innovative Approach: Introducing a language model that incorporates a latent stochastic process for text generation is a novel and impactful contribution. 2. Performance Improvement: The results demonstrate that TC outperforms baseline methods in tasks such as text infilling discourse coherence and forced long text generation showing the effectiveness of the proposed model. 3. Comprehensive Experiments: The paper conducts a thorough evaluation with various metrics including human evaluations to assess the efficacy of TC across different text generation tasks and domains.  Weak Points: 1. Complexity: The paper introduces various concepts including Brownian bridges contrastive objectives and latent dynamics which may be challenging for readers unfamiliar with these topics. The paper could provide clearer explanations or more intuitive examples to aid understanding. 2. Missing Comparisons: While the paper compares TC to domainspecific methods and GPT2 finetuning additional comparisons with other stateoftheart language models or text generation approaches could provide a more comprehensive evaluation of TCs performance.  Questions and Feedback to the Author: 1. Clarity on Methodology: Can the paper provide more straightforward examples or visual aids to clarify the concepts of Brownian bridges latent dynamics and contrastive objectives for readers with varying levels of expertise in the field 2. Generalization: How do you foresee the scalability and generalizability of Time Control beyond text generation tasks such as for multilingual or multimodal applications 3. Robustness: Have you considered conducting experiments to evaluate the robustness of TC to variations in input data quality domain complexity or noisy text inputs  3) Review Summary: The paper presents a promising approach with Time Control a language model that incorporates latent stochastic processes to improve the coherence and structure of long text generation. The comprehensive evaluation and positive results suggest that TC outperforms existing methods in various text generation tasks. Although there are complexity and clarity concerns the significant performance gains and innovative methodology support the papers recommendation for acceptance after addressing the minor weaknesses and providing additional clarifications on key concepts.", "xFOyMwWPkz": "Summary: The paper presents a novel method for quantitatively evaluating the status of individual network units in convolutional neural networks (CNNs) using algebraic topological tools. The proposed method called feature entropy measures the degree of chaos of global spatial patterns hidden in units for a specific category. The study demonstrates that feature entropy can provide an accurate indication of unit status in different network models and scenarios like weightrescaling operations. The paper also shows through experiments that feature entropy decreases as the layer goes deeper shares a similar trend with loss during training and can discriminate networks with varying generalization abilities based on the effectiveness of feature representations. Strengths: 1. Innovative Approach: The paper introduces a novel method using algebraic topological tools to evaluate unit status in CNNs contributing to the field of deep learning research. 2. Comprehensive Analysis: The study provides detailed and thorough explanations of the proposed method including the underlying concepts calculations and experimental results. 3. Experimental Validation: The experiments conducted support the effectiveness and reliability of the proposed feature entropy method in indicating unit status and discriminating between networks with different generalization abilities. 4. Robustness Tests: The paper explores the robustness of the proposed method by conducting rescaling and randomness tests demonstrating its stability and effectiveness in various scenarios. Weaknesses: 1. Complexity: The method utilizing algebraic topological tools may be challenging for readers unfamiliar with the field potentially limiting the accessibility of the paper. 2. Generalization: While the experiments show promising results the generalization of the proposed method across different CNN architectures and datasets needs to be further examined. 3. Visualization: The paper lacks visual aids such as figures or diagrams to help readers better understand the concepts and processes described in the text. 4. Clarity: The paper contains technical terms and concepts that may require a high level of expertise in mathematics and deep learning which could make it less accessible to a broader audience. Question to the Author: 1. Is the proposed feature entropy method applicable to other deep learning architectures beyond CNNs Have you considered applying it to different types of neural networks 2. How might the computational complexity of the method scale with larger datasets or more complex network structures Have you conducted any scalability experiments 3. Could the feature entropy method be used for realtime monitoring or adjustment of neural networks during training or inference phases Feedback to the Author: 1. Consider providing more visual aids such as diagrams or graphs to enhance the readability and understanding of the proposed method and results. 2. Address the potential limitations and challenges of implementing the feature entropy method in practical applications to facilitate a broader adoption of the technique. 3. Discuss the implications of the proposed method in the context of realworld applications and potential future research directions to further validate its effectiveness and applicability.", "u6s8dSporO8": "Summary: The paper introduces a novel method called Group Equivariant Neural Posterior Estimation (GNPE) to incorporate exact and approximate equivariances in simulationbased inference for blackbox scientific models. GNPE aims to standardize the pose of data under joint transformations of parameters and data. The study applies GNPE to the challenging task of gravitationalwave (GW) parameter inference of binary black hole systems achieving stateoftheart accuracy while significantly reducing inference times compared to traditional methods like MCMC. By iteratively estimating the pose of the data GNPE simplifies the inference process by transforming the data based on pose estimates and sampling the joint posterior over parameters and pose proxies using Gibbs sampling. Strengths: 1. Innovative Approach: Introducing GNPE as a method to incorporate equivariances into simulationbased inference is a novel and valuable contribution to the field. 2. RealWorld Application: The application of GNPE to GW parameter inference demonstrates its effectiveness in solving complex scientific problems and achieving significant improvements in accuracy and efficiency. 3. Comprehensive Exposition: The paper provides detailed explanations of the methodology algorithms and application scenarios allowing for a thorough understanding of the proposed approach. 4. Comparative Analysis: The comparison with baseline methods such as NPE and chained NPE provides a clear demonstration of the superior performance of GNPE in achieving accurate posterior estimation. Weaknesses: 1. Complexity: The paper introduces a sophisticated method that may be challenging for readers unfamiliar with the concepts of equivariance and simulationbased inference. 2. Technical Jargon: The extensive use of technical terms and equations may make it difficult for readers from diverse backgrounds to grasp the content fully. 3. Limited Discussion on Limitations: While the benefits and applications of GNPE are wellpresented the paper lacks a detailed discussion on potential limitations or challenges of the proposed method in different scenarios. Questions to the Authors: 1. Can you elaborate on the computational resources required for training and implementing GNPE particularly in the GW parameter inference application 2. How generalizable is the GNPE method to other scientific domains with different types of equivariances and complex data models 3. Have you considered the robustness of GNPE to potential noise and uncertainties in realworld data especially in scenarios where equivariances may not hold perfectly 4. How scalable is the GNPE method in terms of handling larger datasets and more complex models beyond the examples presented in the paper Feedback to the Authors: 1. Provide a concise summary of the key takeaways and contributions of the paper at the beginning to help readers quickly understand the papers significance. 2. Consider providing visual aids or simplified examples to make the concepts more accessible to readers with varying levels of expertise. 3. Strengthen the discussion section by addressing potential limitations or tradeoffs of the GNPE method to provide a more balanced view of its applicability in different contexts. Review Summary: The paper presents a novel method GNPE for integrating equivariances into simulationbased inference with a particular focus on gravitationalwave parameter inference. The approach appears to offer significant advantages in accuracy and efficiency over traditional methods. While the paper is wellstructured and provides thorough explanations there is room for improvement in terms of making the content more accessible to a broader audience and discussing potential limitations of the proposed method. Overall the innovative approach and promising results make the paper a valuable contribution to the field of inference in scientific modeling.", "vdbidlOkeF0": "1) Summary of the Paper: The paper introduces a novel method called the Scaled Density Ratio Estimator (SDRE) for accurately estimating density ratios in machine learning tasks. The authors address the challenges of existing methods such as binary classifierbased density ratio estimators in cases where the densities are wellseparated leading to poor performance especially when both First Order Discrepancy (FOD) and Higher Order Discrepancy (HOD) are present. They propose SDRE which leverages the scaledBregman divergence to scale densities by another measure and estimate the ratio using a multiclass logistic regression model. The paper presents theoretical justifications empirical evaluations on various datasets and demonstrates the applicability of SDRE in improving mutual information estimation and representation learning tasks. 2) Strong Points:  The paper addresses a significant problem in density ratio estimation and introduces a novel method in SDRE.  The theoretical justification for SDRE and comparisons with existing methods are wellexplained.  Empirical evaluations on synthetic and real datasets demonstrate the superiority of SDRE especially in scenarios with both FOD and HOD discrepancies.  The application of SDRE to improve mutual information estimation and representation learning tasks shows promising results. Weak Points:  The paper could provide more detailed explanations of the construction schemes for scaling measures to enhance clarity.  While the comparison with existing methods is thorough more indepth analysis on the limitations of SDRE and areas for future improvement could be beneficial.  Clearer discussions on practical implications computational complexity and potential tradeoffs of using SDRE in realworld applications may enhance the practicality of the proposed method. Questions and Feedback to the Author: 1. Could you provide more insights into the computational efficiency and scalability of SDRE in comparison to existing methods especially in handling large datasets or highdimensional scenarios 2. How does SDRE perform under scenarios with noisy or incomplete data and have you considered robustness analysis in such cases 3. Considering the impact of FOD and HOD discrepancies on the performance of SDRE are there any known scenarios where the proposed method might still face challenges or limitations 3) Review Summary: The paper thoroughly addresses the challenges of density ratio estimation under both FOD and HOD discrepancies and introduces a novel framework SDRE to overcome these issues. With theoretical justifications comprehensive empirical evaluations and successful applications in mutual information estimation and representation learning tasks SDRE outperforms existing methods. The unique approach of leveraging scaledBregman divergence and multiclass logistic regression sets SDRE apart and proves its effectiveness. Considering the strengths in theoretical grounding and empirical results I recommend the acceptance of this paper for its contribution to advancing density ratio estimation techniques in machine learning. The clarity and significance of the proposed method along with its practical applications make it a valuable addition to the field.", "wQ7RCayXUSl": " 1) Summary of the Paper: The paper introduces the MSG algorithm for offline reinforcement learning (RL) with the aim to relax the overly pessimistic assumption of support constraints common in existing RL methods. MSG employs an ensemble of independently updated Qfunctions to estimate uncertainties and optimize the policy with respect to a lowerconfidence bound (LCB). The theoretical analysis highlights the importance of independence in Qfunctions and demonstrates the quality of uncertainties obtained through ensembling. Empirical evaluations showcase that MSG surpasses stateoftheart methods particularly excelling in challenging domains like antmaze. Additionally the paper investigates the effectiveness of efficient ensembling approaches and ensemble ablations.  2) Strong and Weak Points Questions and Feedback:  Strong Points:  Theoretical Foundation: The papers theoretical analysis on infinitewidth networks and ensembling is comprehensive and wellsupported providing valuable insights into the behavior of uncertainties in RL.  Empirical Results: The empirical comparisons in benchmark domains and the demonstration of MSGs superior performance especially in challenging tasks like antmaze are strong points that demonstrate the effectiveness of the proposed approach.  Experimental Rigor: The detailed experiments hyperparameter search procedures and comparisons with stateoftheart methods provide a robust evaluation framework for assessing the performance of MSG.  Practical Advice: The inclusion of practical hyperparameter tuning advice adds value to readers aiming to apply the algorithm in new domains.  Weak Points:  Further Explanations: While the paper offers a detailed theoretical and empirical analysis providing further explanations or visual aids to simplify complex concepts for readers less familiar with the domain could enhance understanding.  Comparison with Baselines: More detailed comparisons with existing methods and additional ablation studies could strengthen the evaluation of MSG.  Questions and Feedback to the Author:  Could you provide more insights into how the proposed method (MSG) compares to other stateoftheart algorithms in terms of sample efficiency and computational complexity  Considering the impressive results in challenging domains like antmaze have you explored the generalizability of MSG to other complex environments or tasks beyond the benchmarks considered in this study  Could you elaborate on any limitations or potential drawbacks of the MSG algorithm particularly in practical applications or scenarios with limited data availability  It would be beneficial to include discussions on the scalability of MSG to largerscale RL problems or realworld applications. Are there any scalability challenges or considerations that need to be addressed  3) Review Summary: The paper presents a compelling approach MSG for offline RL that leverages ensemble Qfunctions to estimate uncertainties and optimize policies. The strong theoretical foundation empirical evidence showcasing superior performance and practical implications make the MSG algorithm a valuable contribution to the field of RL. Further insights into comparative evaluations scalability limitations and realworld applicability could enhance the papers comprehensiveness. Overall based on the rigorous analysis and impressive results I recommend the paper for publication pending minor revisions to address the weak points identified.", "fpU10jwpPvw": " Summary of the Paper: The paper introduces Folded Hamiltonian Monte Carlo (FHMC) within a Bayesian framework for unsupervised and semisupervised Generative Adversarial Networks (GANs). FHMC is proposed as an alternative to Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) to sample weights of generators in order to mitigate mode collapse and efficiently produce data samples with adequate entropy especially in highdimensional and highly correlated data settings. The paper demonstrates the effectiveness of FHMC through mathematical analysis experimental results on synthetic and natural image datasets and comparisons with stateoftheart methods.  Strengths of the Paper: 1. Innovative Approach: FHMC introduces a novel sampling strategy within the Bayesian GAN framework to address challenges such as mode collapse and efficiency in highdimensional data exploration. 2. Theoretical Rigor: The paper includes mathematical proofs and theoretical analysis to support the functionality and validity of FHMC in sampling the target distribution. 3. Experimental Validation: Extensive experiments on both synthetic multimodal datasets and natural image benchmarks show that FHMC outperforms existing methods in terms of test error rates runtime efficiency Inception Score (IS) and Frechet Inception Distance scores (FID). 4. Clear Presentation: The paper is wellstructured with explanations of the problem formulation background methodology theoretical analysis and experimental results providing a comprehensive overview of the proposed model.  Weaknesses of the Paper: 1. Hyperparameters Sensitivity: The paper acknowledges that hyperparameters of FHMC can impact its performance suggesting a potential area for improvement. 2. Comparative Analysis: While the paper compares FHMC with existing methods further detailed discussions on the strengths and limitations of each approach could enhance the analysis. 3. Generalizability: The papers focus on specific datasets and settings may limit the generalizability of the proposed FHMC approach to diverse applications and scenarios.  Questions and Feedback to the Author: 1. Scalability: How does the scalability of FHMC model with increasing dimensions compare to traditional SGHMC or other sampling methods 2. Computational Cost: Can you provide insights into the computational cost involved in implementing FHMC compared to other Bayesian GAN frameworks 3. Extension: Have you considered potential extensions or modifications to FHMC to address hyperparameter sensitivity and improve its robustness across various datasets 4. RealWorld Applications: How do you envision the practical application and adoption of FHMC in realworld scenarios beyond image generation tasks  Review Summary: The paper presents a wellstructured and innovative approach with FHMC within a Bayesian GAN framework to address challenges in sampling highdimensional multimodal data for generative tasks. The theoretical analysis experimental results and comparisons with existing methods substantiate the efficacy of the proposed FHMC model. Despite some limitations regarding hyperparameters and generalizability the papers contributions rigorous analysis and superior performance metrics support a positive recommendation for publication. Further exploration into scalability computational efficiency and practical applications could enhance the impact of the proposed model in diverse domains.", "gdegUuC_fxR": " Summary of the Paper: The paper introduces a novel acceleratedgradientbased Markov Chain Monte Carlo (MCMC) method named HFHR inspired by Nesterovs Accelerated Gradient algorithm for optimization. HFHR is designed to enhance sampling efficiency by incorporating acceleration techniques. The method is derived by formulating Nesterovs Accelerated Gradient for strongly convex functions as a HessianFree HighResolution Ordinary Differential Equation (ODE) in combination with appropriate noise injection. The paper conducts a thorough theoretical analysis and numerical experiments to validate the effectiveness of the proposed algorithm in accelerating the sampling process.  Strong Points: 1. Innovative Concept: The paper introduces a unique concept of using acceleration techniques from optimization to enhance sampling efficiency which can potentially lead to significant improvements in MCMC methods. 2. Theoretical Analysis: The paper provides an indepth theoretical analysis including the derivation of HFHR dynamics convergence results and iteration complexity bounds demonstrating a solid theoretical foundation for the proposed method. 3. Empirical Validation: The empirical experiments conducted validate the theoretical results and demonstrate the practical effectiveness of the HFHR algorithm showcasing its accelerated convergence in comparison to traditional methods. 4. Comprehensive Expositions: The paper provides detailed expositions on the construction of HFHR dynamics discretization algorithms and numerical experiments ensuring a thorough understanding of the proposed method.  Weak Points: 1. Complexity: The paper delves into intricate mathematical formulations and derivations which may make it challenging for readers without a strong background in optimization and stochastic processes to fully grasp the content. 2. Limited Application Domains: The focus on logstronglyconcave target measures in the theoretical analysis may limit the generalizability of the proposed method to more diverse sampling scenarios. 3. Verification Scope: While the numerical experiments are insightful the application domain of Bayesian neural networks may not fully align with the logstronglyconcave assumption potentially limiting the applicability of the results to more complex models.  Questions and Feedback to the Author: 1. Generalization: How do you envision extending the HFHR algorithm to handle nonlogstronglyconcave target measures and more diverse sampling scenarios beyond the scope of the current study 2. Practical Implementation: Are there any practical considerations or challenges in implementing the HFHR algorithm in realworld applications especially when dealing with highdimensional or nonconvex problems 3. Comparison: How does the HFHR algorithm compare in terms of computational efficiency and scalability with existing stateoftheart MCMC methods especially in scenarios with large datasets and complex target distributions  Review Summary: The paper presents a novel acceleratedgradientbased MCMC method HFHR inspired by optimization techniques. The strong theoretical foundation rigorous analysis and empirical validation underscore the potential of HFHR to significantly improve sampling efficiency. While the complexity of the content and the limited scope of applicability in the current study are concerns the innovative approach and promising results justify further exploration and potential applicability in broader sampling scenarios. Therefore I recommend acceptance after addressing the weak points and providing clarification on the extension and practical implementation aspects.", "ucASPPD9GKN": " Summary of the Paper: The paper challenges the popular notion that graph neural networks (GNNs) are limited by homophily assumptions and are not suitable for heterophilous graphs. The authors empirically demonstrate that the graph convolutional network (GCN) model can achieve strong performance on certain heterophilous graphs under specific conditions. They investigate the conditions under which GCNs can learn similar embeddings for nodes with the same label providing theoretical understandings and empirical observations. Through experiments on synthetic and realworld datasets the paper reveals that GCNs can excel even on heterophilous graphs when certain assumptions are met contradicting the prevailing belief about the necessity of homophily for GNN performance.  Strong Points: 1) Empirical Findings: The paper provides compelling empirical evidence that challenges the conventional belief about GCNs performance on heterophilous graphs. 2) Theoretical Insights: The authors offer detailed theoretical underpinnings to explain why GCNs can work well on heterophilous graphs under specific conditions. 3) Experimental Validation: Through experiments on synthetic and realworld datasets the authors validate their theoretical findings and provide a comprehensive analysis of GCN performances. 4) Structured Presentation: The paper is wellstructured with clear sections and subsections allowing for a systematic presentation of the research findings.  Weak Points: 1) Clarity and Conciseness: The paper is quite detailed and dense making it challenging for readers to grasp the key points concisely. Simplifying the language in some sections could enhance readability. 2) Repetitive Explanations: There is some redundancy in the explanations especially between the theoretical derivations and the empirical investigations. Streamlining the content could improve the overall coherence of the paper. 3) Limited Generalizability: While the findings are intriguing the study focuses primarily on the GCN model and may lack generalizability across a broader set of GNN architectures.  Questions and Feedback to the Author: 1) Could you clarify the practical implications of your findings for the design and application of GNN models beyond GCN especially for more complex architectures 2) Have you considered potential extensions or refinements to your proposed conditions for achieving good performance on heterophilous graphs How might these conditions be generalized to other GNN models 3) How do you plan to address the limitations of \"bad heterophily\" and enable GNNs to cope with the challenges posed by such graph scenarios Are there specific research directions or applications where your findings could be particularly impactful  Review Summary: The paper presents a thoughtprovoking investigation into the performance of GCNs on heterophilous graphs challenging established beliefs about GNNs reliance on homophily assumptions. The authors provide a solid mix of empirical evidence theoretical insights and experimental validation to support their claims. While the paper is thorough and wellstructured there is room for enhancing clarity and conciseness. Overall the research offers valuable contributions to the understanding of GNN behavior on diverse graph structures and opens up avenues for further exploration and refinement in GNN design and applications. Based on the quality of the presented research and its potential impact on the GNN field I recommend acceptance of the paper with minor revisions on the presentation and clarity aspects.", "o-1v9hdSult": "Summarize the paper: The paper introduces methods for providing contrastive explanations in sequential decisionmaking settings where the AI model may be inscrutable to users. The focus is on generating explanations in terms of userspecified concepts to enhance interpretability of AI systems. The approach involves constructing localized symbolic models based on user vocabulary terms to explain decisions made by the AI system. The methods are tested on Atari games and Sokoban variants with evaluations conducted through computational experiments and user studies. Strong Points: 1. Innovative Approach: The paper introduces a novel approach to address the challenge of providing contrastive explanations in sequential decisionmaking settings. 2. UserCentric: The emphasis on generating explanations in userspecified terms makes the system more interpretable and userfriendly. 3. Evaluation: The paper provides comprehensive evaluations through systematic user studies and computational experiments demonstrating the effectiveness of the proposed methods. 4. Detailed Explanations: The paper provides detailed explanations of the methodology algorithms and evaluation process offering a clear understanding of the research. Weak Points: 1. Assumptions: The paper relies on several assumptions in the model which might impact the generalization and robustness of the approach. 2. Complexity: The methodology involves complex probabilistic calculations and algorithms which may be challenging for readers without a strong background in AI. 3. Evaluation Limitation: The user studies were limited to a specific group of participants potentially reducing the generalizability of the findings. Question to the Author: 1. Can you elaborate on how the assumptions made in the probability calculations impact the reliability and applicability of the proposed approach 2. How do you plan to address the limitations identified in the user studies to enhance the generalizability of the research findings 3. How do you envision scaling up the methodology to handle more complex AI systems and decisionmaking scenarios Feedback to the Author: 1. Consider providing a more indepth discussion on the implications of the assumptions made in the probabilistic calculations to improve the transparency of the methodology. 2. Enhance the discussion section by exploring potential extensions of the research such as addressing stochastic settings incorporating partial observability and handling temporal aspects in decisionmaking. Review Summary: The paper presents a novel methodology for generating contrastive explanations in sequential decisionmaking settings focusing on userspecified concepts for interpretability. While the approach demonstrates innovation and promising results through evaluations there are concerns about the assumptions made and the complexity of the methodology. The author should address these issues to enhance the reliability and applicability of the research. Overall with further refinement and validation the proposed methods have the potential to advance the field of explainable AI systems.", "j-63FSNcO5a": "Summary: The paper introduces DisCo a framework for learning disentangled representations by leveraging pretrained generative models without sacrificing generation quality. DisCo focuses on discovering traversal directions as factors for disentangled representation learning and uses a contrastive approach to model image variations based on target disentangled representations. The framework consists of a Navigator to provide candidate traversal directions and a \\xe2\\x88\\x86Contrastor to extract representations of image variations. Two key techniques an entropybased domination loss and hard negatives flipping strategy are proposed to enhance disentangled representation learning. The paper evaluates DisCo on GAN VAE and Flow models on various datasets demonstrating stateoftheart results compared to both discoveringbased and typical methods. Strengths: 1. Novel Approach: DisCo offers a unique perspective on disentangled representation learning by contrasting image variations using pretrained generative models. 2. Comprehensive Evaluation: The paper rigorously evaluates DisCo on various datasets and generative models showcasing superior performance compared to baseline methods. 3. Key Techniques: The proposed entropybased domination loss and hard negatives flipping strategy contribute significantly to enhancing disentanglement. Weaknesses: 1. Ablation Study: While the paper conducts an ablation study more detailed discussions on the effects and implications of each key technique could further enhance the understanding of their contributions. 2. Limitation in Flow Model: The limitation in applying DisCo to Flow models raises questions about the frameworks adaptability to different generative models. Further investigations or explanations on this limitation would be beneficial. Questions for the Authors: 1. Can the authors elaborate on the specific challenges faced when applying DisCo to Flow models and is there a potential solution or workaround for these issues in future iterations of the framework Feedback for the Authors: 1. Providing a more indepth analysis of the ablation study results and their implications could enhance the papers contribution and understanding in the field. Review Summary: The paper introduces DisCo a novel framework for learning disentangled representations leveraging pretrained generative models. The proposed approach along with key techniques demonstrates superior performance compared to baseline methods. However some limitations in applying DisCo to Flow models and the detailed discussion on key techniques could be further addressed. Overall the paper makes a significant contribution to the field of disentangled representation learning and warrants publication with minor revisions.", "uSE03demja": " Review of Paper: \"RenderingInvariant State Prediction for Parameter Identification of Physical Systems from Videos\"  1) Summary: This paper addresses the challenge of identifying parameters characterizing a physical systems dynamic motion directly from videos with inaccessible rendering configurations. The proposed method combines domain randomization state estimation and rendering gradients to train a renderinginvariant stateprediction (RISP) network. By formulating a new loss function utilizing rendering gradients the RISP network is trained to infer state differences independent of rendering configurations. The method is evaluated in rigidbody and deformablebody simulation environments across tasks like state estimation system identification imitation learning and visuomotor control. The results indicate superior performance compared to existing methods especially in terms of generalizability to unknown rendering configurations.  2) Evaluation: Strong Points:  Innovative Approach: The integration of domain randomization state estimation and rendering gradients is a novel and effective approach to address the problem.  Efficient Training: The proposed loss function leveraging rendering gradients is efficiently integrated into modern deep learning frameworks enhancing the networks robustness to rendering variations.  Extensive Evaluation: The method is evaluated across multiple tasks and environments demonstrating its effectiveness and superiority over existing techniques. Weak Points:  Simulation Dependency: Dependency on a differentiable simulator may limit practical application in realworld scenarios with complex dynamics not accurately modeled.  Camera Parameters Requirement: Assuming known camera parameters might not always be feasible in realworld applications potentially restricting the methods generalizability. Questions: 1. How does the method scale with more complex physical systems involving fluid dynamics or intricate aerodynamic effects 2. Are there plans to address the limitation of requiring known camera parameters to improve robustness in realworld applications 3. Could the method benefit from incorporating additional sources of supervision or feedback to enhance performance in challenging scenarios Feedback to the Author:  Provide a detailed discussion on the scalability of the method to more complex physical systems and realworld applications.  Consider exploring strategies to mitigate the dependency on known camera parameters to enhance the methods applicability.  3) Review Summary: The paper presents a novel approach to parameter identification of physical systems from videos under unknown rendering configurations. Methodologically innovative the integration of domain randomization state estimation and rendering gradients demonstrates remarkable performance across various tasks and environments. Despite dependencies on simulations and known camera parameters the approach showcases notable advancements and robustness compared to existing methods. Overall the paper provides valuable contributions to the field of parameter identification from videos warranting further exploration and potential realworld implementation.  Recommendation: Based on the innovative methodology extensive evaluation and superior performance demonstrated in the paper I recommend acceptance with minor revisions focusing on addressing the limitations and potential enhancements for realworld applicability. The proposed method offers significant advancements in the field and has the potential for impactful contributions upon addressing the identified suggestions.", "nsjkNB2oKsQ": "Peer Review Summary: The paper delves into the realm of deep reinforcement learning (RL) algorithms with delayed rewards. It begins by defining environments with delayed rewards and the challenges posed by their nonMarkovian nature. A new offpolicy RL framework with a novel Qfunction formulation is introduced underpinned by theoretical convergence guarantees. Moreover a HCdecomposition rule of the Qfunction is proposed to enhance training efficiency and stability in tasks with highdimensional state spaces. Extensive experiments showcase the superior performance of the algorithms compared to existing work. Strengths: 1. Novelty and Significance: Addressing the underexplored domain of RL with delayed rewards is commendable. The paper fills a crucial gap by devising a novel algorithmic framework that shows promising results. 2. Theoretical Foundation: The paper grounds its proposal in thorough theoretical analysis ensuring robustness and reliability in addressing challenges associated with delayed reward settings. 3. Algorithmic Innovation: The introduction of the HCdecomposition framework is a notable contribution enhancing the practicality and efficiency of the proposed RL algorithm for highdimensional state spaces. 4. Experimental Validation: The extensive experiments validating the proposed algorithms against existing baselines exhibit a comprehensive evaluation methodology and provide compelling evidence for the superior performance of the proposed approach. 5. Clarity and Structure: The paper is wellstructured and presents the complex concepts in a clear and organized manner facilitating comprehension for readers. Weaknesses: 1. Evaluation Diversification: While the experiments are extensive diversifying the evaluation to include additional benchmark tasks or scenarios could further strengthen the paper\u2019s impact and generalizability. 2. Reproducibility: Detailed information on the codebase hyperparameters and datasets utilized in the experiments would enhance the reproducibility of the results for the research community. 3. Comparison Scope: Although the comparative evaluation demonstrates the superiority of the proposed algorithms a deeper discussion on the specific strengths and limitations relative to each baseline would provide additional insights. Questions and Feedback: 1. Could you provide more insights into the scalability of the proposed algorithm across different problem domains with varied complexities 2. How does the proposed HCdecomposition framework handle scenarios with noisy or incomplete historical information in the delayed reward setting 3. Considering the practical implementation of the algorithm how computationally intensive is the HCdecomposition methodology especially in scenarios with large state spaces Review Summary: The paper presents a comprehensive study on deep reinforcement learning algorithms in delayed reward environments introducing a novel offpolicy RL framework and HCdecomposition technique for enhanced training efficiency. The theoretical underpinnings coupled with extensive experiments showcasing improved performance over existing methods underscore the papers importance and impact. With a strong foundation in theory innovative algorithmic contributions and robust experimental validation the paper offers valuable insights into tackling challenges posed by delayed rewards in RL settings. Addressing minor concerns related to result reproducibility further diversification in evaluation and indepth comparison discussions could further bolster the papers excellence. Overall the paper demonstrates high academic merit and contributes significantly to the field of RL with delayed rewards. Recommendation: Based on the strengths exhibited by the paper in terms of theoretical innovation algorithmic contributions and experimental validation I recommend acceptance for publication with minor revisions to address the weaknesses highlighted above.", "oVfIKuhqfC": " Summary of the Paper: The paper introduces a new approach called Diffusion Bridge Mixture Transport (DBMT) for generative modeling based on diffusion processes. It presents a method to construct diffusion processes targeting desired data distributions without relying on timereversal arguments. The paper discusses the theoretical framework assumptions and proofs related to the proposed DBMT method. Additionally it explores the concept of diffusion bridges diffusion mixtures SDE classes drift adjustments and training objectives relevant to the DBMT approach. The authors provide a detailed analysis of the proposed method comparing it with existing approaches like Denoising Diffusion Probabilistic Modeling (DDPM) and Diffusion TimeReversal Transport (DTRT). The paper concludes by explaining the computational considerations and practical applications of the DBMT in computer vision scenarios.  Strengths of the Paper: 1. Innovative Approach: The introduction of DBMT as a novel method for generative modeling through diffusion processes is a significant contribution to the field. 2. Theoretical Depth: The paper delves into theoretical concepts such as diffusion bridges mixtures SDE classes and drift adjustments providing a comprehensive understanding of the proposed method. 3. Comparative Analysis: The comparison with existing approaches like DDPM and DTRT helps in highlighting the advantages and uniqueness of the DBMT method. 4. Training Objectives: The development of new training objectives especially focusing on nonfactorial transitions presents an interesting avenue for enhancing generative modeling techniques. 5. Practical Application: The exploration of scalable simulation and inference techniques from spatial statistics in the context of computer vision applications enhances the practical relevance of the proposed method.  Weaknesses and Questions for the Authors: 1. Empirical Evaluation: The paper lacks empirical evaluation or experimental results to validate the proposed DBMT method. It would be beneficial to include empirical benchmarks to demonstrate the effectiveness of the approach. 2. Implementation Details: More clarity on the implementation of the proposed method especially in practical scenarios would enhance the understanding of how DBMT can be applied in realworld settings. 3. Scalability: While the paper discusses the scalability of the method it would be useful to elaborate on the computational complexity and efficiency compared to existing approaches. 4. Clarification on Training Objectives: Providing more insights into how the new training objectives improve upon existing methods and their impact on model performance would strengthen the paper. 5. Potential Limitations: Addressing potential limitations or challenges associated with applying the DBMT method especially in complex scenarios would provide a more nuanced discussion.  Review Summary: The paper introduces an innovative approach DBMT for generative modeling based on diffusion processes. It demonstrates a strong theoretical foundation and proposes new training objectives that offer valuable insights into improving generative modeling techniques. While the paper is comprehensive in its theoretical exploration the lack of empirical validation and practical implementation details limits the applicability of the proposed method. The authors are encouraged to conduct empirical evaluations provide more clarity on implementation aspects and address potential limitations to strengthen the papers impact and relevance in the field of generative modeling.", "mTcO4-QCOB": " Paper Summary The paper focuses on the astuteness of explanation methods in the context of machine learning models. It introduces the concept of explainer astuteness which assesses the probability that an explainer provides similar explanations for similar data inputs. The paper formalizes the notion of explainer astuteness establishes theoretical connections between astuteness and Lipschitzness and evaluates the results empirically on simulated and real datasets.  Strong Points 1. Theoretical Foundation: The paper presents a solid theoretical foundation by introducing and formalizing explainer astuteness and establishing connections to probabilistic Lipschitzness. 2. Comprehensive Analysis: The paper thoroughly analyzes the behavior of various explanation models provides theoretical results for different types of explainers and evaluates the results empirically on multiple datasets. 3. Existing Work Comparison: The paper effectively positions its work in the context of existing research on explainers highlighting similarities and differences with prior studies. 4. Experimental Validation: The paper performs experiments to demonstrate the utility of theoretical results in predicting lower bounds on explainer astuteness providing empirical evidence to support the theoretical claims. 5. Clear Presentation: The paper structure is welldefined with clear sections definitions theorems and experimental details provided to enhance understanding.  Weak Points 1. Complexity: The papers subject matter is highly technical which may limit accessibility to readers with a nonexpert background in the field. 2. Implementation Details: While the theoretical results are sound the practical implementation of the proposed methods and their potential limitations could be further discussed.  Questions and Feedback to the Author 1. Could you elaborate on the practical implications of the theoretical findings presented in the paper for realworld applications of explanation methods in machine learning settings 2. How generalizable are the theoretical results provided for different types of explainers and are there specific limitations or assumptions that could affect the applicability of the lower bound guarantees in varied scenarios 3. It would be beneficial to discuss potential extensions or further research directions based on the current findings to address any limitations or areas for improvement in the proposed methodology.  Review Summary The paper offers a significant contribution by formalizing explainer astuteness and establishing a theoretical framework connecting astuteness to Lipschitzness in blackbox prediction functions. The strong theoretical foundation comprehensive analysis and empirical validation enhance the credibility of the research. Despite the complexity of the subject matter the paper effectively communicates its findings. Overall based on the strength of the theoretical results and empirical validation I recommend the paper for publication with some minor clarifications and potential extensions suggested in the feedback.", "yfe1VMYAXa4": " Peer Review 1) Summary: The paper introduces OntoProtein a framework that incorporates gene ontology (GO) knowledge graphs into protein pretraining models. The model integrates external biological knowledge into protein representations via contrastive learning with knowledgeaware negative sampling. Experimental results show that OntoProtein outperforms existing methods in tasks like proteinprotein interaction prediction and protein function prediction demonstrating the effectiveness of incorporating structured knowledge into protein language models. 2) Strengths:  Innovative Approach: The paper introduces a novel framework that integrates gene ontology knowledge graphs into protein pretraining models offering a unique perspective in protein representation learning.  Experimental Results: Extensive experiments and comparisons with stateoftheart methods demonstrate that OntoProtein provides promising improvements in various protein tasks proving the effectiveness of incorporating external knowledge.  Knowledge Graph Construction: The creation of the ProteinKG25 dataset which aligns gene ontology with protein sequences provides a valuable resource for training and evaluating protein models. Weaknesses:  Evaluation Metrics: While the experimental results show improvement in various tasks more indepth analysis and comparison with a wider range of baseline methods could provide a clearer picture of OntoProteins performance.  Model Complexity: The paper introduces a detailed methodology with several components which could potentially limit the models scalability and practical implementation. Question and Feedback to the Author:  Could you provide more insights into how the knowledgeaware negative sampling method in OntoProtein contributes to better protein representations Are there any potential drawbacks or limitations to this approach  How does the scalability of OntoProtein compare to existing protein language models considering the additional computational overhead from incorporating external knowledge graphs  Have you considered conducting user studies or expert evaluations to validate the practical utility of OntoProtein in realworld protein analysis scenarios 3) Review Summary: The paper presents an innovative approach OntoProtein which integrates gene ontology knowledge graphs into protein pretraining models. While the experimental results demonstrate the effectiveness of incorporating external biological knowledge in improving protein representations there are opportunities for further analysis and validation. The methodology is comprehensive but may raise concerns about model complexity and scalability. Overall the paper contributes to the field of protein representation learning but additional studies and comparisons could enhance the understanding and applicability of OntoProtein. Therefore I recommend a minor revision to address the weaknesses highlighted and provide more depth in the evaluation and discussion sections.  Overall the paper introduces a novel and impactful framework in protein representation learning. It provides valuable insights into the integration of gene ontology knowledge graphs into protein pretraining models opening new avenues for enhanced protein analysis and understanding.", "kSwqMH0zn1F": "Summary: The paper introduces PipeGCN a novel technique for efficient distributed training of Graph Convolutional Networks (GCNs). PipeGCN aims to hide the communication overhead by pipelining interpartition communication with intrapartition computation. The paper provides a theoretical convergence analysis and proves that the convergence rate of PipeGCN with both stale features and feature gradients is close to that of vanilla distributed GCN training without any staleness. The proposed smoothing method further enhances PipeGCNs convergence. Extensive experiments show that PipeGCN significantly boosts training throughput while maintaining accuracy compared to existing methods. Strengths: 1. Innovative Approach: PipeGCN introduces a novel technique to address the communication overhead in distributed GCN training which has the potential to significantly boost training efficiency. 2. Theoretical Analysis: The paper provides a theoretical convergence analysis which is crucial for understanding the effectiveness of PipeGCN and proves its convergence rate with both stale features and feature gradients. 3. Empirical Validation: Extensive experiments on largescale datasets demonstrate the effectiveness of PipeGCN in improving training throughput while maintaining accuracy compared to existing methods. 4. Code Availability: The availability of the code on GitHub enhances reproducibility and allows other researchers to validate and build upon the proposed method. Weaknesses: 1. Lack of Direct Comparison: The paper does not provide a direct comparison with some relevant existing methods such as NeuGraph AliGraph and P3 as their code is not publicly available. Including such comparisons could have offered a more comprehensive evaluation. 2. Single Decay Rate: The use of a single decay rate for smoothing method may limit the adaptability of the technique across different datasets with varying complexities and convergence characteristics. Investigating the sensitivity to different decay rates could enhance the methods flexibility. 3. Overfitting Analysis: While the paper briefly touches on overfitting mitigation with the proposed smoothing method a more detailed analysis and discussion on how PipeGCN handles generalization across different datasets could strengthen the findings. Questions and Feedback: 1. What computational overhead does PipeGCN introduce compared to existing methods and how does it affect the overall efficiency of the training process 2. Could the authors elaborate on the adaptability of PipeGCN to different graph structures or datasets and whether the proposed smoothing method is datasetdependent 3. Could further insights be provided on the scalability of PipeGCN to even larger datasets or distributed settings such as multiserver configurations Review Summary: The paper presents a novel and promising approach PipeGCN for efficient distributed training of GCNs. It addresses a critical issue in training largescale Graph Convolutional Networks and provides a comprehensive theoretical analysis along with empirical validation. The proposed method demonstrates a significant improvement in training throughput while maintaining accuracy. However the paper could benefit from further comparative analysis with existing methods and a more detailed investigation into the adaptability and generalization capabilities of PipeGCN. Overall based on the innovative approach theoretical rigor and promising results presented I recommend acceptance with minor revisions to address the identified weaknesses and questions.", "shbAgEsk3qM": "PeerReview 1) Summary: The paper delves into the theory of function approximation in reinforcement learning (RL) using overparameterized linear representations. It explores the convergence properties of various value estimation algorithms namely Residual Minimization (RM) Temporal Difference (TD) learning and Fitted Value Iteration (FVI) and provides a unified interpretation of their behavior. The analysis reveals distinct fixed points achieved by these algorithms in the overparameterized regime. Furthermore the authors introduce two novel regularizers aimed at improving the stability and generalization of deep RL models showing promising results in empirical evaluations. 2) Strong Points: a) Theoretical Exploration: The paper provides a comprehensive exploration of the convergence properties of value estimation algorithms in the overparameterized setting shedding light on the behaviors of RM TD and FVI. b) Unified View: The unification of TD RM and FVI under a common constrained optimization framework adds a novel and insightful perspective to the field. c) Novel Regularizers: The introduction of regularizers based on orthogonal projections and feature normalization presents innovative approaches to enhance stability and generalization in deep RL models. d) Empirical Evaluation: The empirical validation of the proposed regularizers showcases their effectiveness in improving the performance of TD and RM algorithms on discrete and continuous control benchmarks. Weak Points: a) Complexity: The paper is highly technical and theoretical which may make it challenging for readers who are not wellversed in RL theory to grasp the content fully. b) Empirical Validation: While the empirical results are promising a deeper analysis of the regularizers impact on various architectures environments and sample sizes could provide more robust insights. Question to Authors: a) Further Research: Are there plans to extend the regularizers to other RL algorithms or investigate their efficacy in more complex environments Feedback for Authors: a) Clarity: Clarifying the connection between the theoretical findings and their practical implications in a more accessible manner would broaden the papers readership. 3) Review Summary: The paper offers a deep dive into the convergence properties of value estimation algorithms in overparameterized RL settings introduces novel regularizers and demonstrates their efficacy through empirical evaluations. The unification of algorithms under a common optimization framework and the empirical validation of the proposed regularizers showcase the paper\u2019s significant contributions to the field. With some improvements in clarity and further exploration of empirical results the paper has the potential to be a key reference in advancing the understanding and practical application of deep RL models. Recommended Action: Based on the comprehensive theoretical analysis the innovative regularizers and promising empirical results I recommend acceptance of the paper with minor revisions to enhance clarity and expand on the empirical validation.", "twv2QlJhXzo": "Peer Review Draft: 1) Summary: The paper proposes an algorithm AILO (Advisoraugmented Imitation Learning from Observations) for addressing imitation learning from observations (ILO) when there is a mismatch in transition dynamics between the expert and learner environments. AILO trains an intermediary policy (advisor) in the learner environment to act as a surrogate expert for the learner aiming to minimize the crossentropy distance between the stateconditional nextstate distributions of the advisor and expert. The authors conduct experiments on MuJoCo locomotion tasks to demonstrate the effectiveness of AILO compared to baseline ILO methods under different dynamics discrepancies. 2) Strengths and Weaknesses: Strong Points:  The paper addresses an important challenge in imitation learning by proposing a novel approach to handle transition dynamics mismatch between the expert and learner environments.  The utilization of concepts from distribution support estimation (RED) to train the advisor provides a sound theoretical basis for the algorithm.  Experimental results on MuJoCo locomotion tasks demonstrate the efficacy of AILO in comparison to baseline ILO methods highlighting the algorithms effectiveness in handling dynamics discrepancies. Weak Points:  The introduction could benefit from a clearer delineation of the existing challenges in ILO and a clearer motivation for the proposed algorithm.  The explanation of the methodology especially relating to the training of the advisor policy could be more detailed and provide additional clarity for readers.  The discussion on related work could be expanded to provide a more comprehensive overview of the existing literature in ILO and related areas. Question and Feedback to the Author: 1. Could you provide more insights into the computational complexity of the AILO algorithm compared to baseline methods particularly in terms of training time and sample efficiency 2. How sensitive is the performance of AILO to hyperparameters and what strategies did you employ to tune these hyperparameters in the experiments 3. It would be beneficial to discuss the scalability and generalizability of AILO to more complex environments beyond the locomotion tasks considered in the experiments. 3) Review Summary: The paper presents a novel algorithm AILO for addressing the challenge of transition dynamics mismatch in imitation learning from observations. By training an intermediary policy (advisor) to mimic the experts state transitions in the learner environment AILO offers a promising solution to improve imitation learning performance under dynamics discrepancies. The theoretical grounding in distribution support estimation (RED) and the conducted experiments on MuJoCo locomotion tasks demonstrate the effectiveness of AILO compared to baseline methods. Despite some areas needing clarification and further details the paper contributes a valuable approach to enhancing ILO in practical settings. Recommendation: The paper provides a strong contribution to the field of imitation learning particularly in addressing transition dynamics disparities. With wellgrounded theoretical foundations and promising experimental results I recommend acceptance pending minor revisions aimed at enhancing clarity and providing additional details on the methodology and related work.", "uc8UsmcInvB": " Summary: The paper introduces the concept of statistically meaningful (SM) approximation to address issues with traditional approximation theory results for neural networks. Specifically it proposes a definition that requires approximating networks to have good statistical learnability going beyond mere expressivity. The study focuses on SM approximation for boolean circuits and Turing machines. The paper proves that overparameterized feedforward neural nets can SM approximate boolean circuits with a sample complexity dependent on the circuit size rather than the network size. Additionally it demonstrates that transformers can SM approximate Turing machines with a sample complexity polynomial in the alphabet size state space size and log of computation time. The paper also introduces new tools for generalization analysis to improve sample complexity bounds.  Strengths:  Innovative Concept: The introduction of statistically meaningful approximation provides a new and valuable perspective that goes beyond traditional expressivity considerations.  Theoretical Contributions: The paper provides rigorous proofs of the sample complexity bounds for SM approximation for both boolean circuits and Turing machines extending the understanding of neural network capabilities.  Methodological Advancements: The development of novel tools for analyzing generalization and bounding the sample complexities offers important contributions to the field.  Weaknesses:  Complexity: The paper is highly technical involving intricate theoretical derivations and proofs that may be challenging for a nonexpert audience to fully comprehend.  Lack of Empirical Validation: The analysis is largely theoretical and some empirical validation or application of the proposed concepts on realworld datasets could strengthen the papers impact.  Clarity: Some sections might benefit from further clarification or simplified explanations to enhance accessibility.  Questions and Feedback: 1. Clarification: It would be beneficial to provide more intuitive explanations or visual aids to help readers grasp the complex theoretical concepts presented in the paper. 2. Empirical Validation: How do you intend to validate the proposed SM approximation concept in practical machine learning scenarios or realworld applications 3. Comparison: Can you provide a comparison between SM approximation and other existing approximation frameworks in terms of practical utility and performance in realworld tasks 4. Future Research: What are the next steps in your research agenda following the successful demonstration of SM approximation for boolean circuits and Turing machines  Review Summary: The paper presents a novel concept of statistically meaningful approximation for neural networks showcasing its application to boolean circuits and Turing machines. It makes significant theoretical contributions with rigorous proofs of sample complexity bounds. While the paper is technical its novel approach and methodological advancements provide valuable insights for the deep learning community. With some improvements in clarity and potential empirical validation the findings could have broader impact and relevance in the field of machine learning. Therefore I recommend acceptance with minor revisions to enhance readability and practical relevance.", "xWRX16GCugt": " Summary of the Paper: The paper introduces Sequoia a software framework aimed at unifying research in Continual Learning (CL) by organizing various settings and methods. It presents a taxonomy of settings based on assumptions creating a hierarchical structure where more general settings serve as parents to those with more specific assumptions. The framework implemented as an opensource Python library allows researchers to develop and evaluate CL methods across different settings. It addresses challenges like evaluation reproducibility and the siloed evolution of Continual Supervised Learning (CSL) and Continual Reinforcement Learning (CRL) fields.  Strengths of the Paper:  Novel Framework: The creation of a unified software framework to organize CL research settings is a significant contribution to the field.  Hierarchical Organization: The hierarchical structure based on assumptions provides clarity and enables researchers to reuse methods across related settings.  OpenSource Implementation: Providing a publicly available software framework (Sequoia) enhances reproducibility and fosters collaboration within the research community.  Clear Application Scenarios: The paper effectively explains the application scenarios of the framework and how it addresses key challenges in CL research.  Weaknesses of the Paper:  Lack of Concrete Case Studies: While the paper introduces the framework and its design principles well it lacks detailed case studies or empirical results to demonstrate the effectiveness of the proposed approach.  Reproducibility Concerns: The statement about ensuring determinism in settings and methods is essential for reproducibility thus this issue should be addressed promptly.  Complexity of Hierarchical Structure: The hierarchical organization of settings based on assumptions can be complex potentially requiring a learning curve for researchers new to the framework.  Limited Comparison and Analysis: There is a limited comparison with existing frameworks or approaches in CL making it challenging to assess the uniqueness and superiority of Sequoia.  Questions and Feedback to the Author: 1. Clarification on Determinism: Can the authors provide insights into the challenges faced in ensuring determinism in settings and methods and the potential strategies being considered to address this issue 2. Empirical Validation: Are there plans to include detailed empirical validation or case studies in future iterations of the paper to showcase the practical benefits of Sequoia 3. Comparison with Existing Tools: How does Sequoia differentiate itself from other existing CL frameworks or tools and what are the advantages it offers over these alternatives 4. UserFriendliness of Framework: How userfriendly is Sequoia for researchers with varying levels of expertise in CL especially in terms of setting up and utilizing the framework efficiently  Review Summary: The paper presents a novel software framework Sequoia for organizing and unifying research in Continual Learning. It introduces a taxonomy of settings based on assumptions enabling hierarchical organization and reuse of methods across related settings. While the paper effectively highlights the potential of the framework and its application in addressing crucial challenges in CL research it would benefit from including more empirical validation and comparisons with existing tools. Overall with the implementation of Sequoia and the proposed approach the paper makes a valuable contribution to the field of Continual Learning providing a promising platform for collaborative research and advancement.", "ofLwshMBL_H": "Peer Review 1. Summary of the Paper: The paper discusses the issue of catastrophic forgetting in continual learning where models struggle to retain previously learned tasks when adapting to new ones. The authors propose a Task Conditional Neural Network (TCNN) model that utilizes a mixture of experts approach to learn and embed new tasks without losing information about previous tasks. The TCNN model employs a probabilistic layer to automatically identify task information during inference enabling continual learning without requiring task information in advance. The paper includes detailed discussions on related work the methodology of TCNN experiments on MNIST and CIFAR100 datasets and analysis of the models performance under unknown task settings and density estimation. 2. Strong Points:  Innovative Approach: The introduction of TCNN with a probabilistic layer to address catastrophic forgetting is a novel and potentially effective solution in continual learning.  Comprehensive Experiments: The paper includes experiments on MNIST and CIFAR100 datasets comparing TCNN with stateoftheart methods and visualizing forgetting rates demonstrating the models efficacy.  Clear Explanations: The paper provides detailed explanations of the proposed TCNN model including task likelihood estimation training and inference processes and analysis of learned tasks and new task adaptation.  Addressing Limitations: The paper acknowledges the limitations of parameter explosion and inference time in TCNN and proposes potential solutions for future improvements.  Significant Contributions: The work contributes to the field of continual learning by offering a method that automates task identification and adaptation without prior task information. Weak Points:  Clarification of Simulated Data: The paper would benefit from a more detailed explanation of how simulated data was used especially in the unknown task settings and density estimation sections.  Comparative Analysis: While the paper compares TCNN with various existing methods more indepth analysis and discussion on the differences in performance and efficiency compared to each method are needed.  Visualization Interpretation: The visualization results such as density estimation could be more clearly interpreted to explain their significance in relation to the models functionality and performance.  Future Work: Though mentioned the paper could offer more specific suggestions for future work and research directions beyond improvements to TCNN itself broadening the impact of the proposed model. Questions and Feedback to the Author: 1. Can you elaborate on how the probabilistic layer in TCNN accurately identifies task information during inference especially in cases where tasks are complex or overlapping 2. How do you envision TCNN being applied in realworld scenarios beyond standard datasets like MNIST and CIFAR100 particularly addressing challenges specific to diverse and dynamic environments 3. Could you provide more insights into the potential impact of TCNN in industries such as healthcare monitoring or autonomous systems where continual learning in changing environments is paramount 3. Review Summary: The paper presents a wellstructured and innovative approach TCNN to address catastrophic forgetting in continual learning. Through detailed experiments and analyses the authors demonstrate the effectiveness of TCNN in automatically adapting to new tasks while retaining information from previous tasks. The comprehensive evaluation against stateoftheart methods on MNIST and CIFAR100 datasets highlights the models superiority. With acknowledgment of its limitations and proposed solutions for future enhancement TCNN shows promising prospects for continual learning in dynamic environments. Overall this paper makes a significant contribution to the field and warrants consideration for publication. Recommendation: Based on the thorough analysis presented I recommend this paper for publication with minor revisions to clarify certain aspects and further enrich the discussion on realworld applications and future research directions.", "pgkwZxLW8b": "Summary: The paper introduces a novel method called federated sampled softmax (FedSS) that aims to efficiently learn image representations on decentralized data with a large number of classes under the federated learning setting. The proposed method involves FL clients sampling negative classes and requesting a subnetwork from the FL server in order to optimize a sampled softmax loss which approximates the global full softmax objective. The paper empirically demonstrates that FedSS significantly reduces the number of parameters transferred to and optimized by client devices while maintaining performance comparable to the standard full softmax method. Strong Points: 1. Innovative Approach: The introduction of FedSS a resourceefficient method for learning image representations on decentralized data with large class spaces under the federated learning setting demonstrates innovation in the field. 2. Empirical Evaluation: The paper provides comprehensive empirical evaluations to support the proposed methods effectiveness. The comparison with alternative methods such as NegOnly PosOnly FedAwS FullSoftmax and Centralized provides a thorough analysis of the proposed approach. 3. Efficiency: The efficiency of the FedSS method is showcased through a reduction in the number of parameters transferred to and optimized by clients thereby reducing communication costs without compromising performance. 4. Clear Explanations: The paper explains the methodology background and experimental setup in detail making it easy for readers to follow along. Weak Points: 1. NonIID Data Mentioned not Elaborated: The paper mentions nonIID data distribution among clients as a challenge without delving deeper into how this affects the proposed method or solutions to mitigate this issue. 2. Additional Experiments: While the experiments conducted are thorough it might add more value to the paper if further experiments were conducted on different datasets or scenarios to showcase the robustness and generalizability of the FedSS method. 3. Scalability Discussion: While the paper discusses the efficiency gains a more indepth discussion on the scalability of the FedSS method in terms of largescale deployments or increased complexity would enhance the completeness of the study. Questions and Feedback: 1. Could you provide more insight into how the FedSS method can address challenges related to nonIID data distribution among clients particularly in the federated learning setting 2. What considerations were taken into account during the experimental setup to ensure the robustness and reliability of the results presented in the paper Review Summary: The paper presents a novel method FedSS for learning image representations on decentralized data with a large number of classes under the federated learning setting. The proposed method showcases efficiency by significantly reducing the number of parameters transferred to and optimized by clients while maintaining performance levels akin to the traditional full softmax method. The empirical evaluations demonstrate the effectiveness of FedSS making it a promising approach for efficient image representation learning in decentralized environments. However additional experiments a deeper discussion on scalability and addressing challenges related to nonIID data could further strengthen the paper. Overall based on the innovative approach clear explanations and empirical results the paper is recommended for publication with minor revisions to address the weak points highlighted.", "wTTjnvGphYj": " Summary of the Paper: The paper introduces a novel approach LSPE (Learnable Structural and Positional Encodings) to enhance the representation power of graph neural networks (GNNs). The main idea is to learn separate structural and positional representations in GNNs aiming to address the issue of the lack of canonical positional information in arbitrary graphs. The LSPE architecture is demonstrated on various sparse and fullyconnected GNNs showing performance improvements on molecular and nonmolecular datasets.  Strengths of the Paper: 1. Innovative Concept: The idea of decoupling structural and positional representations in GNNs is innovative and addresses a crucial limitation in current GNN architectures. 2. Comprehensive Review: The paper provides a detailed overview of related work theoretical foundations and existing limitations which sets a strong context for the proposed architecture. 3. Empirical Demonstrations: The paper offers comprehensive numerical experiments on standard benchmark datasets showcasing significant performance gains with the LSPE architecture. 4. Generic and Applicable: The LSPE architecture is demonstrated to be applicable across diverse GNN architectures making it a versatile and practical solution for improving GNN performance.  Weaknesses of the Paper: 1. Complexity: The paper introduces several new concepts and equations making it challenging for readers unfamiliar with GNNs to grasp the detailed technical aspects quickly. 2. Evaluation Metrics Discussion: While the paper presents impressive performance gains a more detailed discussion on the choice of evaluation metrics and comparisons with existing stateoftheart models would strengthen the analysis. 3. Positional Loss: The importance and impacts of the positional loss component could be further elaborated to provide a better understanding of its influence on the overall model performance.  Questions for the Authors: 1. How did you determine the optimal values for hyperparameters in the LSPE architecture such as the number of positional encoding steps (k) for random walk features 2. Can you provide insights into potential future research directions or extensions of the LSPE architecture beyond the experimental validation presented in the paper  Feedback for the Authors: 1. It would be beneficial to include a more detailed discussion on the implications of the positional loss component and its significance in enhancing the networks performance. 2. Visual aids such as diagrams or figures illustrating the LSPE architecture could enhance the understanding of the proposed approach for readers.  Review Summary: The paper introduces a novel architecture LSPE which aims to enhance the representation power of GNNs by decoupling structural and positional representations. It provides a thorough exploration of related work theoretical foundations and empirical evaluations demonstrating significant performance gains across diverse datasets. Despite the technical complexity and some areas for improvement in the discussion the LSPE architecture presents a promising approach in advancing GNN models and warrants further investigation and application in future research endeavors. Hence based on the innovative concept rigorous experimentation and potential for impact in the GNN domain I recommend accepting this paper for publication.", "yql6px0bcT": " Summary: The paper introduces a novel approach called Decentralized CrossEntropy Method (DecentCEM) to address issues faced by the traditional centralized CrossEntropy Method (CEM) in modelbased reinforcement learning. DecentCEM enhances sample efficiency and avoids local optima by decentralizing the sampling process across an ensemble of CEM instances each improving its own sampling distribution independently. The paper illustrates the effectiveness of DecentCEM through a onedimensional multimodal optimization task and extends it to sequential decisionmaking problems showing improved performance over traditional centralized CEM approaches.  Strengths: 1. Innovation: The proposal of DecentCEM is a significant contribution to the field of reinforcement learning addressing key limitations of traditional CEM methods. 2. Theoretical Soundness: The paper provides theoretical support for DecentCEM demonstrating almost sure convergence to a local optimum. 3. Empirical Validation: Extensive experiments and benchmarking with established methods in various environments demonstrate the effectiveness of DecentCEM. 4. Comprehensive Experiments: The paper includes detailed experiments results ablation studies and comparisons with stateoftheart methods providing a thorough evaluation of the proposed approach.  Weaknesses: 1. Sample Size in Convergence: The paper assumes a polynomial growth in the number of samples for convergence which contrasts with the limited practical sample sizes used in experiments. 2. No Discussion on Scalability: While the proposed approach shows promise in the provided experiments scalability considerations or potential limitations in largerscale environments are not explicitly addressed. 3. Limited RealWorld Validation: While the paper extensively covers simulated experiments realworld validation or scenarios involving more complex environments could further strengthen the findings.  Questions to Author: 1. Can you elaborate on how DecentCEM would perform in environments with significantly larger state and action spaces compared to the benchmark environments used in the study 2. How does the computational complexity of DecentCEM scale with the ensemble size of CEM instances especially when dealing with more complex or highdimensional tasks 3. Have you explored potential applications of DecentCEM in realworld scenarios such as robotics or autonomous systems that require planning in complex environments  Feedback: Overall the paper presents a novel and promising approach in DecentCEM addressing limitations of traditional CEM in reinforcement learning. Enhancements in theoretical soundness and empirical validation make the paper valuable to the research community. Further investigations into scalability smaller sample sizes and practical realworld applications could strengthen the contributions of DecentCEM in various settings. Additional clarity on convergence assumptions and future directions for broader application contexts would enhance the impact of the work. Great job on the comprehensive analysis and detailed experimentation provided in the paper.", "iaxWbVx-CG_": " Peer Review 1) Summary of the Paper: The paper introduces a novel approach called Hierarchical Cross Contrastive Learning (HCCL) in the field of selfsupervised learning (SSL) for computer vision. The key idea behind HCCL is to leverage hidden layers of the projection head to improve the representations learned by the conventional contrastive loss. By comparing latent features across different levels and views HCCL aims to enhance the generalization ability of visual representations learned in SSL frameworks. Extensive experiments across various benchmark datasets demonstrate the superior performance of HCCL compared to previous methods in tasks such as classification detection segmentation and fewshot learning. 2) Strengths and Weaknesses Questions and Feedback to the Authors: Strengths:  Innovative Approach: HCCL addresses the limitations of conventional contrastive learning by leveraging hidden layers of the projection head which is a novel and significant contribution to the field.  Comprehensive Experiments: The paper demonstrates the efficacy of HCCL through extensive experiments across multiple tasks and benchmark datasets showcasing its superior performance compared to existing methods.  General Applicability: HCCL is shown to be a simple and generic method that can be applied across different SSL frameworks enhancing its practical utility.  Detailed Method Description: The paper provides a comprehensive description of the HCCL method including framework design implementation details and algorithm pseudocode which enhances reproducibility. Weaknesses:  Theoretical Justification: While the empirical results are strong a deeper theoretical analysis or explanation behind the effectiveness of HCCL especially in relation to other contrastive learning methods could add more depth to the paper.  Comparison Metrics: It would be beneficial to include more detailed comparisons with stateoftheart methods including statistical significance analysis to provide a clearer understanding of HCCLs superiority.  Scalability Considerations: The paper could discuss the scalability of HCCL in terms of computational efficiency and memory requirements especially in largescale applications. Questions and Feedback to the Authors:  Can you provide insights into the scalability of HCCL in terms of computational efficiency and memory usage particularly in scenarios with largescale datasets or complex models  Have you explored the interpretability of the representations learned by HCCL and how they contribute to downstream tasks A discussion on the interpretability aspects would add value to the paper.  It would be beneficial to elaborate on any limitations or potential challenges associated with the HCCL method both from a theoretical and practical perspective. 3) Review Summary: The paper presents a novel approach HCCL that enhances selfsupervised learning by leveraging hidden layers of the projection head. The comprehensive experiments demonstrate the superior performance of HCCL across various computer vision tasks and datasets establishing its effectiveness in improving visual representations. While the paper is wellstructured and provides detailed descriptions of the method and experiments further exploration of theoretical justifications scalability considerations and interpretability aspects would strengthen the paper. Overall the innovative approach extensive experiments and general applicability make HCCL a promising method in the field of selfsupervised learning for computer vision.  Recommendation: Based on the innovative approach comprehensive experiments demonstrating superiority and clarity of presentation I recommend this paper for acceptance with minor revisions to address the identified weaknesses and questions.", "zNHzqZ9wrRB": " Review of \"TorchMDNET: Equivariant Transformer for Quantum Mechanical Properties Prediction\"  Summary: The paper introduces TorchMDNET an equivariant Transformer (ET) architecture for predicting quantum mechanical properties. The study focuses on enhancing accuracy while maintaining computational efficiency in predicting quantum mechanical properties using machine learning potentials. The architecture leverages an attention mechanism and graph data structure to encode atomic interactions. Extensive analysis is conducted including attention weight analysis and evaluation on benchmark datasets like QM9 MD17 and ANI1.  Strong Points: 1. Innovative Approach: The paper introduces a novel ET architecture to predict quantum mechanical properties demonstrating stateoftheart accuracy and efficiency over existing methods. 2. Detailed Methodology: The paper provides an indepth explanation of the ET architecture including components such as the embedding layer modified attention mechanism and update layers making it transparent for replication. 3. Extensive Evaluation: The study extensively evaluates the models performance on benchmark datasets like QM9 MD17 and ANI1 showcasing its superiority over existing approaches in accuracy and computational efficiency. 4. Insightful Analysis: The attention weight analysis provides valuable insights into the models behavior such as focusing on specific atom types and the impact of training data dynamics on attention patterns. 5. Comparative Studies: Ablation studies and comparisons with existing models validate the effectiveness of the proposed ET architecture considering the impact of equivariant features dataset diversity and attention patterns.  Weak Points: 1. Complexity: The detailed technical content may be challenging for readers not wellversed in deep learning or quantum mechanics potentially limiting the accessibility of the paper to a broader audience. 2. Notable Comparators: The discussion could benefit from a more explicit comparison with recent advancements in quantum mechanical property prediction emphasizing the unique advantages of TorchMDNET over competing models.  Questions and Feedback: 1. Clarification on Model Size: Can you provide more insights into how model size (number of parameters) affects the accuracy and efficiency of the ET architecture especially in comparison to smaller models like PaiNN and NequIP 2. Generalization and Transferability: How does the TorchMDNET architecture perform on unseen datasets or data from diverse domains outside the benchmark datasets Does it exhibit transfer learning capabilities  Review Summary: The paper presents a cuttingedge approach to quantum mechanical property prediction using the TorchMDNET architecture showcasing significant advancements in accuracy and efficiency over existing models. The detailed methodology comprehensive evaluation and insightful analysis of attention weights contribute to the credibility and significance of the study. While the complexity of the content and the need for comparisons with recent advancements pose some challenges the innovative nature of TorchMDNET and its strong performance warrant its publication. Therefore I recommend this paper for publication in its current form with minor revisions to address the highlighted points.", "kcrIligNnl": " Summary: The paper proposes a novel method for molecular conformation generation that directly outputs the coordinates of atoms without predicting interatomic distances. The method employs a variational autoencoder (VAE) framework and utilizes a loss function that is invariant to rotation and translation of atom coordinates. The model iteratively refines conformations using multiple blocks each refining the output of the previous one. Experimental results demonstrate the effectiveness of the proposed method on various datasets outperforming existing baselines on molecular conformation generation.  Strengths: 1. Innovative Approach: The paper introduces a unique method that directly generates atom coordinates addressing limitations of existing distancebased methods. 2. Empirical Results: The experimental results show that the proposed method achieves stateoftheart performance on multiple datasets demonstrating its effectiveness. 3. Advanced Model Design: The utilization of advanced architectural components like GATv2 and GN block enhances the modeling capabilities of the method. 4. Efficiency: The proposed method significantly outperforms existing methods in terms of sample efficiency as demonstrated by reduced decoding time. 5. Property Prediction: The paper also includes property prediction experiments showing the applicability of generated conformations.  Weaknesses: 1. Comparison with ConfGF: The method is slightly less effective compared to ConfGF on smallscale QM9 dataset in terms of COV and MAT scores. 2. Retrieval of Strongest Arguments: While the method proves effective on datasets with more heavy atoms a more detailed analysis or explanation of these findings could enhance the papers impact. 3. Ablation Study Limitations: The ablation study could be further extended or refined to provide deeper insights and strengthen the claims.  Questions: 1. Can you provide further insight into how the proposed method addresses the challenge of rotatable rings in molecular conformation generation 2. How does the model complexity impact the computational efficiency of the method especially in largescale settings  Feedback to the Authors: The paper presents a significant contribution to the field of molecular conformation generation by introducing a new approach that directly outputs atom coordinates. The novel design of the method along with the experimental results demonstrates its effectiveness and potential. However addressing the identified weaknesses providing additional insights into specific areas and further refining the ablation study could enhance the papers clarity and impact.  Overall Evaluation: The paper presents a strong contribution to the field with its innovative approach and empirical results. With minor revisions to clarify certain points and further analysis of specific findings the paper has the potential to make a significant impact. Therefore based on the comprehensive analysis of the paper I recommend acceptance after minor revisions.", "wxVpa5z4DU1": " Review of \"Tradeoff Between Accuracy and Privacy in Deep Ensemble Learning\"  Summary: The paper empirically explores the tradeoff between accuracy and privacy concerning membership inference attacks in deep ensembles. Through extensive experimentation with various datasets and model architectures the authors demonstrate how the improved accuracy in deep ensembles can inadvertently increase susceptibility to membership inference attacks. They identify that factors like prediction confidence and agreement among models significantly impact the effectiveness of these attacks. The study further evaluates defense mechanisms such as regularization and differential privacy showing a clear tradeoff where enhancing privacy reduces ensemble accuracy. The experiments also showcase the impact of ensembling techniques like bagging and stacking on this tradeoff.  Strong Points: 1. Detailed Analysis: The paper offers a comprehensive analysis of the accuracyprivacy tradeoff in deep ensembles providing insights into how ensembling affects susceptibility to membership inference attacks. 2. Experimental Rigor: The study covers a wide range of datasets and model architectures ensuring a robust evaluation of the tradeoff across various scenarios. 3. Innovative Approach: By investigating the impact of factors like prediction confidence and model agreement the paper sheds light on the root causes of the tradeoff offering valuable contributions to the research domain. 4. Defensive Mechanism Evaluation: The exploration of defense mechanisms including regularization and differential privacy enriches the study by highlighting practical strategies to mitigate privacy risks associated with deep ensembles.  Weak Points: 1. Limited Theoretical Analysis: The paper could benefit from further theoretical discussions or formal proofs to strengthen the arguments and provide deeper insights into the observed phenomena. 2. Generalization Concerns: While the experiments cover a diverse set of datasets more discussion on the generalizability of findings across various scenarios or model architectures would enhance the papers impact.  Question and Feedback to the Author: 1. Can you provide more insights into the practical implications of the observed accuracyprivacy tradeoff in realworld applications of deep ensemble learning 2. How do you envision overcoming the tradeoff dilemma to strike a balance between accuracy and privacy effectively in future research  Review Summary: The paper presents a thorough empirical investigation into the accuracyprivacy tradeoff in deep ensemble learning highlighting the nuanced relationship between improving accuracy and vulnerability to membership inference attacks. The detailed analysis of factors influencing this tradeoff and the evaluation of defense mechanisms enrich the study. However further theoretical exploration and considerations on generalizability could enhance the papers impact. Considering the innovative approach and valuable insights provided I recommend accepting the paper with minor revisions to address the identified weak points.", "m22XrToDacC": " Summary of the Paper: The paper introduces the Distributionally Robust Recourse Action (DiRRAc) framework which addresses the challenge of explaining machine learning decisions under model shifts. The proposed framework aims to generate robust recourse actions that are valid under model shifts particularly focusing on mixture shifts of model parameters. By utilizing a distributionally robust optimization approach the paper provides a method to efficiently find and solve the DiRRAc problem using a projected gradient descent algorithm. Additionally the paper discusses extensions of the DiRRAc framework to handle mixture weight uncertainty and minimize the worstcase component probability.  Strengths of the Paper: 1. Innovative Framework: The proposed DiRRAc framework is innovative and addresses a critical issue in machine learning explanations by considering robustness to model shifts. 2. Theoretical Contributions: The paper provides solid theoretical foundations for the proposed framework including detailed derivations and proofs. 3. Practical Relevance: The use of synthetic and realworld datasets in numerical experiments enhances the practical relevance of the proposed framework and demonstrates its effectiveness. 4. Extensions: The exploration of extensions such as handling mixture weight uncertainty and minimizing worstcase probabilities showcases the versatility of the framework.  Weaknesses of the Paper: 1. Clarity in Explanations: Some sections especially those detailing the mathematics and derivations might be challenging for nonexperts to follow. Simplifying complex concepts could enhance the readability of the paper. 2. Evaluation Metrics: While the numerical experiments demonstrate the frameworks benefits a more extensive comparative analysis with existing approaches could strengthen the evaluation. 3. Application Scenarios: The paper could benefit from more detailed discussions on potential realworld applications and scenarios where the DiRRAc framework could be particularly useful.  Question and Feedback to the Authors: 1. How does the DiRRAc framework compare to existing stateoftheart approaches in terms of scalability and interpretability 2. Could additional information be provided on the computational complexity of the proposed algorithms to give readers an idea of the feasibility in largescale applications 3. Considering the increasing focus on ethical AI and bias mitigation how does the robustness of DiRRAc impact fairness and bias considerations in machine learning models  Review Summary: The paper presents a novel and theoretically sound framework the Distributionally Robust Recourse Action (DiRRAc) aimed at explaining algorithmic decisions under model shifts. With robust mathematical foundations and practical experiments the paper demonstrates the effectiveness of the proposed framework. However improvements in clarity discussion of practical applications and broader comparative analyses could strengthen the paper. Overall given the innovative nature and theoretical robustness of the DiRRAc framework I recommend this paper for publication after addressing minor concerns and enhancing clarity for readers.", "nhN-fqxmNGx": " Summary of the Paper The paper aims to compare the performance of six different methods  Lasso Elastic net SCAD thresholded Lasso forward selection and forward backward selection  for variable selection in highdimensional data analysis. The comparison is based on theoretical analysis of the expected Hamming error under certain assumptions about the data structure and sparsity. The paper provides comprehensive discussions on the strengths and weaknesses of each method in terms of variable selection accuracy.  Strong Points of the Paper 1. Thorough Theoretical Analysis: The paper provides a detailed theoretical comparison of the six methods deriving explicit expressions for the expected Hamming error and phase diagrams which offer valuable insights into their performances. 2. Novel Contribution: The paper introduces a focus on the Hamming error a measure that is not commonly studied in existing literature making the research unique and potentially impactful for realworld applications. 3. Comparative Analysis: By comparing the performance of multiple methods through phase diagrams and theoretical evaluations the paper presents a comprehensive analysis that highlights the pros and cons of each method under different scenarios.  Weak Points of the Paper 1. Complexity: The paper is highly technical and might be challenging for readers who are unfamiliar with the advanced theoretical concepts and mathematical notations used in the analysis. 2. Lack of Empirical Validation: While the theoretical comparisons are thorough the paper could benefit from empirical validation to demonstrate the practical utility of the findings on real datasets. 3. Limited Practical Guidelines: Despite providing detailed theoretical results the paper could enhance its impact by offering more practical guidelines on the selection of methods based on different data characteristics.  Questions and Feedback for the Author 1. Could you elaborate on how the theoretical results in your study could be practically applied in realworld scenarios of highdimensional data analysis 2. Have you considered conducting empirical experiments to validate the theoretical findings and provide insights into how the methods perform on actual data sets 3. Considering the intricate nature of the paper could you provide a more intuitive explanation or visualization for readers less familiar with advanced statistical concepts  Review Summary The paper provides a rigorous theoretical comparison of variable selection methods in highdimensional data analysis focusing on the Hamming error as a measure of performance. While the study offers valuable insights into the strengths and weaknesses of different methods the complexity of the analysis and the lack of empirical validation might limit its accessibility and practical relevance. Overall the paper is a significant contribution to the theoretical understanding of variable selection methods but could benefit from additional clarity and empirical validation to enhance its practical impact.", "t8O-4LKFVx": " 1) Summary of the Paper: The paper introduces a novel approach conformal training (ConfTr) which integrates conformal prediction (CP) into the training procedure of deep learning models. This method aims to provide better control over the predicted confidence sets by optimizing specific objectives defined on these sets. The paper demonstrates the effectiveness of ConfTr in reducing inefficiency of stateoftheart CP methods such as threshold CP (THR) and Adaptive Prediction Sets (APS) while also allowing for shaping the confidence sets based on various criteria such as classconditional inefficiency coverage confusion and overlap between groups of classes. Experimental results show significant improvements over standard training methods and other related works in terms of inefficiency reduction and control over confidence set composition.  2) Analysis of Strong and Weak Points Questions and Feedback:  Strong Points:  The paper addresses an important issue in deep learning models the lack of control over confidence sets which is crucial for highstake applications like medical diagnosis.  The method proposed ConfTr offers a comprehensive solution by integrating CP into the training process allowing for optimization of confidence sets.  The experiments are detailed and wellstructured showcasing the effectiveness of ConfTr in reducing inefficiency and shaping confidence sets.  The paper provides clear explanations for different steps of the proposed method making it easier to understand the technical details.  Weak Points:  The paper is highly technical and might be challenging for readers without a strong background in deep learning and conformal prediction. More explanation on the basic concepts and motivations behind the method could benefit the clarity.  While the experiments and results are comprehensive more comparative analysis with existing methods could further strengthen the validation of the proposed approach.  The ethical and reproducibility statements are commendable they could be enhanced by including details on the specific datasets used and any privacy or bias considerations.  Questions and Feedback to the Author: 1. Could you provide more insights into the interpretability of the confidence sets generated using ConfTr How can these be utilized to enhance decisionmaking in realworld applications like medical diagnosis 2. How does ConfTr perform when faced with noisy or imbalanced datasets Have you conducted experiments on such scenarios 3. It would be beneficial to discuss the computational overhead introduced by integrating CP into the training procedure. How scalable is ConfTr for largescale models and datasets 4. Consider further discussing the limitations of ConfTr if any and potential areas for future research to address these limitations.  3) Review Summary: The paper presents a wellstructured and technically sound approach ConfTr to integrate conformal prediction into the training of deep learning models. The method shows promise in reducing inefficiency and providing finegrained control over confidence sets which is crucial for applications such as medical diagnosis. The experiments demonstrate the effectiveness of ConfTr in improving the performance of stateoftheart CP methods. Overall the paper provides valuable insights into enhancing the reliability and interpretability of deep learning models and opens avenues for future research in this domain.  Recommendation: Based on the comprehensive analysis of the paper I recommend that the paper be accepted for publication after addressing the minor points highlighted in the review above. The method proposed ConfTr shows potential to bridge the gap between model training and confidence set optimization offering significant advantages for highstake AI applications. Further discussion on possible limitations and expanding the comparative analysis could enhance the overall impact of the research.", "fStt6fyzrK": " Summary of the Paper: The paper proposes a new robust training algorithm named MRTAdapt for enhancing the robustness of DNNbased semantic segmentation methods against natural variations. It leverages modelbased robust training algorithms and generative adversarial networks to minimize natural variation effects from both the image and label sides. Extensive experiments on realworld and synthetic datasets demonstrate that the proposed algorithm outperforms multiple stateoftheart models under various natural variations.  Strengths of the Paper: 1. Innovative Approach: The paper introduces a novel approach combining modelbased robust training and GANs to improve the robustness of semantic segmentation models against natural variations. 2. Comprehensive Experiments: Extensive experimental results are provided on both Cityscapes and Synthia datasets showcasing the superior performance of the proposed algorithm compared to existing methods. 3. Theoretical Foundation: The paper provides a solid theoretical foundation explaining the semantic meaning invariance and the formulation of the modelbased robust adaptive training algorithm.  Weaknesses of the Paper: 1. Limited Comparison Methods: While the paper compares the proposed method with AdaptSegNet ADVENT and FDA methods the comparison could be more comprehensive by including a wider range of stateoftheart domain adaptation techniques. 2. Computational Resource Constraints: There are limitations related to computational resources which may affect the scalability and generalizability of the proposed algorithm. 3. Evaluation Metrics: While pixelwise accuracy and mIoU are commonly used metrics discussing additional evaluation metrics or providing visual demonstrations of the segmentation results could enhance the papers impact.  Questions for the Authors: 1. How does the proposed algorithm perform in scenarios with complex natural variations or combinations of different types of variations 2. What computational resources are required for training and deploying the model and how scalable is the proposed algorithm to larger datasets 3. Can the proposed algorithm be applied to tasks beyond semantic segmentation and if so what modifications or adaptations would be necessary  Feedback to the Authors: 1. Strengthen the comparison with additional stateoftheart methods to provide a more robust evaluation of the proposed algorithm. 2. Consider discussing the practical implications and potential applications of the proposed method in realworld scenarios. 3. Include visual examples or qualitative analysis of segmentation results to enhance the understanding of the algorithms performance.  Review Summary: The paper introduces a promising modelbased robust training algorithm for enhancing the robustness of semantic segmentation models against natural variations. While the approach is innovative and the experimental results demonstrate superior performance the paper could benefit from expanding the comparison methods and addressing scalability concerns. Overall with further refinements and validations the proposed algorithm shows potential for significant contributions to the field of semantic image segmentation.", "xLfAgCroImw": "Summary of the Paper: The paper presents a novel energybased approach for addressing valuation problems in cooperative games specifically focusing on cooperative game theory applied to machine learning applications. The proposed method involves meanfield variational inference to recover traditional gametheoretic valuation criteria such as the Shapley value and Banzhaf value. The paper introduces the concept of the Variational Index as a valuation criterion generated through fixedpoint iteration for multiple steps. Empirical demonstrations are provided to showcase the effectiveness of the Variational Index in various synthetic and realworld valuation problems. Strengths: 1. Innovative Methodology: The paper introduces a novel energybased treatment for cooperative games providing an alternative perspective on traditional gametheoretic valuation criteria. 2. Theoretical Justification: The paper provides a sound theoretical basis for the proposed approach linking it to principles of maximum entropy and meanfield variational inference. 3. Empirical Validation: Empirical experiments are conducted to demonstrate the advantages of the proposed Variational Index over conventional valuation criteria showing improved decoupling error and valuation performance. Weaknesses: 1. Lack of Comparative Analysis: While the paper demonstrates the effectiveness of the proposed method through experiments a more indepth comparative analysis with existing stateoftheart methods could further strengthen the evaluation of the Variational Index. 2. Complexity and Clarity: The paper delves into advanced mathematical formulations and algorithms which may make it challenging for readers less familiar with game theory and variational inference to grasp the concepts easily. Improved clarity and simpler explanations could enhance the accessibility of the paper. Question and Feedback to the Author: 1. Clarification on Experimental Results: Could the authors provide additional insights into how the proposed Variational Index compares to existing methods in terms of computational efficiency and scalability especially with larger datasets or a higher number of players 2. Potential RealWorld Applications: It would be interesting to explore potential applications of the Variational Index beyond the synthetic and dataspecific scenarios discussed in the paper. How might this method be adopted in more complex machine learning tasks or cooperative systems Review Summary: The paper introduces a novel energybased approach for addressing valuation problems in cooperative games presenting the Variational Index as a new valuation criterion. While the paper shows promising results through empirical experiments further comparative analysis and clarity enhancements are recommended. The innovative methodology and theoretical justification support a positive recommendation assuming the author addresses the weaknesses identified.", "lyLVzukXi08": " Peer Review 1. Summary: The paper introduces a novel Bayesian metalearning approach named Neural Variational Dropout Processes (NVDPs) for robust metalearning. NVDPs utilize a taskspecific dropout posterior distribution and a lowrank product of Bernoulli experts metamodel for memoryefficient mapping of dropout rates. The paper also introduces a novel prior to optimize the conditional dropout posterior in variational inference. Experimental evaluations on fewshot learning tasks such as 1D regression image inpainting and classification demonstrate the superior performance of NVDPs compared to existing methods. 2. Strengths:  Innovative Methodology: The paper presents a novel approach NVDPs for Bayesian metalearning incorporating taskspecific dropout posterior modeling and a unique prior for effective optimization.  Memoryefficient Design: The use of a lowrank product of Bernoulli experts metamodel for efficient mapping of dropout rates is a strong aspect of the methodology.  Comprehensive Experimental Evaluation: The comparison with existing methods on various fewshot learning tasks and datasets showcases the effectiveness of NVDPs.  Theoretical Foundations: The paper provides a detailed review of existing Bayesian metalearning methods and offers a clear theoretical framework for NVDPs. 3. Weaknesses:  Complexity of Approach: The methodology may be complex for readers unfamiliar with Bayesian metalearning or deep learning concepts making it challenging for wider adoption.  Evaluation Data: While the paper presents promising results on the selected fewshot learning tasks a more extensive evaluation on diverse datasets and tasks could strengthen the generalizability of NVDPs.  Comparative Metrics: The paper primarily focuses on loglikelihood as the evaluation metric including additional evaluation metrics commonly used in the field could provide a more holistic assessment of the proposed methods performance. 4. Questions and Feedback:  Scalability and Efficiency: How does the proposed NVDPs approach fare in terms of scalability and computational efficiency particularly when dealing with larger datasets and more complex tasks  Sensitivity to Hyperparameters: How sensitive is the performance of NVDPs to hyperparameters especially in realworld applications where data and tasks may vary significantly  Theoretical Insight: Can the authors provide additional insights into the theoretical foundations supporting the design choices of NVDPs particularly regarding the taskspecific dropout posterior and variational prior  Open Source Implementation: Considering the complexity of the methodology providing an opensource implementation of NVDPs would be beneficial for the research community to replicate and build upon the results.  Review Summary The paper presents a novel Bayesian metalearning approach NVDPs which offers a unique methodology incorporating a taskspecific dropout posterior and a novel prior for effective model adaptation in fewshot learning tasks. The comprehensive experimental evaluation demonstrates the superior performance of NVDPs compared to existing methods. While the approach is innovative and shows promising results addressing issues related to complexity scalability and generalization could further enhance the impact and applicability of NVDPs. With the strong theoretical foundations and empirical evidence provided the paper contributes significantly to the field of Bayesian metalearning. Recommendation for acceptance is based on the methodological innovation detailed experimental validation and strong theoretical underpinning.", "w1UbdvWH_R3": " Summary of the Paper: The paper investigates the Neural Collapse (NC) phenomenon in deep neural networks trained with mean squared error (MSE) loss. It empirically establishes the emergence of NC in MSEtrained deep nets through experiments on canonical networks and benchmark datasets. The analytical tractability of MSE loss allows for deeper insights into NC compared to crossentropy (CE) loss. The paper makes three main contributions: a novel decomposition of the MSE loss experiments demonstrating negligible deviation from the leastsquares classifier during training and the derivation of exact dynamics predicting NC along the central path.  Strengths of the Paper: 1. Thorough Investigation: The paper provides a detailed exploration of Neural Collapse in deep nets trained with MSE loss offering new insights into the phenomenon. 2. Novel Contributions: The proposed decomposition of the MSE loss and the identification of the central path introduce innovative theoretical constructs to understand the behavior of classifiers in deep neural networks. 3. Empirical Validation: Conducting experiments on benchmark datasets and networks to validate theoretical findings strengthens the papers credibility.  Weaknesses of the Paper: 1. Complexity: The paper delves into intricate mathematical concepts which may make it challenging for readers without a strong background in theoretical machine learning. 2. Experimental Results Presentation: The extensive experiments presented in the appendix could potentially overwhelm readers and detract from the main message of the paper.  Questions for the Authors: 1. Can you clarify the practical implications of identifying and studying the central path in the context of Neural Collapse 2. How do you envision future research leveraging the theoretical constructs and insights developed in this work for advancements in deep learning theory or practice  Feedback for the Authors: 1. Consider simplifying the presentation of experimental results to highlight key findings without overwhelming readers. 2. Providing more explicit connections between theoretical findings and practical applications could enhance the papers impact and relevance to a wider audience.  Review Summary: The paper provides a comprehensive investigation of Neural Collapse in deep neural networks trained with mean squared error loss. With novel contributions such as a decomposition of the MSE loss and the concept of the central path the paper offers valuable insights into the behavior of classifiers in deep nets. While the complexity of the topic and extensive experimental results may pose challenges for some readers the robust theoretical foundations and empirical validation strengthen the papers contributions. With minor adjustments in presentation and clearer connections to practical implications this work has the potential to make significant contributions to the field of deep learning theory and optimization. Overall I recommend acceptance with minor revisions for improved clarity and impact.", "uydP1ykieNv": " Summary of the Paper: The paper introduces EnsembleinOne (EIO) a method to enhance the robustness of deep learning systems against adversarial attacks. It proposes incorporating a random gated network (RGN) with multipath random gated blocks to construct an ensemble within a single network. By diversifying vulnerabilities among the numerous paths within the RGN EIO aims to improve robustness and scalability while reducing computational overhead.  Strong Points: 1. Innovative Approach: The concept of integrating an ensemble within a single network through random gating is a novel and promising approach to improve robustness. 2. Scalability: EIO addresses the scalability issue of traditional ensemble methods by exponentially increasing the paths within the RGN with network depth. 3. Efficiency: The method demonstrates better accuracyrobustness tradeoffs than adversarial training with minimal computational overhead. 4. Experimental Results: Extensive experiments showcase the effectiveness of EIO outperforming previous ensemble methods and achieving high adversarial accuracy under both blackbox and whitebox attacks.  Weak Points: 1. Clarity and Explanation: The paper contains technical details that may be challenging for readers unfamiliar with the topic. Simplifying the language and providing additional explanations could improve accessibility. 2. Comparison to Existing Methods: While the paper provides comparisons to previous ensemble methods and adversarial training a more detailed analysis and discussion on the specific advantages of EIO over these methods would strengthen the paper. 3. Deployment Considerations: Further exploration into the deployment of models derived from the RGN such as stability and realworld applicability would add depth to the research.  Questions and Feedback to the Author: 1. Can you elaborate on the practical implications and realworld applications of implementing the EIO method in deep learning systems 2. How does the computational efficiency and overhead of training and deploying an RGNbased ensemble compare to traditional ensemble methods in resourceconstrained environments 3. Have you considered the potential impact of hyperparameter choices on the effectiveness of EIO in different scenarios How robust is the method with varying hyperparameters  Review Summary: The paper presents a novel approach EnsembleinOne (EIO) to enhance the robustness of deep learning systems against adversarial attacks. Despite the complexity of the topic the paper provides clear motivation methodology and experimental results to support its claims. The innovative concept of constructing an ensemble within a single network shows promising results in terms of robustness and efficiency. However some aspects such as practical deployment considerations and comparisons to existing methods could benefit from further discussion. Overall the paper contributes valuable insights and presents a strong case for the effectiveness of the proposed EIO method. With minor refinements in clarity and additional analysis the paper has the potential to make a significant impact in the field of adversarial defense in deep learning.", "xkjqJYqRJy": " Summary of the Paper The paper investigates the use of isotropic Gaussian priors in Bayesian neural network (BNN) inference and explores whether these priors accurately reflect the true beliefs of weight distributions or provide optimal performance. The authors study empirical weight distributions of neural networks trained using stochastic gradient descent (SGD) to propose bettersuited priors. The findings suggest that correlated Gaussian priors are more suitable for ResNets while heavytailed priors are more effective for fully connected neural networks (FCNNs). The paper provides experimental evidence that these alternative priors can improve classification performance on various image datasets.  Strong Points of the Paper  Empirical Analysis: The empirical analysis of weight distributions in different neural network architectures provides valuable insights to propose alternative priors.  Experimental Results: The experiments show that implementing correlated Gaussian and heavytailed priors can lead to improved classification performance challenging the standard isotropic Gaussian priors.  Thorough Evaluation Metrics: The paper evaluates the performance of the proposed priors using a variety of metrics like test error likelihood estimation calibration and outofdistribution detection providing a comprehensive assessment of the priors effectiveness.  Relevance: The studys findings have practical implications for improving BNN inference by reconsidering the choice of priors based on empirical evidence.  OpenSource Library: Making the library available on GitHub encourages collaboration and further research in the field fostering reproducibility and transparency.  Weak Points of the Paper  Limited Conclusions: The paper acknowledges the difficulty in drawing strong conclusions about the cold posterior effect in largescale image models hinting at the presence of other factors that may influence the effect.  Complex Phenomenon: The paper states that the cold posterior effect is a complex phenomenon and further investigation is needed to understand its implications fully.  Question to the Author  Have you considered analyzing the impact of different priors on tasks other than image classification to understand the generalizability of your findings  Feedback to the Author The paper provides valuable insights into the impact of priors on BNN performance through empirical analysis and experimental validation. The detailed experiments and thorough evaluation metrics enhance the credibility of the findings. Strengthening the discussion on the limitations and future directions will add depth to the papers conclusions. Overall the paper contributes significantly to understanding the role of priors in BNNs and opens avenues for further research in this area.  Review Summary Based on the papers wellconducted empirical analysis and experiments as well as the relevance of its findings for improving BNN inference I recommend accepting this paper with minor revisions to address the limited conclusions and provide more clarity on the complex nature of the cold posterior effect. The study makes a valuable contribution to the field and stimulates further exploration into Bayesian neural network priors.", "qyzTEWWM0Pp": " Summary of the Paper: The paper introduces Multiresolution Equivariant Graph Variational Autoencoders (MGVAE) a hierarchical generative model for learning and generating graphs in a multiresolution and equivariant manner. The proposed model employs higherorder message passing to encode the graph partition it into mutually exclusive clusters and coarsen it into lower resolutions to create a hierarchy of latent distributions. MGVAE is endtoend permutation equivariant with respect to node ordering and achieves competitive results in various generative tasks including general graph generation molecular generation unsupervised molecular representation learning link prediction on citation graphs and graphbased image generation.  Strengths:  Innovative Approach: The paper introduces a novel hierarchical generative model for graphs utilizing a multiresolution framework filling a gap in existing graph generative models.  Theoretical Framework: The theoretical underpinning of MGVAE is welldescribed especially in terms of permutation equivariance message passing and hierarchical latent structure.  Experimental Results: The paper presents extensive experiments on molecular graph generation and general graph generation tasks comparing MGVAE against stateoftheart models and demonstrating competitive performance.  Weaknesses:  Complexity: The paper is highly technical and may pose a steep learning curve for readers not wellversed in graph neural networks and variational autoencoders.  Clarity: Some parts of the paper especially related to the mathematical formulations and notations could benefit from more concise explanations or intuitive examples.  Evaluation Metrics: While the paper discusses various evaluation metrics for the generated graphs a more detailed analysis on the limitations of these metrics or potential biases in the evaluation could be beneficial for a holistic assessment.  Questions and Feedback to the Author:  Question: How does the performance of MGVAE scale with the size of the input graphs especially in terms of computational complexity and training time  Feedback: Providing visual aids such as diagrams or flowcharts could enhance the understanding of the model architecture and workflow aiding readers in grasping the complex hierarchical structure of MGVAE more effectively.  Review Summary: The paper introduces a sophisticated hierarchical generative model MGVAE for multiresolution graph generation with strong theoretical foundations and competitive empirical results. While the paper is wellstructured and comprehensive it could benefit from simplifying complex sections for better readability. The experimental evaluation showcases the models effectiveness across diverse tasks. Overall the paper makes a valuable contribution to the field of graph generative models and warrants consideration for publication after minor revisions to enhance clarity and accessibility.", "gJcEM8sxHK": " Summary of the Paper: The paper investigates the capability of large pretrained language models (LMs) to learn and map conceptual domains onto a grounded world representation. Specifically the authors test the models ability to learn and generalize spatial terms cardinal directions and color terms by providing textual prompts that represent grid worlds and RGB color spaces. The study focuses on whether LMs can map learned concepts to unseen or related concepts with limited training examples. The results highlight the success of the largest model (GPT3) in grounding concepts across various domains while smaller models struggle with generalization.  Strong Points: 1. Methodological Rigor: The paper provides a detailed experimental design including model selection training data incontext learning methodology and evaluation metrics. This enhances the reproducibility and reliability of the study. 2. Insightful Analysis: The error analysis conducted reveals important trends in model behavior such as the ability of the largest model to provide indomain responses even when incorrect. This enhances the understanding of the models reasoning process. 3. Novel Contribution: The study addresses an important aspect of grounding language models in the real world offering insights into the potential of textonly models to learn and generalize grounded concepts efficiently.  Weak Points: 1. Limited Evaluation: The study focuses on a specific set of grounded concepts (spatial terms cardinal directions and color terms) that have clear textual representations. It may benefit from exploring a wider range of grounded domains to enhance the applicability of the findings. 2. Model Comparison: While the paper compares the performance of models of varying sizes a comparison with other stateoftheart models or approaches in grounding language would provide valuable context. 3. Generalization Scope: The study primarily focuses on how well LMs generalize to unseen concepts within a limited context. Further exploration into the scope and limitations of generalization across diverse domains could strengthen the findings.  Questions and Feedback to the Authors: 1. How do you envision extending this research to incorporate more complex or abstract grounded concepts beyond spatial terms and color domains 2. Have you considered exploring the impact of different training data sources or pretraining strategies on the models ability to ground concepts 3. How do you propose leveraging the insights from this study to enhance the practical application of grounded language models in realworld scenarios  Review Summary: The paper presents a thorough investigation into the ability of large language models to learn and map conceptual domains onto grounded representations. The study demonstrates that despite limitations the largest model GPT3 shows promising results in grounding concepts based on textonly inputs. The robust experimental design and insightful error analysis contribute to the understanding of the models behavior. While the study addresses important aspects of grounding language models further exploration of diverse grounded domains and practical implications would enhance the impact of the research. Overall the paper makes a valuable contribution to the field of grounded language models and warrants consideration for publication with some minor enhancements and clarifications.", "wu5yYUutDGW": " Summarize the paper: The paper introduces a novel Boundaryaware SelfSupervised Learning (BaSSL) framework for video scene segmentation. The focus is on learning contextual relationships between shots without relying on ground truth annotations. The framework includes pseudoboundary discovery using dynamic time warping three boundaryaware pretext tasks (ShotScene Matching Contextual Group Matching Pseudoboundary Prediction) and masked shot modeling. Extensive experiments on the MovieNetSSeg dataset show that BaSSL achieves new stateoftheart results in video scene segmentation.  Strong Points: 1. Innovative Approach: The paper proposes a unique selfsupervised framework specifically tailored for video scene segmentation showing a novel way to leverage pretext tasks for learning contextual relationships between shots. 2. Comprehensive Analysis: The paper thoroughly evaluates the proposed framework through extensive ablations and comparison with stateoftheart methods providing detailed insights into the effectiveness of different components and tasks. 3. Experimental Results: The framework achieves new stateoftheart performance on the MovieNetSSeg benchmark demonstrating the effectiveness of the proposed method. 4. Reproducibility: The paper provides detailed experimental settings metrics and results making the method reproducible. The promise to release the code and trained models further enhances reproducibility.  Weak Points: 1. Ethical Considerations: While briefly addressing potential applications and ethical considerations such as computational resources and bias the paper could delve deeper into the ethical implications of selfsupervised learning in video analysis. 2. Clarity in Presentation: The paper at times delves into technical details that may be challenging for readers not wellversed in video scene segmentation or selfsupervised learning which could hinder clarity for a broader audience.  Questions and Feedback to the Authors: 1. The pseudoboundary discovery method using dynamic time warping is a key component of your framework. Could you elaborate on how sensitive the method is to noise or incorrect pseudoboundaries especially during pretraining with longer durations 2. In the ablation studies when investigating the impact of individual pretext tasks did you observe any specific scenarios or types of scenes where certain tasks had more significant impacts on performance  Review Summary: The paper presents an advanced Boundaryaware SelfSupervised Learning (BaSSL) framework for video scene segmentation showcasing innovation in learning contextual relationships between shots. The comprehensive analysis and experimental results demonstrate the effectiveness of the proposed framework culminating in new stateoftheart performance on the MovieNetSSeg dataset. While the paper excels in methodology and experimental evaluation further clarity in presentation and deeper exploration of ethical implications could enhance its impact. Hence based on the strong methodology innovative approach and compelling experimental results I recommend acceptance after addressing the minor concerns raised.", "mk8AzPcd3x": " Summary of the Paper: The paper proposes a novel graph shortest distance embedding method called Betweenness Centralitybased Distance Resampling (BCDR) to improve the efficiency and accuracy of Shortest Distance Queries (SDQs) in graphs. The method leverages Betweenness Centralitybased random walk and Distance Resampling to better explore long distance correlations and preserve shortest distances during the mapping from graph to the embedding space. The theoretical background methodology and empirical performance of BCDR are thoroughly described and compared with existing embeddingbased methods on realworld graph datasets.  Strong Points: 1. Innovative Methodology: The paper introduces a novel approach BCDR combining Betweenness Centralitybased random walk and Distance Resampling to address the shortcomings of existing embeddingbased methods for SDQs. 2. Theoretical Justification: The paper provides clear theoretical explanations and proofs for the proposed methods effectiveness particularly emphasizing the importance of exploration distance and distance relation preservation. 3. Empirical Evaluation: The experimental evaluation demonstrates the superior performance of BCDR compared to existing methods on realworld graph datasets with divergent diameters showcasing its efficacy in solving SDQ problems. 4. Comprehensive Analysis: The paper includes indepth discussions on exploration distance distance relation preservation parameter setups datasets and practical applications enhancing the understanding and applicability of the proposed method.  Weak Points: 1. Limited Visualization: While the paper provides theoretical and empirical results visual aids such as figures or diagrams could enhance the presentation of complex concepts algorithms and experimental outcomes for better comprehension. 2. Parameter Sensitivity: The sensitivity of the proposed method to hyperparameters like the weight decay coefficient could be further elaborated or experimentally explored to provide insights into optimal parameter tuning for different datasets.  Questions and Feedback to the Author: 1. Clarity and Visualization: Can the authors provide visual representations or diagrams to illustrate the concepts of Betweenness Centralitybased random walk and Distance Resampling making it easier for readers to grasp the methodology 2. Hyperparameter Sensitivity: Have you conducted sensitivity analysis on hyperparameters particularly the weight decay coefficient \u03b1 to understand its impact on the models performance across different datasets 3. Practical Applications: How do you envision the practical applications of BCDR in realworld scenarios beyond graph shortest distance embedding such as in network analysis social relationship modeling or other relevant fields  Review Summary: The paper introduces a promising method BCDR for graph shortest distance embedding addressing key limitations of existing techniques and demonstrating superior performance on realworld datasets. The innovative approach comprehensive theoretical background and empirical results highlight the potential of BCDR in advancing graph representation learning and solving SDQ problems effectively. With minor enhancements in visualization and sensitivity analysis the paper can further strengthen its contributions and applicability in diverse graphrelated applications. Based on the strengths and potential impact of the proposed method I recommend acceptance of this paper for publication.", "fvLLcIYmXb": " Peer Review of the Paper \"An Axial Shifted MLP Architecture for Computer Vision\"  1) Summary of the Paper: The paper introduces an innovative architecture called Axial Shifted MLP (ASMLP) for computer vision tasks. In contrast to the global spatial feature encoding in MLPMixer ASMLP focuses on local feature interactions by axially shifting channels of the feature map enabling information flow from different axial directions. This approach allows ASMLP to capture local dependencies effectively achieving a similar local receptive field as traditional CNN architectures. The ASMLP architecture surpasses existing MLPbased models and competes with transformerbased models demonstrating impressive performance in image classification. Moreover ASMLP is extended to downstream tasks like object detection and semantic segmentation achieving competitive results.  2) Strong and Weak Points Questions Feedback:  Strong Points:  Innovative Approach: The introduction of axial shifting to capture local dependencies in MLP architecture is a novel and effective approach.  Competitive Performance: ASMLP achieves high accuracy on ImageNet1K and competitive results in object detection and semantic segmentation establishing a strong baseline for MLPbased architectures.  WellStructured Experiments: The paper provides detailed experimental results comparisons with CNN and transformer models and ablations to demonstrate the effectiveness of ASMLP.  Code Availability: The availability of the code on GitHub promotes reproducibility and facilitates further research.  Weak Points:  Complexity Discussion: The theoretical discussion on complexity comparisons with other architectures could be expanded for better clarity.  Evaluation Metrics: While the paper presents Top1 accuracy and model size additional metrics like Top5 accuracy or performance on specific classes could provide a more comprehensive evaluation.  Baseline Comparison: A direct comparison with all major MLPbased architectures in addition to transformerbased models would enhance the evaluations completeness.  Qualitative Analysis: Including qualitative visualizations on object detection and semantic segmentation tasks could help in better understanding the models behavior.  Questions and Feedback to the Author: 1. How does the ASMLP architecture handle scale variations in input images especially in downstream tasks like object detection 2. Could you elaborate on the interpretability of the ASMLP architecture especially in capturing spatial features in segmentation tasks 3. Have you considered exploring transfer learning scenarios with ASMLP to analyze its adaptability to diverse datasets  3) Review Summary: The paper presents an innovative ASMLP architecture for computer vision tasks showcasing significant improvements in local feature extraction and achieving competitive performance with transformerbased models. The comprehensive experimental results and detailed analyses demonstrate the effectiveness of ASMLP in image classification object detection and semantic segmentation. The code availability enhances reproducibility. However further clarity on complexity comparisons inclusion of additional evaluation metrics and qualitative visualizations could enhance the papers thoroughness. Overall the innovative approach and compelling results make this paper a valuable contribution to the field of MLPbased architectures.  Recommendation: Based on the thorough analysis of the paper I recommend accepting this submission with minor revisions to address the weak points identified in the review. The innovative approach competitive performance and wellstructured experiments make this paper a valuable addition to the field of computer vision architectures.", "rRg0ghtqRw2": " Summary of the Paper: The paper introduces a novel method for Unsupervised Environment Design (UED) named Adversarially Compounding Complexity by Editing Levels (ACCEL). ACCEL aims to overcome the limitations of current UED methods by evolving a curriculum through editing previously selected levels. The approach is evaluated on challenging procedurally generated grid world environments demonstrating the ability to produce curricula that start simple but become increasingly complex leading to agents capable of zeroshot transfer to challenging outofdistribution environments.  Strong Points: 1. Innovative Approach: ACCEL introduces a novel method by combining evolutionary processes with regretbased curators to produce effective curricula for training agents. 2. Empirical Results: The paper provides comprehensive experimental results demonstrating the effectiveness of ACCEL in quickly facilitating learning on challenging environments and achieving superior performance in zeroshot transfer scenarios. 3. Theoretical Robustness Guarantees: ACCEL maintains theoretical benefits by focusing on high regret levels offering stability and robustness in training agents. 4. Comparative Analysis: The paper effectively compares ACCEL with existing methods showcasing its superior performance in various experiments. 5. Clear Presentation: The paper is wellstructured with detailed explanations of methodology and results making it easy to follow and understand.  Weak Points: 1. Limited Experiments: The experiments focus primarily on navigation tasks limiting the scope of analysis to a specific set of environments. 2. Lack of Diversity Optimization: ACCEL does not explicitly optimize for diversity in the level replay buffer which could be a potential limitation in scaling the approach to more complex scenarios. 3. Inductive Bias: The reliance on starting with a simple base case (empty rooms) may not be applicable to all environments potentially limiting the generalizability of the method.  Questions to the Author: 1. How does ACCEL adapt to environments with diverse characteristics and complexities beyond the navigation tasks considered in the experiments 2. What potential challenges do you foresee in scaling ACCEL to more complex and diverse environments considering the lack of explicit diversity optimization 3. Have you explored the application of ACCEL in realworld scenarios beyond simulated grid world environments and if so what were the outcomes  Feedback to the Author: 1. Consider expanding the scope of experiments to cover a wider range of environments to assess the applicability and scalability of ACCEL in diverse scenarios. 2. Address potential limitations such as the lack of diversity optimization and the inductive bias to enhance the robustness and generalizability of the method. 3. Discuss the implications of ACCEL for realworld applications and provide insights into future directions for research.  Review Summary: The paper introduces ACCEL a novel method for UED that evolves a curriculum through editing levels to train agents capable of zeroshot transfer. Despite its innovative approach and promising empirical results the limited scope of experiments and potential limitations in diversity optimization and inductive bias should be addressed to enhance the methods applicability and robustness. Overall the paper showcases a valuable contribution to UED research and sets a foundation for further exploration in diverse and realworld environments.", "k32ZY1CmE0": " Summary of the Paper: The paper provides a detailed theoretical treatment of the exploding and vanishing gradient problem encountered in training recurrent neural networks (RNNs) particularly when dealing with chaotic time series data. The authors establish a connection between the RNN dynamics and the behavior of loss gradients during training by relating them to Lyapunov exponents. They prove that RNNs converging to stable fixed points or cycles have bounded gradients while those exhibiting chaotic dynamics always lead to diverging gradients. The paper offers insights into the problem and proposes a training technique called \"sparsely forced BPTT\" tailored for chaotic data where trajectories are guided back on track based on the system\u2019s local divergence rates measured through the Lyapunov spectrum.  Strong Points: 1. Theoretical Treatment: The paper offers a comprehensive theoretical analysis of the exploding and vanishing gradient problem establishing a clear connection between RNN dynamics and loss gradients. 2. Innovative Training Technique: Introducing the concept of \"sparsely forced BPTT\" as a method to address the exploding gradient issue in chaotic data training is a novel and significant contribution. 3. Empirical Evaluation: Providing empirical examples on chaotic systems like the Lorenz attractor and the Duffing oscillator as well as realworld chaotic weather data strengthens the theoretical findings with practical relevance. 4. Relevance of Chaotic Dynamics: Highlighting the importance of chaotic phenomena in nature and society and addressing their integration into RNN training algorithms adds value to the research.  Weak Points: 1. Complexity: The paper delves deeply into theoretical concepts potentially making it challenging for readers with limited background in mathematics or neural networks to grasp the content effectively. 2. Detailed Mathematical Proofs: While the theoretical derivations are thorough the extensive mathematical details could be daunting for nonexperts in the field.  Questions and Feedback to the Author: 1. Clarity: Considering the complexity of the topic could additional visual aids such as diagrams or illustrations be provided to enhance the understanding of the proposed theories and methods 2. Applicability: How adaptable is the proposed \"sparsely forced BPTT\" training technique to different types of RNN architectures and varying data complexities beyond chaotic time series 3. Validation: Have the findings been tested on a wider range of chaotic systems or data sets to generalize the effectiveness of the proposed training approach  Review Summary: Overall the paper presents a substantial contribution to addressing the challenging issue of exploding and vanishing gradients in training RNNs on chaotic time series data. The theoretical foundations are solid and the proposed \"sparsely forced BPTT\" technique offers a promising solution to mitigate gradient explosions effectively. While the empirical validation on chaotic systems and realworld data supports the theoretical claims the complex nature of the topic and detailed mathematical proofs could potentially hinder accessibility for a broader audience. Enhancements in visual aids broader validation and considerations for practical implementation could further strengthen the impact and applicability of the research findings. Based on the comprehensive theoretical analysis and innovative training approach presented I recommend this paper for publication with minor revisions to enhance clarity and accessibility.", "nWlk4jwupZ": "Summary: The paper introduces ScheduleNet an RLbased decentralized constructive scheduler designed to coordinate multiple agents in completing tasks with minimum completion time. The proposed method formulates multiagent scheduling problems (mSPs) as eventbased Markov decision processes (MDPs) and utilizes reinforcement learning to derive a decentralized decisionmaking policy. ScheduleNet uses a state representation with agenttask graphs node embeddings and assignment probabilities to efficiently solve mSPs. The effectiveness of ScheduleNet is demonstrated through experimentation on multiple traveling salesmen problem (mTSP) and jobshop scheduling problem (JSP) instances showcasing superior performance compared to heuristic approaches and existing deep RL methods. Strengths: 1. Innovative Approach: ScheduleNet introduces a novel method for decentralized decisionmaking in multiagent scheduling problems using reinforcement learning providing a fresh perspective in the field. 2. Effective Representation: The utilization of agenttask graphs and node embeddings with typeaware graph attention enhances the ability to capture complex relationships among entities in the scheduling problems. 3. Performance Validation: The empirical results showcase ScheduleNets superiority over existing approaches validating its effectiveness in solving practical and largescale scheduling problems. 4. Transferability  Generalizability: The ability of ScheduleNet to adapt to different problem sizes and distributions as demonstrated on mTSP and JSP instances highlights its versatility and applicability in various scenarios. Weaknesses: 1. Limited Comparison: While ScheduleNet outperforms baseline algorithms only a minimal number of baselines are considered. Additional comparisons with a broader range of methods could provide further insights into the proposed approachs strengths and weaknesses. 2. Evaluation Scope: The performance evaluation on publicly available benchmarks could be supplemented with more diversified datasets to ensure the generalizability of ScheduleNet across different types of scheduling problems. 3. Complexity Concerns: The complexity of the proposed method particularly the node embedding computation using typeaware graph attention raises concerns about scalability and practical implementation in realworld scenarios. Questions  Feedback: 1. Have you considered testing ScheduleNet on additional benchmark datasets to evaluate its performance across a wider range of problem instances 2. How does the computational complexity of ScheduleNet compare to other existing solutions for multiagent scheduling problems particularly in terms of scalability and efficiency 3. Could you provide more insights into the robustness of ScheduleNet in handling noisy or incomplete input information especially in dynamic scheduling scenarios Review Summary: The paper presents an innovative and effective approach ScheduleNet for decentralized multiagent scheduling problems using reinforcement learning. While demonstrating promising results and showcasing competitive advantages over existing methods additional evaluation on diverse datasets and further analysis of computational complexity and robustness are warranted. Overall the paper offers a significant contribution to the field of multiagent scheduling and with some enhancements as suggested ScheduleNet could stand out as a versatile and practical scheduling solution.", "zq1iJkNk3uN": " Summary: The paper introduces a novel training paradigm called Data efficient CLIP (DeCLIP) to address the datahungry nature of Contrastive LanguageImage Pretraining (CLIP). DeCLIP leverages intrinsic supervision within imagetext pairs more efficiently by incorporating selfsupervision within each modality multiview supervision across modalities and nearestneighbor supervision from similar pairs. Extensive experiments demonstrate that DeCLIP achieves higher zeroshot accuracy on ImageNet and outperforms CLIP on downstream visual datasets using significantly less data.  Strengths: 1. Innovative Approach: The proposal of using selfsupervision multiview supervision and nearestneighbor supervision in DeCLIP is a novel and effective way to utilize intrinsic supervision within imagetext pairs. 2. Performance Improvement: DeCLIP achieves superior zeroshot recognition accuracy on ImageNet and outperforms CLIP on 8 out of 11 downstream datasets demonstrating the effectiveness of the proposed approach. 3. Efficiency: DeCLIP achieves these results using significantly less data making it more dataefficient than CLIP. 4. Comprehensive Experiments: The paper provides detailed experiments and evaluations showcasing the performance of DeCLIP on various datasets and model architectures.  Weaknesses: 1. Complexity: The approach of incorporating multiple forms of supervision can increase complexity and training time compared to traditional methods which might limit scalability. 2. Limited Discussion on Failure Cases: The paper focuses on the successes of DeCLIP but lacks a thorough discussion on cases where the method might fail or limitations of the proposed approach. 3. Theoretical Justification: While the results are compelling more theoretical justification or analysis of why the proposed supervisory signals lead to improved performance would enhance the paper.  Question to the Author:  Can you provide insights into how the proposed nearestneighbor supervision method handles noisy or nonmatching pairs effectively  How does DeCLIP perform when dealing with extreme variations or noisy data in the imagetext pairs  Feedback to the Author:  The paper is wellstructured and presents a clear argument for the proposed approach. It would be beneficial to include a more indepth discussion on the theoretical underpinnings of the proposed supervisory signals.  It would be helpful to provide visualizations or case studies showcasing how DeCLIP handles challenging cases or variations in imagetext pairs. Overall based on the innovative approach compelling results and comprehensive experimentation I recommend this paper for acceptance with minor revisions to address the points mentioned above.", "ms7xJWbf8Ku": " Summary of the Paper: The paper introduces efficient packing algorithms to accelerate the pretraining of BERT by up to 2x without compromising accuracy. By exploiting the skewed distribution of sequence lengths in the Wikipedia dataset used for pretraining BERT the authors demonstrate that removing padding tokens can significantly increase the processing speed. They present two packing algorithms SPFHP and NNLSHP which efficiently pack datasets with millions of sequences. Additionally the authors suggest adjustments to the BERT model and hyperparameters to maintain convergence behavior with packed sequences. Experimental results show that the proposed packing methods achieve nearoptimal compression efficiency and speedup while preserving accuracy.  Strengths of the Paper: 1. Innovative Approach: The paper introduces novel packing algorithms that efficiently pack sequences to accelerate BERT pretraining without compromising accuracy. 2. Comprehensive Analysis: The paper provides detailed algorithms model adjustments and hyperparameter suggestions to implement and optimize the packing process effectively. 3. Experimental Validation: The authors back their claims with empirical evidence demonstrating the effectiveness of packing on the Wikipedia dataset and its impact on speedup and convergence behavior.  Weaknesses of the Paper: 1. Complexity: The paper includes intricate technical details and algorithms which may be challenging for readers without a strong background in NLP and machine learning to follow. 2. Limited Benchmarking: While the experiments showcase the effectiveness of the packing algorithms on the Wikipedia dataset more extensive benchmarking on diverse datasets and tasks could further validate the approach. 3. Reproducibility: While the authors provide code for the algorithms in the appendix and on GitHub more emphasis on reproducibility practices like opensourcing the experiment code and data could enhance the papers credibility.  Questions for the Authors: 1. Have you considered further exploring the impact of the proposed packing algorithms on different NLP tasks and datasets beyond the Wikipedia dataset 2. How do the proposed packing methods compare with other stateoftheart approaches in terms of both speedup and accuracy preservation 3. What are the implications of adjusting the hyperparameters for the BERT model when transitioning to a packed sequence processing approach  Feedback to the Authors: 1. Enhance the clarity of the paper by providing more intuition and examples to aid readers in understanding the packing algorithms and their benefits. 2. Consider expanding the discussion section to include potential limitations of the packing approach and suggestions for future research directions. 3. Strengthen the reproducibility aspect by releasing all experiment code datasets and models to promote transparency and facilitate reproducibility by the research community.  Review Summary: The paper presents innovative packing algorithms and model adjustments to accelerate BERT pretraining while maintaining accuracy. Despite its technical complexity the paper provides valuable insights and empirical evidence to support the proposed approach. To enhance the papers impact addressing the weaknesses highlighted and providing more context on the implications of the packing algorithms would strengthen its contribution to the field of natural language processing. Overall the paper showcases a promising methodology for improving the efficiency of pretraining large language models and warrants further exploration and validation.", "lu_DAxnWsh": " Summary: The paper explores the difference in training approaches required for neural networks to excel in System 1 tasks (quick and unconscious) compared to System 2 tasks (conscious multistep reasoning). It argues that traditional training approaches for System 1 tasks are insufficient for System 2 tasks and proposes an alternative method by introducing intermediate computations during training. The paper demonstrates the effectiveness of this approach through theoretical proofs and empirical experiments using Transformers for solving algorithmic tasks like binary addition. The results show that explicitly guiding neural networks through intermediate computations can enhance their performance on algorithmic tasks.  Strengths: 1. Innovative Approach: The paper presents a novel perspective on utilizing intermediate computations to enhance the capabilities of neural networks in tackling System 2 tasks. 2. Theoretical Rigor: The paper provides solid theoretical foundations by proving the generality of the approach and demonstrating the feasibility of using Transformers for algorithmic tasks. 3. Empirical Validation: The empirical experiments support the theoretical claims by showing the effectiveness of the proposed method in training neural networks for binary addition tasks. 4. Clear Explanations: The paper effectively explains the concepts methodologies and results making it accessible to readers with varying levels of expertise in the subject matter.  Weaknesses: 1. Limited Application: The focus on binary addition as the main task limits the generalizability of the findings to a broader range of algorithmic tasks. 2. OverReliance on Transformers: While the use of Transformers is common in current research exploring the effectiveness of other architectures could provide additional insights. 3. Lack of Comparison: The paper does not compare the proposed approach with existing methods for algorithmic tasks which could provide further context and validation.  Questions and Feedback: 1. Regarding Applicability: How transferable do you think the findings regarding binary addition are to other algorithmic tasks especially those involving more complex operations 2. Comparison with Existing Methods: Have you considered comparing the performance of your proposed approach with existing methods that focus on enhancing neural networks for algorithmic tasks 3. Scalability: Do you anticipate any challenges in scaling up the approach to larger and more complex algorithmic tasks and if so how do you plan to address them  Review Summary: The paper presents an innovative approach that addresses the challenge of training neural networks for algorithmic System 2 tasks by introducing intermediate computations. It combines theoretical proofs with empirical experiments using Transformers to demonstrate the effectiveness of guiding neural networks through intermediate steps. While the paper is wellstructured and provides clear explanations of the methodology and results further exploration into the generalizability of the approach comparisons with existing methods and considerations for scaling up would enhance the studys impact and applicability in the field of artificial intelligence and machine learning. Overall I recommend this paper for publication due to its innovative contribution to the intersection of neural networks and algorithmic reasoning.", "y1PXylgrXZ": " Summary of the Paper: The paper introduces the IBPMonDEQ layer which aims to provide certified robustness for deep equilibrium models by computing upper and lower interval bounds on the output. It proposes an IBPinspired fixedpoint equation that guarantees unique fixedpoint solutions under specific parameterizations of the layer. The work compares models with IBPMonDEQ layers to fully explicit models showing comparable `8 certified robustness. Theoretical analyses and experiments validate the effectiveness of the proposed method.  Strengths of the Paper:  Innovative Contribution: The paper introduces a novel approach the IBPMonDEQ layer which addresses the important issue of certified adversarial robustness for DEQ models.  Theoretical Foundation: The work provides a strong theoretical foundation demonstrating the unique fixedpoint existence under specific parameterizations using elaborate mathematical proofs.  Comparison and Validation: Empirical comparisons validate the proposed method against fully explicit models showing competitive `8 certified robustness thus contributing to the practical relevance of the work.  Weaknesses of the Paper:  Complexity: The paper delves into intricate mathematical concepts and derivations making it challenging for readers unfamiliar with the specific models and methodologies.  Limited Practical Discussion: More discussion on the practical implications and potential use cases of IBPMonDEQ layers in realworld applications could provide additional insights.  Experimental Setup: While the experiments demonstrate the effectiveness of the proposed method including more detailed information on hyperparameter settings and convergence behavior could enhance reproducibility.  Questions for the Authors: 1. Can you elaborate on the potential scalability and computational efficiency of the IBPMonDEQ layers for larger and more complex models compared to fully explicit networks 2. How does the proposed method perform under different perturbation types or adversarial attack scenarios beyond `8 perturbations 3. Have you considered assessing the interpretability and generalizability of the IBPMonDEQ layers in comparison to explicit models across various tasks and datasets  Feedback for the Authors: 1. Provide more intuitive explanations and examples to help readers comprehend the complex mathematical derivations and models presented in the paper. 2. Consider discussing the implications of your findings on the broader field of deep learning specifically addressing the advancements in certified robustness for implicit models like DEQs.  Review Summary: The paper presents a novel approach the IBPMonDEQ layer to certify the robustness of deep equilibrium models. With a strong theoretical basis and empirical validation the proposed method showcases promising results in achieving `8 certified robustness comparable to fully explicit models. While the complexity of the technical details may pose challenges for some readers the innovative contributions and rigorous analyses make this work significant for the field of deep learning and certified robustness research.  Recommendation: Based on the innovative methodology theoretical rigor and empirical validation presented in the paper I recommend acceptance for publication after addressing the minor issues related to complexity and practical discussions. The work contributes valuable insights to the field of deep learning and certified robustness methods showcasing potential for wider applicability and impact.", "ptxGmKMLH_": "Summary of the Paper: The paper investigates the effectiveness of a prototype classifier in fewshot learning without the need for training a new linear classifier or using metalearning algorithms. It compares the performance of the prototype classifier when directly constructing prototypes from feature vectors extracted using pretrained models against traditional linear evaluationbased approaches and the prototypical network. The study explores how different feature transformation methods can impact the performance of the prototype classifier focusing on reducing the variance of the norm of feature vectors to improve classification accuracy. The paper presents a novel generalization bound for the prototype classifier and experimentally evaluates several normalization methods to minimize the variance of the norm. Strong Points: 1. Thorough Analysis: The paper provides a comprehensive analysis of the prototype classifier in fewshot learning comparing it with established methods like the prototypical network and linear evaluationbased approaches. 2. Novel Contribution: The derivation of a novel generalization bound and investigation into feature transformation methods to enhance performance are significant contributions to the field. 3. Experimental Evaluation: The experimental evaluation on benchmark datasets like miniImageNet tieredImageNet CIFARFS and FC100 adds credibility to the theoretical findings and helps validate the proposed approaches. 4. Clarity and Detail: The paper is wellstructured providing a detailed explanation of theoretical analyses experimental settings results and comparisons enhancing the understanding of the research. Weak Points: 1. Technical Jargon: The paper contains technical terms and equations that may be challenging for readers not familiar with the domain which could affect the accessibility and understanding of the research. 2. Assumptions: The validity of some assumptions made in the theoretical analysis such as the classconditioned distribution of features and assumptions about feature transformations could impact the practical applicability of the proposed methods. 3. Limited Practical Application: While the theoretical analysis and experimental results are insightful the realworld implementation and scalability of the proposed feature transformation methods may need further exploration. Questions and Feedback to the Author: 1. Can you elaborate on the implications of the variance normalization method performing worse than applying no feature transformation in certain settings 2. How do you plan to address the challenge of translating the theoretical insights and experimental findings into practical applications for realworld fewshot learning scenarios 3. Have you considered investigating the impact of incorporating additional regularization techniques or optimization strategies to further enhance the performance of the prototype classifier Review Summary: The paper presents a detailed investigation into the prototype classifier for fewshot learning offering novel insights and experimental evidence on feature transformation methods to improve classifier performance. While the research is thorough and sheds light on the importance of variance reduction in feature vectors addressing technical complexity and bridging the gap between theoretical analysis and practical application could strengthen the papers impact. Overall the study contributes valuable findings that advance the understanding of fewshot learning methodologies. Further studies can focus on refining the proposed methods for broader applicability and exploring additional optimization strategies for enhanced performance. Based on the papers thorough analysis insightful contributions and experimental validation I recommend acceptance after addressing the outlined concerns and suggestions.", "z8xVlqWwRrK": "Summary: The paper introduces Eventbased Variational Distributions for Exploration (EVaDE) a novel approach designed to facilitate efficient exploration in ModelBased Reinforcement Learning (MBRL) specifically in objectbased domains. EVaDE comprises three types of noisy eventbased convolutional layers inserted into deep neural networks to induce variational distributions over model parameters. These layers aid in generating exploratory trajectories focusing on interactions between objectsevents and positional importance in objectbased domains. The proposed method is empirically evaluated on a suite of Atari games showcasing its effectiveness in achieving higher scores compared to baselines like CURL and vanillaSimPLe within a limited interaction budget. Strengths: 1. Innovative Approach: EVaDE introduces a unique approach by incorporating noisy eventbased convolutional layers into neural networks for exploration providing a novel solution to the explorationexploitation tradeoff in MBRL. 2. Empirical Evaluation: The paper provides a thorough empirical evaluation of EVaDE on a set of Atari games demonstrating improved performance compared to existing baselines in a low data regime showcasing the effectiveness of the proposed method. 3. Theoretical Foundation: The paper includes theoretical insights and proofs to support the design choices and capabilities of EVaDE layers strengthening the understanding of how these layers enable efficient exploration. 4. Ablation Studies: Conducting ablation studies to evaluate the impact of individual EVaDE layers on exploration provides valuable insights into the contribution of each layer and their combined effects. Weaknesses: 1. Complexity: The papers technical nature and intricate details may make it challenging for less technically inclined readers to grasp the methodology fully. 2. Clarity: Some sections of the paper could benefit from improved clarity and organization especially in presenting the methodology and experimental results. 3. Generalization: While the paper demonstrates efficacy on a suite of Atari games more diverse benchmarks or realworld applications could enhance the generalizability of EVaDE across different domains. 4. Future Work: It would be beneficial to discuss potential extensions or limitations of EVaDE in addressing more complex tasks beyond the scope of Atari games. Questions and Feedback to the Author: 1. Scalability: How does the proposed EVaDE method scale with increasing complexity in terms of state and action spaces or in different domains beyond the Atari games 2. Impact of Noisy Layers: Can the author provide more insights into the impact of Gaussian dropouts in the noisy layers and how they influence the exploration strategies in EVaDE 3. Comparative Analysis: How does EVaDE fare in terms of computational efficiency compared to existing methods especially in handling highdimensional state spaces or diverse reward functions 4. Future Directions: Are there plans to extend the evaluation of EVaDE to more complex environments or realworld applications to further validate its efficacy and robustness Review Summary: Overall the paper presents a novel and effective approach EVaDE for exploration in MBRL particularly in objectbased domains. The method showcases promising results in Atari games outperforming existing baselines under restricted interaction budgets. However improvements in presentation clarity generalization across diverse domains and discussions on scalability and efficiency could enhance the impact of EVaDE. The comprehensive theoretical foundation and empirical evaluation strengthen the papers contribution to the field. Therefore with minor revisions and deeper exploration into scalability and efficiency aspects EVaDE has the potential to make a significant impact in reinforcement learning research.", "lbauk6wK2-y": "Summary: The paper introduces a framework called Object Pursuit aimed at continuously learning objectcentric representations through selfsupervised discovery and manipulation of objects. The framework leverages interactions for sampling diverse variations of objects and training signals learning objectcentric representations one by one in random order and preventing forgetting through reidentification mechanisms. The proposed method uses a hypernetwork to generate discriminative weights for object representations and focuses on learning to represent objects efficiently with a lowdimensional manifold. The paper presents an extensive study of the frameworks key features characterization of the learned representations and demonstrates improved label efficiency in downstream tasks. Strengths: 1. Innovative Approach: The paper presents a novel approach to learning objectcentric representations through selfsupervised discovery and manipulation of objects. 2. Comprehensive Experiments: The paper conducts extensive experiments to analyze the proposed framework including studies on pursuit dynamics object reidentification forgetting prevention and label efficiency. 3. Code Availability: The authors provide code and trained models publicly accessible on GitHub enhancing reproducibility and transparency of the research. 4. Detailed Discussions: The paper provides detailed discussions on related works continual learning methods hypernetworks and the methodological aspects of the proposed Object Pursuit framework. Weaknesses: 1. Theoretical Background: The paper could benefit from more theoretical foundations to support the proposed framework and its design decisions. 2. Evaluation Metrics: It would be beneficial to include additional evaluation metrics or benchmarks to provide a broader perspective on the performance of the Object Pursuit framework. Questions and Feedback to the Author: 1. Could you provide more insights into the scalability of the proposed Object Pursuit framework when dealing with a large number of objects in complex scenes 2. How does the Object Pursuit framework compare to existing approaches in terms of computational efficiency and scalability for realworld applications 3. Considering the potential applications of the learned objectcentric representations have you explored scenarios beyond visual tasks where these representations could be beneficial Review Summary: The paper presents a novel framework Object Pursuit for continuously learning objectcentric representations through interactions with individual objects. The comprehensive experiments and detailed discussions demonstrate the effectiveness of the proposed method in improving label efficiency for downstream tasks. The availability of code and trained models enhances the reproducibility of the research. However the paper could be strengthened by providing more theoretical background and expanding the evaluation metrics. Overall based on the innovative approach thorough experiments and code availability I recommend acceptance of the paper with minor revisions to address the discussed points.", "zNR43c03lRy": " Summary: The paper presents a novel approach for semisupervised part segmentation tasks by generating highquality images with a pretrained GAN and labeling them with an automatic annotator. The annotator learns to label object parts in generated images reducing the optimization problem to a gradient matching problem which is solved efficiently with an iterative algorithm. The method outperforms other semisupervised competitors especially in scenarios with a limited amount of labeled examples. The approach allows for a wide range of labeled data including real synthetic and outofdomain data.  Strengths: 1. Innovative Approach: The paper introduces a novel method for semisupervised part segmentation that leverages GANs and automatic annotators showing promising results in scenarios with limited labeled data. 2. Efficient Optimization: By reducing the nestedloop optimization problem to a gradient matching problem the method efficiently learns annotators without the need for manual labeling of generated images. 3. Broad Applicability: The method can utilize a variety of labeled data sources including real synthetic and outofdomain images making it applicable in diverse settings. 4. Performance Improvement: The method outperforms other semisupervised segmentation methods in scenarios with a small number of labeled examples demonstrating its effectiveness in challenging conditions.  Weaknesses: 1. Limited by GAN Performance: The methods performance is limited by the quality of the GAN used especially in modeling complex scenes. 2. SingleClass Focus: The approach focuses on part segmentation for specific classes and avoids multiobject scenes limiting its generalizability to broader segmentation tasks. 3. Complexity and Memory Intensity: While the method reduces the nestedloop optimization problem to a simpler gradient matching problem further discussion on the complexity and memory intensity is warranted. 4. Comparative Analysis: The paper could benefit from a more detailed comparison with existing methods particularly in terms of computational efficiency and scalability.  Questions and Feedback: 1. Scalability: How does the proposed method scale with increasing dataset sizes or more complex scenes Are there any limitations in terms of computational efficiency and memory requirements 2. Generalizability: Can the method be extended to tackle multiobject scenes and broader semantic segmentation tasks beyond specific classes How might it perform in such scenarios 3. Evaluation Metrics: Could the paper provide more details on the choice of evaluation metrics and how the method compares to stateoftheart approaches in terms of computation time and resource requirements 4. Code Availability: Is the implementation of the proposed method available for replication and further experimentation by the research community  Review Summary: The paper presents an innovative approach for semisupervised part segmentation using GANs and automatic annotators showcasing superior performance in scenarios with limited labeled examples. While the method demonstrates efficiency and effectiveness in learning annotators without manual labeling of generated images there are considerations regarding GAN performance limitations scalability to more complex scenes and generalizability to multiobject segmentation tasks. The paper could benefit from a more detailed discussion on computational efficiency scalability and comparisons with existing methods. Overall the method shows promise in advancing semisupervised part segmentation tasks but further investigation is needed for broader applicability and performance in more complex visual settings.", "pgKE5Q-CF2": " 1. Summary of the Paper The paper presents a new recommendation method called NeuronEnhanced Autoencoder Based Collaborative Filtering (NEAECF). NEAECF utilizes an additional neural network designed in an elementwise manner to enhance the reconstruction capability of the autoencoder in collaborative filtering. The paper provides theoretical analysis to investigate the generalization ability of autoencoder and deep learning in collaborative filtering. Empirical results on MovieLens datasets and other benchmarks show promising performance compared to existing methods.  2. Strengths and Weaknesses Questions and Feedback Strong Points:  The paper introduces a novel approach NEAECF which addresses the limitations of existing autoencoderbased collaborative filtering methods.  The theoretical analysis provided for NEAECF offers valuable insights into the generalization ability and prediction performance in collaborative filtering.  Empirical results demonstrate the effectiveness of NEAECF on benchmark datasets showcasing its stateoftheart performance. Weak Points:  The paper lacks a detailed comparison with a wider range of stateoftheart methods in collaborative filtering. Including comprehensive comparative experiments would strengthen the validity of the proposed method.  While the theoretical analysis is comprehensive some sections may benefit from clearer explanations for readers less familiar with the underlying concepts.  Further discussions on the scalability and computational complexity of NEAECF could enhance the practical relevance of the proposed method. Questions and Feedback to the Authors: 1. Can you provide more insights into the computational efficiency and scalability of NEAECF especially when dealing with large datasets 2. How does NEAECF perform in scenarios with extremely sparse data or in the presence of cold start problems Have you evaluated its robustness in such situations 3. Considering the potential applications of NEAECF beyond collaborative filtering do you plan to explore its adaptability to other recommendation systems or domains  3. Review Summary The paper introduces NEAECF a novel method in collaborative filtering that combines an elementwise neural network with an autoencoder to enhance recommendation accuracy. The theoretical analysis and empirical results validate the superiority of NEAECF in terms of prediction performance over existing methods. While the paper provides significant contributions to the field further exploration on its computational aspects and robustness is recommended to enhance its practical applicability. Overall the innovative approach and comprehensive validation make NEAECF a promising recommendation method in collaborative filtering.", "p-BhZSz59o4": " Peer Review for \"BEIT: Bidirectional Encoder representation from Image Transformers\"  1. Summary: The paper introduces a selfsupervised vision representation model called BEIT which stands for Bidirectional Encoder representation from Image Transformers. Inspired by the success of BERT in natural language processing the authors propose a new pretraining task called masked image modeling (MIM) to pretrain vision Transformers. The model utilizes image patches and visual tokens to recover the original image by predicting masked image patches. The paper demonstrates competitive results in image classification and semantic segmentation tasks compared to previous pretraining methods. Extensive experiments highlight the effectiveness of BEIT in leveraging selfsupervised learning for vision representation.  2. Strengths:  Innovative Approach: Introducing a masked image modeling task for pretraining vision Transformers is a novel and promising approach.  Theoretical Explanation: Providing a theoretical explanation from the perspective of variational autoencoder adds depth to the methodology.  Comprehensive Experiments: Conducting extensive experiments on downstream tasks and including ablation studies demonstrate a thorough evaluation of the model.  Empirical Results: Achieving competitive results in image classification and semantic segmentation highlights the effectiveness of the proposed model.  Efficiency: Demonstrating improvements in convergence speed and stability of finetuning reduces training costs on end tasks making the model practical.  3. Weaknesses:  Clarity: Some sections are dense and could benefit from simplification or additional visual aids to enhance reader understanding.  Comparison: While the performance of BEIT is compared with previous methods it would be beneficial to provide a more detailed discussion on the specific advantages of BEIT over each method.  Data Size and Model Size: The authors mention scaling up BEIT in terms of data and model size in the future but it would be helpful to provide more insights into potential challenges and considerations for such scaling.  Feedback and Questions: 1. Further Analysis: Additional analysis comparing BEIT with other stateoftheart vision Transformers on specific tasks can provide deeper insights into the models strengths. 2. Data Efficiency: Could the authors elaborate on the potential implications of BEIT in scenarios with limited labeled data such as in transfer learning tasks 3. Robustness Testing: Have robustness tests been conducted to evaluate the models performance under different noise levels or perturbations in input data  Review Summary: The paper presents an innovative selfsupervised vision representation model BEIT showing competitive results in image classification and semantic segmentation tasks. With a wellstructured methodology and comprehensive experiments the paper convincingly demonstrates the effectiveness of BEIT in pretraining vision Transformers. However further clarity in certain sections more detailed comparisons with existing methods and insights into scalability challenges would enhance the depth and impact of the study. Considering the models innovative approach comprehensive experiments and promising results I recommend the paper for publication pending minor revisions to address the identified weaknesses. The contributions and potential impact of BEIT on advancing selfsupervised learning in computer vision make it valuable for the research community.", "mZsZy481_F": " Summary of the Paper: The paper introduces the Fewshot ROBust (FROB) model for classification and OutofDistribution (OoD) detection in the fewshot setting. FROB aims to address the challenges of rarity and limited samples in fewshot settings as well as adversarial attacks. The model proposes a methodology for improved robustness and reliable confidence prediction for fewshot OoD detection. It combines the support boundary of the normal class distribution with fewshot Outlier Exposure (OE) and implements a selfsupervised learning approach for confidence boundary methodology based on generative and discriminative models. The main contribution of FROB is the utilization of the generated boundary in a selfsupervised manner to impose low confidence at this learned boundary which improves robustness for fewshot OoD detection. The paper demonstrates that FROB achieves generalization to unseen anomalies and adversarial attacks maintaining competitive indistribution accuracy.  Strong Points: 1. Innovative Approach: The paper presents a novel methodology FROB for fewshot OoD detection by incorporating a selfsupervised learning approach and generating boundaries to improve robustness. 2. Robustness Analysis: The evaluation of FROB on different datasets demonstrates its robustness to the number of fewshot samples outperforming benchmarks. 3. Performance Improvement: FROB shows competitive performance and outperforms benchmarks in terms of robustness to OoD samples and variability as shown through AUCtype metrics comparison. 4. Effective Boundary Generation: The effectiveness of using the generated boundary (O(z)) is highlighted in enhancing the fewshot OoD detection performance even with a small number of fewshot samples.  Weak Points: 1. Complex Terminology: The paper uses technical language that may be challenging for readers not wellversed in the field potentially hindering broader understanding. 2. Evaluation Details: More details on the experimental setup training procedures and computational complexity required for implementing FROB could provide a clearer understanding of its practical application. 3. Comparative Analysis: While the paper compares FROB to various benchmarks more indepth analysis and discussion on the limitations or shortcomings of existing approaches could strengthen the paper.  Questions to the Authors: 1. Can you provide more insights into the training process of FROB and its computational requirements 2. How does FROB perform in scenarios with highly imbalanced class distributions or in the presence of noisy data 3. Have you considered deploying FROB in realworld applications and if so what are the challenges or considerations for practical implementation  Feedback to the Authors: 1. Consider providing more detailed explanations and examples to aid understanding especially for readers unfamiliar with the technical jargon. 2. Enhance the discussion on the implications of the results in realworld scenarios to highlight the significance and potential applications of FROB. 3. Strengthen the paper by addressing potential limitations or areas for future research to further improve the models effectiveness.  Review Summary: The paper presents an innovative approach FROB for fewshot OoD detection demonstrating competitive performance and robustness compared to benchmarks. While the methodology and results exhibit promise in addressing the challenges of fewshot settings and adversarial attacks further clarity in explanations and deeper analysis of experimental findings would enhance the overall impact and understanding of FROB. Considering the strengths and potential impact of the approach with the authors addressing the outlined recommendations I recommend acceptance of the paper pending minor revisions and clarifications to strengthen its impact and contribution to the field.", "xP3cPq2hQC": "Peer Review 1) Summary: The paper introduces GromovWasserstein Imitation Learning (GWIL) a method for crossdomain imitation learning that leverages the GromovWasserstein distance to align and compare states between different spaces of agents. It addresses the challenge of comparing trajectories and stationary distributions between expert and imitation agents living in distinct systems. The paper relaxes the assumptions of CrossDomain IL and proposes a benchmark and method that does not require access to proxy tasks. The authors demonstrate the effectiveness of GWIL in various continuous control domains showing promising results in nontrivial settings. 2) Strengths:  The paper introduces a novel approach GWIL for crossdomain imitation learning expanding the applicability of IL to different morphologies.  The theoretical foundation of optimal transport distances and the use of GromovWasserstein for alignment is wellexplained and justified.  The experiments conducted are thorough covering tasks with varying degrees of transformation between expert and agent domains.  The paper highlights possibilities and limitations of GWIL providing valuable insights for future research. Weaknesses:  The paper could benefit from a more detailed comparison with existing methods in the field of imitation learning.  Providing a more extensive discussion on the computational complexity and scalability of GWIL would further enhance the understanding of its practical implications.  The implications of potential failure cases or scenarios where GWIL may not be as effective need to be addressed to provide a more holistic view of the methods performance. Questions and Feedback:  Are there specific scenarios or domains where GWIL may not perform as well How does the method handle complex environments with highdimensional state and action spaces  Could the paper include a more extensive discussion on the computational efficiency and scalability of GWIL in realworld applications  Considering the limitations mentioned in the paper how do you envision overcoming or mitigating these challenges in future implementations of GWIL 3) Review Summary: The paper presents an innovative approach GWIL for crossdomain imitation learning using optimal transport distances. The theoretical foundation and experimental results provide strong support for the methods efficacy in various continuous control domains. While the paper offers valuable insights into the possibilities and limitations of GWIL additional discussion on the methods comparison with existing approaches and practical implications as well as addressing potential failure cases would further strengthen the analysis. Overall the paper showcases the potential of GWIL in expanding the applicability of imitation learning to diverse agent morphologies and environments making it a valuable contribution to the field. Recommendation: Strong Acceptance.", "hm2tNDdgaFK": " 1) Summary of the Paper: The paper presents a novel neural framework named Chiral InterRotoInvariant Neural Network (ChIRo) designed to enable representation learning on molecules with defined stereochemistry. ChIRo processes torsion angles of 3D molecular conformers explicitly modeling conformational flexibility by integrating an invariance to rotations about internal molecular bonds into the architecture. The paper explores multiple tasks to benchmark ChIRos ability to learn the effects of chirality and compares it with 2D and 3D graph neural networks (GNNs).  2) Strengths and Weaknesses of the Paper with Questions and Feedback:  Strong Points:  Innovative Approach: The paper introduces a novel neural framework that explicitly models tetrahedral chirality and conformational flexibility addressing a significant gap in the current literature.  Comprehensive Evaluation: The paper conducts a thorough evaluation of ChIRo across multiple tasks demonstrating its stateoftheart performance in learning chiral representations from 3D molecular structures.  Clear Explanations: The paper provides clear explanations of the methodology used including the design of the network components and the rationale behind them.  Weak Points:  Lack of Comparison to More Baselines: While the paper compares ChIRo with some 2D and 3D GNN baselines a broader comparison with additional stateoftheart methods could provide a more comprehensive analysis.  Challenging Technical Details: The technical details provided especially regarding the torsion encoder may be challenging for readers unfamiliar with molecular stereochemistry and neural network architectures.  Questions for the Authors: 1. Can you provide more insights into the computational efficiency of ChIRo compared to other GNN models especially when dealing with large datasets or complex molecular structures 2. How does the performance of ChIRo scale with the size and complexity of the molecules being analyzed Have you encountered any limitations in terms of scalability or generalizability 3. Are there any specific parameters or hyperparameters that have a significant impact on ChIRos performance that you would recommend tuning for different tasks or datasets  Feedback for the Authors:  The paper could benefit from including more visual aids or diagrams to help readers understand the complex concepts such as the torsion angles and their processing more intuitively.  Clarify and simplify the technical jargon where possible to make the paper more accessible to a broader audience.  Consider expanding the discussion section to provide deeper insights into the implications of the studys findings and potential future research directions.  3) Review Summary: The paper presents a novel neural network model Chiral InterRotoInvariant Neural Network (ChIRo) designed to learn chiral representations from 3D molecular structures. The innovative approach of explicitly modeling tetrahedral chirality and conformational flexibility sets ChIRo apart from existing models. The comprehensive evaluation demonstrates ChIRos superior performance in tasks dependent on molecular chirality showcasing its stateoftheart capabilities. While the paper could benefit from a broader comparison with additional baselines and simplification of technical details for better reader comprehension the significant contributions and promising results of ChIRo make it a strong candidate for publication. Overall Recommendation: Strongly Recommend Acceptance. The papers novel approach thorough evaluation and potential impact on the field make it a valuable contribution to the scientific community.", "xf0B7-7MRo6": " PeerReview:  1) Summary: The paper presents a novel model named AIRNet which combines adaptive and implicit lowrank regularization for matrix completion problems. The model dynamically adjusts the regularization according to the recovered matrix addressing the challenge of accurately encoding priors for complex natural signals. The adaptive regularization vanishes at the end preventing saturation issues. Theoretical analyses and numerical experiments demonstrate the effectiveness of AIRNet especially when dealing with nonrandomly chosen missing elements.  2) Strengths and Weaknesses:  Strong Points:  Innovation: The combination of adaptive and implicit regularization is a novel approach to address the challenges of matrix completion.  Theoretical Foundation: The paper provides rigorous theoretical analyses to support the effectiveness of the proposed model.  Experimental Validation: Extensive numerical experiments showcase the superiority of AIRNet in various data types and missing patterns.  Flexibility: AIRNet offers flexibility in selecting neural networks for matrix representation extending its applicability to solve general inverse problems.  Weak Points:  Complexity: The model architecture and mathematical formulations might be challenging for readers unfamiliar with advanced regularization methods and neural networks.  Experimental Reproducibility: Detailed information on hyperparameters selection and implementation nuances could enhance the reproducibility of the experimental results.  Questions and Feedback to the Author: 1. Model Interpretability: Can you provide insights into the interpretability of the learned regularization weights or matrices Lr and Lc How can researchers or practitioners interpret the adaptive regularization learned by AIRNet 2. Scalability: Are there specific scalability considerations or limitations when applying AIRNet to larger datasets or more complex matrix completion scenarios 3. Comparison with Baselines: The paper mentions comparisons with existing methods like KNN SVD PNMC DMF and RDMF. Could you elaborate on the specific advantages of AIRNet over these baseline methods in terms of performance adaptability or computational efficiency  3) Review Summary: The paper introduces a cuttingedge model AIRNet which intelligently combines adaptive and implicit regularization for matrix completion tasks. The theoretical analyses and extensive experiments confirm the models effectiveness in recovering matrices with nonrandomly chosen missing elements. The innovative approach coupled with strong theoretical foundation and robust experimental validation positions AIRNet as a promising solution for complex matrix completion problems. Despite some complexities in the model description and experiment reproducibility concerns the significant contributions of AIRNet to the field of regularization techniques and matrix completion warrant consideration for publication. Overall the paper presents a wellstructured and informative study offering important insights into the development of regularization models for complex signals. The additional clarifications and improvements suggested can further enhance the impact and understanding of the proposed AIRNet model.", "xwAw8QZkpWZ": " Summary of the Paper The paper introduces SAFER a behavioral prior learning algorithm designed to accelerate policy learning on complex control tasks under safety constraints. SAFER utilizes a latent safety variable to efficiently generalize safe behaviors for policy learning in safetycritical environments. The algorithm is evaluated on a suite of safetycritical robotic grasping tasks inspired by the Operation game.  Strong Points  Innovative Approach: SAFER introduces a novel framework that leverages a latent safety variable to promote safe policy learning in complex control tasks.  Empirical Evaluation: The paper provides a comprehensive evaluation of SAFER on a set of safetycritical robotic grasping tasks demonstrating its effectiveness in achieving higher success rates and enforcing safety compared to baseline methods.  Safety Assurance Calibration: The study on the calibration of safety assurances shows that SAFERs safety bounds are wellcalibrated ensuring a good balance between success and safety.  Weak Points  Complexity: The paper delves into intricate technical details making it challenging for readers unfamiliar with the field to grasp the concepts effectively. Simplification of the language or additional explanatory examples could enhance the readability.  Experimental Validation: While the evaluation results show the effectiveness of SAFER further exploration on a wider range of tasks and comparison with more diverse baseline methods could strengthen the conclusions.  Question and Feedback to the Author  Question: How does SAFER handle situations with dynamic or evolving safety constraints such as in environments where safety requirements change over time  Feedback: Including visual aids or descriptive examples could help better illustrate the key concepts and make the paper more accessible to a wider audience.  Review Summary The paper introduces SAFER a novel behavioral prior learning algorithm that accelerates safe policy learning in complex control tasks. The empirical evaluation showcases SAFERs superior performance in promoting safe policies and achieving high success rates. The study on safety assurance calibration further strengthens the credibility of SAFERs safety guarantees. Despite the complexity of the technical details the innovative approach and promising results make SAFER a valuable contribution to the field of safe reinforcement learning. Overall considering the strong points and potential areas of improvement I recommend accepting this paper for publication pending minor revisions and clarification to enhance readability and understanding.", "pjqqxepwoMy": " Summary The paper presents a novel framework called Variational Latent Oracle Guiding (VLOG) for deep reinforcement learning (DRL) to leverage oracle observations to facilitate learning in partially observable environments. The work introduces a latent vector to represent the environmental state and models the prior and posterior distributions of the latent vector using executor and oracle observations respectively. A detailed algorithm is proposed for VLOG which provides a taskagnostic method for automatically adjusting the hyperparameter beta in the loss function. Empirical evaluations demonstrate the effectiveness of VLOG in various tasks including maze navigation MiniAtar video games and Mahjong both in online and offline RL scenarios.  Strengths 1. Innovative Concept: The idea of leveraging oracle observations using a Bayesian framework is novel and addresses an important aspect in RL. 2. Rigor: The paper outlines a clear problem definition introduces a theoretical basis and provides a structured solution with a detailed algorithm. 3. Empirical Evaluation: The experimental evaluation covers a diverse range of tasks and scenarios demonstrating the effectiveness of VLOG and comparing it against existing methods. 4. Reproducibility: The paper offers detailed information on network structures hyperparameters data augmentation and the taskagnostic latent bottleneck control method enhancing reproducibility.  Weaknesses 1. Clarity of Exposition: While the paper is comprehensive the complexity of the concepts introduced could be challenging for readers not familiar with Bayesian methods and DRL. 2. Experiment Limitations: The evaluations are primarily focused on showcasing the performance of VLOG and there is limited discussion on potential failure cases or limitations. 3. Generalization: The paper focuses on specific tasks like Maze MiniAtar and Mahjong and could benefit from discussing applicability to a wider range of environments.  Questions for the Authors 1. Have you considered any indepth analysis on why VLOG outperforms other oracleguiding methods in the MiniAtar environment apart from the stochastic modeling advantage 2. How sensitive is the performance of VLOG on MiniAtar to the choice of the taskagnostic method for adjusting beta particularly in comparison to a linear or exponential scheduler 3. Considering the unique challenges of Mahjong are there any plans to study the scalability and adaptability of VLOG across other complex imperfectinformation games or reallife applications  Feedback  The paper would benefit from a clearer presentation of results and discussions to emphasize the key insights gained from the empirical evaluation.  Providing a discussion section focused on potential failure cases or circumstances where VLOG may not be suitable would enhance the completeness of the work.  Recommendation Overall I recommend accepting this paper with minor revisions focusing on enhancing clarity for readers less familiar with Bayesian theory and DRL concepts and incorporating a more detailed discussion on the generalization potential and limitations of VLOG. The work showcases a promising framework that can advance research in leveraging oracle observations for improved decisionmaking in RL.", "kOu3-S3wJ7": "Summary of the Paper: The paper introduces GRIN a novel graph neural network architecture for multivariate time series imputation. The proposed method aims to reconstruct missing data by learning spatiotemporal representations through message passing in sequences of graphs where edges represent relationships among different channels. GRIN outperforms stateoftheart methods in imputation tasks on realworld benchmarks showing mean absolute error improvements often exceeding 20. Strong Points: 1. Innovative Approach: The paper introduces a novel framework using graph neural networks for multivariate time series imputation addressing a significant challenge in realworld applications. 2. Empirical Evaluation: The empirical results demonstrate that GRIN achieves stateoftheart performance across different datasets showcasing its effectiveness in various scenarios. 3. Bidirectional Model: The bidirectional architecture enhances the imputation performance showing the importance of considering both forward and backward dynamics in the imputation process. 4. Ablation Study: The ablation study provides valuable insights into the contribution of different components of the proposed model highlighting the importance of the spatial decoder and bidirectional architecture. 5. Virtual Sensing Experiment: The virtual sensing experiment further demonstrates the capability of GRIN in inferring trends for unseen sensors showcasing the applicability and generalizability of the proposed method. Weak Points: 1. Complexity: The complexity of the model architecture may hinder its interpretability and practical deployment in some realworld settings. 2. Training Time: The models training time might be significantly high especially on large datasets which could limit its scalability to handle realtime applications. 3. Generalization: The paper could provide more clarity on how well the proposed method generalizes to different types of datasets and applications beyond the ones considered in the study. Questions for the Author: 1. Could you provide insights into the interpretability of the GRIN model and how it handles complex spatiotemporal dependencies in realworld scenarios 2. How does the training time of the model scale with dataset size and complexity and what strategies are in place to mitigate potential scalability issues Feedback for the Author: 1. Consider providing more details on the practical implications of implementing GRIN in realworld applications and insights into how the model can be enhanced for better performance in diverse scenarios. 2. It would be beneficial to discuss potential limitations of the model and provide guidance on optimizing hyperparameters and training strategies to improve efficiency and generalization. Review Summary: The paper introduces an innovative graph neural network architecture GRIN for multivariate time series imputation demonstrating superior performance over stateoftheart methods on various datasets. Despite its complexity and potential scalability issues the bidirectional model and ablation study showcase the importance of spatial decoding in enhancing imputation accuracy. The results from the virtual sensing experiment further validate the models ability to infer trends for unseen sensors. Addressing questions of interpretability training time scalability and generalization will further enhance the impact and applicability of GRIN in realworld settings. Overall the paper presents a robust and promising approach in the field of time series imputation with significant potential for practical applications.", "lgOylcEZQgr": " Summary: The paper introduces an online unsupervised prototypical network (OUPN) for learning visual representations and object categories simultaneously in an unsupervised manner. The model leverages a prototypebased memory network with a control component that decides when to create new class prototypes. The proposed method is evaluated on different datasets including RoamingRooms RoamingOmniglot and RoamingImageNet showcasing its superior performance compared to stateoftheart selfsupervised learning methods like SimCLR SwAV and SimSiam.  Strengths: 1. Innovative Approach: The paper addresses the challenging problem of online unsupervised representation learning in nonstationary environments offering a unique solution through the OUPN model. 2. Simultaneous Learning: The model achieves simultaneous learning of visual representations and object categories overcoming the shortcomings of existing selfsupervised learning methods. 3. Flexibility: The proposed online clustering procedure is flexible and adaptive allowing the model to create new clusters onthefly with minimal examples. 4. Evaluation: Comprehensive evaluation on diverse datasets demonstrates the robustness and effectiveness of the proposed method showcasing superior performance compared to competitive selfsupervised learning methods.  Weaknesses: 1. Complexity: The models architecture and inference processes involve several intricate steps which may make it challenging for implementation and understanding by practitioners. 2. Assumption Violation: The paper points out the limitations of existing selfsupervised learning methods in nonstationary settings but more detailed insights on the limitations and shortcomings of OUPN are needed for a comprehensive analysis. 3. Evaluation Metrics: While the evaluation metrics used in the experiments are extensive additional insights into the models performance on specific challenging scenarios could enhance the papers robustness.  Questions  Feedback to the Author: 1. Can you provide more details on the limitations or challenges faced by the proposed OUPN model How does the model handle scenarios with high variability or noise 2. Could you elaborate on the interpretability of the learned representations and prototypes in realworld applications How can the models outputs be effectively utilized by downstream tasks 3. Considering the success of the OUPN model in nonstationary environments how scalable is the model in terms of computational resources and training time 4. It would be beneficial to include discussions on the potential realworld applications of the OUPN model and how it could revolutionize unsupervised learning in dynamic environments.  Review Summary: The paper presents a novel approach the OUPN model for online unsupervised representation learning in nonstationary environments. The models ability to learn visual representations and object categories simultaneously sets it apart from existing selfsupervised learning methods. The comprehensive evaluation on diverse datasets demonstrates the models robustness and superior performance. Despite some complexity in the architecture and inference processes the innovative nature of the model and its potential for practical applications make it a valuable contribution to the field. Therefore I recommend accepting this paper for publication.", "kAa9eDS0RdO": "Peer Review 1) Summary: The paper proposes the ConceptTransformer a deep learning module that generalizes attention from lowlevel features to highlevel concepts for interpretability in classification tasks. The ConceptTransformer utilizes crossattention between input features and domainrelevant concepts to provide plausible and faithful explanations for model predictions. The study evaluates the module on three benchmark datasets showcasing its effectiveness in improving accuracy and extracting conceptbased explanations. Results demonstrate the benefits of incorporating domain knowledge through highlevel concepts in enhancing performance and interpretability. 2) Strengths:  The paper addresses a crucial problem of interpretability in deep learning models proposing a novel ConceptTransformer module to provide meaningful explanations for model predictions.  The approach is wellmotivated utilizing domainrelevant highlevel concepts to enhance the interpretability of attention mechanisms in deep learning models.  The experiments are comprehensive evaluating the ConceptTransformer on various image benchmark datasets and showcasing improvements in accuracy and interpretability.  The paper provides detailed explanations of the model architecture training procedure and evaluation results making it easy for readers to understand and replicate the findings.  The proposed module offers a versatile and flexible solution that can be integrated with different deep learning classifiers without significant changes to the training pipeline. Weaknesses:  While the paper extensively discusses the benefits and design of the ConceptTransformer more detailed comparisons with existing interpretable models could further strengthen the evaluation.  The paper could benefit from a more indepth discussion on the scalability and generalizability of the proposed approach to different types of datasets and applications.  Further analysis on the potential limitations or failure cases of the ConceptTransformer in complex scenarios could provide a more thorough understanding of its performance under diverse conditions. Questions and Feedback: 1. Can you provide more insights into how the ConceptTransformer handles complex and nuanced concepts that may not align directly with known domain knowledge 2. How does the ConceptTransformer perform when integrated with more complex deep learning architectures beyond the ones mentioned in the paper Have you explored its compatibility with stateoftheart models in various domains 3. Could you discuss any challenges or limitations encountered during the development and evaluation of the ConceptTransformer and how they were addressed 4. How do you envision the ConceptTransformer contributing to interpretability in realworld applications beyond the benchmark datasets used in this study 3) Review Summary: The paper presents a novel and promising approach the ConceptTransformer for enhancing the interpretability of deep learning models through the use of highlevel concepts. The proposed module demonstrates significant improvements in accuracy and provides faithful and plausible explanations for model predictions. While the paper is wellstructured and detailed in its methodology and results further discussions on scalability comparison with existing models and realworld applicability could enrich the study. Overall the ConceptTransformer presents a valuable contribution to the field of interpretable deep learning and aligns well with the research goal of enhancing model transparency and domain understanding. Recommendation: Based on the innovative approach comprehensive evaluation and potential impact on model interpretability I recommend the acceptance of the paper with minor revisions addressing the mentioned points for further clarity and depth.", "jT5vnpqlrSN": " Summary: The paper proposes the Graph Inference Representation (GIR) framework to enhance the encoding of position information within graph neural networks (GNNs). The GIR model follows the anchorbased approach and aims at conveying distance information implicitly while propagating messages in MPNNs. The framework is designed to improve the positionaware embeddings of nodes in graphs and experimental results show its superiority over existing positionaware GNN methods.  Strengths: 1. Innovation: The paper introduces a novel approach the GIR framework which addresses the limitations of existing methods by conveying distance information implicitly in MPNN message passing. 2. Theoretical Foundation: The paper provides clear theoretical implications especially in relation to the BellmanFord algorithm and the concept of indicatability which supports the proposed methodology. 3. Experimental Results: The empirical evaluation on synthetic and realworld datasets demonstrates the effectiveness of the GIR framework in improving positionaware embeddings compared to existing methods. 4. Comprehensive Experiments: The experiments include various settings and comparisons with baselines showcasing the versatility and superiority of the GIR framework in positionaware tasks. 5. Clear Structure: The paper is wellorganized with detailed explanations of the proposed GIR framework methodology and experimental setup.  Weaknesses: 1. Complexity: The paper delves into detailed theoretical discussions and concepts which might make it challenging for readers without a strong background in graph neural networks and algorithms. 2. Clarity in Methodology: While the paper provides a comprehensive explanation of the GIR framework some sections could be further simplified for easier understanding by a broader audience. 3. Limitation of Experiments: The experiments are conducted on relatively smallscale datasets limiting the generalizability of the results to larger more complex graph structures.  Questions and Feedback to the Author: 1. Could you provide more insights into how the GIR framework could be scaled or adapted to handle larger and more complex graph structures in practical scenarios 2. Can you elaborate on the computational efficiency of the GIR framework compared to existing positionaware GNN methods particularly in terms of scalability and training time 3. It would be beneficial to include a discussion on the interpretability of the learned embeddings using the GIR framework and how insights from these embeddings could be used for downstream graph tasks.  Review Summary: The paper presents a novel framework the Graph Inference Representation (GIR) for enhancing positionaware embeddings in graph neural networks. The theoretical foundations are solid and the experimental results demonstrate the effectiveness of the proposed methodology. Considering the strong innovation theoretical grounding and promising experimental results I recommend accepting this paper with minor revisions to improve clarity and potentially conduct further experiments on largerscale datasets to enhance the generalizability of the findings.", "xCVJMsPv3RT": " Peer Review:  1) Summary of the Paper: The paper proposes a method called DroQ a variant of REDQ to enhance the computational efficiency of randomized ensembled double Qlearning (REDQ) in continuousaction reinforcement learning benchmarks. DroQ achieves improved computational efficiency by utilizing a small ensemble of dropout Qfunctions in place of a large Qfunction ensemble used in REDQ. Experimental results demonstrate that DroQ attains comparable sample efficiency as REDQ superior computational efficiency to REDQ and comparable computational efficiency to Soft ActorCritic (SAC).  2) Evaluation of the Paper:  Strong Points: 1. Innovative Contribution: The proposed DroQ method presents a novel approach to addressing the computational efficiency limitations of REDQ leading to improved efficiency without compromising sample efficiency. 2. Experimental Rigor: The paper provides a detailed experimental evaluation that compares DroQ with baseline methods showcasing its performance across various benchmarks and illustrating its efficiency gains. 3. Simplicity: DroQs simplicity of implementation by incorporating dropout and layer normalization makes it easy to replicate and extend. This can be appealing for practical applications and further research.  Weak Points: 1. Clarity and Structure: Although the paper is informative the extensive technical details might make it challenging for all readers to understand the methodology especially those not deeply familiar with reinforcement learning concepts. Simplifying the language and providing more intuitive explanations could enhance accessibility. 2. Comparison and Baselines: While the paper compares DroQ with other baseline methods providing more insights into the specific scenarios or tasks where DroQ outperforms or underperforms compared to each baseline method could further enhance the analysis.  Questions: 1. Integration into Practical Applications: How adaptable is DroQ to realworld RL applications particularly those with constraints on computational resources like edge devices or scenarios requiring rapid experimentation 2. Scalability and Generalizability: Does DroQs enhanced computational efficiency hold across a wider range of environments and tasks or are there specific conditions where it might face challenges  Feedback to the Author: 1. Enhanced Explanation: Consider simplifying the technical descriptions and providing more intuitive examples to aid readers in understanding the methodology and its significance. 2. Comparative Analysis: Provide deeper insights into the strengths and limitations of DroQ compared to each baseline method highlighting specific use cases where DroQ excels.  3) Review Summary: The paper presents an innovative approach DroQ to enhance the computational efficiency of REDQ in continuousaction reinforcement learning. The experimental results showcase DroQs effectiveness in achieving computational efficiency improvements while maintaining comparable sample efficiency with REDQ. While the paper demonstrates strong technical expertise and experimental rigor enhancements in clarity comparative analysis with baselines and insights into realworld applicability would further strengthen the contribution of DroQ to the field of reinforcement learning. Hence with some refinements in presentation and additional comparative insights the paper holds promise for advancing the efficiency of RL methods.  By addressing the listed points the paper stands to foster a deeper understanding of DroQs efficacy and practical applicability in varied reinforcement learning contexts.", "uqBOne3LUKy": " Summary: The paper investigates the compatibility of importance weighting with overparameterized neural networks in handling distribution shifts. It challenges the notion that importance weighting is ineffective due to overparameterization proposing that the choice of loss function plays a crucial role. The authors introduce polynomiallytailed losses and demonstrate that they restore the effects of importance reweighting in correcting distribution shifts in overparameterized models. The theoretical analysis characterizes the behavior of gradient descent on reweighted polynomiallytailed losses and provides insights into the advantages of using biased importance weights. Empirical experiments on both label shift and subpopulation shift datasets validate the effectiveness of the proposed approach outperforming reweighted crossentropy and matching or exceeding stateoftheart methods.  Strengths: 1. Theoretical Contributions: The paper provides a strong theoretical foundation analyzing the behavior of gradient descent on polynomiallytailed losses and demonstrating the impact of importance weights. 2. Novel Approach: Introducing polynomiallytailed losses as a remedy for importance weighting issues is innovative and addresses a critical problem in distribution shift correction. 3. Empirical Validation: The experimental results support the theoretical claims showing significant improvements in test accuracy compared to existing methods. 4. Comprehensive Analysis: The paper covers a wide range of topics from theoretical results to empirical evaluations providing a holistic view of the proposed approach.  Weaknesses: 1. Assumptions and Complexity: The theoretical analysis involves several assumptions and intricate mathematical formulations which may pose challenges for some readers in understanding the core concepts. 2. Experimental Setup: The experiments focus on specific datasets and scenarios limiting the generalizability of the findings to a broader range of applications. 3. Technical Jargon: While the paper is aimed at a specialized audience some sections may be difficult to follow for readers without a strong background in machine learning theory.  Questions: 1. How would you address the scalability of your proposed method to larger more complex datasets or realworld applications 2. Have you considered the potential drawbacks or limitations of using polynomiallytailed losses in certain scenarios or with specific types of data distributions  Feedback: The paper presents a compelling argument for the compatibility of importance weighting with overparameterized neural networks offering a novel perspective on addressing distribution shifts. The theoretical analysis is thorough and the empirical validation lends strong support to the proposed approach. To enhance the paper consider providing more insights into the practical implications of using polynomiallytailed losses in realworld settings and discussing the tradeoffs or challenges that may arise when applying this methodology. Clarifying the implications of the findings for practitioners and researchers in the field would further strengthen the impact of your work.", "uoBAKAFkVKx": " Summary: The paper introduces a novel black box optimization technique called Hypothesistest Driven Coordinate Ascent (HDCA) for learning policies in stochastic environments without computing or estimating gradients. HDCA combines coordinate ascent with hypothesis testing and performs well in distributed settings. The approach was evaluated on various problems from the MuJoCo physics simulator and OpenAI Gym framework showcasing equivalent or superior results to standard reinforcement learning benchmarks.  Strengths: 1. Innovative Approach: HDCA introduces a novel method that does not rely on gradients making it suitable for stochastic environments and distributed settings. 2. Efficiency: The paper highlights the scalability of HDCA demonstrating quicker convergence with fewer iterations compared to other methods which is beneficial for training time. 3. Empirical Analysis: The experiments conducted on different RL environments provide a comprehensive evaluation of the proposed techniques performance and robustness. 4. Clear Presentation: The paper is wellstructured offering detailed insights into the methodology experiments and comparison with existing techniques.  Weaknesses: 1. Complexity: While the approach is innovative the use of hypothesis testing and coordinate ascent might introduce complexity for readers unfamiliar with these concepts. Providing more intuitive explanations could improve understanding. 2. Generalization: The evaluation focuses on specific RL environments and it would be beneficial to showcase the techniques performance across a broader range of diverse tasks for a more comprehensive assessment. 3. Comparison Methods: While the comparison with existing techniques like ES and ARS is insightful a more detailed discussion on the limitations of these methods and how HDCA addresses them would enhance the papers clarity.  Questions  Feedback: 1. Clarification on Hypothesis Testing: Could you provide a more intuitive explanation of how hypothesis testing is incorporated in the HDCA approach for policy updates 2. Scalability Concerns: Have you considered the impact of scaling HDCA to more complex or larger domains in terms of computational efficiency and convergence speed 3. Robustness Evaluation: How does HDCA perform in terms of robustness to variations in the environment and noise levels compared to gradientbased approaches  Review Summary: The paper presents a novel gradientfree optimization technique HDCA for learning policies in stochastic environments showcasing promising results in various RL tasks. The innovative approach efficient convergence and clear experimental validation make HDCA a valuable addition to the field of reinforcement learning. However efforts to simplify complex concepts further generalizing the evaluation and addressing scalability concerns would enhance the papers impact. Overall the paper is wellstructured methodologically sound and provides valuable insights for researchers in the field.  Recommendation: Based on the strengths weaknesses and the insightful contribution of the paper I recommend this work for publication after addressing the questions raised and providing clearer explanations of the methodology for broader understanding. A few improvements can enhance the papers impact and make it even more beneficial for the research community.", "vkaMaq95_rX": " Summary The paper proposes a framework called EXACT that allows for training Graph Neural Networks (GNNs) with compressed activations to address the challenge of high memory usage associated with large graphs. EXACT offers potential for significant memory savings while maintaining acceptable levels of accuracy and time overhead. The authors provide an optimized GPU implementation that supports training GNNs with compressed activations and systematically analyze the tradeoff among memory saving time overhead and accuracy drop. They experimentally show that EXACT can reduce the memory footprint by up to 32\u00d7 with a minimal accuracy drop of 0.20.5 and a time overhead of 1025 across different models and datasets. The framework is implemented as an extension for Pytorch Geometric and Pytorch and the code is available at a Github repository.  Strengths 1. Novel Contribution: The paper offers a unique framework EXACT for training GNNs with compressed activations addressing a crucial challenge in graph learning. 2. Extensive Analysis: The authors systematically analyze the tradeoffs between memory saving time overhead and accuracy drop providing valuable insights into the performance of the proposed framework. 3. Practical Implications: The demonstration of memory savings up to 32\u00d7 and acceptable accuracy drops showcases the practical utility of EXACT for training GNNs on large graphs. 4. Experimental Validations: The paper provides detailed experiments on various models and datasets supporting the claims made about the effectiveness of the proposed framework.  Weaknesses 1. Complexity of Implementation: The frameworks implementation might be challenging for some users due to the specialized operations required for compressed activations. 2. Limited Benchmarking: The experiments focus on a few datasets and models additional benchmarking on diverse datasets and models could further validate the frameworks effectiveness. 3. Specific Hardware Requirements: The paper focuses on GPU implementations which may limit its applicability to users with different hardware setups.  Questions for Authors 1. Can you provide insights into how the proposed framework EXACT could potentially scale in distributed settings for training GNNs on even larger graphs 2. How do you envision integrating EXACT with historical embeddingbased methods to further enhance training efficiency and memory savings 3. Have you considered the implications of EXACT on interpretability and explainability of GNN models especially with compressed activations  Feedback for Authors 1. Consider addressing the implementation complexity by providing detailed guidelines or demonstrations to facilitate wider adoption of the framework. 2. Expand the benchmarking to include a more extensive range of datasets and models to showcase the frameworks robustness across different scenarios. 3. Discuss potential strategies to make the framework more hardwareagnostic to broaden its accessibility to users with varying hardware configurations. Overall the paper presents a valuable contribution in the domain of training GNNs with compressed activations offering significant memory savings potential without compromising accuracy significantly. With further refinement and broader validation EXACT could become a pivotal tool for scaling GNN training on large graphs efficiently.", "l9tb1bKyfMn": " Peer Review of \"Novel Lightweighted SelfAttention Mechanism: Lowrelation Multihead SelfAttention for Computer Vision\"  1) Summary of the Paper: The paper introduces a novel lightweighted selfattention mechanism Lowrelation Multihead SelfAttention (LMSA) designed to reduce the computational complexity associated with traditional selfattention mechanisms in the domain of computer vision. The paper explores how dimensionality consistency within the standard selfattention mechanism may not be necessary and presents experimental results showing improved accuracy in image classification tasks while reducing computational consumption. The approach is compared against existing models like Swin and Cswin showcasing promising results in terms of efficiency and model performance.  2) Evaluation of the Paper:  Strong Points:  Innovative Approach: The introduction of LMSA as a lightweight selfattention mechanism is a novel and potentially impactful contribution to the field of computer vision.  Experimental Validation: The paper presents comprehensive experimental results demonstrating the effectiveness of LMSA in improving model accuracy while reducing computational complexity.  Comparative Analysis: The comparison against existing models like Swin and Cswin adds credibility to the proposed approach by demonstrating its superiority.  Detailed Methodology and Complexity Analysis: The complexity analysis and data efficiency analysis provide a thorough understanding of the proposed approachs benefits.  Versatility: The paper showcases the adaptability of LMSA to different datasets and tasks demonstrating its potential for widespread applicability.  Weak Points:  Clarity and Coherence: In certain sections the paper can be dense and challenging to follow due to the complexity of the subject matter. Simplifying the language and increasing clarity could improve reader comprehension.  Evaluation Metrics: While the paper includes detailed results from experiments it could benefit from a more comprehensive discussion on evaluation metrics and comparisons with existing stateoftheart methods.  Questionable LMSA Performance in Ablation Study: The paper acknowledges the potential negative impact of extreme compression on the models performance. Further exploration on finding the optimal compression ratio would address this concern.  Feedback to the Authors: 1. The paper would benefit from clearer organization and improved coherence to make it more accessible to a broader audience. 2. Provide more insights into the choice of evaluation metrics and comparisons with stateoftheart methods to strengthen the papers foundation. 3. Further investigate the impact of different compression ratios on model performance in the ablation study to determine the optimal dimensionality settings for LMSA.  3) Review Summary: The paper introduces a promising novel lightweighted selfattention mechanism LMSA aimed at reducing computational complexity in computer vision tasks. The experimental validation supports the efficacy of LMSA in improving model accuracy while lowering computational consumption. Despite some areas for improvement in presentation clarity and additional exploration of compression ratios the innovative approach and detailed analysis make this paper a valuable contribution to the field. Thus based on the comprehensive evaluation the paper demonstrates substantial potential indicating that postaddressing minor concerns it merits publication in highquality journals for wider dissemination within the research community.", "xENf4QUL4LW": " Summary: The paper proposes a novel method for robust learning with noisy labels by incorporating uncertainty in sample selection. Instead of relying solely on smallloss criteria for sample selection the method uses interval estimation of losses to handle both small and largeloss data points effectively. By employing concentration inequalities and statistical confidence bounds the approach aims to better explore underrepresented but correctly labeled data thus improving the generalization of deep neural networks in the presence of label noise. Experimental results demonstrate the superior performance of the proposed method compared to stateoftheart approaches on both synthetic and realworld noisy datasets.  Strengths: 1. Innovative Methodology: The paper presents a novel approach to robust learning with noisy labels by incorporating uncertainty in sample selection addressing the limitations of existing methods. 2. Thorough Experiments: The extensive experimental evaluation on synthetic balancedimbalanced datasets and realworld noisy datasets showcases the effectiveness and robustness of the proposed method across various noise types and datasets. 3. Clear Description and Detailed Explanations: The paper provides a clear and detailed explanation of the proposed method including theoretical analyses algorithmic details and experimental setups making it comprehensive and reproducible. 4. Ablation Study and Comparative Analysis: Conducting ablation studies to investigate sensitivity to hyperparameters and comparing the proposed method against stateoftheart baselines provide a comprehensive evaluation of the methods performance.  Weaknesses: 1. Complexity: The method proposed in the paper involves several intricate concepts and methodologies which might make it challenging for readers without a deep understanding of the topic to follow. 2. Limited RealWorld Experiments: While experiments on realworld noisy datasets demonstrate the efficacy of the method additional realworld experiments covering a broader range of benchmarks could further strengthen the validation of the proposed approach.  Questions and Feedback to the Author: 1. Explanation of Hyperparameters: Could the paper provide more insights into the process of determining hyperparameters like \u03c3 and \u03c4min How do these hyperparameters affect the performance of the method 2. Scalability Concerns: How does the proposed method scale with larger more complex datasets or deep neural network architectures Are there considerations for practical implementation in scenarios with highdimensional data 3. Practical Utility: How does the proposed method handle scenarios where noisy labels are prevalent but clean data is limited or sparse Can it adapt to such challenging conditions effectively 4. Comparison with StateoftheArt: How does the proposed method compare with the latest advancements in semisupervised learning and other noise handling techniques in deep learning  Review Summary: The paper presents a novel and innovative method for enhancing deep learning models robustness to noisy labels by addressing uncertainty in sample selection. The comprehensive experimental evaluation showcases the methods effectiveness across various noise types and datasets. Overall the paper is wellwritten technically sound and contributes significantly to the field of learning with noisy labels. Addressing minor concerns and expanding realworld experiments could further strengthen the papers impact and practical applicability. Based on the presented strengths I recommend the paper for publication after addressing the identified weaknesses and questions.", "sMqybmUh_u8": " Paper Summary The paper discusses a novel approach to hierarchical reinforcement learning (HRL) in the metaRL setting where a learner extracts a latent hierarchical structure during metatraining. The study focuses on a tabular setting where natural hierarchical structure is embedded in the transition dynamics of tasks. The paper introduces \"diversity conditions\" and a tractable algorithm to recover the latent hierarchy efficiently. It also provides regret bounds on a learner using the recovered hierarchy for solving metatest tasks.  Strong Points 1. Innovative Approach: The paper addresses an important gap in the field by proposing a method to learn hierarchies in the metaRL setting with provable guarantees which is a novel and significant contribution. 2. Theoretical Rigor: The paper is wellstructured and grounded in theoretical analysis providing robust arguments for the proposed methods and algorithms. 3. Coverage Conditions: The introduction of \"diversity conditions\" to ensure coverage of the latent hierarchy is a key strength as it offers a systematic way to learn the hierarchy effectively. 4. Regret Bounds: The paper successfully provides regret bounds for learners using the recovered hierarchy for downstream tasks which is crucial for assessing the performance of the proposed approach.  Weak Points 1. Complexity: The paper involves intricate theoretical concepts and assumptions which might pose challenges for readers not wellversed in reinforcement learning and metaRL. 2. Practical Implementation: While the proposed algorithm and theoretical analysis are thorough the practical implementation and scalability of the approach in realworld applications are not extensively discussed. 3. Clarification: The paper could benefit from clearer explanations and additional illustrative examples to enhance the understanding of the concepts presented. 4. Empirical Validation: The paper lacks empirical validation of the proposed approach through experiments on benchmark tasks which could strengthen the credibility of the findings.  Questions for the Author 1. Can you further elaborate on the scalability and computational efficiency of your proposed algorithm in handling larger and more complex tasks 2. How do you envision the practical application of the proposed hierarchylearning algorithm in realworld scenarios with continuous state and action spaces 3. Have you considered conducting experiments to empirically validate the performance of your method on benchmark tasks compared to existing hierarchical RL approaches 4. How do you plan to address the complexity and practical implementation challenges faced by potential users or developers wanting to adopt your approach in their systems  Review Summary The paper presents a rigorous analysis of hierarchical reinforcement learning in the metaRL setting introducing novel diversity conditions and a tractable algorithm for hierarchy recovery. Despite the complex theoretical nature of the work the proposed methods and regret bounds provide valuable insights into learning hierarchies efficiently. However the lack of empirical validation practical implementation considerations and potential scalability issues are notable limitations. Addressing these concerns would strengthen the applicability and impact of the proposed approach in practical settings. This paper is recommended for publication pending clarifications and enhancements in the aforementioned areas.", "rTAclwH46Tb": " Summary The paper proposes Eigencurve a novel learning rate scheduler that considers the eigenvalue distribution of the Hessian matrix of the objective function. This scheduler achieves minimax optimal convergence rates for stochastic gradient descent (SGD) on quadratic objectives particularly when the eigenvalue distribution of the Hessian is skewed. The paper provides theoretical analysis and experimental results showing that Eigencurve outperforms traditional learning rate schedules such as step decay on image classification tasks. The paper also introduces practical variations inspired by Eigencurve namely Elastic Step Decay and Cosinepower Decay which exhibit superior or comparable performance to cosine decay in experiments. Theoretical analyses and empirical validations demonstrate the effectiveness of Eigencurve in achieving optimal convergence rates.  Strengths  Theoretical Contribution: The paper provides a novel learning rate scheduler that bridges the gap between theoretical analysis and practical performance offering minimax optimal convergence rates for SGD on specific quadratic objectives.  Experimental Validation: The empirical results on image classification tasks demonstrate the superior performance of Eigencurve compared to traditional schedules like step decay.  Practical Applications: The paper introduces practical schedulers based on Eigencurve making the concept applicable to realworld scenarios.  Clear Presentation: The paper is wellorganized with detailed explanations theoretical analyses and experimental results to support the proposed scheduler.  Weaknesses  Complexity: The reliance on precomputed eigenvalue distributions for implementing Eigencurve might introduce complexity and computational overhead especially for large models.  Sensitivity: The sensitivity of Eigencurve to the eigenvalue distribution estimation may limit its ease of use in diverse scenarios where such distributions are not readily available.  Limitations: While the paper outlines the advantages of Eigencurve it would be beneficial to discuss potential limitations or boundary conditions under which the approach may not perform optimally.  Questions and Feedback to the Author 1. Could you elaborate on the computational overhead involved in precomputing eigenvalue distributions for large models and possible strategies to mitigate it 2. How sensitive is Eigencurves performance to variations in the estimated eigenvalue distributions especially in scenarios where precise estimation is challenging 3. Considering practical implementation what are the main bottlenecks or challenges that might hinder the adoption of Eigencurve in realworld deep learning tasks 4. Have you considered scenarios or settings where Eigencurve might not offer significant advantages or might underperform compared to traditional learning rate schedules  Review Summary The paper presents a significant contribution by introducing Eigencurve a novel learning rate scheduler that leverages the eigenvalue distribution of the Hessian matrix for optimal convergence rates in SGD on quadratic objectives. The theoretical analyses empirical results and practical applications showcase the effectiveness of Eigencurve in achieving superior performance compared to traditional schedules. Despite potential complexities and sensitivity to eigenvalue distribution estimation Eigencurve offers a promising approach to enhance training efficiency for deep neural networks. Hence based on the comprehensive assessment of the paper I recommend acceptance after addressing the minor concerns and providing further insights into the limitations and practical implications of Eigencurve.", "s3V9I71JvkD": " PeerReview Draft:  Summary: The paper proposes a novel method Semisupervised Meta ActorCritic (SMAC) for offline metareinforcement learning that aims to mitigate the distribution shift in context parameters encountered in offline metaRL scenarios. SMAC utilizes selfsupervised online training to bridge the distributional shift issue by collecting additional interactions without requiring ground truth reward labels. The method is evaluated on simulated and robot manipulation tasks demonstrating its effectiveness in improving adaptive capabilities compared to prior offline metaRL methods.  Strengths:  Novel Concept: The paper introduces a unique approach to address the distribution shift challenge in offline metaRL by leveraging selfsupervised online training.  Comprehensive Experiments: The method is thoroughly evaluated on a variety of simulated tasks and a complex robot manipulation domain demonstrating significant improvements over prior offline metaRL methods.  Clear Explanations: The paper provides detailed explanations of concepts algorithms and experiments allowing for better understanding and reproducibility.  Weaknesses:  Assumption on Data Collection: The assumption that the agent can autonomously collect online data without ground truth rewards may limit the practical applicability of the proposed method in realworld scenarios where such data collection could be challenging.  Experimental Validation: While the method shows promising results in simulated tasks further validation in more complex realworld settings could strengthen the applicability and robustness of the proposed approach.  Questions to Author: 1. How do you envision extending the proposed method to address challenges related to autonomous data collection in realworld environments 2. Can you elaborate on the computational efficiency and scalability of the SMAC algorithm especially when applied to tasks with larger state and action spaces 3. Have you considered potential limitations or drawbacks of utilizing selfsupervised online training and how do you plan to address them in future research  Feedback to Author:  Clarity and Structure: The clarity of presentation and the organization of the manuscript are commendable. Consider highlighting the practical implications of your findings in the conclusion section.  Reproducibility: Providing detailed instructions for reproducing experiments and making the source code available greatly enhances the credibility and reproducibility of your research.  Review Summary: The paper presents a novel approach SMAC for addressing distribution shift challenges in offline metaRL through selfsupervised online training. The method demonstrates superior performance compared to existing offline metaRL methods on simulated tasks and a robot manipulation domain. The experimental results highlight the efficacy of SMAC in improving adaptive capabilities and bridging distributional shifts. Despite certain assumptions on data collection the proposed method shows promise for enhancing metaRL performance. The clear presentation and comprehensive evaluation make this paper a valuable contribution to the field of offline metaRL. Strengthening validation on realworld tasks and addressing potential limitations could further enhance the practical utility of the proposed approach. Overall I recommend this paper for acceptance with minor revisions addressing the questions and feedback outlined.", "qO-PN1zjmi_": " Summary of the Paper The paper introduces a new ensemblebased procedure for semisupervised novelty detection (SSND) called Ensembles with Regularized Disagreement (ERD). The proposed method leverages a mixture of unlabeled indistribution and outofdistribution samples to enhance detection performance on near outofdistribution data. ERD focuses on promoting diversity on outofdistribution data while maintaining agreement on indistribution data. The authors conducted extensive comparisons with stateoftheart SSND methods on standard image datasets (SVHNCIFAR10CIFAR100) and medical image datasets demonstrating significant performance gains with negligible computational cost increase.  Strong Points of the Paper 1. Innovative Solution: ERD introduces a novel approach for semisupervised novelty detection that leverages unlabeled data effectively to enhance OOD detection performance. 2. Extensive Comparisons: The paper provides a comprehensive comparison of the proposed method with existing stateoftheart SSND methods on various datasets showcasing significant improvements. 3. Theoretical Justification: The paper provides theoretical arguments and empirical evidence to support the effectiveness of the ERD method particularly focusing on the importance of regularizing disagreement in ensembles for OOD detection. 4. Practical Implications: The discussion on the practical scenarios where ERD can be applied and the flexibility it offers in dealing with new diseases or anomalies is insightful and relevant.  Weak Points of the Paper 1. Complexity of Explanations: Some sections such as the theoretical justifications in Sections 3.2 and 3.3 might be challenging for nonexperts to fully grasp potentially hindering the accessibility of the paper. 2. Experimental Setup Details: While the paper discusses the datasets and baselines used more details on the experimental setup hyperparameters and specific results could enhance the clarity and reproducibility of the findings. 3. Clarity of Figures: The explanation accompanying the figures could be enhanced to better explain the concepts illustrated in the figure especially for readers who are not familiar with the underlying methodology.  Question to the Author  Could you elaborate on how the performance of ERD is affected by the size and ratio of OOD samples in the unlabeled set particularly in scenarios with varying levels of OOD representation  Feedback to the Authors The paper presents a valuable contribution to the field of semisupervised novelty detection by proposing a novel ensemblebased method ERD. The extensive experimental evaluation and theoretical justifications provide strong support for the effectiveness of the proposed approach. However the paper could benefit from clearer explanations in certain sections and additional details in the experimental setup for better reproducibility and understanding. Overall the paper is wellstructured and highlights the practical relevance and performance advantages of ERD in challenging OOD detection scenarios.  Review Summary The paper introduces ERD a new ensemblebased approach for semisupervised novelty detection showing significant performance gains with minimal computational cost increase compared to existing methods. The proposed method effectively leverages unlabeled data to improve near outofdistribution detection and is theoretically welljustified. While the paper demonstrates innovation and practical relevance some sections could be clarified further to enhance accessibility for a wider audience. Overall based on the strong experimental results and theoretical underpinnings I recommend the acceptance of this paper for publication.", "yjMQuLLcGWK": " Paper Summary: The paper introduces FPDETR a new method that addresses the limitations of existing detection transformers by proposing a fully pretrained encoderonly transformer model for object detection. The key innovation lies in leveraging query positional embeddings as visual prompts to aid the model in attending to target areas and recognizing objects. The proposed task adapter enhances the prompt ability of query positional embeddings and bridges the gap between pretraining and finetuning. Experimental results on COCO dataset demonstrate the superiority of FPDETR in terms of performance robustness and generalization to small datasets.  Strengths: 1. Innovative Approach: The paper introduces a novel method that fully pretrains an encoderonly transformer for object detection addressing the limitations of traditional detection transformers. 2. Experimental Results: The experiments on the COCO dataset show that FPDETR achieves competitive performance better robustness against corruptions and improved generalization to small datasets. 3. Clear Explanations: The paper provides detailed explanations of the proposed method task adapter and their role in improving detection performance. The methodology is welldescribed and easy to follow. 4. Comparative Analysis: Comparative analysis with existing methods like Deformable DETR and UPDETR and performance comparisons provide a clear understanding of the superiority of FPDETR.  Weaknesses: 1. Incomplete Evaluation: While the experiments on COCO dataset are extensive further evaluation on a wider range of datasets or benchmark tasks could strengthen the validity of the proposed method. 2. Lack of Analysis: The paper could benefit from a more indepth analysis of the limitations of existing methods and a detailed discussion on how FPDETR overcomes these limitations. 3. Limited Insights: The paper could provide more insights into the design choices of the task adapter its working mechanism and its impact on the models performance.  Questions for the Authors: 1. Could you provide more insights into the decisionmaking process behind choosing the specific configurations for the task adapter such as the selfattention layer 2. How do you plan to evaluate the scalability and adaptability of FPDETR on larger and more diverse datasets beyond COCO and Cityscapes 3. Have you considered conducting user studies or perceptual evaluations to validate the improved robustness and generalization aspects claimed for FPDETR  Feedback to the Authors: 1. Consider expanding the discussion section to provide more insights into the implications of the proposed method and the potential future directions for research. 2. Enhance the explanation of the task adapter to provide a more detailed understanding of its functionality and the role it plays in improving model performance. Overall the paper presents a promising method with significant potential for advancing the field of object detection using detection transformers. The comprehensive experimental evaluations support the claims made and further improvements in analysis and discussions could enhance the overall impact of the research.", "vQmIksuciu2": " PeerReview of \"Dynamic Filter Pruning with Explainable AI for Accelerating CNNS\" 1. Summary: The paper introduces a novel approach to dynamic filter pruning in Convolutional Neural Networks (CNNs) that incorporates explainable AI and early coarse prediction in intermediate layers to reduce latency in inference without significant overhead. By employing an early exit mechanism and selecting only relevant filters based on coarse prediction the proposed architecture aims to reduce both average and longestpath latency of inference. The paper provides theoretical background describes the proposed architecture and evaluates the approach using standard datasets and deep learning models. 2. Strengths and Weaknesses: Strong Points:  Innovative Approach: Introducing early coarse prediction to guide filter selection is a novel and innovative approach in dynamic filter pruning.  Clear Explanations: The paper provides detailed explanations of the methodology ranking criteria and training processes making it easy to follow for readers.  Experimental Evaluation: The work includes a thorough evaluation using various deep learning models and datasets demonstrating effectiveness on both CPU and GPU platforms. Weak Points:  Lack of Comparison: While the paper discusses the architectures advantages over static pruning approaches more direct comparisons with existing dynamic pruning methods could further highlight the uniqueness and performance of the proposed approach.  Implementation Details: More details on the implementation aspects such as computational complexity comparison and scalability to larger models would enhance the practical insights for potential adopters. Questions and Feedback to the Author: 1. Could you provide more insights into the computational overhead incurred by the dynamic pruning mechanism especially in comparison to static pruning methods 2. How does the proposed architecture perform in scenarios with varying complexity levels of datasets and model sizes 3. Have you considered exploring the potential scalability of your approach to larger and more complex CNN architectures beyond the tested models 3. Review Summary: The paper presents a novel approach to dynamic filter pruning in CNNs utilizing explainable AI and early coarse prediction to reduce latency in inference tasks. The thorough theoretical background detailed description of the proposed architecture and comprehensive evaluation results showcase the significance of this work. The innovative methodology and promising results support the recommendation for acceptance. Further improvements could include more direct comparisons with existing dynamic pruning techniques and detailed insights into computational overhead and scalability aspects. Overall the paper makes a valuable contribution to the field of dynamic filter pruning and demonstrates the potential for significant latency reduction in CNN inference tasks.  Recommendation: Accept with Minor Revisions", "nkaba3ND7B5": " Summary: The paper introduces a new framework for Autonomous Reinforcement Learning (ARL) to address the discrepancy between episodic RL environments and realworld continual learning scenarios such as those encountered by robots. The authors propose benchmark tasks reflective of autonomous learning challenges and evaluate various algorithms in these settings. They distinguish between deployment and continuing evaluation scenarios and demonstrate the limitations of traditional RL algorithms in autonomous settings highlighting the need for more autonomyfocused algorithms.  Strong Points: 1. Novel Framework: The paper introduces a novel framework for Autonomous Reinforcement Learning addressing a critical gap in existing RL research. 2. Benchmark Tasks: The authors provide a set of diverse and challenging benchmark tasks reflective of the hurdles introduced by lack of human intervention in autonomous RL. 3. Evaluation: The paper evaluates existing algorithms under the new framework highlighting the shortcomings and the necessity for algorithms focusing on autonomy. 4. Conceptual Clarity: The paper effectively articulates the concepts and challenges associated with ARL providing clear definitions examples and illustrations.  Weak Points: 1. Methodological Rigor: More details on algorithm implementations hyperparameters and experimental setups would strengthen the paper. 2. Visualization: While visualizations aid in understanding the concepts they could be further refined to enhance clarity. 3. Comparative Analysis: The paper lacks a more detailed comparative analysis between the performance of various algorithms in different benchmark tasks.  Questions and Feedback to the Author: 1. Implementation Details: Can you provide more clarity on the implementation specifics of the algorithms tested in the benchmark tasks 2. Further Experiments: Have you considered conducting additional experiments to further validate the efficacy of the proposed framework and benchmark tasks 3. Algorithm Improvements: What are the key areas where existing algorithms fall short in autonomous settings and how can they be improved upon  Review Summary: The paper presents a welldefined and significant contribution to the field of Reinforcement Learning by introducing a new framework for Autonomous Reinforcement Learning. The benchmark tasks and evaluation metrics provided showcase the challenges faced in realworld autonomous settings. While the paper is comprehensive and wellarticulated some aspects could be strengthened such as providing more detailed experimental methodologies and comparative analyses. Overall the paper offers valuable insights into the need for autonomyfocused RL algorithms and lays a solid foundation for future research in this domain. Considering the strengths and potential impact of the work I recommend acceptance after addressing the weak points raised above.", "per0G3dnkYh": " Summary The paper focuses on enhancing normalizing flows ability to capture tail behavior for improving generative modeling. The authors introduce a novel type of triangular flows called marginal tailadaptive flows (mTAFs). They prove theoretical insights on controlling marginal tailedness and propose a datadriven permutation scheme to capture heavy and lighttailed marginals. The authors conduct an empirical analysis on synthetic data to demonstrate the superior performance of mTAF compared to other flow models. They also propose a sampling strategy for generating tail samples.  Strengths  The paper addresses an important problem of learning heavytailed distributions making a valuable contribution to the field.  The theoretical insights and proofs provided are detailed and rigorously explained enhancing the credibility of the proposed mTAFs.  The experimental evaluation on synthetic data solidly supports the effectiveness of mTAFs showing improved performance over other flow models.  The sampling strategy for generating tail samples is a valuable addition providing a practical application of the proposed method.  Weaknesses  The paper could benefit from further elaboration on the limitations and potential pitfalls of mTAFs especially when applied to realworld datasets.  The discussion on the generalizability of the proposed permutation scheme could be expanded to address its potential limitations and implications in varied scenarios.  While the reproducibility statement highlights documentation of source code additional details on the availability of data and models used for experiments would enhance reproducibility.  Questions to the Authors 1. Can you elaborate on the scalability of mTAFs to larger and more complex datasets beyond the synthetic data used in the experiments 2. How does the proposed mTAF method compare to stateoftheart models in terms of computational efficiency and memory requirements 3. Have you considered exploring the performance of mTAFs on realworld datasets with varying degrees of tail behavior for a more comprehensive evaluation  Feedback for the Authors  Addressing limitations and practical implications of mTAFs when applied to realworld datasets would strengthen the papers applicability.  Providing additional insights on the generalizability of the proposed permutation scheme and its flexibility could enhance the methods adaptability to diverse model architectures and data characteristics.  Including a more detailed discussion on the potential tradeoffs between model complexity and expressive base distributions in the context of normalizing flows would enrich the theoretical framework of mTAFs.  Review Summary The paper presents a wellformulated research study on improving normalizing flows through the novel concept of mTAFs to capture tail behavior effectively. The rigorous theoretical foundation empirical evaluation and practical sampling strategy demonstrate the effectiveness of mTAFs. While the paper showcases notable strengths in terms of innovation and methodology addressing limitations exploring realworld applications and further discussing the generalizability of the proposed method would provide a more comprehensive understanding of mTAFs. Overall I recommend considering this paper for publication given its solid contributions to the field of generative modeling and distribution learning.", "nwKXyFvaUm": " Summary: The paper introduces a novel approach called \"federated averaging with diverse client selection (DivFL)\" for client selection in federated learning. DivFL aims to select a diverse and representative subset of clients in each communication round to improve convergence learning efficiency and fairness. The approach utilizes a submodular facility location function to select clients that provide representative gradient information for server aggregation. The paper provides theoretical analysis of convergence applies DivFL to synthetic and real datasets and compares its performance with baseline client selection strategies.  Strong Points: 1. Innovative Approach: The paper introduces a novel method for client selection in federated learning which aims to improve learning efficiency convergence and fairness by selecting diverse clients. 2. Thorough Analysis: The paper provides a detailed theoretical analysis of the proposed approachs convergence properties demonstrating its effectiveness in heterogeneous settings. 3. Empirical Validation: The empirical results on synthetic and real datasets showcase the superiority of DivFL over existing client selection methods in terms of convergence fairness and learning efficiency. 4. Communication Efficiency: The paper addresses the communication cost issue by proposing a communicationefficient version of DivFL without sacrificing performance.  Weak Points: 1. Communication Cost Consideration: While the paper introduces a communicationefficient version of DivFL more insights on practical implementation details and potential tradeoffs in terms of communication costs could enhance the robustness of the approach. 2. Extension to Other Federated Learning Methods: While discussing the extension of the proposed approach to other federated optimization methods the paper could provide more concrete examples or experiments validating this extension. 3. Realworld Scalability: The scalability of the proposed approach to largescale federated learning settings involving a greater number of clients could be further explored to assess its performance in scenarios with a higher client count. 4. Generalizability: Addressing potential limitations or assumptions made in the heterogeneous settings could enhance the generalizability of DivFL across different federated learning environments.  Question for the Authors: 1. Given the promising results on synthetic and real datasets have you considered any practical applications or industry use cases where DivFL could be particularly beneficial compared to traditional client selection methods in federated learning  Feedback: The paper presents a novel and wellanalyzed approach to client selection in federated learning with significant improvements in convergence learning efficiency and fairness. Addressing potential scalability issues and providing further insights on practical implementation details could strengthen the applicability of DivFL in diverse federated learning scenarios. Consider discussing potential realworld applications and extending the analysis to largerscale federated learning settings for a more comprehensive evaluation of the proposed approach.", "pWBNOgdeURp": " Summary of the Paper: The paper presents a novel approach to pruning deep neural networks using Koopman operator theory. The main contributions of the paper include defining a new class of theoretically motivated pruning algorithms based on Koopman modes decomposition showing the equivalence between Koopmanbased pruning and traditional magnitude and gradientbased pruning and providing insights into the dynamics of how these pruning methods perform during the early and late stages of training.  Strong Points: 1. Innovative Pruning Approach: Introducing a new class of pruning algorithms based on Koopman operator theory is a novel and intriguing contribution to the field of deep learning. 2. Theoretical Foundation: The theoretical motivation for the proposed pruning algorithms adds depth to the discussion making the approach more robust and potentially insightful. 3. Unified Framework: By showing the equivalence between Koopmanbased pruning and traditional magnitude and gradientbased pruning the paper unifies seemingly disparate methods under a common framework which enhances the understanding of pruning techniques. 4. Reproducibility: The authors have been transparent in methodology and provided detailed steps for reproducing the results ensuring the validity and replicability of their findings.  Weak Points: 1. Excessive Technical Detail: The paper includes a significant amount of technical information which might make it challenging for readers without a strong background in dynamical systems theory to comprehend the concepts and significance fully. 2. Limited Experimental Validation: While the paper discusses the equivalence of pruning methods and insights into dynamics there is room for more comprehensive experimental validation to support the claims made.  Questions for the Authors: 1. Can you elaborate on how the proposed Koopmanbased pruning algorithm can be practically implemented in largescale deep neural networks considering the computational requirements and scalability aspect 2. Have you considered comparing the proposed Koopman pruning method with other stateoftheart pruning techniques to evaluate its performance comprehensively across different types of networks and tasks  Feedback for the Authors: 1. Consider providing a more structured and intuitive explanation of the Koopman mode decomposition and its relevance to deep neural network pruning for the benefit of a wider audience. 2. Enhance the papers impact by including more diverse experiments and benchmarks to showcase the effectiveness and versatility of the proposed pruning approach.  Review Summary: The paper introduces a novel approach to pruning deep neural networks based on Koopman operator theory presenting a new class of theoretically motivated pruning algorithms. While the theoretical foundation and unified framework for pruning methods are strong points the paper could benefit from more diversified experimental validation and clearer explanations to make the concepts accessible to a broader audience. The authors are encouraged to address these points to enhance the papers impact and utility in the field of deep learning research.", "zRb7IWkTZAU": "1) Summary: The paper proposes a novel approach for training reinforcement learning agents on robotic manipulation tasks using natural language descriptions as goals in a zeroshot setting. The method leverages CLIP a largescale vision and language model to ground objects in images based on textual descriptions. By factoring the grounding of objects and spatial relationships the model provides an offtheshelf reward signal using raw pixels as input. This allows for training a languageconditioned policy on multiple tasks without the need for access to state information or expert demonstrations. Experimental results demonstrate the efficacy of the framework on various manipulation tasks. 2) Strengths and Weaknesses: Strong points:  Innovative Approach: The use of natural language descriptions for goal specification in a zeroshot setting is a unique and innovative approach.  Utilization of CLIP: Leveraging CLIP for grounding objects in images based on textual descriptions is a noteworthy contribution.  Effective Reward Model: The proposed spatially grounded visuolinguistic reward model shows promising results in training robotic manipulation tasks.  LanguageConditioned MultiTask Policy: The ability to deploy a single policy for multiple tasks based on language descriptions is a significant advancement.  Comprehensive Experiments: The paper provides thorough experiments showcasing the effectiveness of the proposed method on various tasks. Weak points:  Complexity: The method involving spatial heuristics and factoring of grounding aspects may be complex and challenging to implement in practical scenarios.  Evaluation Metrics: While the experiments demonstrate success in zeroshot learning more indepth evaluation metrics and comparisons with existing methods could strengthen the paper.  RealWorld Applicability: The scalability and applicability of the proposed approach in realworld scenarios with complex environments may need further exploration.  Ethical Considerations: The paper briefly touches on ethical perils in leveraging largescale datasets but a more detailed discussion on potential bias and mitigation strategies would enhance the paper. Questions and Feedback to the Author: 1. Could you elaborate on how the proposed method addresses generalization to unseen tasks and variations in scene configurations 2. How robust is the spatialsalience scheme proposed for factored grounding of what vs wherehow aspects in handling complex manipulation tasks 3. Have you considered the computational efficiency and scalability of the proposed method for deployment in realworld robotic systems 4. The paper briefly mentions ethical considerations related to dataset bias. Could you provide more insights into potential biases and strategies for ensuring fairness in robotic systems trained using largescale datasets 3) Review Summary: The paper presents an innovative approach leveraging CLIP for training reinforcement learning agents on robotic manipulation tasks using natural language descriptions as goals in a zeroshot setting. The spatially grounded visuolinguistic reward model and languageconditioned multitask policy demonstrate promising results in task learning and generalization. The comprehensive experiments validate the effectiveness of the proposed framework. However addressing the complexity of the method enhancing evaluation metrics exploring realworld applicability and further discussing ethical considerations would strengthen the paper. Overall the paper makes significant contributions to the field of reinforcement learning for robotic manipulation tasks and warrants consideration for publication after addressing the mentioned points.", "wkMG8cdvh7-": " Summary of the Paper: The paper investigates Graph Injection Attack (GIA) on Graph Neural Networks (GNNs) in comparison to Graph Modification Attack (GMA). It explores the power and limitations of GIA highlighting its flexibility but also the potential pitfall of damaging homophily. To address these issues the paper introduces the concept of homophily unnoticeability and proposes Harmonious Adversarial Objective (HAO) to enforce GIA to retain homophily during attacks. Extensive experiments are conducted showing that GIA with HAO can break homophilybased defenses and outperform previous GIA attacks significantly.  Strong Points: 1. Comprehensive Investigation: The paper provides a thorough comparison between GIA and GMA exploring their advantages limitations and the impact on homophily. 2. Novel Concept: The introduction of homophily unnoticeability and HAO is a novel contribution to address the potential pitfalls of GIA attacks. 3. Theoretical Grounding: Theorems are formulated to support the findings and claims made in the paper providing a strong theoretical basis for the proposed concepts. 4. Empirical Validation: The extensive experiments conducted to verify the effectiveness of HAO and its superiority in breaking defenses demonstrate the practical relevance of the proposed methods.  Weak Points: 1. Complexity of Concepts: The paper delves into intricate concepts related to adversarial attacks on GNNs which may be challenging for readers unfamiliar with the field. 2. Terminology Usage: Some terms and notations used in the paper can be quite technical making it less accessible to a broader audience. 3. Lack of Discussion on Limitations: While the paper provides insights into the limitations of GIA attacks further discussion on potential drawbacks or challenges in implementing HAO could enhance the completeness of the study.  Questions to the Author: 1. How does the proposed HAO approach compare to existing techniques in terms of computational efficiency and scalability for largescale graphs 2. Can the proposed methods be generalized to other types of adversarial attacks or different applications beyond GNNs 3. Have you considered realworld scenarios or practical applications where GIA attacks could pose significant risks and how the proposed solutions would fare in such contexts  Feedback to the Author:  Consider providing more explanations or examples to elucidate complex concepts for readers with varying expertise levels.  Discuss potential limitations or challenges in implementing the proposed methods to provide a wellrounded perspective on the research.  Review Summary: The paper presents a detailed exploration of Graph Injection Attacks on Graph Neural Networks introducing novel concepts like homophily unnoticeability and HAO to address inherent weaknesses in such attacks. Strong theoretical grounding and empirical validation support the effectiveness of the proposed methods. However improving clarity in the presentation and discussing the practical implications and limitations of the proposed approaches would enhance the impact and applicability of the research. Overall the paper makes significant contributions to the field of adversarial attacks on GNNs and offers promising avenues for further research and development.", "uwnOHjgUrTa": "Peer Review 1) Summary: The paper introduces a novel training procedure DNN Quantization with Attention (DQA) that aims to improve the performance of existing quantization methods for Deep Neural Networks (DNNs). The proposed method involves using a learnable linear combination of high medium and lowbit quantization during training to smooth the loss function and aid convergence. Experimental results on CIFAR10 CIFAR100 and ImageNet datasets demonstrate that DQA enhances the accuracy of stateoftheart quantization methods. The paper provides a comprehensive overview of existing quantization techniques the challenges of training quantized DNNs and the methodology behind DQA. 2) Strengths and Weaknesses: Strong Points:  The paper addresses an important problem of improving the performance of quantized DNNs.  The proposed DQA method is innovative and wellmotivated providing a systematic way to train lowbit quantized DNNs.  The paper is wellstructured providing an indepth review of existing quantization methods and clear explanations of the proposed methodology.  Experimental results demonstrate the effectiveness of DQA in improving accuracy compared to other quantization methods on various datasets.  The comparison with existing methods like BinaryRelax and the visualization of loss surfaces add value to the study. Weak Points:  The paper lacks a detailed discussion of the computational complexity and scalability of the proposed method.  While the experimental results are promising it would be beneficial to include more indepth analysis of the results and provide insights into the limitations of DQA.  The paper could benefit from additional comparisons with a broader range of stateoftheart quantization methods to showcase the versatility and robustness of DQA.  The paper can be further enhanced by discussing potential realworld applications and implications of DQA in the context of IoT devices or edge computing. Questions and Feedback to Author: 1. Could you provide more insights into the computational overhead introduced by the DQA method in comparison to traditional quantization techniques 2. How does the DQA method generalize to different DNN architectures beyond ResNet18 and MobileNetV2 and can it adapt to more complex networks 3. Have you considered potential tradeoffs between accuracy improvement and computational efficiency when using the DQA method on resourceconstrained devices 4. How does the proposed method handle nonlinear activation functions and batch normalization layers during the quantization process 3) Review Summary: The paper introduces a novel training procedure DNN Quantization with Attention (DQA) to enhance the performance of existing quantization methods for DNNs. The study is wellstructured providing a detailed overview of related work the methodology behind DQA and comprehensive experimental results on multiple datasets. The proposed method shows promise in improving the accuracy of quantized DNNs compared to stateoftheart techniques. However additional analysis discussions on computational complexity scalability and broader comparisons with existing methods could strengthen the study. Overall the paper is wellwritten contributes to the field of neural network quantization and warrants further exploration and validation. Recommendation: Based on the innovative approach clear presentation and promising results I recommend this paper for acceptance after addressing minor revisions and incorporating additional discussions and analyses as suggested above.", "z1-I6rOKv1S": " Summary The paper proposes autoregressive quantile flows a new class of normalizing flow models trained using an innovative objective based on proper scoring rules. These quantile flows aim to simplify the computation of determinants of Jacobians in normalizing flows allowing for more efficient training and better support for various neural architectures. The authors demonstrate the effectiveness of this approach in tasks such as time series forecasting object detection density estimation and generation tasks. They also introduce quantile flow regression which improves probabilistic predictions by parameterizing predictive distributions with flows.  Strengths 1. Innovative Approach: The paper presents a novel objective based on proper scoring rules for training normalizing flows offering a unique perspective compared to traditional maximum likelihood methods. 2. Broad Applicability: The proposed autoregressive quantile flows show promising results in various tasks including time series forecasting object detection density estimation and generation modeling demonstrating the versatility of the method. 3. Efficiency: By simplifying the computation of Jacobian determinants the proposed quantile flows offer a more efficient training process enabling the use of more diverse neural architectures.  Weaknesses 1. Technical Depth: The complex mathematical formulations and technical details in the paper may be challenging for readers not wellversed in flow models and neural networks. 2. Limited Clarity: Some sections could benefit from improved clarity and better organization to enhance understanding for readers.  Questions and Feedback  Could you provide more insights into the scalability and computational efficiency of autoregressive quantile flows compared to traditional methods  How does the proposed approach handle highdimensional data and model interpretability  Can you elaborate on the potential limitations or challenges of implementing the proposed objectives in complex realworld scenarios  Review Summary The paper introduces a promising approach with autoregressive quantile flows that offer a new perspective on training normalizing flow models. The method shows significant potential in various applications providing improved probabilistic predictions and enhancing efficiency in training neural architectures. Despite some technical complexities and areas for clarification the innovative nature of the proposed quantile flows combined with the demonstrated improvements in different tasks make this paper a valuable contribution to the field of machine learning. Therefore I recommend acceptance after addressing the minor weaknesses identified.", "zXM0b4hi5_B": " Summary: The paper investigates the relationships between perceptual distances unsupervised image representations image probability distributions and human perception. It explores how using perceptual distances in machine learning models affects performance particularly in autoencoders. The study connects various concepts such as efficient coding hypothesis distance measurements image distributions and human perception.  Strong Points: 1. Comprehensive Investigation: The paper delves deep into the relationships between perceptual distances image representations and probability distributions offering a thorough investigation into these interconnected concepts. 2. Experimental Validation: Experimental demonstrations are provided to support the theoretical concepts discussed in the paper enhancing the credibility of the findings. 3. Theoretical Soundness: The theoretical framework presented involving equalization models sensitivity analysis and optimization strategies is solid and wellargued providing a strong foundation for the experimental work. 4. Relevance: The study addresses an important aspect of machine learning linking it with human visual perception and natural image statistics which could have implications for image processing applications. 5. Insightful Observations: The observations made regarding the impact of using perceptual metrics in training correlations between different distances and the regularization effect are thoughtprovoking and contribute valuable insights to the field.  Weak Points: 1. Clarity in Certain Sections: Some sections like the appendix detailing experiments could be complex to understand for nonexpert readers. Simplifying the terminology and providing clearer explanations might improve readability. 2. Limited Comparison: While the experiments are detailed and insightful a comparison with existing literature or methods could have strengthened the papers contribution by highlighting its novelty or advancements. 3. Integration of Results: The paper could benefit from a more cohesive integration of experimental results with the theoretical discussions to enhance the flow and coherence of the argument presented.  Questions and Feedback to the Author: 1. Could you discuss potential realworld applications or implications of the findings presented in the paper 2. How do you envision further extending this research to practical scenarios or industrial applications for image processing or machine learning tasks 3. Have you considered exploring the computational efficiency or scalability of using perceptual distances in largescale machine learning models  Review Summary: The paper provides a comprehensive exploration of the relationships between perceptual distances image distributions and unsupervised learning models offering valuable insights into the interplay between these concepts. The experimental validation supported by a strong theoretical framework enhances the credibility of the study. While certain sections may be challenging for nonexperts overall the paper presents important findings that could advance the understanding of machine learning applications in image processing tasks. Based on the strength of the research I recommend acceptance with minor revisions to enhance clarity and integrate results more effectively.", "qXa0nhTRZGV": " 1) Summary of the Paper: The paper discusses the SharpnessAware Minimization (SAM) training method which focuses on worstcase weight perturbations to improve generalization in various settings. It analyzes the implicit bias of SAM over diagonal linear networks provides convergence proofs for SAM with stochastic gradients for nonconvex objectives and discusses its behavior in deep networks. Additionally the paper explores the benefits of SAM in the noisy label setting including interpretation as a gradient reweighting scheme and its improvement in adversarial training scenarios.  2) Strong and Weak Points Questions and Feedback:  Strong Points: 1. Theoretical Analysis: The paper provides a thorough theoretical analysis of SAM discussing objectives implicit bias convergence proofs and generalization behavior for various scenarios. 2. Empirical Evidence: Empirical results are presented to support the theoretical findings showcasing the performance of SAM compared to traditional methods in both linear and nonlinear models. 3. Insights into Noisy Labels and Adversarial Training: The paper draws insightful parallels between overfitting in noisy label settings and adversarial training highlighting the potential of SAM in addressing robust generalization challenges.  Weak Points: 1. Conclusive Justifications: The paper acknowledges that existing justifications for SAMs success are not entirely conclusive suggesting a need for further clarification. 2. Complexity of Analysis: The technical analysis although detailed and extensive may be challenging for readers unfamiliar with the topic potentially limiting accessibility.  Questions and Feedback:  Question: Can the paper delve deeper into the practical implications of SAM particularly in realworld applications or scenarios beyond the experimental setups discussed  Feedback: To enhance clarity and understanding the paper could benefit from visual aids or simplified examples to illustrate key concepts and results especially for readers with limited technical background.  3) Review Summary: The paper presents a comprehensive analysis of SharpnessAware Minimization (SAM) and its implications for generalization in deep learning. It offers valuable theoretical insights supported by empirical evidence and suggests practical applications in addressing challenges such as noisy labels and adversarial training. While the analysis is thorough further clarification on the conclusive justifications and practical implications of SAM could enhance the papers impact and accessibility. Overall the paper provides a significant contribution to the understanding of optimization techniques for robust generalization in machine learning.  Recommendation: Based on the thorough analysis and significant contribution to the field I recommend the paper for publication with minor revisions to address the clarity of justifications and practical implications.", "jgAl403zfau": "1) Summary: The paper introduces HardwareAware Latency Pruning (HALP) which formulates structural pruning as a global resource allocation optimization problem to maximize accuracy while constraining latency under a predefined budget. It leverages a latency lookup table to track latency reduction potential and a global saliency score to gauge accuracy drop. HALP surpasses prior work in pruning efficacy and accuracyefficiency tradeoff demonstrating improvements across various networks on ImageNet and VOC datasets. 2) Strong Points:  Innovative Approach: The method of formulating structural pruning as a resource allocation optimization problem is innovative and effective.  Efficiency: The proposed algorithm HALP shows improved pruning efficacy and accuracyefficiency tradeoff compared to prior art.  Experimental Verification: The paper provides comprehensive experiments across different network architectures and datasets demonstrating the efficacy and generalization of HALP.  Scalability: HALPs scalability to different architectures and hardware platforms is a strong point showcasing its versatility. Weak Points:  Complexity: The method described is quite complex particularly in terms of the augmentation of the knapsack solver and neuron grouping which may make it challenging for broad adoption without detailed understanding.  Discussion of Limitations: The paper lacks a clear discussion of potential limitations or cases where HALP may not perform optimally.  Comparison Metrics: While the paper compares HALP with prior methods in terms of accuracy and latency speedups it might benefit from including energy efficiency metrics for a more comprehensive comparison. Question and Feedback to the Author:  Question: Could you provide more details on how HALP could potentially be extended or modified to account for energy efficiency metrics in addition to accuracy and latency speedups  Feedback: Consider expanding the discussion section to include a detailed analysis of the limitations of HALP and potential future research directions to address those limitations. 3) Review Summary: The paper presents an innovative approach HALP that formulates structural pruning as a global resource allocation optimization problem to maximize accuracy while constraining latency under a predefined budget. The thorough experimental evaluation showcases the efficacy and efficiency of HALP across various network architectures and datasets highlighting its scalability and generalization to different hardware platforms. While the complexity of the method may pose some adoption challenges the strong results and innovative approach make this paper a valuable contribution to the field of network pruning. With some improvements in discussing limitations and potential extensions the paper could further enhance its impact. Recommendation: I recommend accepting this paper with minor revisions. The innovative approach comprehensive experimental evaluation and scalability of HALP make it a valuable contribution to the field of network pruning. With some improvements in discussing limitations and potential future work the paper can further enhance its impact.", "gKprVaCyQmA": "Review of \"There are Free Lunches\" Theorem 1) Summary: The paper introduces a new theorem \"There are Free Lunches\" (TAFL) challenging the NoFreeLunch (NFL) Theorems by proposing that some algorithms can excel in all tasks when given in a specific order. It suggests that by accumulating solved tasks the difficulty of new tasks decreases. Sections cover the introduction to NFL Theorems presentation of TAFL Theorem task order analysis implications and an example demonstrating TAFL Theorems application in solving binary classification tasks. 2) Strengths:  Original Contribution: The introduction of TAFL Theorem is a novel concept that challenges existing beliefs in algorithm performance.  Elaborate Explanation: The paper provides a detailed explanation of concepts theorems and implications aiding in understanding the proposed idea.  Application Example: The example demonstrating the application of TAFL Theorem in solving binary classification tasks adds practicality to the theoretical framework.  Structured Sections: The paper is wellstructured with clear sections making it easy to follow the argument progression. 3) Weak Points:  Clarity of Proofs: The proof of the TAFL Theorem could be further simplified for better understanding by readers from diverse backgrounds.  Relation to Existing Literature: While the paper introduces a new theorem more comparison with existing research on optimization algorithms and task order strategies could strengthen the argument. Questions and Feedback to the Author:  Can you provide additional insights into how the proposed TAFL Theorem compares to existing taskadaptation methods like MAML  It would be beneficial to include a discussion on the computational complexity or practical implementation challenges of utilizing the TAFL Theorem in realworld scenarios.  Have you considered conducting experiments to validate the TAFL Theorems effectiveness in comparison to traditional algorithms on a benchmark dataset Review Summary: The paper presents a compelling argument challenging the universality of algorithm performance with the introduction of the TAFL Theorem. The clear articulation of ideas coupled with the practical example enhances the papers impact. While the novelty and clarity are commendable addressing the weaknesses by providing further simplification of proofs and deeper discussions on practical applications and comparisons with existing methods could strengthen the contribution. Recommendation: I recommend the paper for acceptance pending minor revisions to address the clarity of proof explanations and further discussions on practical implementation challenges and comparisons with existing literature.", "lzupY5zjaU9": "1) Summary of the paper: The paper introduces a new metaprocedure COMPRESS aimed at improving the efficiency of thinning algorithms used in distribution compression. Thinning algorithms aim to represent a probability distribution accurately using a small number of representative points. COMPRESS is designed to speed up any thinning algorithm while maintaining error guarantees up to a factor of 4. The paper demonstrates the effectiveness of COMPRESS in reducing runtime while preserving quality compared to traditional thinning approaches. 2) Strong Points: a) Novelty and Contribution: The introduction of COMPRESS as a metaprocedure to enhance the efficiency of thinning algorithms is a significant contribution to the field of distribution compression. The ability to speed up thinning without sacrificing accuracy is valuable. b) Theoretical Foundation: The paper provides a strong theoretical foundation with detailed definitions theorems and proofs to support the effectiveness of COMPRESS in reducing runtime while maintaining error guarantees. This adds credibility to the proposed approach. c) Empirical Evaluation: The paper presents thorough experiments showcasing the effectiveness of COMPRESS in various scenarios. Results demonstrate that COMPRESS can significantly reduce computational time while maintaining or even improving accuracy compared to traditional approaches. d) Application Diversity: The use of various types of targets such as mixture of Gaussians and MCMC summaries adds to the versatility of COMPRESS. Demonstrating the effectiveness across different scenarios strengthens the applicability of the proposed approach. Weak Points: a) Complexity: The paper includes detailed theoretical and algorithmic content which may be challenging for readers not familiar with the specific technical concepts related to thinning algorithms RKHS and MMD. Simplifying explanations or providing more intuitive examples could enhance readability for a broader audience. b) Empirical Evaluation: While the empirical evaluation is comprehensive providing insights into scenarios where COMPRESS may not perform as well could offer a more balanced view. Discussing limitations or failure cases could help understand the boundaries of the proposed approach. Question to the Author: 1. Have you considered the scalability of COMPRESS to even larger datasets or higher dimensions than those tested in your experiments How do you foresee the performance of COMPRESS in such scenarios Feedback to the Author: 1. Consider including discussions on potential limitations or scenarios where COMPRESS may not provide significant improvements. Providing a comprehensive view of the strengths and weaknesses of the approach can enhance the papers impact. 3) Review Summary: The paper introduces a novel metaprocedure COMPRESS designed to speed up thinning algorithms in distribution compression while maintaining error guarantees. Through rigorous theoretical foundations comprehensive experiments and empirical evaluations the paper demonstrates the effectiveness of COMPRESS in reducing computational time while preserving or improving accuracy. The theoretical underpinnings experimental evaluation and practical applications of COMPRESS make a strong case for its potential impact in the field. However the paper could benefit from additional discussions on scalability and potential limitations to provide a more wellrounded perspective to readers. Recommendation: Based on the strong theoretical foundation innovative approach and promising empirical results I recommend accepting this paper for publication with minor revisions that address the suggested feedback and question to the author for further clarification and completeness.", "on54StZqGQ_": " PeerReview of Paper Titled \"Certifiably Robust Defense Mechanisms Against Adversarial Attacks\"  1. Summary: The paper discusses the limitations of certifiably robust neural network defenses against adversarial examples. It points out that these defenses are overly cautious and can erroneously flag nonadversarial inputs as adversarial leading to unnecessary input rejections and reduced model utility. New attacks called degradation attacks are introduced which aim to force certifiably robust classifiers to reject inputs unnecessarily. The study provides empirical evidence showing the efficacy of these attacks against stateoftheart defense mechanisms like GloRo Nets and Randomized Smoothing.  2. Strengths and Weaknesses:  Strong Points: 1. Innovative Approach: The paper introduces a novel concept of degradation attacks that exploit the weaknesses of certifiably robust defenses. 2. Detailed Explanations: The authors provide a thorough explanation of the problem algorithms and evaluation methodologies used in the study. 3. Empirical Evidence: The empirical demonstration of the attacks on stateoftheart defense mechanisms strengthens the credibility of the findings. 4. Relevance: The study addresses a critical issue in the field of neural network security and provides insights that can potentially enhance future defense mechanisms.  Weak Points: 1. Complexity: The technical nature of the paper might make it challenging for readers unfamiliar with the domain to understand the nuances of the attacks and defenses discussed. 2. Lack of Comparison: The paper could benefit from a comparison with other existing defense mechanisms to provide a broader perspective on the effectiveness of certifiably robust defenses.  Questions and Feedback: 1. Can you elaborate on the computational complexity of the proposed degradation attacks and how it compares to existing attack methodologies 2. Have you considered exploring the impact of varying hyperparameters of the models on the susceptibility to degradation attacks such as different radii values or noise parameters 3. It would be valuable to discuss the realworld implications of the findings. How do you envision the industry or academia utilizing this knowledge to improve the robustness of neural network defenses  3. Review Summary: The paper presents a compelling argument regarding the overcautious nature of certifiably robust neural network defenses introducing degradation attacks as a means to exploit this vulnerability. The authors provide a comprehensive analysis through examples and experiments showcasing the efficacy of these attacks against stateoftheart defense mechanisms. The empirical evaluation supports the claims and emphasizes the importance of addressing these limitations in neural network security. The work is significant as it highlights a critical issue in adversarial defense mechanisms and suggests avenues for future research and improvement. However the technical complexity of the paper might pose a challenge for readers unfamiliar with the field. Overall the study offers valuable insights and prompts further investigation to enhance the robustness of neural network defenses against adversarial attacks.  This peerreview provides a thorough evaluation of the paper highlighting its strengths weaknesses and offering constructive feedback for the authors to consider.", "hGXij5rfiHw": " Summary of the Paper: The paper proposes a novel learning strategy called Discovering Invariant Rationales (DIR) for constructing intrinsically interpretable Graph Neural Networks (GNNs). DIR aims to discover causal features that are stable across different distribution shifts by creating multiple interventional distributions. The authors conduct experiments on synthetic and realworld datasets to validate the effectiveness of DIR in terms of interpretability and generalization ability on graph classification tasks.  Strong Points: 1) Innovative Approach: The paper introduces a novel strategy DIR addressing the limitation of existing rationalization methods by focusing on stable causal patterns across different distributions. 2) Theoretical Foundation: The paper is wellfounded theoretically emphasizing the causal view of datagenerating processes and providing a principled approach to discovering invariant rationales. 3) Comprehensive Experiments: The authors conduct experiments on both synthetic and real datasets showcasing the superiority of DIR over existing baselines in terms of interpretability and generalization ability. 4) Learning Dynamics Analysis: The study reveals intriguing twostage learning dynamics adaption and fitting shedding light on the training process of DIR. 5) Comparison with Baselines: The comparison with various baselines including interpretable and robustinvariant learning methods demonstrates the effectiveness and superiority of DIR in different scenarios.  Weak Points: 1) Complexity of Approach: The approach introduced in the paper particularly the formulation of interventional distributions and learning strategy may be complex for readers not wellversed in causal reasoning and GNNs. 2) Limited RealWorld Application: While the study showcases promising results on synthetic and real datasets the practical application and scalability of DIR in complex realworld scenarios need further exploration. 3) Lack of Human Study: The paper lacks human studies or realworld applications to demonstrate the impact of DIR in practical scenarios beyond the benchmark datasets.  Questions for the Authors: 1) How does DIR adapt to more complex and dynamic realworld scenarios where causal relations may change over time 2) Have you considered conducting user studies or experiments in realworld applications to validate the effectiveness and usability of DIR outside of controlled experimental settings 3) How scalable is the DIR approach to larger and more diverse graph datasets and what are the computational constraints or limitations when applying DIR to realworld largescale applications  Feedback to the Authors: 1) Consider providing additional insights on the scalability and computational efficiency of DIR for largescale graph datasets. 2) Enhance the discussion on the realworld applications of DIR potentially with case studies or practical usecases to showcase its applicability beyond the current experimental setup. 3) Provide more guidance and examples on how researchers or practitioners can implement and adapt the DIR approach in their own research or applications.  Review Summary: Overall the paper presents a novel and promising approach DIR for constructing intrinsically interpretable Graph Neural Networks. By focusing on stable causal patterns DIR outperforms existing methods in terms of interpretability and generalization. The theoretical foundation and experimental validation provide strong support for the proposed approach. Addressing the weakness in existing methods related to spurious features and distribution shifts DIR shows potential for practical application in various domains. However further exploration on realworld applications scalability and usability is recommended to enhance the impact and relevance of the proposed approach.", "vEIVxSN8Xhx": " Summary of the Paper: The paper proposes a novel logpolar space convolution (LPSC) layer to address the issue of small local receptive fields in lower layers of convolutional neural networks caused by the quadratic increase in parameters with larger kernel sizes. LPSC utilizes elliptical convolution kernels that adaptively divide the local receptive field into regions based on relative directions and logarithmic distances leading to an exponential growth in the singlelayer receptive field without increasing the number of parameters. The LPSC method is applied by pooling in logpolar space replacing conventional and dilated convolutions in different network architectures. Experimental results demonstrate the effectiveness of LPSC on various tasks and datasets outperforming other convolution methods in certain instances.  Strong Points: 1. Innovative Approach: The paper introduces a novel method LPSC which effectively increases local receptive fields without a significant increase in parameters. 2. Implementation Details: The paper provides comprehensive details on the LPSC method including kernel structure mapping to logpolar space and incorporation into various network architectures. 3. Empirical Evaluation: The experimental results demonstrate the superiority of LPSC over conventional and dilated convolutions in tasks like image classification and semantic segmentation across different datasets. 4. Complexity Analysis: The paper discusses the computational complexity and FLOPs comparison showing efficiency gains with LPSC making it a viable alternative for lightweight models.  Weak Points: 1. Hyperparameter Sensitivity: The introduction of additional hyperparameters (Lr L\u03b8 and g) for LPSC could pose challenges in practical implementation and tuning potentially impacting model robustness. 2. Memory Overhead: The paper acknowledges the large memory overhead incurred by implementing LPSC via logpolar space pooling which could limit its scalability to larger datasets or models. 3. Evaluation Scope: While the experimental results are encouraging further analysis and comparisons with more advanced convolution methods or diverse datasets could strengthen the papers claims.  Question and Feedback: 1. Question: Considering the tradeoff between accuracy improvements and memory performance how does LPSC compare to other advanced convolutional methods that aim to increase receptive fields efficiently 2. Feedback: It would be beneficial to include discussions on the potential limitations of LPSC in handling complex spatial dependencies or diverse datasets to provide a more comprehensive view of its applicability.  Review Summary: The paper presents a novel logpolar space convolution method LPSC as an effective solution for increasing local receptive fields in CNNs without a significant rise in parameters. Despite the hyperparameter sensitivity and memory overhead challenges LPSC showcases promising results in image classification and segmentation tasks. With its innovative approach and efficiency gains LPSC offers a viable alternative to conventional and dilated convolutions. However a more extensive evaluation and comparison with advanced methods are recommended to solidify its position in convolutional neural network research. Overall I recommend acceptance of the paper with minor revisions to address the mentioned weak points and provide a more indepth discussion on the applicability of LPSC in varied scenarios.", "w4cXZDDib1H": " Summary of the Paper: The paper introduces a novel object detection architecture called Vision and Detection Transformers (ViDT) which integrates Vision Transformers (ViT) and Detection Transformers (DETR) to create an efficient object detector. ViDT utilizes a reconfigured attention module to leverage the Swin Transformer architecture for standalone object detection. It also incorporates an encoderfree neck structure for multiscale features and additional optimization techniques. The paper presents ViDT as a scalable and effective object detector demonstrating superior performance on the Microsoft COCO benchmark dataset.  Strong Points: 1. Innovative Approach: The integration of ViT and DETR to form ViDT represents an innovative approach to object detection showcasing the potential of fully transformerbased architectures. 2. Efficient Design: ViDT introduces a reconfigured attention module and an encoderfree neck structure addressing computational overhead issues and improving the scalability of transformer models. 3. Performance Evaluation: The paper provides detailed evaluation results on the Microsoft COCO benchmark demonstrating that ViDT achieves the best tradeoff between accuracy and speed compared to existing transformerbased detectors. 4. Comprehensive Experiments: Extensive ablation studies on the reconfigured attention module additional techniques and knowledge distillation add depth to the analysis and showcase the benefits of various components. 5. Code and Models Availability: The authors have made the code and trained models publicly available on GitHub promoting reproducibility and further research in the field.  Weak Points: 1. Complexity: The paper is highly technical and may be challenging for readers unfamiliar with transformer architectures to grasp fully. 2. Comparative Analysis: While comparisons with existing methods are insightful a more extensive discussion on the limitations and shortcomings of ViDT in specific scenarios could have added value. 3. Discussion on RealWorld Applications: The paper focuses primarily on benchmark performance expanding the discussion to realworld applications and practical implications could enhance the papers relevance.  Questions and Feedback to the Author: 1. How does ViDT compare to traditional CNNbased object detectors in terms of computational efficiency and scalability 2. Could the paper elaborate on potential limitations or challenges in deploying ViDT in realworld scenarios where data and computational resources are limited 3. How do the proposed components of ViDT contribute individually to the overall performance and are there any tradeoffs involved in their incorporation 4. Are there specific use cases or domains where ViDT has shown superior performance compared to existing object detection methods 5. Considering the fastevolving nature of deep learning research how does the proposed ViDT architecture align with upcoming trends in computer vision research  Review Summary: Overall the paper presents a significant contribution to the field of object detection by introducing ViDT a fusion of ViT and DETR for enhanced object detection performance. The novel components extensive experiments and detailed analysis showcase the effectiveness and efficiency of the proposed architecture. However further discussion on practical applications comparative analysis with traditional methods and implications of ViDT in realworld scenarios could enhance the papers impact. The availability of code and models adds credibility and encourages further exploration. As such I recommend the paper for publication given its technical depth innovative approach and strong experimental validation.", "gI7KCy4UDN9": "1) Summary: The paper addresses the issue of nondeterministic calculations in learned image compression models due to floatingpoint arithmetic which affects the consistency of entropy parameter prediction for arithmetic encoding. The proposed solution involves utilizing posttraining quantization to make the models inference integerarithmeticonly achieving crossplatform consistency. The authors further delve into discretizing the entropy parameters for deterministic CDF indexing and discuss encoding and decoding with Gaussian mixture models. 2) Strengths and Weaknesses: Strengths:  Welldefined problem statement and clear objectives.  Detailed explanation of the proposed method and its implementation specifics.  Thorough evaluation and comparison with existing techniques.  Explicit algorithms and pseudocode provided for better understanding.  Comprehensive coverage of different aspects of learned image compression. Weaknesses:  Lack of detailed description on the benchmark dataset or evaluation metrics.  Could provide more insight into realworld applications or practical implementations.  Potential room for further experimentation or exploration of alternative methods.  Limited discussion on potential challenges or drawbacks of the proposed approach. Questions to Authors: 1. What datasets were used in the evaluation of the proposed method Did you consider any specific benchmark datasets 2. How does the proposed approach compare with existing stateoftheart models in terms of compression efficiency and speed 3. Have you considered any limitations or potential drawbacks of the posttraining quantization method especially in practical deployment scenarios 4. Can you elaborate on the computational resources required for implementing the proposed method particularly in terms of hardware compatibility and efficiency Feedback to Authors:  Consider providing more detailed insights into the experimental setup datasets and evaluation metrics used for a more comprehensive analysis.  Further discussion on practical implications scalability and realworld applications of the method could enhance the overall impact of the study.  Acknowledging potential limitations and challenges in the current approach would contribute to a more balanced assessment of the proposed solution. 3) Review Summary: The paper introduces an effective methodology for addressing nondeterministic calculations in learned image compression models emphasizing crossplatform consistency through posttraining quantization. The detailed explanation of the proposed method algorithms and comprehensive evaluation provide a strong foundation for the research. Incorporating additional insights into the practical implications limitations and potential improvements would further enhance the studys relevance and applicability. Based on the thorough analysis presented I recommend the paper for publication with minor revisions to address the outlined considerations.", "fYor2QIp_3": " Peer Review for the Paper \"Improving Protein Function Prediction using Hierarchical Features of Gene Ontology Terms\" 1. Summary: The paper presents a novel method to enhance Protein Function Prediction (PFP) by leveraging hierarchical features of Gene Ontology (GO) terms. The proposed method combines a language model for encoding protein sequences with a Graph Convolutional Network (GCN) for representing GO terms. By incorporating nodewise representations containing the complete hierarchical information of GO terms the algorithm demonstrates effectiveness in largescale graphs outperforming existing PFP approaches. 2. Strengths:  Innovation: The integration of a pretrained Language Model (LM) and GCN to capture highlevel biophysical features for PFP is innovative.  Hierarchical Representation: The use of nodewise representations to incorporate the entire hierarchical information of GO terms addresses limitations of previous GCN models.  Performance: The experimental results show superiority over stateoftheart approaches particularly in complex domains like BPO. Weaknesses:  Evaluation: While the paper presents impressive performance results a deeper analysis of the models interpretability and robustness to different datasets could enhance the credibility of the findings.  Comparison to Baselines: A more detailed comparison to baseline models showcasing specific areas of improvement and limitations would provide a better understanding of the models advancements.  Scalability: The scalability of the proposed model to even larger datasets or more diverse protein function prediction tasks should be discussed to highlight potential limitations. Questions and Feedback: 1. Data Generalization: How well does the proposed method generalize to unseen datasets with different characteristics such as varying degrees of label imbalance or annotation sparsity 2. Interpretability: Can the model provide insights into the biological relevance of the predicted protein functions allowing researchers to understand the underlying reasoning behind the predictions 3. Computational Efficiency: Have you considered the computational resources required for training and inference in deploying the proposed model at scale How does it compare to existing methods in terms of efficiency  Review Summary: The paper introduces an innovative approach to improving Protein Function Prediction using hierarchical features of Gene Ontology terms. The incorporation of nodewise representations and a pretrained LM demonstrates superior performance over existing stateoftheart methods especially in complex domains. The methods effectiveness in largescale graph structures is a notable contribution to the field of PFP. However further investigations into model interpretability generalizability and scalability would strengthen the paper. Overall the paper presents a compelling advancement in PFP methodology and merits consideration for publication after addressing the minor concerns raised.", "sWqjiqlUDso": " Summary: The paper proposes a novel causal graphbased fair prediction framework under the pathspecific causal fairness definition aiming to eliminate unfair paths and ensure fair predictions. The framework integrates graph structure learning and fairness regularization to exclude unfair pathways in the causal graph. It also generalizes to scenarios where sensitive attributes can be nonroot nodes addressing realworld applications challenges.  Strengths: 1. Innovative Approach: The paper introduces a unique approach that combines graph structure learning and fairness regularization to achieve pathspecific causal fairness. 2. Theoretical Analysis: The paper provides a thorough theoretical analysis presenting a comprehensive understanding of the proposed frameworks generalization error and its impact on model performance. 3. Experimental Validation: The experiments conducted on synthetic and realworld datasets demonstrate the effectiveness of the proposed framework in achieving a balance between utility and fairness. 4. Generalization to NonRoot Node Case: The paper successfully extends the proposed framework to handle scenarios where sensitive attributes are nonroot nodes showcasing the frameworks adaptability to diverse realworld applications.  Weaknesses: 1. Complexity: The papers approach involving graph structure learning and fairness regularization might be computationally intensive potentially limiting its scalability to larger datasets. 2. Assumption: The paper assumes no latent confounders in the dataset which might not hold true in practical scenarios. Addressing latent confounders would enhance the applicability of the proposed framework. 3. Experimentation: While the experiments provide valuable insights additional diverse realworld datasets could further validate the frameworks robustness across different domains.  Questions to the Authors: 1. How does the proposed framework perform in terms of computational efficiency especially when applied to larger datasets 2. Have you considered any mechanisms to handle the presence of latent confounders in the dataset which might impact the graph structure learning and fairness regularization 3. Could you provide insights into how the frameworks performance might vary across different realworld applications with varying complexities and dataset sizes  Feedback to the Authors:  The paper presents a novel and wellstructured framework for achieving algorithm fairness through causal graphbased approaches.  Consider discussing the computational complexity implications and scalability of the proposed framework for diverse realworld applications.  Addressing the presence of latent confounders in the dataset would strengthen the frameworks robustness and applicability.  Review Summary: The paper introduces a cuttingedge approach to algorithm fairness through a causal graphbased framework showcasing theoretical rigor experimental validation and generalization to diverse scenarios. While demonstrating promising results addressing computational efficiency latent confounders and exploring broader datasets would enhance the frameworks practical utility. Overall the paper offers valuable insights into advancing algorithm fairness in machine learning models.", "kiNEOCSEzt": " Summary of the Paper: The paper discusses the implications of the actions taken by a recommender system (RS) on user preferences. The paper argues that RS designers need to consider how their systems choices influence user preferences and how longhorizon optimization techniques like reinforcement learning can manipulate preferences for higher engagement. They propose a method to estimate the shifts in user preferences induced by different RS policies before deployment and introduce the concept of \"safe shifts\" to define a trust region for behavior deemed safe. The paper explores how recommender systems that optimize to avoid manipulative behaviors and stay within the trust region can produce higher engagement while avoiding unwanted preference shifts. The work includes a detailed analysis of related literature the method for estimating user preference shifts and the evaluation of metrics for identifying undesirable shifts.   Strengths of the Paper:  Comprehensive Analysis: The paper provides a detailed analysis of how RS actions can influence user preferences the challenges in estimating these shifts and the proposed framework for evaluating and optimizing behavior.  Novel Approach: The concept of \"safe shifts\" to define a trust region and the methodology for estimating and evaluating preference shifts are innovative and can have significant implications in the field of recommender systems.  Experimental Setup: The use of simulated testing environments and ground truth human dynamics for evaluating the proposed methods is a strength and provides a solid foundation for the results obtained.  Weaknesses of the Paper:  Limitations in Realworld Application: The paper acknowledges the caveat of conducting evaluation in simulated environments and raising concerns about the assumed dynamics model. Addressing potential limitations in realworld application would enhance the practicality of the proposed methods.  Incomplete Validation: Given the use of estimated metrics with simulated interactions validating the effectiveness of the proposed method under realworld conditions would strengthen the work.  Complexity and Ethical Implications: The philosophical and ethical challenges related to defining unwanted shifts and safe behaviors are complex and require further exploration. Discussing potential ethical implications and mitigating strategies in more detail could enhance the paper.  Questions for the Authors: 1. How do you envision the practical application of your proposed framework in realworld recommender systems What steps need to be taken to address the limitations identified in the paper 2. Have you considered conducting user studies or field trials to validate the efficacy of your method in influencing user preferences positively while avoiding unwanted shifts in a live RS environment 3. How do you plan to address the ethical considerations raised by manipulating user preferences and ensuring that the trust region defined for safe shifts is inclusive and does not compromise user autonomy  Feedback for the Authors:  The papers focus on anticipating and evaluating preference shifts in RS is commendable. Enhancing the validation methodology exploring realworld applications and addressing the ethical implications in more detail would strengthen the research.  Consider discussing how the proposed method can be integrated with existing RS frameworks and potential challenges that might arise in deploying it in practice.   Review Summary: The paper presents a novel approach to estimate and evaluate shifts in user preferences induced by recommender system policies. The concept of \"safe shifts\" and the methodology proposed for preference estimation and evaluation are innovative and have the potential to address concerns related to manipulative behaviors in RS. The comprehensive analysis experimental setup and the examination of related work add value to the research. However addressing limitations in realworld applicability conducting user studies for validation and further exploring the ethical implications of the proposed framework could enhance the impact and practicality of the research. Overall the paper provides a solid foundation for future research and advancements in the field of recommender systems.", "rWXfFogxRJN": " Paper Summary: The paper proposes a novel Automated Data Augmentation (AutoDA) method called AdaAug to learn adaptive augmentation policies in a classdependent and potentially instancedependent manner. The method efficiently captures datasetspecific inductive biases by optimizing augmentation policies using a recognition model through an exploitandexplore procedure. AdaAug outperforms existing AutoDA baselines and achieves stateoftheart performance on datasets like CIFAR10 CIFAR100 and SVHN while also demonstrating effective transfer to unseen datasets.  Strengths: 1. Innovative Methodology: AdaAug introduces a unique approach by learning adaptive augmentation policies in a classdependent manner addressing limitations of existing AutoDA methods. 2. Efficient Workflow: The exploitandexplore strategy using a recognition model enables efficient optimization of augmentation policies showcasing promising results on various datasets. 3. Experimental Analysis: The paper includes comprehensive experiments demonstrating the superior performance of AdaAug comparing it with existing AutoDA methods on a range of datasets. 4. Transfer Learning: AdaAug showcases the effectiveness of learned policies by successfully transferring them to unseen datasets enhancing the generalization capability of models.  Weaknesses: 1. Complexity Concerns: The paper highlights some technical challenges in learning adaptive augmentation policies such as the large search space and nondifferentiability of operations which could potentially limit the practical implementation of AdaAug. 2. Experimental Reproducibility: While the experimental results are promising more detailed insights or visualizations could strengthen the understanding of how AdaAug performs compared to existing methods.  Questions to the Authors: 1. Scalability: How does AdaAug perform in terms of computational efficiency on larger datasets or more complex architectures compared to existing AutoDA methods 2. Robustness: Have you performed sensitivity analyses to evaluate the robustness of AdaAug to hyperparameters or different search configurations 3. RealWorld Implementation: In realworld scenarios what challenges could arise when deploying AdaAug for various computer vision tasks and how could these challenges be addressed  Feedback: 1. Clarity: The paper provides a detailed analysis of the proposed AdaAug method. It would be beneficial to include more visualizations or examples to aid in understanding the adaptive augmentation policies learned by AdaAug. 2. Future Directions: Consider discussing potential future research directions or applications of AdaAug beyond image classification tasks to enhance the impact of the proposed methodology.  Review Summary: The paper introduces a novel AdaAug method for automated adaptive data augmentation showcasing superior performance and effective transfer to unseen datasets. While the method addresses key limitations of existing AutoDA approaches further analysis on scalability robustness and realworld implementation challenges would strengthen the paper. Overall AdaAug presents a significant advancement in automating the generation of datasetspecific augmentation policies warranting further exploration in the field of representation learning and computer vision tasks.", "vr4Wo33bd1": " Summary of the Paper The paper introduces the novel concept of semisupervised longtailed recognition aiming to address challenges in traditional longtailed recognition by leveraging unlabeled data. The proposed method combines feature embedding and classifier training using an alternate sampling framework. By iterating through different stages significant improvements in recognition accuracy are demonstrated on two benchmark datasets  CIFAR10SSLT and ImageNetSSLT.  Strengths of the Paper 1. Innovative Approach: The paper presents a novel paradigm of semisupervised longtailed recognition addressing realworld data scenarios with a pragmatic solution. 2. Thorough Literature Review: The introduction and related work sections provide a comprehensive overview of existing research on longtailed recognition and semisupervised learning setting the stage for the proposed method. 3. Technical Depth: The paper delves into technical details explaining model decoupling alternate learning stages and the rationale behind each step providing clarity to readers. 4. Experimental Rigor: The experiments are wellstructured comparing the proposed method against baseline techniques on both CIFAR10SSLT and ImageNetSSLT datasets showcasing the effectiveness of the approach.  Weaknesses of the Paper 1. Complexity: The method involves multiple stages and intricate training schemes which might make implementation challenging for readers without deep expertise in the field. 2. Lack of Visual Aids: While the paper is technically detailed the absence of visual aids such as diagrams or illustrations could make it harder for readers to grasp the method at a glance. 3. Limited Dataset Description: While the proposed datasets are manually curated providing more detailed information on the datasets particularly about the specific classes and distribution could enhance the understanding of the experimental setup. 4. Discussion on Failure Cases: While the results highlight the improvements achieved by the proposed method a discussion on potential failure cases or scenarios where the method might underperform could add depth to the analysis.  Questions for the Author 1. Can you explain the rationale behind the choice of ResNet18 for CIFAR10SSLT and ImageNetSSLT and the decision to scale up to ResNet50 for ImageNetSSLT Were there specific reasons behind these architecture choices 2. How does the proposed method handle noisy or mislabeled pseudo labels during the semisupervised learning process Are there mechanisms in place to detect and mitigate the impact of incorrect labels on training 3. In light of the results how do you envision the practical applicability of the proposed semisupervised longtailed recognition method in realworld computer vision tasks with imbalanced data distributions 4. Have you considered further experiments or case studies on more diverse datasets to test the generalizability and scalability of the proposed method across different types of data distributions and recognition tasks  Review Summary The paper introduces a novel methodology for semisupervised longtailed recognition demonstrating significant improvements over baseline methods on benchmark datasets. While the approach showcases innovation and technical depth potential challenges lie in complexity and lack of visualization. Addressing these areas while enriching the dataset descriptions and discussing potential failure cases could further strengthen the paper. The proposed method holds promise for addressing realworld imbalanced data scenarios paving the way for advancements in recognition tasks. Hence I recommend the paper for publication pending minor revisions and clarifications as suggested.", "ybsh6zEzIKA": " PeerReview:  Summary: The paper proposes a novel input mixing schema called ifMixup for improving the generalization of Graph Neural Networks (GNNs). The ifMixup method leverages Mixup regularization techniques typically used in image and text domains and adapts it for graphs. The key contributions include proposing a mixup scheme that is free of manifold intrusion issues and demonstrating improved performance in graph classification tasks compared to existing baseline methods.  Strengths: 1. Innovative Methodology: The introduction of ifMixup as an input mixing schema for graph classification is a novel and innovative approach in the context of GNN regularization and augmentation. 2. Theoretical Foundations: The paper provides a strong theoretical basis for the proposed method demonstrating that the mixing strategy is capable of recovering source graphs and ensuring the absence of manifold intrusion. 3. Experimental Validation: The experimental evaluation on benchmark datasets showcases the superiority of ifMixup over existing methods demonstrating its effectiveness in improving predictive accuracy without suffering from underfitting oversmoothing or overfitting issues. 4. Comprehensive Analysis: The paper includes detailed comparisons with multiple baselines ablation studies on mixing ratios and network depth and evaluation with different GNN architectures providing a thorough analysis of the ifMixup method.  Weaknesses: 1. Complexity of Presentation: The detailed theoretical proofs and technical content may be challenging for readers not wellversed in graph theory and GNNs. Simplifying certain sections for a broader audience could enhance the accessibility of the paper. 2. Lack of Comparative Analysis: While the experimental results show the superiority of ifMixup further comparative analysis with stateoftheart methods or largerscale datasets could strengthen the claims regarding the methods performance.  Questions: 1. Scalability: How does the performance of ifMixup scale with larger and more complex graphs in realworld scenarios or industrial applications 2. Robustness: Have you evaluated the robustness of ifMixup to noisy or incomplete graph data as might be encountered in practical applications 3. Extension to Other Domains: Do you foresee the applicability of ifMixup beyond graph classification tasks such as in graph generation or link prediction problems  Feedback to the Author:  Consider providing a more detailed explanation of the practical implications and potential applications of the ifMixup method in realworld scenarios.  Enhance the readability of the paper by further clarifying complex technical aspects while maintaining the rigor of the theoretical foundations.  Include discussions on the limitations and potential extensions of ifMixup to inspire future research directions.  Review Summary: The paper introduces a novel input mixing schema ifMixup for improving the generalization of Graph Neural Networks. With strong theoretical foundations and promising experimental results ifMixup demonstrates superior performance in graph classification tasks. While showcasing innovative contributions the paper could benefit from simplifying technical complexities for broader readership and expanding the discussion on realworld applications and scalability. The thorough analysis and comprehensive comparisons emphasize the robustness and effectiveness of the proposed method making it a valuable addition to the field of GNN regularization techniques. Overall the paper is wellstructured methodologically sound and presents a significant advancement in GNN optimization. Therefore I recommend acceptance of this paper for publication pending minor revisions to enhance clarity and broaden the discussion.", "wbPObLm6ueA": " Summary: The paper introduces Shifty algorithms a novel approach designed to address the challenge of maintaining fairness in machine learning models when faced with demographic shift between training and deployment environments. Shifty algorithms aim to provide highconfidence fairness guarantees under demographic shift without relying on data from deployment environments. The evaluation of Shifty algorithms was conducted using the UCI Adult Census dataset and realworld student data demonstrating the effectiveness of Shifty in avoiding biased outcomes compared to existing methods.  Strengths: 1. Innovative Approach: The paper introduces a novel concept of Shifty algorithms to tackle fairness challenges in machine learning models under demographic shift. 2. Concrete Problem Statement: The paper clearly outlines the impact of demographic shift on fairness guarantees and formulates the problem statement. 3. Theoretical Framework: The paper provides a solid theoretical framework for assessing fairness under demographic shift and elaborates on the methods used to compute highconfidence upper bounds. 4. Evaluation and Results: The paper presents a comprehensive evaluation of Shifty algorithms with detailed experiments using realworld datasets providing a clear comparison with existing fair ML algorithms. 5. Empirical Validity: The results demonstrate that Shiftyttest outperforms existing algorithms in maintaining fairness guarantees under demographic shift without compromising accuracy.  Weaknesses: 1. Complexity and Specificity: The paper delves into complex mathematical formulations and algorithms potentially making it difficult for readers less familiar with the domain to grasp the content. 2. Assumptions: The effectiveness of Shifty algorithms heavily relies on the assumption that the user has accurate information regarding demographic shift which may not always be feasible in practical scenarios. 3. Aiming for High Confidence: The requirement for high confidence in fairness guarantees might lead to challenging scenarios where the algorithm returns no solution as described in the evaluation section.  Questions to the Author: 1. Could you elaborate on how the Shifty algorithm handles scenarios where the assumptions about demographic shift are not accurately known or specified How robust is the algorithm in such cases 2. How do you envision implementing and incorporating the Shifty algorithm into realworld applications or industries where demographic shift is a common challenge in ensuring fairness in machine learning models  Feedback:  Clarification and Simplicity: Consider providing more intuitive examples or visual aids to simplify complex mathematical formulations for easier understanding by a broader audience.  Robustness and RealWorld Application: Highlight the robustness of the Shifty algorithm in scenarios where assumptions about demographic shift might be uncertain or evolving for practical implementation.  Review Summary: The paper presents a novel approach Shifty algorithms for ensuring highconfidence fairness guarantees in machine learning models under demographic shift. While the theoretical framework and evaluation results are solid there is a need to balance complexity with clarity for broader accessibility. The paper presents valuable insights into addressing fairness challenges in ML applications showcasing the effectiveness of Shifty algorithms. With further clarification on handling uncertainty in assumptions and realworld applicability this work provides a strong foundation in advancing fairness in machine learning under demographic shift.", "nf3A0WZsXS5": " Summary of the Paper: The paper introduces a novel method called SurrealGAN (SemiSupervised Representation Learning via GAN) that aims to dissect diseaserelated heterogeneity in neuroimaging data. The proposed method utilizes crosssectional imaging data and employs a semisupervised clustering approach to infer disease severity along continuous dimensions. Key components of SurrealGAN include transformation functions regularization techniques to ensure meaningful imaging patterns parallel mapping functions and inverse reconstruction for inferring representative disease patterns. The paper validates the model through semisynthetic experiments and demonstrates its potential in capturing biologically plausible imaging patterns in Alzheimers disease.  Strong Points: 1. Innovative Methodology: The paper introduces a novel approach SurrealGAN to address the limitations of existing methods in modeling diseaserelated heterogeneity in neuroimaging data. 2. Comprehensive Analysis: The paper provides indepth details on the methodology regularization techniques and implementation details making it easier for readers to understand and replicate the study. 3. Validation: The method is validated through semisynthetic experiments and applied to real data from Alzheimers disease patients demonstrating significant correlations between the proposed imaging signatures and clinical variables. 4. Robustness Testing: The paper conducts robustness testing under various conditions showcasing the models performance in different scenarios including pattern overlapping scarce regions and noisy data. 5. Comparative Analysis: The paper compares the proposed SurrealGAN method with other existing models demonstrating its superiority in capturing diseaserelated patterns from neuroimaging data.  Weak Points: 1. Complexity: The paper presents a highly technical and complex methodology which might be challenging for readers with limited expertise in machine learning and neuroimaging. 2. Lack of Visualization: While the paper provides detailed explanations of the methodology visual aids such as diagrams and figures could enhance the understanding of the model architecture and results for readers.  Questions and Feedback: 1. Are there specific scenarios or types of neurological diseases where SurrealGAN might be particularly effective given its ability to capture diseaserelated heterogeneity 2. How do the parameters such as \u03b3 \u03ba \\xce\\xb6 \u03bb \u00b5 and \\xce\\xb7 impact the performance of SurrealGAN and are there guidelines for choosing their optimal values 3. Can the model be easily extended or adapted for applications beyond Alzheimers disease to capture heterogeneity in other neurological or neuropsychiatric conditions  Review Summary: The paper presents a novel method SurrealGAN for dissecting diseaserelated heterogeneity in neuroimaging data overcoming limitations of existing models by offering a more continuous and interpretable representation of disease severity. The paper is wellstructured comprehensive in its methodology and provides robust validations showcasing the models potential in capturing biologically plausible imaging patterns in Alzheimers disease. However the complexity of the methodology and the lack of visual aids might pose challenges for readers. Overall based on the innovative approach comprehensive analysis and strong validation results I recommend this paper for publication after addressing the minor weaknesses noted.", "iEx3PiooLy": " Summary: The paper proposes a novel approach for perceiving and manipulating 3D articulated objects using objectcentric actionable visual priors. The system VATMART utilizes a perceptioninteraction framework to learn actionable visual representations for interacting with articulated objects. Experiments conducted on the PartNetMobility dataset demonstrate the effectiveness and generalization capabilities of the proposed approach to novel shapes unseen object categories and realworld data.  Strengths: 1. Innovative Approach: The paper introduces a novel concept of actionable visual priors for manipulating 3D articulated objects bridging the gap between perception and interaction. 2. Comprehensive Technical Details: The paper provides detailed descriptions of the proposed framework VATMART including input representations outputs RL policy perception networks and training strategies enhancing the reproducibility of the work. 3. Experimental Validation: The experiments conducted on the PartNetMobility dataset and realworld data demonstrate the effectiveness and generalization capabilities of the proposed method providing quantitative and qualitative evidence of its performance. 4. Ethical Considerations and Reproducibility: The paper includes an ethics statement acknowledging the potential applications and ethical considerations of the research. The commitment to releasing code data and pretrained model weights upon acceptance enhances reproducibility.  Weaknesses: 1. Lack of Baseline Comparisons: While the paper demonstrates the effectiveness of the proposed approach it would be beneficial to include more extensive comparisons with existing methods or variations of the proposed approach to showcase its superiority comprehensively. 2. Complexity: The technical details provided in the paper might be overwhelming for readers not deeply familiar with the field potentially hindering understanding for a broader audience. 3. Future Directions: The paper briefly touches on future directions but a more detailed discussion on potential extensions limitations and areas for further research could enhance the completeness of the work.  Question to the Authors: 1. Can you provide insights into how the proposed approach could be adapted or enhanced for applications involving humanrobot interaction especially in dynamic environments  Feedback to the Authors: 1. Consider expanding the comparison section with more baseline methods to provide a more thorough evaluation of the proposed approach. 2. Enhance the accessibility of the technical details for readers with varying levels of expertise in the field. 3. Provide a more detailed discussion on future directions and potential limitations to guide further research in the field.  Review Summary: The paper introduces an innovative approach for perceiving and manipulating 3D articulated objects using objectcentric actionable visual priors. With detailed technical descriptions and comprehensive experimental validation the proposed framework VATMART shows promising results in terms of effectiveness and generalization capabilities. While the paper demonstrates strong contributions including ethical considerations and reproducibility addressing the weaknesses such as expanding baseline comparisons and enhancing accessibility of technical details would strengthen the overall impact and understanding of the research. Therefore I recommend acceptance of the paper pending minor revisions based on the feedback provided.", "yhCp5RcZD7": " Summary The paper proposes a novel representation for detailed 3D geometry called implicit displacement fields (IDFs). The method disentangles a complex surface into a smooth base surface and a highfrequency implicit displacement field enhancing geometric details. The approach is motivated by displacement mapping with the key innovation of extending it to a continuous function in the R3 domain. The paper introduces a neural architecture that creates a geometrically interpretable frequency hierarchy in the neural implicit shape representation. Systematic evaluations demonstrate superior representational power and stability in training.  Strengths 1. Innovative Method: The paper introduces a novel approach IDF for detailed 3D geometry representation extending classic displacement mapping to continuous functions. 2. Unsupervised Disentanglement: The disentanglement of detailed shapes into a smooth base surface and highfrequency displacement fields is a robust and unsupervised approach. 3. Theoretical Grounding: The method is theoretically grounded providing insights into the network architectures design and the training strategies. 4. Superior Representation: Results show the proposed method outperforms baseline approaches in detail representation tasks.  Weaknesses 1. Complexity: The method involves multiple components including network design training strategy and detail transfer which may make it challenging to implement for new users. 2. Limitation of Detail Transfer: The necessity to prealign shapes for detail transfer and possible degradation in performance when estimating base and displacements jointly are noted limitations.  Questions and Feedback for the Authors  Clarification: Can you provide more insights into any limitations or scalability issues of the proposed method especially in handling more complex and larger datasets  Comparison to Existing Techniques: How does the proposed IDF method compare to stateoftheart methods in terms of computational efficiency and scalability  Review Summary The paper presents a novel and theoretically grounded method IDF for detailed 3D geometry representation. The approach shows superior performance in representing geometric details with a robust unsupervised disentanglement strategy. However the complexity of the method and limitations in detail transfer could be areas for further investigation and improvement. Overall the paper is wellstructured thorough and makes a significant contribution to the field of 3D geometry representation. Thus I recommend acceptance with minor revisions to address the weaknesses highlighted above.", "nLb60uXd6Np": " Paper Summary: The paper introduces rotation and permutationequivariant architectures for deep learning on small point clouds utilizing geometric algebra and attention mechanisms. These architectures are applied to solve sample problems in physics chemistry and biology. By systematically combining vector scalar and other geometric inputs the authors aim to achieve rotation invariance and permutation equivariance crucial in physical sciences applications involving small point clouds.  Strong Points: 1. Innovative Approach: The integration of geometric algebra and attention mechanisms for achieving rotation and permutation equivariance is a novel and valuable contribution to the field of deep learning particularly in the context of small point cloud data. 2. Domain Relevance: The paper addresses relevant applications in physics chemistry and biology providing concrete examples that demonstrate the utility of the proposed architectures. 3. Clarity and Detail: The paper is wellstructured providing a comprehensive overview of the problem domain related work methodologies and results. Detailed descriptions diagrams and examples are included to enhance understanding. 4. Results and Analysis: Detailed results and comparisons with baseline models are provided showing the effectiveness of the proposed architectures in various applications. The discussion of attention maps also adds depth to the analysis.  Weak Points: 1. Complexity: The use of geometric algebra and attention mechanisms may make the proposed architectures complex for those less familiar with these concepts. Further simplification or explanation may be needed for broader accessibility. 2. Scalability Concerns: The computational complexity and memory requirements especially when generating combinations of input points could potentially limit scalability in realworld applications with larger datasets. 3. Generalization Testing: Only training set error is considered in the protein coarsegrain backmapping task. Incorporating typical data splitting for validation and testing would provide a more comprehensive evaluation of model generalization.  Questions and Feedback to the Author: 1. Scalability: Have you considered strategies to address the computational complexity and memory scaling concerns when working with larger datasets 2. Interpretability: Could you provide more insights into how the learned attention weights translate to meaningful interpretations or offer practical benefits in realworld applications 3. Generalization: How do you foresee the transferability of these architectures to other domains or datasets beyond the specific applications discussed in the paper  Review Summary: The paper presents a wellstructured and innovative approach leveraging geometric algebra and attention mechanisms to build rotation and permutationequivariant architectures for deep learning on small point clouds. Strong points include the novel methodology relevance to varied scientific domains and detailed analyses. However concerns regarding complexity scalability and generalization testing were noted. Overall the paper demonstrates promising results and significant potential for advancing geometric deep learning in scientific applications. Addressing scalability and generalization considerations could further enhance the impact and applicability of the proposed architectures. Therefore based on the innovative nature of the research clear presentation and promising results I recommend acceptance pending revisions to address scalability and generalization concerns.", "fWK3qhAtbbk": " PeerReview of \"Optimizing Time Series Forecasting with Exponential Partitioning of Data for LSTM Models\"  Summary: The paper explores the limitations of current methods for aggregating time series data for Long ShortTerm Memory (LSTM) models proposing an Exponential partitioning approach. The authors argue that existing uniform partitioning and averaging of data may not be optimal due to the decreasing significance of older data leading to a novel methodology that assigns exponentially increasing weights to historical data. The study evaluates the proposed method against seven public datasets demonstrating significant accuracy improvements ranging from 6 to 27 by using nonuniform partitioning and alternative aggregation functions like median or maximum. Experimental results and analyses support the effectiveness of the proposed approach in enhancing LSTM model performance for time series forecasting.  Strong Points: 1. Innovation: The paper presents a novel approach (Exponential partitioning) to address the limitations of existing data aggregation methods for LSTM models providing a fresh perspective on improving time series forecasting accuracy. 2. Experimental Rigor: The study conducts thorough experiments on multiple datasets allowing for a comprehensive assessment of the proposed methodologys performance across different scenarios. 3. Clear Presentation: The structure of the paper is logical starting from motivation to methods and results analysis which aids in understanding the research objectives methodologies and outcomes.  Weak Points: 1. Limited Comparison: While the paper compares exponential partitioning with existing uniform partitioning and different aggregation functions it could benefit from a more extensive comparison with other advanced forecasting models or techniques to showcase the superiority of the proposed method. 2. Theoretical Depth: The paper could offer a more indepth theoretical explanation of why exponential partitioning outperforms uniform partitioning providing insights into the underlying principles driving the improved forecasting accuracy. 3. Code Availability: The paper mentions the availability of the Exponential partitioning source code but lacks detailed information on the implementation and practical guidance for readers to replicate the experiments easily.  Question to the Author: 1. Have you considered conducting comparative experiments with other advanced forecasting models like ARIMA Prophet or hybrid models to provide a more comprehensive evaluation of the proposed Exponential partitioning methods effectiveness 2. How do you plan to further validate and refine the Exponential partitioning approach in future research and do you anticipate any limitations or challenges in scaling this method to diverse time series forecasting tasks  Feedback to the Author: 1. Enhance the paper by providing a detailed theoretical background on why exponential partitioning is more effective in preserving historical data significance compared to uniform partitioning. 2. Consider expanding the discussion section to address the implications of the study findings on practical applications and potential future research directions for optimizing time series forecasting models.  Review Summary: The paper presents a compelling argument for optimizing time series forecasting with Exponential partitioning showcasing significant accuracy improvements over traditional uniform partitioning methods. While the studys experimental results are robust the comparison scope could be extended to include a broader range of forecasting models for a more comprehensive evaluation. Addressing theoretical underpinnings code availability details and discussing practical implications and future research avenues would strengthen the overall impact and relevance of the study. Based on the novelty of the proposed approach and the promising results achieved I recommend major revisions to enhance the depth and scope of the manuscript positioning it as a valuable contribution to the field of time series forecasting and LSTM model optimization.", "s6roE3ZocH1": "1) Summary of the Paper: The paper introduces a genetic algorithm with constrained molecular inverse design for optimizing specific properties of molecules while maintaining structural constraints. The algorithm utilizes graph and SELFIES descriptors to generate valid molecules through a twophase optimization process. The first phase focuses on ensuring structural similarity within the population while the second phase involves bioptimization for property and constraint conditions. The study conducts experiments comparing the proposed method with various baseline models demonstrating its effectiveness in generating molecules that satisfy specific properties while adhering to structural constraints. 2) Strong Points:  The paper addresses a significant challenge in molecular design by introducing a genetic algorithm with constrained molecular inverse design.  The use of graph and SELFIES descriptors for molecular generation allows for the creation of valid molecules while maintaining structural constraints.  The twophase optimization approach effectively balances constraint satisfaction and property optimization.  The experiments comparing the proposed method with baseline models provide empirical evidence supporting the algorithms effectiveness in generating optimized molecules with specific properties. Weak Points:  The paper could benefit from a more detailed explanation of the datasets used for experimentation and the selection criteria for the target molecules.  The genetic operators section could be expanded to provide more insight into how the crossover and mutation operations are specifically implemented within the algorithm.  The discussion on the limitations or potential challenges of the proposed method could be further developed to enhance the papers comprehensiveness. Questions and Feedback to the Author:  Can you provide more information on the specific criteria used to evaluate the success of the molecular optimization process in terms of properties like LogP SAScore and RingPenalty  How do you handle issues related to diversity and novelty in generated molecules to ensure a wide exploration of the chemical space  It would be helpful to elaborate on the computational complexity of the proposed algorithm and its scalability when dealing with larger datasets or more complex molecular structures. 3) Review Summary: The paper presents a novel genetic algorithm for constrained optimization in molecular inverse design which effectively balances property optimization with structural constraints. The approach using graph and SELFIES descriptors along with the twophase optimization strategy demonstrates promise in generating valid molecules that satisfy specific properties while adhering to structural requirements. The comparative experiments highlight the algorithms superiority over baseline models underscoring its practical value in molecular design applications. However further elaboration on experimental methodologies algorithm scalability and future research directions could enhance the papers overall impact and thoroughness. Based on the strengths exhibited in addressing the challenging task of constrained molecular optimization and the demonstrated effectiveness in generating valid molecules with optimized properties I recommend this paper for acceptance after addressing the suggested enhancements.", "gxRcqTbJpVW": " Summary The paper introduces Orthogonality Preserving Pruning (OPP) a regularizationbased structured pruning method aimed at maintaining dynamical isometry during pruning in modern deep neural networks. OPP encourages kernel orthogonality among important filters and drives unimportant weights towards zero. The method also includes regularization of batch normalization parameters to better preserve dynamical isometry. Empirical results demonstrate that OPP outperforms existing solutions competing well with ideal dynamical isometry recovery methods on linear networks and delivering significant performance gains on nonlinear networks and largescale deep networks like ResNets on ImageNet. Overall OPP is the first method known to effectively maintain dynamical isometry during pruning for largescale deep neural networks.  Strong Points  Technical Contributions: The paper introduces a novel method OPP that effectively maintains dynamical isometry in pruning modern deep networks through innovative regularization techniques.  Empirical Results: The method demonstrates impressive performance gains over existing solutions on various networks and datasets showcasing its effectiveness in realworld applications.  Scalability: OPP is scalable to modern deep networks like ResNets on ImageNet making it applicable to largescale deep learning tasks.  Indepth Analysis: The paper provides thorough discussions on the methodology related work and experimental results enhancing the understanding of the proposed method.  Weak Points  Comparative Analysis: It would be beneficial to include more detailed comparisons with existing methods particularly discussing the limitations of OPP in certain scenarios or architectures.  Evaluation Metrics: While accuracy comparisons are extensively discussed additional metrics such as inference speed or model size reduction could provide a more holistic view of the methods efficacy.  Reproducibility: Providing more details on the implementation code availability and hyperparameter settings could enhance the reproducibility and credibility of the results.  Questions for the Author 1. Can you elaborate on the potential limitations or failure cases of OPP especially in scenarios where dynamical isometry maintenance may be more challenging 2. How does OPP compare in terms of computational efficiency or model size reduction compared to other pruning methods especially for deployment on resourceconstrained devices 3. Have you considered conducting experiments on datasets or architectures with different characteristics to assess the generalizability and robustness of OPP  Feedback The paper presents a significant contribution to the field of structured pruning by introducing a novel method OPP aimed at maintaining dynamical isometry during pruning in modern deep networks. The experimental results support the effectiveness of OPP showcasing its competitive performance compared to existing solutions. To strengthen the paper additional discussion on limitations reproducibility details and broader comparison with related methods would be beneficial. Overall the research is wellpresented and addresses an important aspect of neural network pruning warranting further exploration and validation.", "tHx6q2dM86s": " Summary of the Paper: The paper introduces HYPOCRITE a method that generates homoglyph adversarial examples for natural language web services in the physical world. These adversarial examples can disrupt normal AI services provided by cloud companies by replacing English characters with similar international characters causing misclassification through blackbox attacks. The paper presents the methodology experimental results and comparison with a baseline approach demonstrating HYPOCRITEs effectiveness in terms of attack success rate and perturbed ratio.  Strong Points: 1. Innovative Approach: Introducing HYPOCRITE for generating homoglyph adversarial examples is a novel and interesting concept. 2. Detailed Experiments: The paper provides extensive experimental results demonstrating the effectiveness of HYPOCRITE compared to a baseline approach under various conditions. 3. User Study: Including a user study to evaluate the recognition difficulty of the adversarial examples adds valuable insights to the study. 4. Clear Organization: The paper is wellstructured with clear sections discussing related work attack model experiments results and conclusion.  Weak Points: 1. Evaluation Metrics: While the paper mentions attack success rate and perturbed rate it could benefit from additional metrics related to the impact of the attacks on the AI models. 2. Research Challenges: The paper briefly mentions research challenges but does not delve deep into potential solutions or strategies to address them. 3. Cost Limitations: Discussing the financial implications of evaluating the results using sentiment analysis APIs is crucial but addressing potential costeffective strategies would enhance the practicality of the approach.  Questions for the Author: 1. Could you elaborate on the specific impact of the homoglyph attacks on the performance and integrity of the AI models used by the cloud service providers 2. How would you address the research challenges mentioned in the paper to improve the effectiveness and efficiency of the HYPOCRITE approach  Feedback to the Author:  Consider expanding the discussion on the implications of the research findings for the AI industry and potential countermeasures against homoglyph attacks.  Provide more insights into the user study results including any trends or patterns observed in users ability to recognize the adversarial examples.  Enhance the paper by suggesting practical solutions to mitigate the financial constraints associated with evaluating results using sentiment analysis APIs.  Further clarify the significance of the proposed methodology in addressing security vulnerabilities in natural language web services.  Review Summary: The paper presents a novel approach HYPOCRITE for generating homoglyph adversarial examples targeting natural language web services. Its detailed experimental evaluation showcases superior performance compared to a baseline method. While the paper is wellorganized and offers valuable insights addressing weaknesses related to evaluation metrics research challenges and cost limitations would strengthen the overall study. With some enhancements and clarifications the paper can significantly contribute to the field of AI security and adversarial attacks.", "vtDzHJOsmfJ": " Summary of the Paper: The paper focuses on addressing fairness issues in supervised learning models particularly in relation to the Equalized Loss (EL) fairness notion where prediction errorloss needs to be equalized across different demographic groups. The study formulates the fairness problem as a nonconvex optimization task and proposes algorithms to efficiently find optimal solutions. These algorithms leverage convex programming tools and are applicable to both regression and classification problems. The paper provides theoretical results algorithms complexity analysis and experimentation on realworld data to validate the effectiveness of the proposed approach.  Strong Points of the Paper: 1. Relevance and Novelty: The paper addresses a critical and contemporary issue related to fairness in machine learning models particularly focusing on Equalized Loss fairness which is a significant contribution to the field. 2. Theoretical Contributions: The study provides rigorous mathematical formulation the relationship between Equalized Loss and Bounded Group Loss fairness notions theoretical results and proofs to support the proposed algorithms effectiveness. 3. Algorithm Development: The proposed algorithms tackle a challenging nonconvex optimization problem efficiently by reducing it to a sequence of convex optimizations demonstrating innovation in solving fairnessconstrained optimization problems. 4. Experimental Validation: The paper validates the proposed algorithms through experiments on realworld data demonstrating the effectiveness of the approach in practical scenarios.  Weak Points of the Paper: 1. Complexity: While the proposed algorithms are theoretically sound and shown to be effective the complexity of the algorithms may not be trivial especially for practitioners not wellversed in optimization theory. Providing more intuitive explanations could aid in better understanding. 2. Empirical Evaluation Limitation: The experiments conducted focus on synthetic data and a specific dataset limiting the generalizability of the results. More diverse datasets and realworld applications could enhance the external validity of the findings. 3. Practical Implementability: While the paper discusses the implementation of the algorithms practical guidance such as code snippets or software packages would enhance the accessibility of the proposed methods for practitioners and researchers.  Questions and Feedback for the Author: 1. Scalability: How do the proposed algorithms scale with larger datasets and more complex feature spaces Have you tested their scalability on datasets with higher dimensions and samples 2. Comparison with Existing Methods: How do the proposed algorithms compare with existing stateoftheart fairnessaware learning methods especially in terms of performance computational efficiency and fairness achieved 3. Extensions: Are there any plans to extend the research to address other fairness notions or explore the application of the proposed algorithms in different domains beyond the ones presented in the paper  Review Summary: The paper presents a rigorous study on fairnessaware supervision learning models under the Equalized Loss (EL) fairness notion. The theoretical contributions algorithm development and experimental validation showcase the papers significance. While the algorithms show promise in addressing fairness issues efforts to improve the practical implementability scalability and comparison with existing methods could enhance the papers impact. Overall based on the strong theoretical foundation innovative algorithms and initial experimental results I recommend acceptance of the paper with minor revisions for improved clarity and applicability.", "uVXEKeqJbNa": " Peer Review  Summary: The paper introduces a novel method called stiffnessaware neural network (SANN) for learning Hamiltonian dynamical systems from data. The proposed method classifies training data into stiff and nonstiff portions based on a stiffnessaware index (SAI) to capture the stiffness of the dynamical system. By applying different time integration strategies and resampling techniques SANN demonstrates improved stability energy preservation and accuracy compared to existing methods. The performance of SANN is evaluated on complex physical systems such as the threebody problem and billiard model.  Strong Points: 1. Novel Methodology: The introduction of SANN along with the concept of SAI to address the challenges associated with learning stiff Hamiltonian dynamics is a significant contribution. The approach of splitting training data based on stiffness classification and using resampling techniques is innovative. 2. Experimental Validation: The paper provides thorough experimental validation on complex physical systems illustrating the superior performance of SANN compared to existing methods. Results in terms of energy preservation and accuracy are wellsupported and clearly presented. 3. Theoretical underpinning: The paper provides detailed insights into the theoretical foundation of the proposed approach including the characterization of stiffness through SAI theoretical comparisons with SI and detailed explanations of the methodology. 4. Exhaustive Experiments and Results: The paper includes detailed experiments on the billiard model and threebody problem showcasing the effectiveness of SANN in learning dynamical systems. The comparison with stateoftheart methods and the ablation study provide comprehensive insights into the performance of the proposed approach.  Weak Points: 1. Complexity: The paper while thorough in its explanations can be quite dense and technical which might make it challenging for readers with limited background knowledge in the subject area to grasp the concepts easily. 2. Reproducibility: The paper lacks a detailed description of the code implementation and the datasets used in the experiments which could hinder reproducibility and further validation of the proposed method by other researchers.  Questions and Feedback to Authors: 1. How does the proposed method scale with larger and more complex dynamical systems compared to the ones tested in the experiments 2. Could you provide more details on the hyperparameters selected for the experiments such as the rationale behind choosing specific partition sizes and replication values in resampling to help readers understand the experimental setup better 3. Given the theoretical comparison between SAI and SI would it be beneficial to provide more insights into the practical implications and limitations of using SAI as a proxy for stiffness classification in realworld scenarios  Review Summary: The paper presents an innovative approach SANN for learning Hamiltonian dynamical systems specifically focusing on handling stiff dynamics. The proposal of a stiffnessaware index SAI and the methodology of splitting training data and applying resampling techniques demonstrate significant improvements in stability and accuracy. Thorough experimental validation on complex physical systems supports the claims made by the authors. Despite the denseness of the content and lack of detailed implementation information the papers theoretical foundations and extensive experimentation make it a valuable contribution to the field of datadriven dynamical systems modeling. Addressing the minor issues would further strengthen the papers impact and accessibility. Overall based on the strong methodology theoretical rigor and compelling experimental results I recommend acceptance of this paper for publication.", "rI0LYgGeYaw": " Review of \"Approximate Dictionary Learning with Algorithm Unrolling: Convergence Analysis and Application to Magnetoencephalography Pattern Learning\"  1) Summary: The paper explores approximate dictionary learning based on algorithm unrolling and compares it to alternating minimization (AM) for a tradeoff between speed and precision. The research addresses the unstable nature of nonsmooth bilevel optimization and evaluates unrollings performance particularly in sparse coding tasks. The study includes theoretical analysis on gradient estimation convergence and practical applications in magnetoencephalography (MEG) for pattern learning.  2) Strong Points:  Theoretical Analysis: The paper provides a comprehensive theoretical analysis on the convergence behavior of gradient estimation demonstrating a deep understanding of the algorithm unrolling process in dictionary learning.  Empirical Validation: The empirical studies on synthetic data and realworld applications particularly in MEG pattern learning provide practical insights into the performance and efficiency of the proposed method.  Stochastic Optimization: Introducing stochastic optimization in the context of dictionary learning showcases a practical and efficient approach to handle large datasets and speed up computations.  Weak Points:  Incomplete Explanation: While the paper covers theoretical aspects in depth explanations on some concepts might be challenging for readers without a strong background in optimization and signal processing.  Lack of Comparative Analysis: The paper could benefit from a more extensive comparison with existing methods in terms of performance metrics efficiency and scalability.  Questions and Feedback: 1. Could you provide more insights into the computational complexity aspect of the proposed method compared to traditional approaches like AM How does the tradeoff between speed and precision impact the overall efficiency in practical applications 2. In realworld scenarios like MEG pattern learning how does the proposed method compare in terms of computational efficiency and accuracy with stateoftheart algorithms currently used in the field 3. Further clarification on the numerical instabilities observed in algorithm unrolling after multiple iterations would be beneficial for readers to understand the limitations and potential solutions in handling such challenges.  3) Review Summary: The paper delves into the intricate details of approximate dictionary learning through algorithm unrolling demonstrating a deep understanding of the convergence behavior tradeoffs between speed and precision and practical implications in pattern learning tasks. The empirical validation in MEG data highlights the methods efficiency and potential for realworld applications. However further comparative analysis and explanations on numerical instabilities would enhance the papers overall impact.  Rating: Highly Recommended for Publication", "xNOVfCCvDpM": " Summary of the Paper: The paper investigates the effectiveness of three types of post hoc model explanations (feature attribution concept activation and training point ranking) for detecting a models reliance on spurious signals in the training data particularly when the spurious signal is unknown at test time. The study includes an empirical methodology using semisynthetic datasets with predefined spurious artifacts to test the reliability of explanation methods for detecting spurious signals. The findings suggest that the post hoc explanation methods tested are ineffective when the spurious artifact is unknown especially for nonvisible artifacts like a background blur. Feature attribution methods are also found to erroneously indicate dependence on spurious signals even when the model does not rely on them.  Strong Points: 1. Rigorous Methodology: The paper employs a comprehensive experimental design with clear definitions measures and metrics for evaluating the effectiveness of explanation methods for detecting spurious signals. 2. Clear Contributions: The paper clearly outlines its contributions highlighting the importance of detecting spurious signals and providing guidance for using explanation methods effectively. 3. Empirical Study and User Study: The inclusion of both an empirical study and a blinded user study adds depth to the analysis offering insights into the practical implications of using post hoc explanations. 4. Comparative Analysis: The comparison of feature attribution concept activation and training point ranking methods across different datasets and model architectures provides a holistic view of their performance.  Weak Points: 1. Limited Scope: The study primarily focuses on DNNs trained on image datasets limiting the generalizability of findings to other data modalities. 2. False Positives: The findings indicate that the methods are susceptible to false positives casting doubt on their reliability for detecting spurious signals. 3. Unclear Generalization: It is not clear how the findings may apply to different types of datasets or tasks beyond the specific image classification scenarios studied.  Questions and Feedback: 1. Clarity on Methodology: Could the paper elaborate on the rationale behind selecting specific performance metrics and similarity functions for evaluating the explanation methods 2. Further Experiments: Have you considered extending the study to include datasets beyond image tasks to validate the findings in different domains 3. Implications for Practice: How do you suggest practitioners navigate the limitations identified in current post hoc explanation methods for detecting spurious signals in realworld applications  Review Summary: The paper presents a wellstructured study focused on evaluating post hoc explanation methods for detecting spurious signals in trained models. While the methodology is robust and the contributions are clear the paper raises concerns regarding the reliability of the tested methods particularly in detecting unknown spurious signals. The findings highlight the challenges faced by practitioners in using existing explanation approaches effectively. Given the limitations and susceptibility to false positives further research is needed to develop robust methods for detecting spurious signals in diverse datasets and applications. Overall the paper provides valuable insights into the limitations and potential improvements in post hoc explanations for model interpretability.", "qj1IZ-6TInc": " Summary The paper presents a method called predictive attacks to defend against eavesdropping through automatic speech recognition systems. The approach aims to obstruct the ASR systems in realtime by forecasting attacks that are effective in the future. The method demonstrates significant disruption to the ASR model especially the DeepSpeech system. Experimental results show that the method is practical and effective in realworld scenarios.  Strengths  Innovative Approach: The concept of predictive attacks for realtime obstruction of ASR systems is novel and addresses a critical concern regarding privacy.  Performance: The method achieves substantial disruption to the DeepSpeech system compared to baselines showcasing its effectiveness in jamming speech recognition.  RealWorld Applicability: The experiments conducted in realistic environments with ambient noise add practical value to the proposed method.  Ethical Considerations: The paper acknowledges ethical considerations ensuring the protection of user privacy and commitment to ethical principles.  Weaknesses  Evaluation Limitation: The method is tested on a limited dataset and may not generalize well to diverse linguistic and cultural backgrounds.  Accuracy Concerns: Acknowledgment of the lack of 100 accuracy and potential reliance on the model raises concerns about deployment in critical settings.  Language and Speech Impediments: Lack of validation on languages and speech impediments limits the generalizability of the method.  Question and Feedback 1. Validation on Diverse Languages: Have you considered testing the method on languages beyond the Western speech data to validate its performance across a broader linguistic spectrum 2. User Interaction and Reliability: How do you plan to ensure that users do not come to rely too heavily on the attack method considering limitations in accuracy and possible vulnerabilities  Review Summary The paper introduces an innovative method of predictive attacks to jam ASR systems in realtime showing promising results in disrupting the DeepSpeech model. The approach addresses a critical privacy concern and demonstrates practical effectiveness in realworld scenarios with ambient noise. However further validation on diverse languages and consideration of user interaction impacts are crucial. Overall the paper presents a valuable contribution to the field of privacy protection in speech recognition but requires additional validation and considerations for realworld deployment.", "gEynpztqZug": "Summary of the Paper: The paper introduces a semisupervised approach named Mako for lifelong machine learning (LML) that addresses the challenge of expensive labeling at the individual task level. Mako leverages data programming to generate labels for unlabeled data and can be applied on top of existing supervised LML frameworks without imposing extra knowledge base storage overhead. The approach shows promise in achieving high accuracy and preventing catastrophic forgetting in LML tasks sequences formed from standard image classification datasets like MNIST CIFAR10 and CIFAR100. Strengths: 1. Innovative Approach: The paper introduces a novel approach Mako that fills a gap in the field of LML by addressing the issue of expensive labeling and lack of labeled data at the individual task level. 2. Empirical Evaluation: The paper presents a comprehensive evaluation of Mako on various partially labeled LML task sequences comparing it to supervised learning on fully labeled data and existing semisupervised LML tools. The empirical results demonstrate the effectiveness of Mako in achieving high accuracy. 3. Modularity: Mako is designed as a wrapper tool that can be mounted on top of any existing LML framework enhancing modularity and allowing for easy integration into different models. 4. Theoretical Guarantees: The paper provides theoretical guarantees for the Mako approach ensuring the accuracy and confidence of the generative model labels in semisupervised learning. 5. Extensive Experiments: The paper conducts experiments on multiple datasets and tasks demonstrating the performance of Mako in different scenarios and comparing it to baseline methods. Weaknesses: 1. Complexity: The paper introduces a complex framework with various components such as automatic hyperparameter search guaranteed data programming with bootstrapping and confidence calibration. This complexity might make it challenging for researchers to implement and replicate the approach. 2. Limited Domain Exploration: The experiments are focused on image classification datasets limiting the generalizability of the approach to other domains or types of tasks. Question to Authors: 1. Can you provide insights into the scalability of the Mako framework when applied to larger datasets or more complex tasks Feedback to Authors: 1. Consider providing a more detailed explanation or illustration of the Mako framework to improve clarity especially for readers less familiar with data programming and semisupervised learning methods. Review Summary: The paper presents a novel semisupervised approach Mako for lifelong machine learning that addresses the challenge of expensive labeling in individual LML tasks. The approach is innovative shows promising results in empirical evaluation and introduces new theoretical guarantees. While the framework is complex its modularity and effectiveness make it a significant contribution to the field of LML. Overall I recommend the paper for publication pending minor revisions to enhance clarity and provide additional insights into scalability.", "hdSn_X7Hfvz": "Paper Summary: The paper investigates probability estimation from highdimensional data using deep neural networks. It introduces a new method Calibrated Probability Estimation (CaPE) which modifies the training process to ensure output probabilities consistent with empirical probabilities. The study includes evaluating existing methodologies on synthetic and realworld datasets concerning precipitation forecasting cancer patient survival prediction and vehicle collision forecasting. The proposed CaPE method outperforms existing approaches on most metrics in both synthetic and realworld data. Strengths: 1. Novel Methodology: The introduction of the CaPE method is a significant contribution to probability estimation using deep neural networks. 2. Comprehensive Evaluation: The paper evaluates existing approaches on synthetic and realworld datasets covering different application areas providing a thorough analysis. 3. Benchmark Datasets: Building synthetic and realworld benchmark datasets adds value to the research allowing for systematic evaluation and comparison of methods. 4. Detailed Explanations: The paper provides detailed explanations of the methodology including algorithms evaluations metrics and comparisons with existing methods. 5. Practical Application: Addressing realworld issues such as weather prediction cancer prognosis and collision detection enhances the practical relevance and impact of the research. Weaknesses: 1. Limited Comparison: While the paper introduces a novel method more extensive comparison with existing calibration techniques could provide a clearer understanding of its advantages over current approaches. 2. Lack of Sensitivity Analysis: It would be beneficial to see how the proposed method performs with variations in hyperparameters or dataset characteristics to assess its robustness. 3. Theoretical Analysis: Some additional theoretical analysis or discussions on the underlying principles supporting the proposed method could enhance the depth of the research findings. Questions for the Author: 1. How does the CaPE method handle imbalanced datasets especially in scenarios like collision prediction where the number of outcomes is very low 2. Could you provide more insights into the computational efficiency and scalability of the proposed CaPE method for handling largescale datasets 3. Have you considered conducting user studies or practical implementation trials to validate the performance of the CaPE method in realworld applications Feedback for the Author: 1. Consider providing more comparative results with existing calibration methods to showcase the superiority of the CaPE approach clearly. 2. Include sensitivity analyses or robustness checks to evaluate the performance of the CaPE method under various conditions. 3. Enhance the discussion section by delving deeper into the practical implications and potential limitations of the proposed methodology. Review Summary: The paper presents a comprehensive study on probability estimation using deep neural networks introducing a novel method named Calibrated Probability Estimation (CaPE). It extensively evaluates existing techniques on synthetic and realworld datasets highlighting the superior performance of CaPE. The papers strengths lie in its innovative methodology detailed evaluations and practical applicability. However further comparative analyses sensitivity studies and theoretical discussions would strengthen the research. Overall the paper makes a valuable contribution to the field of probability estimation warranting publication after addressing the suggested improvements.", "jGmNTfiXwGb": " Summary: The paper introduces a novel methodology for approximating offline algorithms in online settings. By using a graphbased labeling scheme and multitask learning model the methodology learns to predict future behavioral structures based on past data. The approach is demonstrated on synthetic and historical stock market data highlighting the utility of the method in generating online approximations of offline algorithms.  Strengths: 1. Innovative Methodology: The paper introduces a unique approach for approximating offline algorithms in online settings using graphbased labeling and multitask learning. 2. Application Diversity: The methodology is showcased on both synthetic and realworld historical stock market data demonstrating its versatility in various domains. 3. Clear Explanations: The paper provides detailed explanations of the methodology algorithms used datasets and model architecture making it accessible to readers. 4. Performance Validation: Results from experiments on synthetic and realworld datasets validate the effectiveness of the proposed methodology. 5. Reproducibility: Availability of code data and configuration scripts under the MIT License enhances reproducibility and transparency.  Weaknesses: 1. Limited Dimensionality: The experiments are confined to 1D settings and the methods applicability to highdimensional or multidimensional data remains to be explored. 2. Simplistic Model Architecture: The paper does not delve deeply into the scalability or complexity aspects of the model architecture used especially regarding larger datasets. 3. Experimental Limitations: The focus on specific types of offline algorithms and relatively small dataset sizes might limit the generalizability of the findings.  Questions: 1. Scalability: How scalable is the proposed methodology to handle larger datasets or highdimensional time series data 2. Generalization: Can the approach be extended to a broader range of offline algorithms beyond the specific ones discussed in the paper 3. Comparison: How does the performance of the proposed method compare with existing approaches for translating offline algorithms to online settings  Feedback: 1. Provide insights into the scalability aspects and address potential limitations when applied to larger datasets. 2. Consider discussing the tradeoffs between model complexity and interpretability in the context of different offline algorithms. 3. Expand on the implications of the methodology beyond the outlined experiments highlighting potential applications and realworld implications.  Review Summary: The paper presents a novel methodology for approximating offline algorithms in online settings demonstrating its efficacy on synthetic and realworld datasets. While the approach is innovative and validated through experiments further investigation into scalability generalizability and implications for diverse offline algorithms would enhance the papers impact. Overall the paper provides a valuable contribution to the field of online approximation of offline algorithms and warrants further exploration and analysis.", "tCx6AefvuPf": " Summary of the Paper: The paper introduces a novel approach for learning Graph Neural Networks (GNNs) with nodelevel privacy guarantees. It addresses the challenge of preserving sensitive information while leveraging the power of GNNs which aggregate data from the neighborhood of each node in a graph for enhanced modeling capabilities. The proposed method DPGNN integrates differential privacy techniques with GNNs to provide accurate solutions while ensuring strong privacy parameters. The paper outlines the problem formulation method algorithm and empirical evaluation on standard benchmark datasets to demonstrate the effectiveness of the approach.  Strengths of the Paper: 1. Innovative Approach: The paper proposes a unique method to maintain nodelevel privacy in GNNs a crucial aspect in scenarios where sensitive information must be protected. 2. Solid Theoretical Framework: The paper provides a formal definition of the problem algorithmic solution and formal privacy guarantees underpinning the proposed method with a strong theoretical foundation. 3. Comprehensive Evaluation: Extensive empirical evaluation on standard benchmarks showcases the effectiveness of the proposed DPGNN method in outperforming both nonprivate baseline methods and alternative differentially private approaches. 4. Clear Exposition: The paper is wellstructured with detailed explanations of the problem formulation algorithm steps and experimental results making it accessible for a broad audience.  Weak Points of the Paper: 1. Clarity in Technical Details: The paper delves into intricate technical aspects such as sensitivity analysis and privacy amplification which may be challenging for readers not wellversed in differential privacy and GNNs. Providing more intuitive explanations or examples could enhance understanding. 2. Relevance of Datasets: While the empirical evaluation is performed on standard benchmarks the paper could benefit from a discussion on the generalizability of the proposed approach to diverse realworld applications and datasets.  Questions and Feedback to the Author: 1. Could you provide a more intuitive example or analogy to help nonexperts understand the core concepts of differential privacy and how it is integrated into the GNN framework 2. How does the proposed DPGNN method compare to other stateoftheart privacypreserving techniques outside of the GNN domain Are there any insights on how it might perform in more complex or dynamic graph structures  Review Summary: The paper presents a significant contribution in the intersection of privacypreserving machine learning and graph neural networks. The proposed DPGNN method offers a novel approach to learning GNNs with nodelevel privacy guarantees demonstrating improved accuracy over standard nonprivate methods while safeguarding sensitive user data. Despite its technical depth the papers rigor in problem formulation algorithm development and empirical evaluation solidify its importance in advancing privacyaware GNN research. Addressing the clarity of technical details and scalability to diverse datasets would further enhance the papers impact and applicability. Overall I recommend the paper for publication based on its innovative methodology strong theoretical foundation and promising empirical results.", "ovRQmeVFbrC": " Summary of the Paper: The paper proposes a novel method called PARS (PseudoLabel Aware Robust Sample Selection) to address the issue of learning with noisy labels in largescale datasets. PARS utilizes a hybrid approach combining sample selection methods noise robust loss functions and label correction techniques in a joint training framework. By leveraging both rawnoisy labels and estimatedrefurbished pseudolabels via selftraining dividing samples into ambiguous and noisy subsets through loss analysis and designing labeldependent noiseaware loss functions PARS aims to achieve robustness to noisy labels. Extensive experiments on noisy CIFAR10 CIFAR100 and Clothing1M datasets show that PARS outperforms existing stateoftheart methods especially in highnoise and lowresource settings.  Strong Points: 1. Innovative Approach: The paper introduces a novel method (PARS) that integrates various techniques to tackle the challenge of learning from noisy labels effectively. 2. Comprehensive Experiments: Extensive experiments on benchmark datasets demonstrate the superiority of PARS over existing methods in handling noisy labels especially in highnoise and lowresource scenarios. 3. Detailed Methodology: The paper provides detailed explanations of the proposed method including robust warmup training sample selection labeldependent noiseaware loss functions selftraining with pseudolabels and data augmentation. 4. Significant Performance Gains: Results showcase impressive improvements in test accuracy particularly in challenging settings with high label noise. 5. Ablation Study: Conducting an ablation study to analyze the importance of various components in PARS adds depth to the evaluation and helps in understanding the contributions of different stages.  Weak Points: 1. Theoretical Insights: While the paper is strong on empirical results more theoretical insights or analyses supporting the effectiveness of the proposed method could enhance the impact of the research. 2. Comparison with Baselines: While the experimental results are promising a more detailed comparison with baselines discussing limitations and insights gained from failures could improve the paper.  Questions and Feedback to the Author: 1. Question: How does PARS handle the generalization performance when faced with unseen or outofdistribution data 2. Feedback: Adding a discussion on the scalability of PARS to larger datasets or different domains could provide insights into the methods applicability beyond the datasets tested.  Review Summary: The paper presents a novel method PARS for learning with noisy labels showcasing significant performance gains over existing stateoftheart methods in noisy label scenarios particularly in the presence of high noise and limited labeled data. The proposed approach is innovative wellexplained and supported by comprehensive experiments. Additionally an ablation study enhances the understanding of the methods components. While the empirical results are compelling the paper could benefit from more theoretical insights and a deeper discussion on scalability and generalization aspects. Overall I recommend acceptance for publication considering the methods contributions and effectiveness demonstrated in the experiments.", "m7zsaLt1Sab": " Summary: The paper introduces a novel perspective to analyze contextualized word embeddings focusing on how models represent context rather than just individual tokens. The authors propose a framework based on category theory and manifold learning methods to study the representation of context in transformerbased language models with an emphasis on disentangling the effect of observed conditions from the central tokens. The study provides theoretical definitions analysis scenarios and empirical results using BERT as a case study.  Strong Points: 1. Innovative Perspective: The paper presents a unique and innovative perspective on contextualized word embeddings shedding light on the representation of context in language models. 2. Theoretical Foundation: The use of category theory and manifold learning methods provides a rigorous mathematical framework to analyze and characterize the representation of context in linguistic models. 3. Empirical Results: The paper includes empirical results from experiments conducted using BERT offering insights into how context is represented and evolves across model components like attention and FFN layers. 4. Comprehensive Approach: The paper covers a wide range of aspects including theoretical formulations analysis scenarios and practical experiments ensuring a holistic understanding of the proposed concept.  Weak Points: 1. Complexity: The papers theoretical and mathematical formulations may be challenging for readers without a strong background in category theory or manifold learning potentially limiting the accessibility of the content. 2. Clarity in Presentation: The complexity of the concepts discussed could benefit from clearer and more concise explanations with an effort to make the content more approachable to a broader audience.  Questions: 1. Can you provide more insights into how the proposed framework can be generalized to other transformerbased models beyond BERT 2. How do you plan to address the complexity of the theoretical formulations to ensure broader accessibility and understanding among readers  Feedback: 1. Consider providing more intuitive explanations alongside the complex theoretical formulations to ensure clarity and understanding for a wider audience. 2. Include a section that discusses the practical implications of the proposed framework on improving the performance of language models in realworld applications.  Review Summary: The paper introduces a novel perspective on contextualized word embeddings focusing on the representation of context in transformerbased language models. While the papers innovative approach and rigorous theoretical foundation are commendable there is a need for improved clarity in presentation to enhance accessibility and understanding. The empirical results provide valuable insights but further clarification and practical implications are necessary to strengthen the papers impact. Overall with some revisions to enhance clarity and accessibility the paper has the potential to significantly contribute to the understanding of contextualized language representation models.", "rl8jF3GENq": " Summary of the Paper: The paper proposes a novel method for detecting synthetic images using a waveletpacket representation focusing on space and frequency domains. The study examines the differences between real and synthetic images in terms of mean and standard deviation of waveletpacket coefficients. The research contributes by introducing boundary wavelets for image analysis in a deep learning context and building classifiers for identifying image sources using wavelet representations. The experiments are conducted on FFHQ CelebA and LSUN datasets showcasing improved or competitive performance.  Strengths of the Paper: 1. Innovative Methodology: The paper introduces a novel approach using waveletpacket representations for synthetic image detection filling a gap in existing literature that mainly focuses on spatial or frequency domain analysis. 2. Comprehensive Experiments: The study evaluates the proposed method on multiple datasets demonstrating improved or competitive performance over existing techniques. 3. Interpretability: The paper provides clear explanations and visualizations (e.g. in Figures 1 and 3) to help understand the differences between real and synthetic images in the wavelet domain. 4. Efficient Implementation: The method is presented as lightweight and efficient with small network sizes and reproducible results over multiple runs. 5. Open Source Code: The authors provide a link to the source code for their wavelet toolbox making the method accessible for further research and applications.  Weaknesses of the Paper: 1. Comparison with StateoftheArt: While the paper mentions improved performance compared to existing methods a more detailed comparison with stateoftheart deepfake detection techniques could further validate the proposed approach. 2. Limited Insight into Limitations: The paper could benefit from a discussion on the limitations and potential challenges of the proposed method especially when applied to realworld scenarios.  Questions for the Authors: 1. How does the proposed waveletpacket approach compare with stateoftheart deepfake detection methods in terms of accuracy computational efficiency and robustness 2. Can the method handle variations in image quality different types of GAN architectures or other forms of image manipulation beyond the datasets tested 3. Have you considered conducting user studies or realworld application testing to assess the practical usability and effectiveness of the proposed approach in detecting deepfake content  Feedback to the Authors:  Elaborate Comparative Analysis: Provide a more detailed comparison with existing deepfake detection techniques showcasing the strengths and limitations of the proposed waveletbased method.  Address Generalizability: Discuss the generalizability of the method beyond the specific datasets used in the experiments and potential challenges in realworld deployment.  Expand on Robustness: Consider exploring the robustness of the approach against adversarial attacks or unknown variations in synthetic images to enhance the reliability of the detection method.  Review Summary: The paper introduces an innovative waveletbased method for detecting synthetic images showcasing improved or competitive performance on various datasets. The research is wellstructured with clear explanations and insightful visualizations. However further detailed comparisons discussion on limitations and exploration of realworld implications would strengthen the papers contribution. The authors are encouraged to address these aspects to enhance the credibility and applicability of their research. Overall considering the methods novelty efficiency and potential impact in deepfake detection I recommend this paper for acceptance pending revisions addressing the mentioned points.", "r9cpyzP-DQ": "Summary of the Paper: The paper introduces a novel paradigm for learning ordinary differential equations (ODEs) with unknown dynamics by utilizing invertible neural networks to relate a target ODE to a base ODE that is easier to integrate. By learning the diffeomorphism between the two vector fields the complexity in modeling the dynamics can be offloaded onto learning the diffeomorphism. This leads to speedups in integrating trajectories from learned ODEs. The paper explores using both linear and neural networkbased dynamics for the base ODE. Strengths of the Paper: 1. Innovative Approach: The paper introduces a novel method for learning ODEs by leveraging invertible neural networks to relate a target ODE to a base ODE. 2. Efficiency: By using a linear base ODE the method achieves substantial speedups in integration compared to traditional methods enhancing efficiency. 3. Theoretical Soundness: The paper provides theoretical background on diffeomorphisms related vector fields and stability properties ensuring a strong foundation for the proposed approach. 4. Experimental Validation: The paper empirically validates the proposed method on synthetic and realworld datasets demonstrating significant speedups in integration and competitive performance. 5. Versatility: By considering both linear and neural networkbased base ODEs the method can handle various types of ODEs efficiently increasing its applicability. Weaknesses of the Paper: 1. Complexity: The paper delves into intricate mathematical concepts such as tangent spaces pushforwards and differential geometry which might be challenging for readers unfamiliar with these topics. 2. Limited Comparison: The paper could benefit from comparing the proposed method with a broader range of stateoftheart ODE learning techniques to provide a more comprehensive evaluation. 3. Practical Implementation: While the approach is theoretically sound practical challenges in implementation such as computational requirements and scalability could be further discussed. Questions to the Authors: 1. How does the proposed method compare in terms of scalability and computational efficiency when dealing with larger and more complex ODE systems 2. Have you explored the sensitivity of the method to hyperparameters network architecture choices and different optimization techniques 3. What potential obstacles or limitations do you foresee in applying the method to realworld complex dynamical systems Feedback to the Authors: 1. Consider providing more insights into the practical implementation aspects computational costs and scalability of the method to enhance the applicability of the approach. 2. Enhance the discussion on the limitations and potential challenges of the proposed method providing a comprehensive view of its capabilities. 3. Broaden the comparison with existing approaches in ODE learning to highlight the unique contributions and advantages of the proposed method. Review Summary: The paper presents an innovative approach utilizing invertible neural networks to learn ODEs by relating a target ODE to a base ODE through diffeomorphisms leading to significant speedups in integration efficiency. Theoretical foundations are wellestablished and empirical validations demonstrate the methods effectiveness. However further discussions on practical implementation scalability and comparative analysis would strengthen the paper. Overall based on the strengths of the approach theoretical soundness and promising empirical results I recommend acceptance after addressing the areas for improvement outlined above.", "zRJu6mU2BaE": " Peer Review:  1) Summary of the Paper: The paper proposes a novel framework called ConFeSS (Contrastive Learning and Feature Selection System) for fewshot learning specifically targeting large domain shifts between base and novel categories. The framework consists of pretraining a feature extracting backbone with contrastive loss on the base category data learning a feature masking module on the target dataset and finetuning the backbone network. The proposed method is evaluated on a crossdomain fewshot learning benchmark introduced by Guo et al. (2020) where it outperforms existing metalearning approaches and achieves competitive results against recent crossdomain methods.  2) Strengths and Weaknesses:  Strengths:  Novel Framework: Introduces a unique framework combining contrastive learning and feature selection for fewshot learning targeting large domain shifts.  Experimental Validation: Extensive experiments on a new crossdomain fewshot learning benchmark demonstrate the efficacy of the proposed method.  Detailed Methodology: Clearly outlines the steps of the framework and provides indepth explanations of each component.  Comparison with Baselines: Provides comprehensive comparisons with existing metalearning methods and recent crossdomain fewshot learning approaches.  Ablation Study: Conducts thorough ablation studies to analyze the impact of individual components and hyperparameters on the model performance.  Weaknesses:  Limited Dataset Variety: The paper focuses on a single crossdomain fewshot learning benchmark diversification in datasets could enhance the generalizability and robustness of the proposed framework.  Theoretical Insights: While the paper mentions some theoretical underpinnings of contrastive learning and feature selection additional theoretical insights could strengthen the justification for design choices.  Computational Complexity: The complexity or efficiency of the proposed framework especially in largerscale applications could be discussed to provide insights into the practical implementation.  Questions and Feedback to the Author: 1. Dataset Generalization: How do you envision extending your framework to accommodate various datasets with different characteristics to enhance its scalability and adaptability across diverse domains 2. Theoretical Foundation: Could you elaborate on the theoretical basis behind the choice of contrastive loss and feature selection module in improving fewshot learning performance possibly discussing how these choices align with existing theoretical frameworks in machine learning 3. Scalability Concerns: Have you explored the scalability of the ConFeSS framework to larger datasets or more complex scenarios and are there any scalability challenges that need to be addressed for realworld applications  3) Review Summary: The paper presents an innovative framework ConFeSS for crossdomain fewshot learning by integrating contrastive learning and feature selection mechanisms. The detailed methodology thorough experimental validation and insightful ablation studies highlight the strength and effectiveness of the proposed approach. While the paper showcases strong empirical results addressing theoretical justifications scalability concerns and dataset diversity considerations would further enrich the contribution. Overall the wellstructured framework and promising experimental outcomes warrant the acceptance of the paper with minor revisions and additional theoretical insights.", "m8uJvVgwRci": " Review of \"Weak Indirect Supervision for Label Synthesis: A Probabilistic Approach\"   1) Summary: The paper addresses the issue of labeled training set creation in machine learning by introducing Weak Indirect Supervision (WIS) a novel approach to synthesizing training labels based on indirect supervision sources with different output label spaces. The key contribution of the paper is the development of the Probabilistic Label Relation Model (PLRM) a probabilistic modeling approach that leverages userprovided label relations to overcome the challenge of mismatched output spaces of indirect supervision sources. The paper also presents a theoretical framework to test the distinguishability of PLRM for unseen labels. Experimental results on image and text classification tasks as well as a commercial advertising system demonstrate the effectiveness of PLRM compared to baselines.   2) Strengths and Weaknesses: Strong Points:  Innovative Problem Formulation: The introduction of the Weak Indirect Supervision (WIS) problem is novel and addresses an important challenge in machine learning.  Probabilistic Label Relation Model (PLRM): The development of PLRM demonstrates a sophisticated and effective approach to tackling the task of synthesizing training labels from indirect supervision sources.  Theoretical Grounding: The establishment of a theoretical framework to test the distinguishability of unseen labels is a significant contribution providing a solid foundation for the proposed approach.  Experimental Validation: The paper provides comprehensive experiments demonstrating the superiority of PLRM over baselines on various tasks enhancing the credibility of the proposed method.  RealWorld Application: The application of the proposed method in a commercial advertising system showcases the practical relevance and potential impact of the research. Weak Points:  Complexity: The paper introduces a complex framework involving probabilistic modeling and theoretical concepts which may pose challenges for readers unfamiliar with advanced machine learning techniques.  Technical Depth: Certain sections especially those explaining the probabilistic modeling and generalization bounds may require a high level of technical understanding potentially limiting accessibility for a broader audience. Question to the Author:  Could you provide more insights into the computational complexity of the PLRM approach compared to traditional methods especially in scenarios with a large number of indirect supervision sources and label relations Feedback to the Author:  Clarifications on specific technical terms and detailed examples of the application of PLRM in various settings could help improve the understandability and readability for a wider audience.   3) Review Summary: The paper presents a pioneering approach in addressing the challenge of creating labeled training sets through Weak Indirect Supervision (WIS) and the development of the Probabilistic Label Relation Model (PLRM). The methodological sophistication of PLRM and the establishment of a theoretical framework for testing distinguishability provide a strong foundation for the proposed research. Extensive experimental validation on image and text classification tasks coupled with a realworld commercial application underlines the practical significance of the work. While the paper excels in innovation theoretical grounding and experimental validation attention to simplifying complex concepts and offering more detailed examples could enhance the accessibility and impact of the research. Overall the paper presents a significant advancement in machine learning methodology with promising implications for practical applications.   Recommendation: Strongly Recommend for Publication. The paper makes a valuable contribution to the field of machine learning by introducing a novel approach to addressing challenges in labeled training data creation. The innovative problem formulation sophisticated methodological development and comprehensive experimental validation support the recommendation for publication. Further enhancements in the clarity of technical explanations and additional illustrative examples could enhance the impact and accessibility of the research.", "wronZ3Mx_d": "Peer Review 1) Summary of the Paper: The paper introduces a novel approach named Deep Kernel Gaussian Process surrogate with Landmark Metafeatures (DKLM) for hyperparameter optimization (HPO) in machine learning models. The authors address the challenges of sample inefficiency in HPO by proposing a method that utilizes metatraining on a set of source tasks to efficiently transfer knowledge to new (unseen) target tasks. DKLM incorporates a deep metafeature network to capture the similarity between hyperparameter configurations and leverage this information to enhance the HPO process. Extensive experiments conducted on various HPO metadatasets demonstrate the superior performance of DKLM compared to existing stateoftheart baselines. 2) Strengths and Weaknesses: Strong Points:  The paper addresses a significant problem in the field of machine learning focusing on improving hyperparameter optimization through efficient transfer learning.  The proposed DKLM method is innovative and introduces a novel approach using deep kernels and metafeatures to enhance the HPO process.  The experimental validation is thorough involving a largescale benchmark with extensive comparisons against existing methods showcasing the empirical superiority of DKLM.  The inclusion of ablation experiments visualization of metafeature representations and resilience to negative transfer analysis adds depth to the evaluation of the proposed method. Weak Points:  Clarity and Organization: Some sections of the paper could be more succinctly written potentially reducing the verbosity of technical details to improve clarity for readers.  Methodological Details: While the paper introduces a novel framework more detailed explanations and illustrations could be provided for complex concepts especially in the preliminaries and technical sections.  Comparison with Baselines: The discussion on how DKLM compares to specific baselines in certain scenarios could be elaborated further to provide clearer insights into the methods performance. Question and Feedback to the Author:  Can you further elaborate on the computational complexity and scalability aspects of the DKLM method particularly in comparison to existing approaches  Given the focus on transfer learning and metatraining how does DKLM perform when faced with dynamic or evolving datasets where task characteristics may change over time  It would be helpful to provide additional insights into realworld applications or scenarios where DKLM could bring substantial improvements in efficiency and effectiveness compared to traditional HPO methods. 3) Review Summary: The paper introduces a novel approach DKLM to address the sample inefficiency in hyperparameter optimization by leveraging transfer learning and deep kernel Gaussian Processes with landmark metafeatures. The method is wellmotivated thoroughly evaluated through extensive experiments and demonstrates significant improvements over existing baselines. Despite some areas for improvement in terms of clarity in presentation and comparisons with baselines the paper presents a solid contribution to the field of machine learning optimization. The innovative nature of DKLM and its empirical superiority make it a promising method for improving HPO practices. Consideration of the queries raised above would further enhance the value and applicability of the research. Recommendation: Based on the novelty experimental validation and potential impact of the proposed method I recommend acceptance of the paper with minor revisions to improve clarity and elaborate on certain methodological aspects for the benefit of the readership.", "z2zmSDKONK": "1) Summary: The paper investigates the training robustness of distributional Reinforcement Learning (RL) algorithms in the context of noisy state observations that agents may encounter during training. It introduces the StateNoisy Markov Decision Process (SNMDP) to model both random and adversarial state observation noises in RL. The study compares the robustness of expectationbased RL and distributional RL in the face of noisy state observations establishing theoretical characteristics and benefits of distributional RL in handling noisy environments. Extensive experiments on various games support the theoretical findings highlighting the robust performance of distributional RL in noisy environments. 2) Strong Points:  The paper effectively introduces the problem of noisy state observations in RL and presents a comprehensive analysis.  The theoretical analysis especially the derivation of contraction and convergence results for distributional RL in the SNMDP setting and Lipschitz continuity properties is detailed and wellexplained.  The experiments are wellstructured and provide empirical evidence supporting the theoretical findings showing the robustness advantages of distributional RL against noisy state observations. Weak Points:  The paper could benefit from providing more detailed explanations in certain sections to make the content more accessible to readers not familiar with distributional RL or noisy environments in RL settings.  While the experiments demonstrate the robustness of distributional RL empirically the paper could elaborate more on the practical implications of the findings and how they can be applied in realworld scenarios. Questions and Feedback to the Author:  Can you elaborate more on the limitations or potential challenges that might arise when deploying distributional RL in realworld applications especially concerning noisy state observations  How does the proposed Lipschitz continuity property in distributional RL impact the algorithms scalability and efficiency in handling noisy environments compared to expectationbased RL  Considering the experimental results what further studies or advancements do you believe are needed to enhance the practical applicability and performance of distributional RL algorithms in noisy environments 3) Review Summary: The paper provides a comprehensive investigation into the training robustness of distributional RL algorithms under noisy state observations offering both theoretical analyses and empirical validations. The theoretical derivations of contraction and convergence properties in the SNMDP setting along with the Lipschitz continuity benefits of distributional RL are well elucidated. The experiments effectively demonstrate the robustness advantage of distributional RL algorithms against noisy state observations. Overall the paper presents a solid contribution to the understanding of distributional RL in noisy environments supported by both theoretical insights and empirical evidence. Recommendation: I recommend accepting the paper with minor revisions to enhance the clarity of certain sections and include a discussion on the practical implications and future directions for the research. The papers thorough theoretical analyses and wellstructured experiments make a valuable contribution to the field of reinforcement learning particularly in addressing the challenges posed by noisy state observations in training environments.", "qjN4h_wwUO": " Summary of the Paper: The paper introduces GradMax a novel method for growing neural networks that focuses on maximizing the gradient norm of newly added neurons without impacting what is already learned. The key idea is to efficiently add new neurons during training to improve training dynamics without requiring costly retraining of the network. The paper presents a detailed explanation of GradMax including the mathematical formulations and the method for optimizing the gradient norm. Experimental results demonstrate the effectiveness of GradMax in various vision tasks and architectures.  Strong Points: 1. Innovative Approach: The paper introduces a unique approach to growing neural networks by maximizing the gradient norm of newly added neurons which sets it apart from conventional methods. 2. Substantial Contributions: GradMax is a significant contribution to the field offering a method that improves training dynamics and accelerates learning without the need for retraining. 3. Theoretical Background: The paper provides a strong theoretical foundation including mathematical formulations derivations and explanations of the GradMax method. 4. Experimental Validation: The effectiveness of GradMax is demonstrated through experiments on various tasks and architectures showcasing improved training dynamics and model performance. 5. Clear Presentation: The paper is wellstructured with detailed sections covering aspects like the problem statement method overview implementation details and thorough analysis of experimental results.  Weak Points: 1.Limited Comparison: The comparison with existing methods could be more comprehensive. A deeper analysis of the limitations and advantages compared to other growing approaches would enhance the paper. 2.Evaluation Metrics: While the paper focuses on training dynamics and optimization additional metrics related to model performance and generalization could provide a more holistic evaluation of GradMax. 3.Complexity: The paper involves intricate technical details and mathematical equations that may pose a challenge for readers not wellversed in the field.  Questions and Feedback to the Author: 1.Clarification on Growing Schedule: How was the growing schedule determined in experiments and are there considerations for optimizing the growth frequency for different tasks or architectures 2.Scalability Issues: Have you considered the scalability of GradMax for larger and more complex neural networks How does GradMax perform with extremely deep or wide architectures 3.RealWorld Applications: Can you provide insights into potential realworld applications or scenarios where GradMax could be particularly beneficial  Review Summary: The paper presents a novel and innovative approach GradMax for growing neural networks that focuses on maximizing the gradient norm of newly added neurons to improve training dynamics. The theoretical derivations and experimental results demonstrate the effectiveness of GradMax in various vision tasks and architectures. While the paper has substantial contributions and clear presentation further comparison with existing methods and evaluation metrics related to model performance could enhance the overall impact. Overall based on the innovative approach strong theoretical foundations and promising experimental results I recommend acceptance of the paper with minor revisions to address the aforementioned weak points and questions provided for clarification.", "o9DnX55PEAo": " Summary: The paper introduces a novel approach to distilling large pretrained language models (PreLMs) into a more efficient architecture called CMOWCBOWHybrid using matrix embeddings. The authors extend the CMOW architecture with bidirectional components pertoken representations and a twosequence encoding scheme. Experimental results demonstrate that the distilled models achieve comparable performance to ELMo and challenge DistilBERT while providing faster inference speed. The study explores general distillation and taskspecific distillation highlighting the effectiveness of bidirectional models and the twosequence encoding scheme.  Strengths:  Innovative Approach: The paper introduces a novel method of distilling PreLMs into an efficient architecture using matrix embeddings providing a unique contribution to the field of natural language processing.  Experimental Rigor: The study provides thorough experimental results comparing different variants of the proposed model and evaluating performance on the GLUE benchmark tasks showcasing the effectiveness of the proposed approach.  Impact: The findings indicate that the distilled models achieve competitive performance with ELMo and DistilBERT while offering faster inference speed making them potentially suitable for deployment in resourceconstrained environments.  Weaknesses:  Linguistic Acceptability: While the distilled models outperform previous crossarchitecture distillation approaches they still fall behind on linguistic acceptability suggesting a limitation that needs further investigation.  Detailed Analysis: The paper lacks indepth analysis on the tradeoff between embedding size and downstream performance which could provide valuable insights into model optimization.  Future Directions: The exploration of additional techniques like pruning and quantization to improve efficiency is suggested but not explored in the current study leaving room for further research.  Questions and Feedback to the Authors: 1. Could you provide more insights into the specific challenges faced in improving linguistic acceptability and potential strategies to address these limitations in future research 2. Considering the tradeoff between embedding size and downstream performance have you explored the impact of varying embedding dimensions on different tasks within the GLUE benchmark 3. The paper suggests future work on pruning and quantization for efficiency improvement. What specific insights do you anticipate these techniques could offer in enhancing the proposed models performance  Review Summary: The paper presents a novel approach to distilling large PreLMs into an efficient orderaware embedding model using a matrixbased architecture. The study demonstrates competitive performance with established models like ELMo and DistilBERT emphasizing the potential for faster inference speed. While the paper showcases the effectiveness of the proposed method further exploration into improving linguistic acceptability and investigating the impact of embedding size on performance could enhance the researchs depth. Overall the innovative approach and promising results warrant consideration for publication after addressing some minor weaknesses and questions raised in this review.", "kl8flCo98nm": " PeerReview of the Paper: \"Estimation of Neural Network Parameters in the Presence of Arbitrary Outliers\"  Summary: The paper addresses the challenge of estimating parameters of a singlelayer neural network with rectified linear unit (ReLU) activation in the presence of arbitrary outliers using a twostep algorithm. The first step involves gradient descent to estimate the norms of weight matrix rows and bias vector while incorporating either median or trimmed mean filters to mitigate the effect of outliers. The second step estimates angles between rows to obtain the weight matrix. The main contribution is in proving the sample complexity for robust estimation requiring \\xce\\xa9(1p4 log d \u03b4) samples to estimate parameters with high probability. The paper includes theoretical guarantees algorithmic details and simulation results.  Strong Points: 1. Novel Contribution: The paper\u2019s main contribution is in providing a sample complexity analysis for robust estimation filling a gap in the literature. 2. Algorithmic Approach: The twostep algorithm using gradient descent and filters to handle outliers is wellexplained and theoretically grounded. 3. Theoretical Analysis: The paper provides detailed theoretical guarantees and error bounds for parameter estimation enhancing the understanding of the algorithm\u2019s performance. 4. Empirical Evaluation: Simulation results demonstrate the effectiveness of the proposed algorithm especially the improved performance with median or trimmed mean filters. 5. Clear Organization: Sections are logically structured covering related work algorithm details theoretical guarantees and empirical validation succinctly.  Weak Points: 1. Clarity on Outliers: Further clarification on the nature and characteristics of the arbitrary outliers would enhance understanding. 2. Comparison Metrics: While the paper discusses error metrics for comparisons more detailed analysis of how these metrics affect performance comparisons could strengthen the discussion. 3. Scalability Considerations: The impact of problem dimension d on scalability and computational complexity could be further discussed. 4. Limitation Discussion: Addressing limitations and potential challenges of the proposed algorithm would provide a wellrounded evaluation.  Questions to the Author: 1. How sensitive is the proposed algorithm to variations in the fraction of outliers (p) and the problem dimension (d) Have you explored the algorithms performance in more varied scenarios 2. Could there be scenarios where the nonnegativity assumption on the bias vector might not hold in practice and how might this impact the algorithms performance 3. Have you considered extending the sample complexity analysis to different types of noise or outliers beyond arbitrary outliers such as structured noise or adversarial attacks  Feedback to the Author: The paper presents a strong contribution in addressing the challenging problem of parameter estimation in the presence of outliers. The theoretical analysis and empirical validation establish the effectiveness of the proposed algorithm. Strengthening the discussion around outliers scalability and comparing with alternative approaches would enhance the papers impact and readability. Additionally addressing potential limitations and exploring extensions to different outlier types would further enrich the work.  Review Summary: This paper provides a significant contribution by addressing the robust parameter estimation of a singlelayer neural network in the presence of outliers. The twostep algorithm along with theoretical guarantees and empirical results showcases the effectiveness of the proposed approach. While the paper is wellstructured and presents clear explanations further discussion on outliers scalability considerations and comparison metrics would improve the depth of analysis. Addressing potential limitations and exploring extensions to different outlier scenarios could further enhance the significance and applicability of the algorithm. Overall based on the strong theoretical foundation and promising empirical results the paper shows promise in advancing the field of robust neural network parameter estimation.", "zBhwgP7kt4": " Summary of the Paper: The paper addresses the problem of dynamic leastsquares regression (LSR) in largescale supervised learning. It introduces a dynamic data structure for maintaining an approximate solution to dynamic LSR efficiently as new incremental data arrives. The paper presents theoretical results and practical experiments demonstrating the effectiveness of the proposed algorithm on both synthetic and realworld datasets.  Strong Points: 1. Theoretical Contributions: The paper provides insightful theoretical results including a negative answer to the complexity of maintaining exact LSR solutions in the dynamic setting and a robust lower bound under the OMv Conjecture. 2. Efficient Algorithm: The proposed dynamic data structure offers a provably efficient solution that almost matches the running time of the static solution demonstrating practical utility. 3. Experimental Evaluation: The experimental results both on synthetic and realworld datasets illustrate the effectiveness of the algorithm compared to baseline methods showcasing its superior performance.  Weak Points: 1. Clarity in Presentation: The paper could benefit from improved clarity in presenting the technical details especially in the theoretical sections to enhance readability for a wider audience. 2. Further Comparison: While the comparisons with baseline methods are insightful additional comparisons with more contemporary algorithms or dynamic regression techniques could further strengthen the evaluation. 3. Discussion on Limitations: It would be valuable to discuss the limitations or potential scenarios where the proposed algorithm may not perform optimally providing a more comprehensive analysis.  Questions to the Author: 1. Can you elaborate on the practical implications of the separation in complexities between static and dynamic LSR problems under high precision as discussed in Theorem 1.1 2. How does the proposed dynamic data structure handle scenarios with a high degree of noise or outliers in the dataset Are there specific considerations or adaptations to address such situations 3. Considering the experimental results how is the algorithms performance impacted when the dataset characteristics differ significantly from the scenarios tested  Feedback to the Author: 1. Strengthen the exposition of theoretical results by providing more intuitive explanations for complex concepts to enhance the papers accessibility to a wider audience. 2. Consider addressing potential edge cases or scenarios where the algorithm may face challenges providing a more nuanced discussion on the algorithms robustness. 3. Enhance the experimental section by including additional datasets or varying parameters to showcase the algorithms versatility and robustness.  Review Summary: The paper offers valuable contributions to the field of dynamic leastsquares regression with a novel dynamic data structure. The theoretical results efficient algorithm design and practical applicability demonstrated through experiments showcase the significance of the work. However improvements in clarity of presentation additional comparisons and discussions on limitations would enhance the impact and comprehensiveness of the paper. Overall based on the theoretical advancements and promising experimental results the paper is recommended for acceptance after addressing the mentioned points for improvement.", "tlkHrUlNTiL": " Summary: The paper presents a novel approach Deep Linearly Gated Networks (DLGN) to address the interpretability issue in deep neural networks (DNNs) with rectified linear units (ReLUs). The DLGN disentangles the computations in a DNN into two mathematically interpretable linearities namely primal and dual linearities. The primal linearity involves linear transformations from input to preactivations that trigger the gates while the dual linearity characterizes the pathwise learning in the weights network using the neural path kernel (NPK). The paper demonstrates that the DLGN recovers more than 83.5 of the performance of stateoftheart DNNs on datasets like CIFAR10 and CIFAR100.  Strengths:  Conceptual Novelty: The disentanglement of gating and weight networks in DLGN presents a unique approach to enhance interpretability in DNNs.  Theoretical Grounding: The paper provides detailed theoretical formulations such as the NPK for infinite width DNNs and insights into pathwise learning.  Empirical Validation: Experimental results on CIFAR10 and CIFAR100 demonstrate the effectiveness of DLGN in recovering DNN performance.  Future Directions: The paper highlights interesting future research directions including the role of depth in the feature and value networks and the potential for DLGN to be a universal spectral approximator.  Weaknesses:  Complexity: Some sections contain technical language and intricate theoretical derivations which might be challenging for nonexperts to follow.  Clarity: The paper could benefit from clearer illustrations or examples to aid in understanding the DLGN concept and its practical implementation.  Evaluation: While the experiments show promising results a more thorough evaluation across various architectures and datasets could further validate the utility of DLGN.  Questions for Authors: 1. Can you provide more insights into the computational complexity of training and inference in DLGN compared to standard DNN architectures 2. How does the choice of hyperparameters such as the number of layers or the initialization strategy impact the performance of DLGN 3. Have you considered investigating the robustness of DLGN to adversarial attacks or noisy inputs  Feedback:  Enhance the readability of technical sections by simplifying complex concepts for a wider audience.  Provide more intuitive explanations or visual aids to clarify the disentanglement process in DLGN.  Broaden the scope of empirical evaluation to encompass a diverse set of architectures and datasets for a comprehensive assessment of DLGNs performance. Overall the paper presents an intriguing approach with potential implications for improving the interpretability of DNNs. Strengthening the clarity of presentation and expanding the experimental validation would further enhance the impact of this work.", "vpiOnyOBTzQ": " Summary: The paper proposes a method to improve longterm stability and outofdistribution (OOD) generalization in dynamical system prediction using deep generative models. The approach focuses on disentangling the domain parameters from the dynamics in the latent space of generative models. This is achieved through supervised disentanglement leveraging Variational Autoencoders (VAEs) and is tested in both phase space and video sequence predictions. The results show that disentangled models adapt better to domain parameters spaces not present in the training data and improve longterm and OOD predictions compared to stateoftheart models.  Strengths: 1. Innovative Approach: The use of supervised disentanglement to separate domain parameters from dynamics is novel and shows promising results. 2. Empirical Rigor: The paper conducts comprehensive experiments on both phase space and video sequences including rigorous OOD testing. 3. Reproducibility: Providing code and datasets for reproducibility enhances the credibility and transparency of the research. 4. Clarity: The paper is wellstructured and clearly explains the methodology results and implications.  Weaknesses: 1. Scope Limitation: The paper focuses solely on dynamical system prediction limiting the broader applicability of the proposed method. 2. Evaluation Metrics: The paper lacks detailed inspection of disentangled representations with appropriate metrics which could provide deeper insights into performance. 3. Mixed Results for MLP: The mixed results for using supervised disentanglement on multilayer perceptron models call for further investigation and discussion.  Questions and Feedback to the Author: 1. It would be beneficial to include more detailed discussions on the limitations of the proposed method particularly in realworld applications beyond dynamical system prediction. 2. Can the authors elaborate on how the disentangled representations could be further interpreted or utilized in downstream tasks 3. How does the proposed method compare with other stateoftheart approaches in terms of computational efficiency and scalability 4. Given the variance observed in model performance depending on hyperparameters how were the hyperparameters selected and optimized to ensure fair model comparisons  Review Summary: The paper introduces an innovative approach to improve longterm stability and OOD generalization in dynamical system prediction using supervised disentanglement in generative models. The empirical results demonstrate the effectiveness of the proposed method particularly in capturing variability and improving predictions in OOD scenarios. Overall the research is wellconducted but further exploration into realworld applications and interpretation of disentangled representations would enhance the impact of the findings. Therefore I recommend acceptance of the paper with minor revisions addressing the mentioned weaknesses and questions.", "t5EmXZ3ZLR": " Summary of the Paper: The paper introduces two novel methods SOSPI and SOSPH for secondorder structured pruning in neural networks. The goal is to reduce the size of networks without compromising performance especially focusing on structured pruning. The main method SOSPH incorporates fast Hessianvector products for efficient evaluation while the SOSPI method is based on the GaussNewton approximation. The paper validates the methods by comparing them to stateoftheart pruning methods and demonstrates their effectiveness on various neural network architectures and datasets including Cifar10100 and ImageNet. Additionally the study explores widening architectural bottlenecks in the pruned networks to further enhance performance.  Strong Points of the Paper:  Innovative Approach: The development of two novel methods for structured pruning in neural networks focusing on secondorder effects represents an innovative contribution to the field.  Theoretical Analysis: The paper thoroughly analyzes the complexity and efficiency of the proposed methods providing theoretical underpinnings to support the practical implementations.  Comparative Study: Conducting extensive benchmarking against stateoftheart methods and showcasing comparable or superior performance at lower computational costs highlights the effectiveness of the proposed methods.  Scalability and Applicability: The scalability and application of the methods to largescale datasets like ImageNet demonstrate their practical utility in realworld scenarios.  Architectural Bottlenecks: The inclusion of a methodology to identify and widen architectural bottlenecks represents a valueadded aspect of the study showing a holistic approach to network optimization.  Weak Points of the Paper:  Complexity Explanation: The technical details in the \"Approximative SecondOrder Derivatives\" and \"Counting of Parameters and MACs\" sections might be challenging for readers not familiar with the specific neural network terminology and concepts.  Experiments and Results: While the paper provides a comprehensive explanation of the methods the results section could be further enhanced with visualization to aid in better understanding and interpretation of the performance metrics.  Practical Implementation: While the theoretical analyses are strong practical aspects such as computational efficiency and realworld applicability could be further emphasized to facilitate adoption by practitioners.  Questions and Feedback to the Author: 1. Clarity of Results: Could you provide visual aids such as graphs or charts to illustrate the experimental results for better comprehension and comparison 2. RealWorld Applications: How do you envision the implementation and adoption of these structured pruning methods in realworld scenarios considering computational constraints and hardware requirements 3. Further Validation: Have you considered conducting experiments on a wider range of neural network architectures and datasets to validate the generalizability and robustness of the proposed methods 4. Comparative Analysis with Unstructured Pruning: How do the benefits of structured pruning in this study compare with unstructured pruning methods in terms of computational efficiency and accuracy improvements  Review Summary: The paper presents novel methods for secondorder structured pruning in neural networks showcasing theoretical advancements practical implementations and promising results. The thorough analysis benchmarking against existing methods and innovative approaches make the study a valuable contribution to the field of neural network optimization. The emphasis on scalability efficiency and identification of architectural bottlenecks demonstrates a comprehensive and systematic approach to network pruning. However improving the clarity of the results presentation focusing on practical implications and extending the experimentation could further enhance the impact and usability of the proposed methods. Overall considering the strong theoretical foundations innovative methodology and promising results I recommend this paper for publication with minor revisions to address the weak points highlighted.", "inSTvgLk2YP": " Summary: The paper proposes a novel approach named MeshInversion for singleview 3D object reconstruction by leveraging generative prior from a pretrained GAN. The method aims to reconstruct highly realistic and faithful 3D objects even from novel views and is robust against occlusion. It achieves stateoftheart results in terms of perceptual metrics and geometric accuracy compared to existing CMRbased methods.  Strengths: 1. Innovative Approach: The utilization of generative prior from a GAN for 3D reconstruction is a novel and promising approach. 2. Robustness: The method shows robustness against occlusion and can generalize well to less common shapes showcasing its versatility. 3. Chamferbased Losses: The introduction of Chamfer texture loss and Chamfer mask loss addresses key challenges in 3Dto2D rendering leading to more accurate and realistic reconstructions. 4. Experimental Validation: The paper provides extensive experimental validation on benchmark datasets showcasing the effectiveness of the proposed method through quantitative and qualitative evaluations.  Weaknesses: 1. Limitations in Artifact Representation: The method may have limitations in representing complex geometric details particularly in articulated regions like wings as a few vertices may restrict the representation of rich texture and geometry. 2. Comparative Analysis: While the paper shows stateoftheart results compared to existing methods a more detailed comparative analysis with additional baseline methods could enhance the evaluation. 3. Complexity Concerns: The complexity of the proposed method particularly in the application of Chamferbased losses could potentially limit its scalability in realworld applications.  Questions: 1. Can you elaborate on the computational overhead involved in training and inference with MeshInversion How does it compare to existing methods in terms of efficiency 2. How does MeshInversion handle cases of extreme articulation or shape complexity such as highly curved surfaces or intricate textures that may challenge the generative prior 3. Have you explored the robustness of MeshInversion to variations in lighting conditions or different camera perspectives which could impact the quality of the reconstructions  Feedback: The paper presents an innovative approach to singleview 3D object reconstruction with promising results. To enhance the manuscript consider addressing the above weaknesses by providing clarity on potential limitations and offering solutions to improve artifact representation. Additionally expanding on the computational efficiency and robustness analysis will strengthen the practical relevance of MeshInversion. Overall the paper demonstrates significant advancements in the field of 3D reconstruction and warrants consideration for publication. I recommend accepting the paper for publication with minor revisions to address the mentioned weaknesses and inquiries. The proposed MeshInversion method shows significant potential for advancing the stateoftheart in singleview textured mesh reconstruction.", "k7efTb0un9z": " Summary of the Paper: The paper introduces a novel Graph Networkbased Scheduler (GNS) to dynamically control the learning rate in deep neural network optimization using reinforcement learning. By leveraging graph neural networks to encode the state representation of the neural network structure the proposed GNS is able to learn the optimal learning rate scheduling rule specific to the task at hand. The study demonstrates significant improvements in performance over traditional scheduling rules and previous learningbased methods on benchmark datasets for both image classification and language understanding tasks.  Strong Points: 1. Innovative Approach: The utilization of graph neural networks to capture the state representation of neural networks for learning rate scheduling is a novel and innovative approach. 2. Generalization: The ability of GNS to generalize across different datasets and network structures is a strong point that highlights its practical applicability. 3. Performance Improvement: The experimental results consistently show that GNS outperforms popular baselines in both image classification and language understanding tasks. 4. Efficiency: The efficient reward collection procedure of GNS speeds up the training process making it more practical for realworld applications. 5. Comprehensive Evaluation: The detailed experimental setup comparison with various baselines and ablation study provide a thorough evaluation of the proposed method.  Weak Points: 1. Complexity: The complexity introduced by the graphbased encoding method may limit the scalability of the approach to very large models or datasets. 2. Validation Frequency: The paper mentions a reduction in validation frequency for efficiency but there might be cases where this approach could lead to suboptimal performance. 3. Limited Comparison: While the comparison with baselines is detailed there could be more discussion on the limitations and drawbacks of those baselines for a more balanced comparison.  Questions to the Author: 1. How does the proposed GNS approach handle the increase in complexity when applied to larger models or datasets and are there any scalability issues noted during the experiments 2. Can you provide more insights into the tradeoffs between reduced validation frequency and potential impact on learning rate adjustments in different optimization scenarios 3. How does the GNS approach perform when faced with noisy or uncertain environments and what are the strategies employed to mitigate such challenges  Feedback: The paper presents a wellstructured and innovative approach for learning rate scheduling using graph neural networks. The thorough evaluation on benchmark datasets and tasks showcases the superiority of the proposed GNS method over traditional and learningbased baselines. Further discussions on scalability robustness and evaluation on more diverse datasets or models could enhance the comprehensiveness of the study. Overall the paper has the potential to make a significant impact in the field of stochastic optimization and deep learning model training.  Review Summary: The paper introduces a novel Graph Networkbased Scheduler (GNS) for learning rate scheduling in deep neural network optimization. Strong points include the innovative approach generalization capability and noticeable performance improvements over existing methods. The paper could benefit from addressing scalability concerns and providing more nuanced comparisons with baselines. Additional insights on handling noisy environments and further experimental evaluations could enhance the impact of the proposed method. Based on the strong contributions and compelling experimental results I recommend this paper for acceptance with minor revisions to address the weak points highlighted.", "kUGYDTJUcuc": "Peer Review  1) Summary: The paper presents a unified visual attention model that combines topdown and bottomup mechanisms for recurrent visual attention. The proposed model leverages image pyramids and Qlearning to select regions of interest in the topdown attention mechanism guiding the policy search in the bottomup approach. By introducing constraints related to entropy and context the model aims to explore the environment effectively. Experimental results on visual classification tasks such as MNIST CIFAR 10 and SVHN datasets demonstrate the superior performance of the proposed model over CNNs and bottomup recurrent attention models. 2) Strengths:  Innovative Approach: The integration of topdown and bottomup attention mechanisms is novel and addresses existing limitations in recurrent visual attention models.  Theoretical Grounding: The paper provides a comprehensive background on reinforcement learning and recurrent attention models laying a strong foundation for the proposed unified model.  Experimental Results: The model outperforms existing approaches on visual classification tasks showcasing its effectiveness and superiority.  Detailed Explanations: The paper offers detailed explanations of the model architecture training process and experimental setups making it easier for readers to follow the research methodology. Weaknesses:  Clarity of Explanations: While the paper provides detailed explanations some sections could benefit from clearer articulation to enhance reader understanding.  Evaluation Metrics: While the experimental results demonstrate the models efficacy a more detailed discussion on evaluation metrics and comparisons with stateoftheart models would further strengthen the paper.  Ethics and Generalization: It would be beneficial to address ethical considerations related to visual attention models and discuss the generalizability of the proposed approach in more diverse contexts. Questions and Feedback to the Author: 1. Can you provide more insights into how the model handles cases of outliers or anomalies in the data 2. Considering the mention of constraints related to entropy and context how sensitive is the model to hyperparameter tuning particularly in realworld applications 3. How does the proposed model compare in terms of computational efficiency and scalability when compared to existing visual attention models 3) Review Summary: The paper presents an innovative unified visual attention model that combines topdown and bottomup mechanisms to enhance performance in recurrent visual attention applications. The experimental results indicate significant improvements over existing models particularly in visual classification tasks. While the paper is thorough in its explanations and presents promising results further clarity in explanations detailed evaluation metrics discussion on ethics and generalizability would enhance the research value. Overall the proposed model shows potential for broad applications in computer vision tasks. Recommendation: Given the innovative approach strong theoretical foundation and promising experimental results I recommend acceptance of the paper for publication with minor revisions related to clarity and additional details on evaluation metrics and generalizability.  This peer review offers a comprehensive evaluation of the paper highlighting its strengths weaknesses questions to the authors review summary and a recommendation for publication.", "m4BAEB_Imy": " Summary: The paper presents a novel pruning algorithm iPrune for reducing computation and memory requirements in binary neural networks while maintaining high accuracy. The algorithm focuses on effectively pruning weights in convolutional and dense layers by reducing the number of inputs per neuron. Experimental results on MNIST and CIFAR10 datasets demonstrate significant reductions in memory (up to 2200x) and computation (up to 70x) with minimal loss in accuracy (less than 3).  Strengths: 1. Innovative Approach: The development of the iPrune algorithm addresses the challenge of efficiently implementing binary neural networks on hardware accelerators showcasing a novel solution. 2. Comprehensive Analysis: The paper provides detailed insights into binarization and pruning techniques along with a thorough evaluation of results on popular datasets like MNIST and CIFAR10. 3. Experimental Rigor: The experiments are meticulously designed showcasing the effectiveness of iPrune in reducing memory and computation requirements while maintaining competitive accuracy levels compared to full precision networks. 4. Comparative Analysis: Comparison with existing pruning techniques and recent works provides a clear context for the performance and efficiency gains achieved by iPrune.  Weaknesses: 1. Evaluation Scope: While the paper effectively demonstrates the benefits of iPrune on MNIST and CIFAR10 expanding the evaluation to diverse datasets could enhance the generalizability of the algorithm. 2. Implementation Complexity: The complexity of implementing iPrune on hardware accelerators like FPGAs may require additional discussion on practical implementation challenges faced. 3. Discussion on Scalability: While the results are promising a discussion on the scalability of iPrune to larger and more complex neural network architectures could strengthen the papers impact.  Questions: 1. Could you elaborate on the potential challenges or limitations in implementing iPrune on realworld hardware accelerators especially in scenarios with varying network complexities 2. How does iPrune perform when applied to neural networks with larger architectures beyond the ones tested on MNIST and CIFAR10 Have you explored its scalability to more complex models 3. Considering the memory and computation gains achieved with iPrune have you conducted any finegrained analysis on energy efficiency improvements that can be expected when porting the algorithm to an FPGA  Feedback to Author: 1. Consider discussing the scalability of iPrune to larger neural network architectures and its practical implementation challenges. 2. Provide insights into the potential energy efficiency improvements and inference time reduction when deploying iPrune on FPGA hardware accelerators. 3. Include a detailed discussion on the implications of iPrune for environmentally friendly AI training and inference practices. Overall the paper presents a promising pruning algorithm with significant memory and computation reduction benefits for binary neural networks. Extending the evaluation to diverse datasets discussing realworld implementation challenges and exploring scalability to larger models would further enhance the impact and applicability of iPrune. Based on the innovative approach rigorous evaluation and comparative analysis provided I recommend this paper for publication with minor revisions and additional discussions on scalability and practical implementation considerations.", "hq7vLjZTJPk": " Peer Review 1) Summary: The paper proposes a communicationefficient distributed stochastic local gradient clipping algorithm for training deep neural networks in a Federated Learning setting. The algorithm utilizes a relaxedsmoothness assumption of the loss landscape particularly suitable for neural networks like LSTM. Theoretical guarantees on linear speedup and reduced communication complexity are provided. Empirical results on benchmark datasets validate the algorithms fast convergence speed. 2) Strengths:  Novel Algorithm Design: The proposed algorithm tackles a pertinent issue in distributed training of neural networks offering new insights into incorporating gradient clipping in a Federated Learning setting.  Theoretical Analysis: The paper provides rigorous theoretical guarantees on the algorithms performance under the relaxed smoothness condition showcasing efforts to bridge theory and practice.  Empirical Validation: Extensive experiments on various datasets and tasks demonstrate the algorithms efficacy in practice substantiating theoretical claims with empirical results.  Comparative Analysis: Comparisons with existing algorithms and a detailed discussion of the algorithms advantages such as reduced communication complexity and linear speedup enhance the papers contribution. Weaknesses:  Complexity of Presentation: The paper delves deep into technical details making it challenging for readers less familiar with the domain to grasp the content easily. Simplifying certain sections could improve accessibility.  Empirical Evaluation Details: While the empirical results are comprehensive additional insights or visualizations could aid in better understanding the algorithms performance in different scenarios.  Impact Clarity: Articulating the practical implications of the algorithms benefits in realworld distributed training scenarios could enhance the papers significance. Questions and Feedback to the Author: 1. Did you consider exploring the algorithms performance with different hyperparameters or tuning strategies in the empirical evaluation to understand its robustness across a broader range of settings 2. Could you elaborate on the challenges faced during the algorithms implementation and any specific scenarios where it demonstrated noticeable improvements over existing methods 3. How do you envision the algorithms adoption in industry applications or realworld distributed training setups considering its theoretical guarantees and empirical performance 3) Review Summary: The paper presents a novel communicationefficient distributed stochastic local gradient clipping algorithm for deep neural network training in a Federated Learning setting. The theoretical analysis provides solid insights into the algorithms efficacy under the relaxed smoothness condition supported by empirical validation on benchmark datasets. While the technical depth is commendable enhancing the papers accessibility and practical implications could amplify its impact. Overall the algorithms novel design theoretical guarantees and empirical validation contribute significantly to the domain of distributed training warranting further exploration and potential realworld applications. Recommendation: Based on the innovative algorithm design thorough theoretical analysis and promising empirical results I recommend accepting this paper with minor revisions to enhance readability and strengthen the discussion on practical implications. Further elaboration on impact and adoption in realworld scenarios can augment the papers significance.", "qwBK94cP1y": " Peerreview of the Paper on Causal Direction Determination using Functional Causal Models and Optimal Transport  1) Summary: The paper discusses a novel framework for identifying causal direction in the bivariate case using Functional Causal Models (FCMs) and optimal transport. It presents a dynamicalsystem view of FCMs and explores the connection between FCMs and optimal transport methods. The proposed algorithm named DIVOT is based on the divergence measure criterion to determine causal relationships between two variables. The method is validated on synthetic and realworld datasets showing stateoftheart results in causal discovery tasks.  2) Strong Points: a) Novel Conceptual Framework: The paper introduces a unique dynamicalsystem perspective of FCMs connecting them with optimal transport for causal direction determination which is a novel and interesting approach. b) Rigorous Methodology: The paper lays out a clear methodology with detailed explanations on the dynamical interpretation of FCMs constraints and optimal transport providing a comprehensive understanding of the proposed algorithm. c) Empirical Evaluation: The experimental results on synthetic and real datasets demonstrate the effectiveness of the proposed method showcasing its robustness efficiency and performance compared to stateoftheart methods.  Weak Points: a) Simplified Explanations: While the paper provides indepth technical details some complex concepts could be simplified to enhance readability for a broader audience. b) Lack of Comparative Analysis: Although the paper mentions comparisons with other methods a more detailed comparative analysis with existing algorithms and methodologies could further strengthen the findings. Question to the Author: 1. How does DIVOT handle cases where there might be confounding variables influencing the causal relationship between the two variables Feedback to the Author: 1. Consider providing more insights into the computational complexity of the DIVOT algorithm and how it scales with larger datasets to address practical implementation challenges. 2. Enhance the clarity of the methods extensibility to more than two variables and its ability to handle complex causal structures for a broader applicability.  3) Review Summary: The paper presents a novel and insightful approach to identifying causal relationships using FCMs and optimal transport showcasing significant contributions to the field of causal discovery. The rigorous methodology combined with comprehensive experiments establishes the efficacy and robustness of the proposed DIVOT algorithm. With minor improvements in clarity and further exploration of practical implications the paper is recommended for acceptance as it offers a valuable advancement in causal discovery research.", "tYRrOdSnVUy": "Summary: The paper proposes NonTransferable Learning (NTL) as a novel approach to protect welltrained deep learning models as intellectual property in the context of Artificial Intelligence as a Service (AIaaS). NTL aims to restrict the model generalization ability to specific data domains enabling ownership verification and applicability authorization. Two types of NTL approaches TargetSpecified NTL and SourceOnly NTL are introduced and validated through extensive experiments across various datasets showcasing the effectiveness of NTL in model protection. Strengths: 1. Innovative Approach: The concept of NTL is novel and addresses a critical issue in protecting deep learning models as intellectual property in the AIaaS landscape. 2. Comprehensive Experiments: Extensive experiments conducted on diverse datasets demonstrate the effectiveness of NTL in ownership verification and applicability authorization. 3. Robustness: NTLbased ownership verification is shown to be resistant to stateoftheart watermark removal methods adding a robust layer to model protection. 4. Customized Data Protection: NTLbased applicability authorization restricts model usage to specific data domains showcasing a datacentric protection approach. Weaknesses: 1. Complexity: The complexity of the proposed NTL approach and its implementation might pose challenges for widespread adoption and practical deployment. 2. Limited RealWorld Application: The paper lacks realworld case studies or practical use cases to demonstrate the applicability of NTL in real AIaaS scenarios. 3. Generalization: The experimental evaluation focuses on specific datasets and classification tasks potentially limiting the generalizability of the proposed approach to other domains or tasks. 4. Scalability: There could be concerns about the scalability of NTL especially when applied to larger and more complex deep learning models or datasets. Question to the Author: 1. How does NTL compare to existing methods of model protection and IP verification in terms of computational overhead and performance impact 2. Have you considered the potential ethical implications of restricting model applicability to certain data domains especially in terms of bias and fairness considerations 3. How feasible would it be to integrate NTLbased protection mechanisms into existing AIaaS platforms and frameworks Feedback: The paper presents a promising approach in NTL for protecting deep learning models as intellectual property. Further insights into the practical implementation scalability and ethical considerations would enhance the completeness of the research. Additionally discussing the potential adoption challenges and the integration of NTL in realworld AIaaS environments could provide valuable insights for future work in this domain. Review Summary: The paper introduces a novel approach in NonTransferable Learning (NTL) for protecting welltrained models as intellectual property in the AIaaS landscape. With comprehensive experiments validating the effectiveness of NTL in ownership verification and applicability authorization the paper addresses a significant concern in model protection. However further exploration into practical implementation details scalability and ethical considerations would strengthen the research. Overall based on the innovative concept thorough experimental validation and potential impact in IP protection the paper merits consideration for publication pending revisions addressing the outlined concerns.", "lQI_mZjvBxj": "PeerReview 1. Summary of the paper: The paper explores the possibility of designing a modelagnostic API for federated learning that allows adhoc groups of dataholders to collaborate and perform federated learning without sharing raw data. The study focuses on the use of knowledge distillation (KD) to design protocols for federated learning and investigates the limitations and performance issues faced by current KDbased algorithms. To address these challenges the authors propose a theoretical framework called Federated Kernel ridge regression to capture model and data heterogeneity. They analyze an alternating knowledge distillation algorithm and its connection to projection onto convex sets revealing issues with slow degradation of performance due to data heterogeneity. The paper also introduces alternative approaches such as averaged knowledge distillation and ensembled knowledge distillation to improve federated learning outcomes. 2. Strengths and Weaknesses: Strong Points:  The paper addresses an important and challenging problem in federated learning focusing on modelagnostic approaches and knowledge distillation.  The theoretical framework proposed by the authors Federated Kernel ridge regression provides valuable insights into model and data heterogeneity in federated learning.  The analysis of the alternating knowledge distillation algorithm and its connection to projections offers new perspectives on the limitations of existing protocols.  The introduction of alternative methods like averaged knowledge distillation and ensembled knowledge distillation presents innovative solutions to address performance degradation in federated learning.  The experiments conducted on both synthetic and realworld datasets provide empirical validation of the proposed frameworks and algorithms. Weak Points:  The paper could benefit from more concrete examples and illustrations to aid in understanding the theoretical concepts introduced especially for readers less familiar with the technical details.  While the experimental results are promising a broader range of benchmarks and datasets could further validate the effectiveness and generalizability of the proposed approaches.  Clarifying the computational complexity and scalability of the proposed algorithms would enhance the practical relevance of the study.  Providing a discussion on potential realworld applications and implications of the findings could add practical value to the research. Questions and Feedback: 1. Could you elaborate on the potential scalability and computational efficiency of the proposed Federated Kernel ridge regression framework in realworld federated learning scenarios 2. How do you plan to address the challenges related to incorporating diverse datasets and model architectures in practical implementations of the modelagnostic federated learning algorithms proposed in the study 3. Have you considered exploring additional optimization techniques or regularization methods to further enhance the performance and convergence speed of the ensembled knowledge distillation approach in federated learning setups 3. Review Summary: Overall the paper presents a comprehensive investigation into the design of modelagnostic federated learning algorithms using knowledge distillation and introduces innovative theoretical frameworks and algorithms to address performance limitations. The theoretical analysis experimental results and proposed alternative methods demonstrate the authors expertise in the domain. Further explorations into realworld applications scalability considerations and potential enhancements to the proposed algorithms would enhance the impact and practical relevance of the study. Based on the strong theoretical foundation innovative methodologies and promising experimental validation I recommend accepting the paper with minor revisions to address the weaknesses and provide practical insights for future research and applications. This review provides detailed feedback on the strengths weaknesses questions and recommendations for the paper on designing a modelagnostic API for federated learning. It offers a comprehensive evaluation of the study and suggests ways to enhance its impact and practical applicability.", "oDFvtxzPOx": " 1) Summary of the Paper: The paper introduces a novel deep learning approach to feature selection that addresses challenges in domains with limited labeled data and high feature correlations. They propose a Selfsupervision Enhanced Feature Selection (SEFS) method that utilizes both labeled and unlabeled data. SEFS pretrains an encoder using unlabeled data with pretext tasks then finetunes the network for relevant feature discovery using labeled data. It also introduces a multivariate Bernoulli gate vector model to consider feature correlations. Experiments on synthetic and realworld datasets show SEFS outperforms stateoftheart benchmarks in predicting target variables.  2) Strong Points:  Innovative Approach: SEFS introduces a novel method that addresses the challenges of feature selection in highdimensional data with limited labeled samples and feature correlations.  Consideration of Correlations: The use of a multivariate Bernoulli gate vector to model feature correlations is a strong point and distinct from existing methods.  Experimental Evaluation: The paper provides thorough experimental evaluation on synthetic and realworld datasets demonstrating superior prediction performance compared to stateoftheart benchmarks.  SelfSupervision: Leveraging unlabeled data through selfsupervised learning enhances feature selection robustness.  Weak Points:  Data Sources: While the use of unrelated unlabeled data in some experiments shows potential further validation with more relevant unlabeled samples could strengthen the overall approach.  Interpretability: While the performance improvement is evident more insights on the interpretability of the features discovered by SEFS could add value.  Benchmark Comparison: While comparisons with existing methods are thorough additional discussions on the limitations and assumptions of benchmarks could provide a more comprehensive analysis.  Questions and Feedback to the Author:  Robustness: How does SEFS perform with varying sizes of labeled and unlabeled data in practical applications  Scalability: Are there scalability considerations when applying SEFS to larger datasets with even higher dimensions  Interpretation: Can the discovered features by SEFS provide insights into the underlying mechanisms or associations in the datasets  3) Review Summary: The paper presents a novel deep learning approach SEFS for feature selection in highdimensional data with limited labeled samples and feature correlations. The method shows superior performance in predicting target variables compared to stateoftheart methods. The consideration of feature correlations through the multivariate Bernoulli gate vector model is a notable strength. However further exploration of interpretability and robustness especially with varying data sizes and scalability considerations could enhance the applicability and impact of SEFS in practical scenarios. Enhanced discussions on benchmark comparisons and potential limitations of the approach would contribute to a more comprehensive assessment. Overall the innovative method rigorous experimental evaluation and promising results position SEFS as a valuable contribution to the field of feature selection.", "ptZfV8tJbpe": "Summary: The paper introduces a novel method for multilabel text classification that aims to model label correlations implicitly through the use of latent label encodings. Unlike previous stateoftheart works that model label correlations explicitly this method uses latent label representations to cooperatively map contextual encodings of these latent labels to actual labels. By doing so the method achieves significant improvements in performance on benchmark datasets and demonstrates the importance of utilizing latent label embeddings for effective labelcorrelation utilization. Strong Points: 1. Innovative Approach: The paper introduces a novel method to address the challenge of modeling label correlations implicitly which provides a fresh perspective in the field of multilabel text classification. 2. Significant Performance Improvement: The proposed method outperforms previous stateoftheart results on two benchmark datasets by a large margin demonstrating the effectiveness of implicit labelcorrelation modeling. 3. Detailed Experiments and Analysis: The paper provides indepth experimental results feature studies and error analyses to support the efficacy of the proposed method. This thorough evaluation enhances the credibility of the research findings. Weak Points: 1. Limited Comparison: While the paper compares the proposed method with previous stateoftheart works and provides compelling results it may benefit from further comparison with a broader range of existing algorithms in the field of multilabel text classification. Question to the Author: 1. Can you elaborate on the computational efficiency of your method compared to other stateoftheart algorithms in the field Is there any tradeoff between performance and computational resources required Feedback to the Author: 1. Providing further insights into the interpretability of latent label embeddings and how they correlate with each other actual labels and context could enhance the comprehensibility and applicability of the proposed method. Review Summary: The paper presents a novel method for multilabel text classification that models label correlations implicitly through latent label encodings. With significant improvements in performance on benchmark datasets and insightful analyses the method demonstrates strong potential in labelcorrelation utilization. Further exploration into the interpretability of latent label embeddings and comparisons with additional algorithms could strengthen the papers contribution to the field. Overall I recommend this paper for publication based on its innovative approach substantial performance gains and thorough experimental evaluation.", "m5EBN92vjN": " Peer Review:  1) Summary of the Paper: The paper introduces a novel network architecture Attention Aware Network (AASeg) designed for realtime semantic image segmentation. AASeg incorporates Spatial Attention (SA) Channel Attention (CA) and Multi Scale Context (MSC) modules to enhance feature extraction and improve segmentation accuracy. The network achieves competitive results on benchmark datasets like Cityscapes ADE20K and Camvid.  2) Strengths and Weaknesses of the Paper: Strong Points:  Innovative Approach: The integration of SA CA and MSC modules distinguishes AASeg as a novel architecture in the field of semantic segmentation.  Comprehensive Experiments: The paper provides detailed analyses quantitative results and an ablation study to demonstrate the effectiveness of the proposed network.  Performance Metrics: Using metrics like Mean IOU FPS and comparison with stateoftheart methods enhances the credibility of the results.  Balanced Tradeoff: AASeg achieves a balance between speed and performance crucial for realworld applications like autonomous vehicles. Weak Points:  Clarity: Some sections especially related to the network architecture and attention mechanisms could be further explained for better comprehension.  Comparative Analysis: While the performance comparison with existing methods is valuable a more detailed discussion on how AASeg specifically improves upon them would be beneficial.  Ablation Study: The ablation study could be expanded to include more experiments to provide a deeper understanding of the networks components. Question  Feedback to the Author: 1. Could you elaborate on how AASeg addresses the limitations of previous architectures in capturing spatial and channel information accurately 2. Can you provide more insights into the decisionmaking process behind the hyperparameters chosen for training the network 3. Considering future work how do you plan to validate the scalability of AASeg on larger more complex datasets  3) Review Summary: The paper presents a promising contribution in the realm of realtime semantic image segmentation through the Attention Aware Network (AASeg). The incorporation of SA CA and MSC modules showcases a novel approach to feature extraction leading to competitive results on benchmark datasets. The detailed experiments performance metrics and comparison with prior methods bolster the credibility of the study. However further clarity in explaining the network architecture and attention mechanisms along with an expanded ablation study could enhance the papers impact. Based on the innovative network design comprehensive analyses and competitive results I recommend the paper for acceptance with minor revisions addressing the points mentioned above. The proposed AASeg architecture demonstrates promising advancements in realtime semantic segmentation making it a valuable addition to the existing literature in the field.", "msRBojTz-Nh": "Peer Review  1. Summary of the Paper: The paper explores the use of learned simulators for turbulent fluid dynamics aiming to capture turbulent dynamics accurately on lowresolution grids. The authors introduce a Dilated ResNet model and compare it with other baseline models in various turbulent and chaotic environments demonstrating superior performance through metrics like stability efficiency generalization and constraint satisfaction. 2. Strengths:  Innovative Approach: The use of learned simulators for turbulent dynamics presents a novel and promising avenue.  Comprehensive Evaluation: The paper extensively evaluates the proposed model across various metrics and compares it with diverse baseline models.  Superior Performance: The Dilated ResNet model consistently outperforms traditional solvers in capturing turbulent dynamics on coarse grids especially in preserving highfrequency information.  Stability Improvements: The incorporation of training noise and temporal downsampling enhances stability and accuracy in trajectory rollouts.  Efficiency: The learned simulators demonstrate faster simulation times on GPU hardware providing a practical advantage over CPUbased solvers like Athena. 3. Weaknesses:  Generalization Challenges: The authors acknowledge limitations in generalizing learned models beyond the training distribution especially in more extreme conditions.  Preserved Quantities: The difficulty in capturing preserved quantities in turbulent environments raises questions about model robustness.  Generalization to Larger Boxes: The observed challenges in generalizing cooling velocities to different box sizes suggest the need for stronger inductive biases or sophisticated dataset augmentation. 4. Questions and Feedback to the Authors:  Can the model be further optimized to improve generalization in extreme conditions beyond what was explored in the paper  How might the models robustness be enhanced to better capture preserved quantities in turbulent simulations  Given the challenges in generalizing to different initial conditions and box sizes what additional strategies or techniques could potentially address these limitations 5. Review Summary: The paper presents a significant advancement in using learned simulators to accurately capture turbulent dynamics on lowresolution grids surpassing traditional solvers in various metrics. The Dilated ResNet model demonstrates superior stability efficiency and performance offering a promising approach for efficient turbulence simulation. While challenges in generalization and capturing preserved quantities exist the studys results highlight the potential of learned models in distilling highresolution simulations into lowresolution algorithms effectively. Therefore based on the papers thorough evaluation and compelling results I recommend its acceptance with minor revisions to address the identified limitations.", "xUdEO_yE-GV": "Summary: The paper proposes a novel approach to using Persistent Homology (PH) for training deep networks to delineate road networks in aerial images and neuronal processes in microscopy scans. The key innovation lies in a novel filtration function that combines thresholdingbased filtration and filtration with height functions. The authors experimentally demonstrate that deep networks trained with their approach yield reconstructions that better preserve the connectivity of the original structures compared to existing methods. Strengths: 1. Innovation: The paper introduces a novel filtration technique that improves the use of Persistent Homology for training deep networks to maintain topology. 2. Empirical Validation: The experimental validation on various datasets shows significant performance improvements compared to existing topological methods. 3. Applicability: The proposed technique is not limited to 2D images as it naturally extends to 3D data enhancing its applicability to a broader range of applications. Weaknesses: 1. Evaluation Metrics: The paper uses multiple connectivityoriented metrics which may lead to some confusion. Consolidating the evaluation metrics could enhance clarity in assessing the results. 2. Complexity: While the proposed approach shows promise it may introduce additional complexity to the training process which could potentially impact scalability and computational efficiency. Questions and Feedback: 1. Clarity: Can you provide more insights into the computational complexity of your proposed method compared to existing techniques 2. Generalization: How well does your approach perform when dealing with noise or variations in the input data 3. Scalability: Have you considered the scalability of your approach to larger datasets or more complex tasks Review Summary: The paper presents a novel approach leveraging Persistent Homology for training deep networks to preserve topology in delineating structures in imagery data. The method demonstrates superior performance compared to existing techniques especially in maintaining connectivity in reconstructions. However addressing concerns about computational complexity generalization to noisy data and scalability would further strengthen the impact and applicability of this innovative approach. Overall the paper presents a significant advancement in utilizing Persistent Homology for topological preservation in deep learning tasks. Additional clarification and validation would enhance the robustness of the proposed methodology.", "wIzUeM3TAU": " Summary of the Paper The paper introduces a novel approach using tensor languages to analyze the separation and approximation power of Graph Neural Networks (GNNs). It provides insights on bounding the separation power of GNN architectures by relating them to the Color Refinement (CR) and kdimensional WeisfeilerLeman (kWL) algorithms. The paper also discusses HigherOrder MessagePassing Neural Networks (kMPNNs) and the implications of tensor expressions on the capabilities of GNNs.  Strong Points 1. Novel Methodology: The use of tensor languages to analyze GNNs is a novel approach that offers a clear and structured way to understand the separation power of GNN architectures. 2. Theoretical Framework: The integration of tensor languages and graph algorithms such as CR and kWL provides a solid theoretical foundation for analyzing the capabilities of GNNs. 3. Toolbox for Analysis: The development of kMPNNs and the identification of treewidth as a key factor influencing separation power offer a practical toolbox for GNN designers to assess and optimize their architectures. 4. Concise Description: The paper provides a comprehensive overview of the methodology results and implications making it accessible for readers interested in GNNs and graph algorithms.  Weak Points 1. Complexity: The paper dives deep into mathematical concepts and formalism which may be challenging for readers not wellversed in graph algorithms or linear algebra. 2. Technical Language: Some sections contain highly technical language that may require a strong background in math and computer science to fully grasp. 3. Generalizability: While the approach is theoretically sound the practical applicability and generalizability of using tensor languages for analyzing GNN architectures in diverse realworld scenarios could be further explored.  Questions and Feedback to the Author 1. Can you provide practical examples or case studies where the proposed tensor language analysis approach has been successfully applied to optimize GNN architectures 2. How do you envision extending this research to incorporate realworld datasets and applications to validate the theoretical findings 3. Have you conducted any experiments or case studies to demonstrate the practical benefits of using tensor languages for analyzing separation power in GNNs  Review Summary The paper presents an innovative methodology using tensor languages to analyze the separation and approximation power of Graph Neural Networks. While the paper offers a comprehensive theoretical framework and introduces valuable concepts such as kMPNNs and treewidth some sections may be challenging for readers with limited background in graph algorithms and linear algebra. Practical applications and experimental validations of the proposed approach could enhance the relevance and impact of the research. Overall the paper is wellstructured and provides valuable insights for researchers in the field of graphbased machine learning.", "g1SzIRLQXMM": " Summary: The paper presents a novel approach to reducing the number of supervised synaptic updates required in deep neural network models to achieve adultlike ventral visual stream processing. The study explores three key strategies: (1) reducing the number of supervised updates (2) optimizing the \"atbirth\" synaptic connectivity information through weight compression and (3) training only critical synapses to maintain high brain predictivity. These strategies are demonstrated on the CORnetS architecture yielding impressive results in brainlikeness predictivity scores with significantly reduced training requirements.  Strengths: 1. Innovative Approach: The paper introduces novel techniques to reduce the supervised updates required for model training offering fresh perspectives on neural network development. 2. Comprehensive Analysis: The study thoroughly dissects different strategies such as weight compression and critical training providing a detailed breakdown of how each contributes to brain predictivity scores. 3. Experimental Validation: The authors validate their approaches through extensive experiments and comparisons with existing models showcasing the effectiveness of the proposed methods.  Weaknesses: 1. Complexity of Presentation: The detailed technical descriptions and scientific jargon might make it challenging for some readers to grasp the concepts easily. Simplifying the language without losing critical information could enhance accessibility. 2. Limited Generalization: While the techniques are effective on the CORnetS architecture the generalization to other architectures like ResNet and MobileNet showed mixed results. Further investigation into the applicability of the methods across different models is needed.  Questions to the Authors: 1. How do you envision further optimizing the proposed techniques to ensure broader applicability across various neural network architectures 2. Can you elaborate on the potential implications of your work on advancing our understanding of biological development processes in the visual system  Feedback for Authors: 1. Consider providing more insights into the potential scalability of the proposed methods and their adaptability to diverse experimental settings and datasets. 2. Enhance the discussion on the practical implications and realworld applications of your findings to broaden the impact of the study.  Review Summary: The paper exhibits a strong foundation in addressing the challenge of reducing supervised synaptic updates for neuroscientific modeling. While the methods are innovative and show promise in enhancing brainlike predictivity scores there is a need for further validation and exploration for broader applicability. With its detailed analysis and experimental rigor the study offers a valuable contribution to the field of visual processing modeling but could benefit from clearer communication to enhance accessibility and practical relevance. Overall the work exhibits strong potential and warrants further research to refine and validate the proposed techniques.", "uxxFrDwrE7Y": " Summary of the Paper: The paper proposes CLSER a novel dual memory experience replay method based on the complementary learning systems theory in the brain to address catastrophic forgetting in continual learning. The method maintains shortterm and longterm semantic memories that interact with the episodic memory. By aligning decision boundaries with semantic memories new knowledge is acquired effectively while consolidating existing knowledge. CLSER is versatile not tied to task boundaries or data distribution assumptions and achieves stateoftheart performance in both standard benchmarks and complex continual learning scenarios.  Strong Points: 1. Innovative Approach: The paper introduces a novel method inspired by the brains complementary learning systems theory addressing the crucial issue of catastrophic forgetting in deep neural networks. 2. Versatility: CLSER is highlighted as a general continual learning method that does not rely on task boundaries or data distribution assumptions making it suitable for a wide range of scenarios. 3. StateoftheArt Performance: The method achieves stateoftheart results on standard benchmarks and more realistic continual learning settings outperforming existing approaches. 4. Comprehensive Analysis: The paper provides a thorough analysis of the proposed methods characteristics including convergence to flatter minima task probabilities and model calibration showcasing its effectiveness.  Weak Points: 1. Theoretical Justification: While the paper presents strong empirical results a deeper theoretical justification of how the method aligns with the brains complementary learning systems theory could enhance the understanding of the proposed approach. 2. Comparison Metrics: It would be beneficial if the paper includes detailed comparisons with existing methods on various metrics like computational complexity memory usage and scalability to provide a more comprehensive evaluation.  Questions for the Authors: 1. Could you elaborate on how the proposed method aligns with the detailed neurophysiological processing principles of the brains complementary learning systems theory A more indepth discussion on this aspect would enhance the understanding of the theoretical basis of CLSER. 2. How does CLSER perform in terms of computational efficiency memory overhead and scalability compared to existing continual learning methods Providing insights into these aspects would be valuable for understanding the practical implications of the proposed approach.  Feedback for the Authors: The paper presents a novel and effective method CLSER for continual learning achieving stateoftheart performance in various scenarios. Strengthening the theoretical underpinnings of the proposed approach providing detailed comparisons on additional metrics and offering insights into computational efficiency would further enhance the paper. Overall the paper makes a significant contribution to the field of continual learning.  Recommendation: Based on the innovative approach versatility stateoftheart performance and comprehensive analysis presented in the paper I recommend acceptance with minor revisions for further clarity and depth in theoretical justification and comparison metrics.", "uHv20yi8saL": " Review Summary  Summary of the Paper The paper introduces a novel monotonic improvement guarantee for optimizing decentralized policies in cooperative MultiAgent Reinforcement Learning (MARL) particularly focusing on two prominent actorcritic algorithms: Independent Proximal Policy Optimization (IPPO) and MultiAgent PPO (MAPPO). It addresses the theoretical understanding and practical implications of using independent ratios in the objective functions of IPPO and MAPPO. The study provides a detailed theoretical analysis empirical results and comparisons between IPPO and MAPPO in various scenarios.  Strong Points 1. Theoretical Analysis: The paper presents a comprehensive theoretical analysis on the monotonic improvement guarantee in MARL specifically focusing on the implications of the trust region constraint over joint policies and independent ratios. 2. Practical Relevance: The practical implications of the theoretical findings are explored through empirical results confirming the effectiveness of independent ratio clipping and trust region enforcement through centralized training. 3. Experimental Results: The empirical evaluations on the StarCraft MultiAgent Challenge (SMAC) demonstrate the effectiveness of ratio clipping in enforcing the trust region constraints with clear discussions on the impact of hyperparameters and the number of optimization epochs. 4. Comparative Analysis: The comparative analysis between IPPO and MAPPO showcases comparable performance reinforcing the importance of enforcing trust region constraints in actorcritic methods for MARL.  Weak Points 1. Complexity: The papers theoretical analysis and discussions may be intricate for readers less familiar with reinforcement learning concepts potentially limiting the accessibility of the findings. 2. Lack of Novelty: While the study addresses an important aspect of trust region optimization in MARL some reviewers might seek more substantial novelty or realworld applications to strengthen the impact of the research.  Questions and Feedback to the Author 1. Have you considered investigating the scalability and robustness of the proposed monotonic improvement guarantee in largerscale MARL scenarios beyond the SMAC benchmark 2. How do the findings of the study contribute to addressing practical challenges in realworld applications of cooperative MARL especially in scenarios with dynamic environments or unknown transition dynamics 3. Can you elaborate on the potential limitations or assumptions made during the theoretical analysis and their implications on the generalizability of the proposed monotonic improvement guarantee In conclusion the paper presents a solid contribution to the field of MARL offering valuable insights into trust region optimization and the practical implications of enforcing centralized trust region constraints. Further exploration of realworld applications and scalability considerations could enhance the impact of the research.  Recommendation Based on the comprehensive theoretical analysis empirical results and comparisons provided in the paper I recommend acceptance of the submission pending minor revisions to enhance clarity and potential extensions to address scalability and practical applications. The studys theoretical depth and practical relevance highlight its significance in advancing the understanding of decentralized policy optimization in cooperative MARL.", "qsZoGvFiJn1": " Summary of the Paper: The paper introduces a novel framework called HINDSIGHT which leverages past LiDAR data from previous traversals of the same route to improve 3D object detection accuracy for selfdriving cars. By extracting contextual information from historical data and storing it in a SparseQuantized Sparse History (SQuaSH) representation the proposed framework enhances the performance of 3D object detectors without requiring additional annotations. The HINDSIGHT framework is compatible with various modern 3D detection architectures and demonstrates significant improvements in detecting challenging objects such as small faraway and occluded ones.  Strong Points: 1. Innovative Approach: The paper proposes a novel concept of utilizing historical LiDAR data to enhance the accuracy of 3D object detection addressing the limitations posed by challenging objects in autonomous driving scenarios. 2. Comprehensive Evaluation: The paper provides thorough experiments and evaluations across multiple realworld datasets and various modern object detection models demonstrating consistent and substantial performance gains using the HINDSIGHT framework. 3. Efficient and Pragmatic Solution: The proposed framework is efficient in terms of storage and latency making it suitable for realtime applications in selfdriving systems. The offline precomputation of SQuaSH tensors and minimal disruption to the perception loop showcase the practicality of the approach. 4. Robustness Analysis: The paper includes robustness analyses such as the impact of localization error quantization sizes and the amount of available past traversals providing insights into the adaptability and resilience of the HINDSIGHT framework.  Weak Points: 1. Unexplored Datasets: The paper mainly focuses on two datasets Lyft Level 5 Perception and nuScenes which may limit the generalizability of the results. There could be value in testing the framework on additional datasets to further validate its effectiveness. 2. Limited Adverse Condition Testing: The paper mentions potential limitations in adverse conditions like snowstorms or accidents and suggests the need for diverse datasets. It would be beneficial to explore the frameworks performance under various adverse scenarios for a more comprehensive assessment.  Questions and Feedback: 1. Can you elaborate on the computational complexity of the proposed HINDSIGHT framework particularly during the extraction and aggregation of historical LiDAR data How scalable is the approach for largescale deployment in autonomous driving systems 2. Have you considered incorporating additional sensor data such as camera images or highdefinition maps into the framework to further enhance object detection capabilities How might the integration of multimodal data sources impact the performance of the HINDSIGHT framework  Review Summary: The paper presents a novel and practical framework HINDSIGHT for improving 3D object detection in selfdriving cars by leveraging historical LiDAR data. The approach demonstrates significant performance gains on challenging objects showcasing its potential for enhancing safety in autonomous driving systems. The comprehensive evaluations robustness analyses and efficient implementation contribute to the strength of the proposed framework. To strengthen the paper further testing on diverse datasets exploration of multimodal data integration and elaboration on computational aspects would be beneficial. Overall the innovative nature of the approach coupled with its promising results suggests that the HINDSIGHT framework could be a valuable enhancement for autonomous driving technology.", "okmZ6-zU6Lz": "Summary: The paper explores the controllability of largescale networked dynamical systems when the complete knowledge of network structure is unavailable. It focuses on learning communitybased representations to understand the ability of a group of control nodes to steer the network to a target state. The paper compares two approaches  a Model Order Reduction (MOR) approach and a Learningbased approach to estimate controllability from coarse summaries of network structure. It uses the stochastic block model (SBM) to characterize network structures and derives insights applicable to realworld scenarios like brain networks and power systems. Strengths: 1. Novel Contribution: The paper proposes a novel learningbased framework for inferring the controllability of fine networks from coarse measurements. 2. Theoretical Rigor: The paper presents a wellstructured theoretical framework deriving error bounds and probabilities to support the proposed methodologies. 3. Relevance: The study addresses a significant contemporary challenge in networked systems with practical applications in diverse fields. Weaknesses: 1. Complexity: The paper delves into complex mathematical formulations which may be challenging for readers who are not wellversed in advanced control theory. 2. Technical Jargon: The paper contains a substantial amount of technical jargon which could hinder understanding for readers outside the specific domain. 3. Lack of Empirical Validation: Although theoretical results are presented there is a need for empirical validation to demonstrate the practical effectiveness of the proposed methods. Questions and Feedback to the Author: 1. Empirical Validation: Will the proposed methods be empirically validated using realworld data to confirm the effectiveness in practical scenarios 2. Applicability: How do you plan to extend this research to other types of networks beyond the ones mentioned in the paper such as social networks or transportation networks 3. Scalability: How scalable are the proposed approaches when dealing with even largerscale networks especially in scenarios like smart cities or InternetofThings (IoT) systems Review Summary: The paper presents a rigorous analysis of controllability in largescale networked systems using both a Model Order Reduction (MOR) approach and a Learningbased approach. The theoretical foundations are strong and the paper addresses a relevant and challenging problem in networked systems. However to strengthen the paper empirical validation and clarification on applicability and scalability are essential. Overall with further validation and additional insights on scalability the paper has the potential to make a significant impact in the field of networked dynamical systems.", "qHsuiKXkUb": "Summary: The paper proposes improvements for diffusion models particularly in the context of highresolution image generation. The main contributions include a new parametrization for estimating unbounded data scores a practical soft truncation trick for handling score scales variation and a novel stochastic differential equation design to enable sampling at high precision. These improvements are applied to enhance the performance of diffusion models such as HNCSN and HDDPM resulting in stateoftheart performance on highresolution image generation tasks. Experimental results demonstrate the effectiveness of the proposed improvements in terms of sample fidelity and density estimation across various benchmark datasets. Strengths: 1. Theorydriven innovation: The paper provides a solid theoretical foundation for addressing the challenges related to highprecision diffusion in image generation models. 2. Comprehensive improvements: The proposed enhancements cover multiple aspects of diffusion models including parametrization optimization and differential equation design contributing to a holistic approach to improving performance. 3. Experimental validation: Extensive experiments and ablation studies are conducted on benchmark datasets demonstrating the effectiveness of the proposed improvements in enhancing both sample quality and density estimation. 4. Stateoftheart performance: The results show that the proposed highprecision diffusion models outperform existing models in terms of various evaluation metrics establishing a new benchmark for image generation tasks. Weaknesses: 1. Complexity: The paper introduces several technical concepts and theoretical frameworks which may pose a challenge for readers not wellversed in diffusion models and stochastic processes. 2. Clarity: Some sections especially related to theoretical aspects and mathematical derivations could benefit from clearer explanations or additional illustrative examples to aid in understanding. Questions and Feedback to the Authors: 1. Scalability: Have you considered the scalability of the proposed improvements to even higher resolutions beyond 256x256 and how do you foresee these methods performing with ultrahighresolution image generation tasks 2. Generalization: How generalizable are the proposed enhancements to other types of generative models beyond diffusion models or are they specifically tailored for diffusionbased approaches Review Summary: The paper presents a comprehensive and wellstructured investigation into improving diffusion models for highresolution image generation. The proposed enhancements including new parametrization optimization method and novel stochastic differential equation show significant performance gains in experiments across various datasets. The thorough evaluation and comparison with existing models establish the effectiveness of the proposed approach making a strong case for adoption in the field. With a rigorous theoretical foundation and empirical validation the paper provides valuable contributions to advancing stateoftheart image generation techniques. Based on the strengths and impact of the proposed improvements I recommend accepting this paper for publication.", "rrWeE9ZDw_": " Summary: The paper proposes a method for autonomously learning objectcentric representations from raw pixel data in complex and highdimensional environments with a focus on planning tasks. The goal is to enable efficient transfer of learned representations between tasks that share similar object types. The approach is demonstrated on a 2D crafting domain and various Minecraft tasks to show the agents capability to form complex longterm plans based on the learned representations.  Strengths: 1. Innovative Approach: The paper introduces a novel method for autonomously learning objectcentric representations directly from pixel data enabling efficient transfer of learned representations. 2. Efficiency: By incorporating additional structure and assuming the existence of objects the proposed framework reduces the number of samples required to learn a new task. 3. Demonstration: The methodology is demonstrated on various domains including the Blocks World a 2D crafting domain and Minecraft tasks showcasing the effectiveness of the approach across different environments.  Weaknesses: 1. Complexity: Despite the highlevel overview provided the detailed technical aspects of the methodology could be challenging for readers not familiar with the domain. 2. Transferability: While the paper discusses transferability of learned representations more concrete evidence or quantitative analysis on the effectiveness of transfer learning could enhance the papers impact. 3. Evaluation: The evaluation of the proposed method could be further strengthened by comparing against existing approaches and providing benchmarks for performance comparison.  Questions: 1. How does the learned representation generalize across various object types in different tasks 2. Can you provide more insights into the computational resources required for training and utilizing the proposed method in realworld scenarios 3. How robust is the methodology to noisy and dynamic environments especially in realtime applications  Feedback: 1. Clarity: Enhance the clarity of explanations especially in technical sections to ensure better understanding for a wider audience. 2. Evaluation: Consider including quantitative measures to evaluate the performance of the proposed method enabling a more robust assessment of its effectiveness. 3. Comparative Analysis: Adding comparisons with existing approaches would provide a clearer understanding of the advancements made by the proposed methodology.  Review Summary: The paper introduces an innovative method for autonomously learning objectcentric representations from raw pixel data showcasing its applicability across different domains. While the proposed approach demonstrates efficiency and transferability benefits addressing potential weaknesses in complexity evaluation and robustness could further enhance the impact and credibility of the paper. Overall the methodology presents a promising direction for optimizing planning tasks in complex environments.", "n6Bc3YElODq": "PeerReview: 1) Summary: The paper introduces a novel approach called ModelBased Opponent Modeling (MBOM) to address the challenge of managing diverse opponents in multiagent environments. MBOM utilizes recursive imagination and Bayesian mixing techniques to predict and capture the policy improvement of opponents. The paper evaluates MBOM in two competitive tasks  Triangle Game and OneonOne against fixed policy na\u00efve learner and reasoning learner opponents demonstrating its effectiveness in adaptation. 2) Strengths:  Innovative Approach: Introducing MBOM with recursive imagination and Bayesian mixing is a novel and interesting approach to opponent modeling in multiagent environments.  Comprehensive Evaluation: The paper provides a detailed experimental evaluation in two diverse competitive tasks demonstrating the effectiveness of MBOM against various types of opponents.  Clearly Defined Methodology: The methodology of MBOM including recursive imagination and Bayesian mixing is well explained making it easy to understand the models functioning.  Comparison with Baselines: The comparison with existing methods like LOLADiCE MetaPG MetaMAPG and PPO provides a solid benchmark for evaluating the performance of MBOM. Weaknesses:  Complexity and Technical Detail: The paper is highly technical and complex which may make it challenging for readers unfamiliar with the domain to follow the intricacies of the proposed MBOM methodology.  Lack of Visual Aids: Some visual aids could help in understanding the concepts better especially the recursive imagination process and Bayesian mixing.  Limited Generalization: The focus on two specific tasks may limit the generalizability of the proposed approach to a broader range of applications in multiagent environments. Questions and Feedback for the Authors: 1. Could you provide more insights into the scalability of MBOM when the size of the joint opponent becomes larger 2. Have you considered experimenting with a wider range of diverse environments beyond the Triangle Game and OneonOne to further validate the adaptability and effectiveness of MBOM across different scenarios 3. How does the computational overhead of MBOM compare to existing methods when dealing with complex opponent interactions and reasoning abilities 3) Review Summary: The paper presents an innovative ModelBased Opponent Modeling (MBOM) approach with recursive imagination and Bayesian mixing techniques to address the challenges of diverse opponents in multiagent environments. The comprehensive evaluation in two competitive tasks showcases the effectiveness of MBOM against fixed policy na\u00efve learner and reasoning learner opponents. Despite the complexity and technical detail the methodology is clearly defined with a solid comparison against baselines. The lack of visual aids and the limited generalization to broader environments are areas for improvement. Overall the novel approach and promising results make MBOM a potential candidate for enhancing adaptive multiagent learning strategies. Recommendation: Based on the innovative methodology comprehensive evaluation and promising results I recommend accepting this paper with minor revisions to address the weaknesses identified.", "hSktDu-h94": "Summary: The paper addresses the misalignment between prediction loss and optimization goal in predictthenoptimize (PTO) problems through the introduction of total group preorder (TGP) and approximate total group preorder (ATGP) losses. These losses aim to improve the performance of PTO problems with strong ranking properties. The paper also proposes an automatic loss function search algorithm APOC to adapt the ATGP loss to different combinatorial structures. Extensive experiments on ranking knapsack and shortest path problems show the superiority of the proposed method over existing approaches. Strengths: 1. Novel Contribution: The paper introduces a new perspective to tackle the optimizationprediction misalignment in PTO problems offering theoretical proof and practical solutions. 2. Rigorous Theoretical Basis: The theoretical framework laid out for TGP strong ranking property and ATGP loss is welldefined and offers a strong foundation for the proposed method. 3. Automatic Search Algorithm: The development of APOC to automatically find suitable loss functions for different optimization problems is innovative and has the potential to streamline PTO problemsolving. 4. Empirical Evidence: The extensive experiments on various realworld datasets validate the effectiveness of the proposed method through comparisons with existing algorithms. 5. Reproducibility: The availability of source code and the inclusion of proofs for key theoretical results enhance the reproducibility and transparency of the research. Weaknesses: 1. Complexity: The paper delves into intricate mathematical formulations and technical details potentially making it challenging for nonspecialists to grasp the core concepts. 2. Challenge in Application: The proposed methods practical implementation and scalability to other PTO problems may encounter obstacles due to the complexity of the algorithm. 3. Limited Comparison: The comparison with baseline methods could be more comprehensive by including a broader range of existing algorithms. 4. Hyperparameter Sensitivity: The sensitivity analysis of hyperparameters could be further elaborated to provide deeper insights for future applications. 5. Ablation Study Impact: The ablation study results could be discussed in more detail to highlight the significance of utilizing wavelet filters for optimal performance. Question: 1. How does the proposed APOC algorithm handle scenarios with highdimensional parameter spaces or complex optimization structures Feedback to Authors: 1. Enhance the discussion on practical implementation challenges and potential scalability of the proposed method to address reallife optimization problems. 2. Consider providing more detailed insights into the impact evaluation of hyperparameters particularly in diverse optimization contexts. 3. Elaborate further on the implications of the ablation study results emphasizing the importance of leveraging wavelet filters in the APOC algorithm for improved outcomes. Review Summary: The paper presents a significant advancement in addressing the misalignment between prediction loss and optimization goals in PTO problems. The theoretical foundations innovative approaches such as ATGP loss and APOC algorithm and comprehensive experimental validations highlight the papers contributions. However there are aspects of complexity scalability and comparative analysis that could be further elaborated for a more holistic understanding. Overall the solid theoretical underpinning and empirical validation make the paper a valuable addition to the field of PTO problems with strong ranking properties. Thus I recommend the paper for publication after addressing the mentioned comments and suggestions.", "mQDpmgFKu1P": " Review of the Paper  1. Summary: The paper introduces a novel attention module called implicit selfattention and a Legendre Memory Unit (LMU) based model to address the limitations of traditional selfattention mechanisms in transformers. The paper investigates the scaling properties of the LMU models in language modeling tasks showcasing their improved performance in terms of memory and computation efficiency compared to transformers and LSTMs.  2. Strengths:  Innovation: The introduction of the LMU and implicit selfattention offers a fresh perspective on addressing the limitations of traditional selfattention mechanisms.  Efficiency: The paper demonstrates notable improvements in memory and computational efficiency showcasing a more linear dependency for memory and computation over sequence length which is a significant contribution to the field.  Experimental Results: The comprehensive experiments and comparisons with transformers and LSTMs highlight the superiority of the LMU models in terms of scaling properties and data efficiency.  2. Weaknesses:  Clarity: Some sections especially those detailing the LMU equations and the details of the model architecture may be challenging to follow without a strong background in the subject matter. Simplifying the explanations could enhance the accessibility of the paper.  Visualization: While the architecture illustration is helpful additional visual aids or diagrams could further clarify the workings of the LMU and implicit selfattention.  3. Question and Feedback to the Author:  Question: Could you provide more insights into the practical implementation challenges or considerations when deploying the LMU model in realworld applications  Feedback: Consider providing more extensive discussions on the interpretability of the LMU models outcomes and any potential limitations that may arise in specific use cases.  Review Summary: The paper presents a novel LMUbased model with implicit selfattention demonstrating improved memory and computation efficiency over traditional selfattention mechanisms in transformers. The experimental results show promising scaling properties and data efficiency of the LMU models in language modeling tasks. Overall the paper makes a significant contribution to the field of natural language processing by introducing an innovative approach to address the limitations of current architectures. The findings support the recommendation for publication based on the papers substantial contributions innovative approach and solid experimental validation. Further enhancements in clarity and visualization could enhance the papers impact and accessibility to a broader audience.", "qnQN4yr6FJz": " Summary: The paper addresses the challenging problem of distributional prediction with incomplete features proposing a novel hybrid approach that combines elements of generative and discriminative models. The proposed method is grounded on a new variational inference framework allowing for versatile application across modern machine learning models like variational autoencoders. Empirical validation confirms the effectiveness of the proposed technique.  Strengths: 1. Innovative Hybrid Approach: The paper introduces a novel hybrid method that leverages the strengths of both generative and discriminative models providing a fresh perspective to tackle the problem of distributional prediction with incomplete features. 2. Versatile Application: The use of blackbox variational inference enables the method to be applied across various modern machine learning architectures enhancing its practical utility and relevance. 3. Theoretical Foundation: The paper is wellgrounded in theoretical concepts outlining clear mathematical formulations and technical challenges which enriches the papers academic value. 4. Empirical Evaluation: The effectiveness of the proposed method is empirically validated through numerical experiments demonstrating its practical applicability and performance.  Weaknesses: 1. Complexity of Notations: The papers heavy use of mathematical notations and technical terminology may hinder readability for nonexpert readers. Simplifying the presentation could enhance accessibility and comprehension for a broader audience. 2. Lack of Comparison Metrics: While the paper evaluates the proposed method against existing approaches providing a more detailed comparison with additional metrics or benchmarks could strengthen the analysis and insight into the methods performance. 3. Scope of Applicability: While the proposed method shows promising results discussing the limitations of the approach such as specific scenarios where it might not perform optimally could provide a more nuanced perspective.  Question:  Could you elaborate on the computational complexity of the proposed method compared to existing approaches especially when applied to largescale datasets  Feedback to the Author:  Considering the complexity of the topic ensuring a balance between technical depth and readability is crucial for appealing to a wider audience.  Providing more insights into the practical implications of the proposed method such as realworld applications or potential industry use cases can enhance the papers impact and relevance.  Review Summary: The paper presents an innovative hybrid approach for distributional prediction with incomplete features addressing a challenging problem in machine learning. While the method shows promise and is supported by sound theoretical foundations and empirical validation improvements in readability comparison metrics and discussions on limitations could further strengthen the paper. Overall the papers novel methodology and empirical validation contribute valuable insights to the field of machine learning and warrant consideration for publication.", "oiZJwC_fyS": "1) Summary of the paper: The paper presents a theoretical contribution to neural network approximation using a novel tropical geometrical viewpoint for structured neural network compression. The authors introduce a bound on the approximation error between two neural networks with ReLU activations propose geometrical compression algorithms based on zonotope reduction and the Kmeans algorithm and analyze their performance theoretically and empirically. 2) Strong and Weak Points Questions Feedback: Strong Points:  The paper offers a unique theoretical contribution by linking tropical geometry with neural network compression.  The theoretical foundation provided including the theorems and propositions is robust and wellexplained.  The proposed compression algorithms based on zonotope reduction and Kmeans algorithm showcase innovative thinking in network compression.  The experimental evaluation on MNIST FashionMNIST and CIFAR datasets with comparisons to existing methods provides compelling evidence of the effectiveness of the proposed geometrical compression methods. Weak Points:  The paper is highly theoretical and may be challenging for readers without a strong background in mathematical concepts such as tropical geometry.  The practical implications of the proposed algorithms could be further elaborated especially regarding realworld applications and computational efficiency.  Clarity could be improved in some sections especially when discussing the algorithms and their applications in neural network compression. Questions and Feedback to the Authors: 1. How do the proposed compression algorithms based on zonotope reduction and the Kmeans algorithm compare in terms of computational efficiency and scalability to existing compression techniques 2. Have you considered exploring the applicability of the geometrical compression methods to more complex neural network architectures beyond the examples provided in the paper 3. Can you provide more insights into the robustness of the proposed algorithms under different settings or datasets with varying complexities 3) Review Summary: The paper offers a novel theoretical standpoint by integrating tropical geometry with the field of neural network approximation and compression. The strong theoretical foundation backed by theorems and propositions innovative geometrical compression algorithms and comprehensive experimental evaluations on various datasets demonstrate the efficacy of the proposed methods. Further research directions could explore the scalability and applicability of the algorithms to more complex neural network structures and datasets. Overall the paper presents a promising approach that opens up new avenues for research in the intersection of tropical geometry and machine learning. Recommendation: Based on the innovative theoretical contributions robust methodology and promising experimental results I recommend accepting this paper with minor revisions focusing on enhancing clarity and practical implications. This work has the potential to make a significant impact within the field of neural network compression and tropical geometry.", "w01vBAcewNX": " 1) Summary of the Paper: The paper explores the problem of utilizing expert data with unobserved confounders for imitation learning and reinforcement learning in a contextual Markov Decision Process (MDP) setup. It defines the tasks of imitation and reinforcement learning using expert data with hidden confounders and tackles the challenges posed by covariate shift between the expert data and the online environment. The paper analyzes the limitations of learning from such data provides theoretical results proposes algorithms to address the challenges and validates the claims empirically on assistive healthcare and recommender system simulation tasks.  2) Strengths and Weaknesses Questions and Feedback:  Strong Points:  Theoretical Analysis: The paper provides a comprehensive theoretical analysis of learning with hidden confounders and covariate shift in imitation and reinforcement learning settings.  Algorithm Proposals: The proposed algorithms such as Corrective Trajectory Sampling (CTS) address the challenges of utilizing expert data effectively in the presence of hidden confounders.  Empirical Validation: The empirical validation on challenging tasks adds credibility to the theoretical claims made in the paper.  Clear Structure: The paper is wellstructured with clear sections explaining the problem contributions methodology and experimental results.  Weak Points:  Complexity: The paper delves into advanced concepts like fdivergences sensitivity analysis and structural causal models which may make it challenging for readers not wellversed in these areas to follow the content.  Transition between Sections: The transitions between different sections could be smoother to enhance the readability and understanding.  Algorithm Implementation Details: While the algorithms are welldescribed more details on their practical implementation and applicability in realworld scenarios could enhance the papers relevance.  Questions and Feedback: 1. Clarification on Experimental Setup: Could you provide more insights into the experimental setup including the hyperparameters used environment configurations and specific details of the simulation tasks 2. Comparison with Existing Methods: How does the proposed approach compare with existing methods for utilizing expert data in the presence of hidden confounders in the literature 3. Practical Considerations: Considering the intricate theoretical framework developed in the paper what practical considerations should researchers or practitioners keep in mind while applying these techniques in realworld scenarios  3) Review Summary: The paper presents a rigorous analysis of utilizing expert data with unobserved confounders in imitation learning and reinforcement learning settings. The theoretical insights algorithmic proposals and empirical validation on assistive healthcare and recommender system tasks demonstrate the significance and feasibility of leveraging confounded expert data. While the paper excels in providing novel contributions and addressing challenging scenarios the complexity of the concepts introduced and the detailed theoretical analyses might limit its accessibility to a broader audience. However with some enhancements in clarifications practical insights and comparisons to existing methods this work can make substantial contributions to the field of reinforcement learning and causal inference. Overall given the importance of addressing confounded expert data in learning settings I recommend accepting this paper for publication with minor revisions to enhance clarity and practical applicability.", "qvUJV2-t_c": " Summary: The paper introduces a line search method called LargeBatch Parabolic Approximation Line Search (LABPAL) for Deep Learning optimization. LABPAL approximates the fullbatch loss with parabolas estimated over several minibatches to derive learning rates during training. The approach leverages recent empirical findings on the behavior of fullbatch loss in the direction of noisy update step directions. Experimental results show that LABPAL outperforms other line search approaches and SGD with a piecewise constant learning rate schedule in terms of validation and test accuracy across models datasets and batch sizes.  Strengths: 1. Novel Approach: The paper presents an innovative line search method based on empirical observations addressing the challenge of determining learning rates in Deep Learning optimizers. 2. Empirical Validation: The approach is backed by extensive empirical studies across different models datasets and noise levels demonstrating the effectiveness of LABPAL in optimizing Deep Learning tasks. 3. Significant Results: LABPAL outperforms existing line search methods and SGD tuned with fixed learning rates showcasing superior performance in terms of validation and test accuracy. 4. Theoretical Background: The paper provides a detailed mathematical and empirical foundation for the proposed approach addressing the challenges associated with line searches in Deep Learning.  Weaknesses: 1. Theoretical Limitations: While the approach is empirically validated a theoretical analysis of the underlying principles is lacking potentially limiting the understanding of the methods convergence properties in all scenarios. 2. Hyperparameter Sensitivity: The sensitivity analysis reveals the impact of hyperparameters on performance suggesting the need for careful tuning which might add complexity in practice. 3. Ethics Statement: The ethics statement provided could be expanded to address potential ethical implications and societal impacts of implementing optimization methods in AI.  Questions to the Authors: 1. Are there any additional datasets or models not covered in the paper where LABPAL was tested and if not are there plans to expand testing to more diverse scenarios 2. How does the proposed approach compare to other stateoftheart optimization techniques in terms of convergence speed and computational efficiency  Feedback: 1. Strengthen the theoretical analysis by providing insights into the convergence properties of the approach under different function types and noise levels. 2. Consider expanding the ethics statement to discuss potential ethical implications of optimizing Deep Learning models and the societal impact it might have.  Review Summary: The paper presents a novel line search method LABPAL for optimizing Deep Learning models based on empirical findings on fullbatch loss behavior. The approach is wellsupported by experiments showing its superior performance over existing methods. However strengthening the theoretical background and expanding the ethics statement could enhance the papers impact and contribute to a more comprehensive evaluation of LABPALs efficacy. Overall the paper offers a valuable contribution to the field of Deep Learning optimization.", "pC00NfsvnSK": " Summary: The paper proposes a novel framework called Generalized Similarity Functions (GSF) to improve zeroshot generalization in offline reinforcement learning (RL) settings. The key idea is to use contrastive learning to train an RL agent to aggregate observations based on their similarity in expected future behavior quantified using generalized value functions. The paper introduces a new offline RL benchmark offline Procgen and demonstrates that GSF outperforms existing stateoftheart offline RL and representation learning methods on this benchmark.  Strengths: 1. Novel Framework: GSF presents a unique and theoreticallymotivated solution to enhance generalization in offline RL showcasing creativity in the field. 2. Theoretical Foundation: The paper provides a solid theoretical foundation for GSF explaining concepts like Generalized Value Functions and their connection to future behavior similarity. 3. Empirical Evaluation: The extensive empirical evaluation on the offline Procgen benchmark demonstrates the effectiveness of GSF compared to existing methods showcasing its practical applicability. 4. Reproducibility: The inclusion of proofs in the appendix plans to opensource the benchmark and provision of source code enhance the reproducibility of the research.  Weaknesses: 1. Complexity: The papers technical depth and complexity may pose challenges for readers not wellversed in reinforcement learning theory which could limit its accessibility to a broader audience. 2. Limited RealWorld Application: While the offline Procgen benchmark is extensive the specific focus on this benchmark might limit the generalizability of the findings to other realworld scenarios.  Questions and Feedback: 1. Clarity on Hyperparameters: Providing more details on how hyperparameters were selected for GSF could enhance the reproducibility of the results. 2. Comparison Metrics: Are there any additional comparison metrics beyond the average scores across games that could provide deeper insights into the performance of GSF 3. Future Extensions: Have the authors considered potential realworld applications or areas where GSF can be further tested to evaluate its broader utility outside of the proposed benchmark  Review Summary: The paper presents a novel and theoreticallygrounded framework GSF for improving zeroshot generalization in offline RL settings. With a strong emphasis on both theoretical foundations and empirical results the paper showcases the performance of GSF on a newly introduced benchmark offline Procgen outperforming existing stateoftheart methods. Despite its technical complexity the paper offers valuable insights into enhancing generalization capabilities in RL agents. Overall based on the strengths exhibited in the innovative approach and robust evaluation results I recommend acceptance of the paper after addressing the suggested improvements and clarifications.", "wIK1fWFXvU9": "Summary: The paper investigates the interaction between noisy labels (NL) and adversarial training (AT) which have been traditionally studied independently. It introduces a new measure based on the number of projected gradient descent (PGD) steps in AT to evaluate the robustness of data points near class boundaries. The study demonstrates that AT with strong smoothing effects can mitigate the impact of NL on model performance and provides a new criterion for sample selection to correct NL. The paper further explores the potential of using the number of PGD steps as a measure to differentiate correctincorrect and typicalrare data points. Strengths: 1. Innovative Approach: The paper introduces a novel method that integrates AT with NL to improve model performance and mitigate the impact of noisy labels. 2. Thorough Analysis: The study provides a comprehensive analysis of the interaction between NL and AT backed by empirical results and quantitative comparisons. 3. Clear Explanations: The paper effectively explains the concepts of AT NL and the proposed measure (PGD steps) using clear examples and figures making it accessible to readers. 4. Practical Applications: The paper offers practical applications of the new measure including a robust annotator and confidence scores which can have realworld implications for improving model training and data annotation. Weaknesses: 1. Limited Scalability Evaluation: While the paper demonstrates the effectiveness of the proposed method on datasets like CIFAR10 and MNIST a broader evaluation on diverse datasets would strengthen the generalizability of the results. 2. Lack of Robustness Analysis: The study could benefit from a deeper analysis of the robustness of the proposed method to different noise levels and types to validate its performance under various scenarios. 3. Incomplete Discussion on Limitations: The paper could benefit from a more detailed discussion on the limitations and potential challenges of implementing the proposed method in realworld applications to provide a more balanced analysis. Questions and Feedback: 1. Could you provide more insight into the scalability of the proposed method in handling larger and more complex datasets beyond CIFAR10 and MNIST 2. How does the proposed method perform under varying noise levels and types and what are the key factors influencing its robustness 3. Have you considered potential challenges or limitations that could affect the practical implementation of the new measure in realworld settings Review Summary: Overall the paper presents an innovative approach that integrates adversarial training with noisy labels to improve model performance and mitigate the impact of label noise. The study offers clear explanations thorough analysis and practical applications of the proposed method. However addressing scalability robustness under diverse noise scenarios and discussing potential limitations would enhance the paper\u2019s overall impact and applicability. Considering the strengths and potential contributions of the study with further developments the proposed method has the potential to be valuable in enhancing the robustness and accuracy of deep learning models in the presence of noisy labels.", "olQbo52II9": " 1) Summary: The paper introduces ECORD a novel reinforcement learning (RL) algorithm for combinatorial optimization on graphs specifically targeting the Maximum Cut problem. ECORD addresses the scalability issues of existing stateoftheart (SOTA) RL algorithms by limiting the use of graph neural networks (GNNs) to a single preprocessing step. The algorithm then utilizes a recurrent unit for fast actiondecoding which results in significant speed and scalability improvements. Experimental results showcase ECORD achieving new SOTA for RL algorithms on the Maximum Cut problem with orders of magnitude improvements in speed and scalability compared to competitors. Moreover ECORD retains strong performance even when scaling to larger graphs with up to 10000 vertices.  2) Strengths and Weaknesses: Strong Points:  Innovation: The proposed ECORD algorithm introduces a new paradigm for RL in combinatorial optimization successfully improving scalability.  Experimental Validation: The paper provides thorough experimental results showcasing the superiority of ECORD compared to existing algorithms.  Efficiency: By restricting GNN usage to a single preprocessing step ECORD achieves significant speed and scalability improvements.  Detailed Methodology: The paper offers detailed descriptions of the algorithm architecture training procedures and ablation studies. Weak Points:  Lack of Theoretical Analysis: The paper lacks indepth theoretical analysis of the algorithm focusing more on empirical validation.  Limited Generalization: The evaluation is primarily focused on the Maximum Cut problem restricting the breadth of applicationdomain.  Benchmark Comparison: While comparisons are made with existing algorithms wider benchmark comparisons with additional stateoftheart methods could further enhance the studys significance.  Clarity on Algorithm Implementation: The detailed descriptions of the algorithm could potentially be further improved for easier understanding by readers. Feedback to Authors: 1. Consider including more theoretical analysis and discussion on the complexity and properties of the ECORD algorithm. 2. Discuss potential applications beyond the Maximum Cut problem to demonstrate the algorithms versatility. 3. Enhance clarity in describing the algorithm architecture and methodology for better reader comprehension. 4. Expand the benchmark comparisons to provide a more comprehensive evaluation against a wider range of stateoftheart methods.  3) Review Summary: The paper presents a novel RL algorithm ECORD designed to address scalability issues in graphbased combinatorial optimization problems specifically focusing on the Maximum Cut problem. The innovation of restricting graph neural network usage to a single preprocessing step and leveraging fast actiondecoding with a recurrent unit proves to be highly effective in improving speed and scalability. Thorough experimental validation showcases ECORDs superior performance and efficiency compared to existing algorithms. While the paper lacks indepth theoretical analysis and broader application demonstrations the welldetailed methodology and robust experimental results indicate the significance of ECORD in the domain of RL for combinatorial optimization on graphs. Therefore based on the strengths and weaknesses identified I recommend accepting the paper for publication after addressing the feedback provided to enhance its quality and impact.", "rczz7TUKIIB": " Review Summary  1) Summary of the Paper: The paper introduces MetaLoss a novel metalearning model designed for improving regression results specifically in the context of forecasting time series data. The proposed model MetaLoss involves joint cotraining of a regressor network and a losslearning network leveraging controlled noise during training to explore optimal loss function behaviors for various scenarios. The paper showcases the effectiveness of MetaLoss in adapting loss functions for different magnitudes of prediction errors demonstrating improved forecasting accuracy across naive and complex performance metrics in realworld datasets.  2) Strengths and Weaknesses:  Strong Points:  Innovative Approach: Introduces a unique metalearning model for regression losses that significantly improves forecasting performance.  RealWorld Application: Grounds the study on realworld data showcasing practical benefits of MetaLoss in forecasting tasks.  Thorough Experimental Analysis: Conducted experiments on diverse datasets demonstrating the effectiveness of MetaLoss in adapting to various performance metrics.  CoTraining Mechanism: Highlighting the cotraining aspect of the model which allows for mutual learning between the regressor and losslearning networks.  Weak Points:  Lack of Comparison Metrics: While the paper presents results comparing MetaLoss with traditional loss functions additional comparison metrics or benchmarks could provide more comprehensive insights.  Theoretical Explanations: Some sections especially related to the models design and training phases may benefit from clearer and more detailed explanations for better understanding by readers.  Visualizations and Plots: More detailed explanations or analyses on the visualized results in figures could provide deeper insights into the behavior of MetaLoss in different scenarios.  Questions and Feedback to the Authors: 1. Could you provide additional insights on how MetaLoss performs in scenarios with varying complexities of time series data beyond the datasets used in the study 2. Have you considered exploring how MetaLoss could handle outlier detection or anomaly forecasting tasks which are crucial in various realworld applications 3. It would be beneficial to discuss the computational costs or scalability aspects of MetaLoss especially when applied to larger datasets or in resourceintensive environments.  3) Recommendation: The paper introduces a novel approach MetaLoss that clearly demonstrates significant improvement in regression tasks particularly in time series forecasting. The thorough experimentation and realworld application provide strong validation for the proposed model. With welldocumented strengths such as innovation practical relevance and robust experimental validation I recommend the acceptance of the paper with minor revisions to address the identified weaknesses and clarifications. The findings and implications presented in the paper contribute valuably to the field of metalearning for regression losses and forecasting tasks.", "lsQCDXjOl3k": "Peer Review: 1) Summary: The paper introduces a novel method called \"unconditional guidance\" as an alternative to \"classifier guidance\" for conditional diffusion models. The study aims to investigate the possibility of achieving a tradeoff between sample quality and diversity without relying on an image classifier. By jointly training a conditional and an unconditional diffusion model the proposed method combines the score estimates of both models achieving similar results to classifier guidance. 2) Strengths and Weaknesses: Strengths:  The paper addresses an important research question and proposes a novel method to enhance sample quality in diffusion models without using classifiers.  The experiments are welldesigned comparing the effectiveness of unconditional guidance in achieving a tradeoff similar to classifier guidance.  The paper is wellstructured with detailed explanations and clear algorithms provided for implementation.  The results demonstrate competitive performance compared to prior work showcasing the effectiveness of unconditional guidance. Weaknesses:  The paper lacks a detailed discussion on the limitations of unconditional guidance such as potential drawbacks in terms of sample diversity or computational efficiency.  While the experiments show promising results further analysis on the implications of using unconditional guidance in realworld scenarios could provide more insights. Questions and Feedback to the Author: 1. How does the computational cost of implementing unconditional guidance compare to classifier guidance in practical settings 2. Have you considered conducting user studies or qualitative assessments to evaluate the impact of decreased sample diversity when using unconditional guidance 3. Can you provide insights into potential applications or use cases where unconditional guidance could be particularly beneficial over classifierguided methods 4. How does the proposed method generalize to other data modalities beyond image synthesis tasks 3) Review Summary: The paper presents a valuable contribution by introducing unconditional guidance as a method to enhance sample quality in diffusion models without relying on classifiers. The thorough experimental evaluation demonstrates the effectiveness of the proposed approach in achieving a tradeoff between sample fidelity and diversity. The structured presentation and detailed algorithms enhance the reproducibility of the research. However further exploration of the limitations and practical implications of unconditional guidance would strengthen the paper. Overall based on the innovative approach clear methodology and promising results I recommend this paper for publication after addressing the feedback and questions raised. Overall Recommendation: Accept with Revisions", "r8S93OsHWEf": " Summary of the Paper The paper introduces a novel approach to improve the generalization and robust accuracy of adversariallytrained networks by incorporating selfsupervised testtime finetuning. The method involves a meta adversarial training strategy to find an optimal starting point for testtime finetuning enhancing the correlation between selfsupervision and classification tasks. Extensive experiments on CIFAR10 STL10 and Tiny ImageNet demonstrate consistent improvements in robust accuracy under various attack strategies including whitebox and blackbox attacks.  Strengths of the Paper 1. Innovative Approach: The proposed method of selfsupervised testtime finetuning combined with meta adversarial training is a novel and promising direction for improving robustness of adversariallytrained networks. 2. Comprehensive Experiments: The paper conducts thorough experiments on different datasets and attack strategies showcasing the methods effectiveness and consistency across various scenarios. 3. Clear Methodology: The methodology is wellexplained with detailed algorithms provided for implementation aiding in reproducibility. 4. Empirical Analysis: The paper includes empirical analysis demonstrating the effectiveness of the proposed approach supported by statistical data and visualizations.  Weaknesses of the Paper 1. Theoretical Background: The paper could benefit from more detailed explanations of the theoretical foundations supporting the proposed method. 2. Comparative Analysis: While the paper compares the method with existing approaches a more indepth comparative analysis and discussions on the differences could enhance the evaluation. 3. Discussion on Limitations: Addressing the potential limitations or constraints of the proposed method would help in understanding its applicability in various contexts. 4. Realworld Applications: The paper could discuss practical use cases and implications of the proposed approach in realworld scenarios for broader impact.  Questions for the Authors 1. Could you provide more insights into how the findings of your study could be applied in practical realworld scenarios particularly in critical applications where adversarial attacks pose a significant threat 2. How does the proposed approach perform in scenarios with highly diverse datasets or complex image distributions that deviate significantly from the training data 3. Have you considered exploring the impact of different selfsupervised tasks or varying hyperparameters on the methods performance to gain further insights into its robustness and applicability 4. What are the computational requirements and scalability considerations associated with implementing the proposed approach on largerscale datasets or more complex neural network architectures  Feedback for the Authors  Clarify the theoretical underpinnings of the method to enhance the understanding of the proposed approach.  Provide additional discussion on the practical implications and realworld applications of the method.  Conduct further experiments or analyses to explore the methods robustness under varying conditions and challenges including scalability and generalizability.  Review Summary Overall the paper presents an innovative method for improving the generalization and robust accuracy of adversariallytrained networks through selfsupervised testtime finetuning and meta adversarial training. The comprehensive experiments and empirical analysis support the effectiveness of the approach. However addressing theoretical foundations comparing with existing methods more comprehensively and discussing the limitations and practical implications further would strengthen the papers contribution to the field. Therefore considering the novelty strong experimental results and potential for improving robustness against adversarial attacks I recommend acceptance of the paper with minor revisions addressing the outlined points.", "i7h4M45tU8": " Summary: The paper proposes a method called Neural Temporal Logic Programming (Neural TLP) to uncover underlying atomic events and their relations that lead to composite events in noisy temporal data settings. The approach involves learning implicit temporal relations between atomic events and then lifting logic rules for composite events by efficiently searching through the combinatorial space of temporal logic rules. The method is evaluated on video and healthcare datasets and outperforms baseline methods for rule discovery.  Strengths:  Innovative Approach: The paper introduces a novel approach Neural TLP which addresses the challenge of uncovering underlying temporal rules in noisy temporal datasets.  Comprehensive Evaluation: The method is thoroughly evaluated on both synthetic video datasets and realworld healthcare data demonstrating its effectiveness in rule discovery.  Detailed Explanation: The paper provides a detailed explanation of the proposed method including parameter learning structure learning and optimization procedures.  Weaknesses:  Complexity: The proposed method involves multiple stages and requires significant computational resources which may limit its scalability.  Evaluation Limitation: While the experiments show promising results the paper could benefit from a more indepth analysis of the limitations and potential failure modes of Neural TLP.  Realworld Applicability: The realworld healthcare dataset used for evaluation is limited to a specific scenario (sepsis patients) which may restrict the generalizability of the proposed method to diverse healthcare settings.  Questions to the Authors: 1. How does the proposed Neural TLP method compare to existing rule discovery approaches in terms of computational efficiency and scalability 2. How does the method handle noise in the temporal data and ensure robustness in rule extraction 3. Have you considered further validation methods or case studies to demonstrate the practical utility of the extracted rules in realworld applications beyond the presented healthcare dataset  Feedback:  The paper would benefit from discussing the interpretability of the extracted rules and how they can be used to enhance decisionmaking or domain understanding.  Including a discussion on the potential ethical implications and bias considerations when deploying the Neural TLP method in realworld settings would add value to the paper.  Providing additional details on the hyperparameters chosen potential tradeoffs and sensitivity analysis would enhance the clarity and reproducibility of the results.  Review Summary: Overall the paper presents a novel method Neural Temporal Logic Programming (Neural TLP) for rule discovery in noisy temporal data settings. While the approach demonstrates promising results in both synthetic and healthcare datasets some aspects such as scalability interpretability and generalizability could be further elaborated. Addressing these points and considering potential practical applications would strengthen the impact and relevance of Neural TLP in diverse temporal data domains.", "xQUe1pOKPam": "Summary: The paper proposes the Graph MultiView Pretraining (GraphMVP) framework for molecular graph representation learning by incorporating 3D geometric information into graph selfsupervised pretraining to enhance the 2D molecular graph representation. The framework utilizes two SSL tasks  contrastive (intermolecule level) and generative (intramolecule level) SSL tasks to leverage the consistency between 2D and 3D views. The proposed VRR (Variational Representation Reconstruction) objective function meets the challenge of measuring the quality of reconstruction on a molecular scale. Comprehensive experiments demonstrate that GraphMVP performs significantly better than existing graph SSL methods. Strengths: 1. Novel Framework: The incorporation of 3D information into graph SSL for molecular representation learning is a novel and promising approach. 2. Theoretical Justifications: The paper provides clear theoretical insights to justify the effectiveness of GraphMVP such as maximizing mutual information and using 3D geometry as privileged information. 3. Empirical Validation: Comprehensive experiments demonstrate the superiority of GraphMVP over existing methods across various downstream tasks supporting its effectiveness. 4. Technical Contributions: The paper introduces innovative SSL strategies like EBMNCE and VRR and combines both contrastive and generative SSL for enhanced performance. 5. Code Availability: The availability of the code on GitHub enhances reproducibility and further research. Weaknesses: 1. Complexity: The paper is highly technical and contains intricate mathematical formulations that might be challenging for some readers to follow. 2. Experimental Setting: The paper focuses on specific tasks and datasets limiting the generalizability of the approach. 3. Comparison: While the paper compares GraphMVP with several baselines and performs ablation studies further comparisons with stateoftheart methods would strengthen the evaluation. Questions: 1. How do you handle the scalability of the proposed approach when dealing with larger datasets or diverse molecular structures 2. How robust is GraphMVP to noise or uncertainties in the data especially when dealing with realworld scenarios with incomplete or noisy 3D information 3. The theoretical insights provided are intriguing. Can you elaborate on potential future work that could further validate the theoretical underpinnings of GraphMVP Feedback: 1. The paper could benefit from providing realworld use cases or applications to showcase the practical significance of GraphMVP. 2. Consider discussing potential limitations or challenges of the proposed framework in handling more complex molecular tasks or scenarios. Review Summary: The paper presents a novel framework GraphMVP for molecular graph representation learning that incorporates 3D geometric information to enhance 2D molecular graph representations via selfsupervised learning. The approach is supported by strong theoretical justifications significant empirical results and innovative SSL strategies. While the technical contributions are commendable and the experimental results are promising further elaboration on scalability robustness and realworld applicability would enhance the papers impact. Overall based on the presented strengths and potential impact I recommend the paper for acceptance subject to minor revisions addressing the weaknesses and questions raised.", "w8HXzn2FyKm": "1) Summary The paper introduces a novel multiagent linear stochastic approximation algorithm driven by Markovian noise and consensustype interaction over timevarying directed graphs with stochastic interaction matrices. The paper provides finitetime bounds on meansquare error deviation from equilibrium points for distributed reinforcement learning algorithms with unidirectional communication. 2) Evaluation Strong Points:  The paper tackles an important problem of relaxing the assumption of bidirectional communication in distributed reinforcement learning algorithms.  The theoretical framework developed in the paper addresses the challenge of consensusbased algorithms with stochastic interaction matrices.  The analysis and results presented demonstrate the convergence and finitetime performance of the proposed algorithms under various conditions.  The paper provides a detailed explanation of the technical innovations and contributions including the handling of timevarying stepsizes and the pushbased distributed algorithm. Weak Points:  The mathematical notations and technical content may be challenging for readers without a strong background in stochastic processes and distributed algorithms.  The paper lacks practical experiments or simulations to validate the theoretical findings and algorithms proposed.  The discussion of future directions could have been further expanded to provide more insights into potential research avenues. Questions  Feedback to the Author: 1. How does the proposed pushbased distributed stochastic approximation algorithm compare to existing algorithms in terms of computational complexity and convergence speed 2. Could you provide more intuitive explanations or examples to help readers understand the practical implications of the theoretical results and algorithms developed 3. Have you considered applying your algorithms to realworld applications or scenarios to demonstrate their effectiveness in practical settings 3) Review Summary The paper introduces novel algorithms for multiagent linear stochastic approximation with consensustype interaction over timevarying graphs with stochastic matrices. The theoretical analyses provide insights into the convergence and finitetime performance of these algorithms addressing the challenge of unidirectional communication in distributed reinforcement learning. The technical innovations including the pushbased variant showcase the significance of the proposed approaches. However the lack of practical validations and complexity of the mathematical content may limit the accessibility of the findings. Overall the paper offers valuable contributions to the field of distributed reinforcement learning and motivates further research in this area. Recommendation: Based on the theoretical rigor and contributions to the field I recommend the paper for publication pending minor revisions to enhance clarity and provide more practical insights into the proposed algorithms.", "qPzR-M6HY8x": " 1) Summary of the Paper: The paper introduces a novel approach InstanceConfidence Embedding (ICE) for handling label noise in multiclass classification tasks. The focus is on modeling instancespecific noise information contrary to the commonly used classconditional noise assumption. ICE proposes an efficient approximation of instancedependent noise to capture instancespecific label corruption. The method utilizes a singlescalar confidence parameter and instance embedding to detect ambiguous or mislabeled instances. Experimental validation is conducted on various image and text classification tasks to demonstrate the utility and effectiveness of ICE.  2) Evaluation of the Paper:  Strong Points: 1. Innovative Approach: ICE proposes a novel method to address label noise by considering instancespecific noise information offering a fresh perspective in the field of multiclass classification. 2. Theoretical Framework: The paper provides a clear theoretical background on the instancedependent noise model and explains the proposed ICE method comprehensively. 3. Experimental Validation: Extensive experiments on image and text classification datasets illustrate the effectiveness of ICE in both improving classification performance and identifying mislabeled or ambiguous instances. 4. Practical Applications: The discussion on possible applications of ICE such as data cleansing learning with rejection and active learning adds value and relevance to the proposed method.  Weak Points: 1. Complexity: The method introduces additional complexity due to the instance embedding approach which may raise concerns about scalability and computational efficiency especially for large datasets. 2. Limited Generalization: The paper acknowledges that ICEs instancespecific nature limits its generalization ability to unseen instances which could be a limitation in some practical scenarios. 3. Comparative Analysis: While the paper compares ICE with several baseline methods a more indepth comparative analysis with stateoftheart techniques could further strengthen the evaluation of ICE. 4. Application Scenarios: The paper could elaborate more on specific scenarios where ICE would be beneficial compared to other existing methods to provide a clearer understanding of its advantages.  Questions and Feedback to the Author: 1. Could you provide more insights into the scalability of ICE particularly in terms of computational efficiency and the impact on training time when dealing with large datasets 2. How does ICE compare to stateoftheart methods in handling label noise and are there specific scenarios or datasets where ICE outperforms existing techniques 3. Can you elaborate on potential tradeoffs or limitations of ICE in practical applications considering the instancespecific nature of the method and its impact on generalization to unseen instances 4. Do you have plans to extend the application of ICE to other domains or explore its combination with different training techniques to enhance its effectiveness further  3) Review Summary: The paper presents a novel and innovative method InstanceConfidence Embedding (ICE) for addressing label noise in multiclass classification tasks. ICE offers a unique perspective by focusing on instancespecific noise information demonstrating promising results in improving classification performance and detecting mislabeled instances through experimental validation. While the method shows potential some challenges remain in terms of complexity generalization and comparison with existing techniques. Considering its practical implications and theoretical foundation with some modifications to address the identified weaknesses ICE could be a valuable addition to the field of label noise handling in classification tasks. Hence I recommend the paper for acceptance after minor revisions and further clarification on the points highlighted above.", "fM8VzFD_2-": " Summary: The paper proposes a novel conditional variational autoencoder approach to derive a lowdimensional embedding space from highdimensional functional connectivity data in neuropsychiatric disorders. By incorporating diagnostic information the method aims to reveal nosological relationships among disorders. The study demonstrates the effectiveness of the approach through simulations and empirical applications on datasets containing autism spectrum disorder major depressive disorder and schizophrenia. The results show consistent nosological relations among the disorders.  Strong Points: 1. Innovative Methodology: The proposed conditional VAE approach with dual utilization of diagnostic information is novel and addresses a key challenge in deriving lowdimensional embeddings from highdimensional data. 2. Empirical Validation: The study applies the method to real neuropsychiatric datasets and demonstrates reliable nosological relationships among the disorders validating the effectiveness of the approach. 3. Thorough Analysis: The paper provides detailed explanations of the methodology validation process and results. The theoretical underpinnings of the approach are well elucidated enhancing understanding. 4. Ethical Considerations: The inclusion of ethical considerations regarding the clinical application of the approach demonstrates a thoughtful approach towards potential implications.  Weak Points: 1. Limitations Acknowledged but not Addressed: While the study acknowledges limitations such as the need for validation on independent datasets and interpretability of results these limitations are not fully addressed. 2. Complexity of Methodology: The intricate technical details of the methodology particularly in equations and mathematical derivations may pose challenges for readers not wellversed in the field.  Questions for the Authors: 1. How do you plan to address the limitations identified particularly regarding the interpretability of the derived lowdimensional embeddings and the potential need for validation on independent datasets 2. Can you provide insights into the computational efficiency and scalability of the proposed approach especially when applied to larger datasets or more complex disorders 3. How do you envision the potential integration of findings from this study into clinical practice considering the complexities and ethical considerations of neuropsychiatric diagnoses  Feedback: The paper presents a novel and technically sound approach for deriving nosological relationships among neuropsychiatric disorders using highdimensional functional connectivity data. The thorough validation process and inclusion of ethical considerations enhance the credibility of the study. Addressing the acknowledged limitations and providing more practical implications of the findings would further strengthen the impact of the research. Overall considering the innovative methodology empirical validation and ethical considerations I recommend acceptance of the paper after addressing the identified limitations and providing further insights into the practical implications of the study.", "qwULHx9zld": " Review of \"Spectral Analysis and Random Features Optimization\"  1) Summary: The paper investigates the spectral behavior of random features kernel matrices under a highdimensional setting with the use of a Gaussian mixture model. It proposes a novel random features technique called Ternary Random Feature (TRF) that optimizes computation and storage efficiency by tuning activation functions and random vectors to ternary values. The main contributions include proving the equivalence of the eigenspectrum of the kernel matrix and designing TRFs with computational and storage advantages while maintaining competitive performance compared to existing methods.  2) Strengths:  Innovative Research: The paper addresses an important issue of scalability in kernel methods and proposes a novel solution TRFs which is a significant contribution to the field.  Theoretical Rigor: The paper provides detailed theoretical analysis including theorems corollaries and algorithms supporting the proposed TRF approach.  Experimental Validation: Empirical evidence is provided through experiments on real datasets showcasing the effectiveness of TRFs in terms of computation storage efficiency and performance.  Comparison with StateoftheArt: The comparison with existing methods including LPRFFs and Nystr\u00f6m approximation demonstrates the superiority of TRFs in terms of memory and performance.  3) Weaknesses:  Clarity: The papers technical details might be challenging for nonexperts to understand fully. Improving clarity in explanations of equations and algorithms would help in comprehension.  RealWorld Applicability: While the proposed TRFs show promise in theory and on experimental datasets the realworld applicability and generalization to a wider range of scenarios need further exploration.  Questions and Feedback to the Authors: 1. Reproducibility: Are the code implementations of the proposed TRF approach and experiments available for reproduction to validate the results 2. Comparative Insights: Can you offer more insights into the scenarios where TRF excels compared to existing methods and provide more detailed performance comparisons with a broader range of datasets  4) Review Summary: The paper presents a substantial contribution to the field of machine learning by proposing a novel random features optimization technique TRFs which offers computational and storage efficiency benefits while maintaining competitive performance. The indepth theoretical analysis supported by empirical validations strengthens the credibility of the proposed approach. With some improvements in clarity and further exploration of practical applicability this work has the potential to impact the advancement of scalable machine learning algorithms. Thus based on the strengths of the paper and its significant contributions I recommend the acceptance of this paper for publication.", "ldkunzUzRWj": "1) Summarize the paper: The paper addresses the issue of classimbalance in pairwise ranking models used for recommendation systems. The authors propose a method called VINS (Vital Negative Sampler) to improve training for ranking models. They highlight the imbalance at the edge and vertex levels caused by the disparity between observed positive interactions and unobserved negative interactions. The paper provides theoretical analysis on the impact of classimbalance on model performance and introduces VINS as an efficient negative sampling method to address these challenges. 2) List strong and weak points of the paper Question and Feedback to the author: Strong Points:  The paper thoroughly investigates the classimbalance issue in pairwise ranking models offering theoretical analysis and proposing a novel solution VINS.  The research presents a systematic understanding of the problem and its impact on model performance with clear explanations and insightful findings.  The proposed VINS method demonstrates significant improvements in training efficiency achieving competitive or superior performance compared to stateoftheart methods.  Detailed experimental results and comparisons on real datasets validate the effectiveness and efficiency of VINS highlighting its potential for practical applications. Weak Points:  In the abstract and introduction the significance of the problem and the novelty of the proposed method could be emphasized more explicitly for readers to grasp the importance of the research.  The paper could provide further details on the computational complexity and scalability of the VINS method when dealing with largescale datasets.  While the theoretical analysis is comprehensive more indepth discussion on the limitations or potential drawbacks of the proposed method would add value to the research. Question to the Authors:  Could you elaborate on how the VINS method could be adapted or optimized for different types of recommendation systems or datasets with varying characteristics Feedback to the Authors:  The paper provides a solid foundation and comprehensive analysis of the classimbalance issue in pairwise ranking models. Strengthening the discussion on the practical implications and potential extensions of the VINS method would further enhance the impact of the research.  Addressing the scalability and generalizability of VINS in diverse scenarios would be valuable for future work in the field of recommendation systems and machine learning. 3) Review Summary: The paper offers a detailed investigation of the classimbalance problem in pairwise ranking models for recommendation systems. The proposed VINS method provides an efficient solution to mitigate imbalance issues leading to improved training efficiency and ranking performance. The theoretical analysis experimental results and comparisons with existing methods showcase the effectiveness of VINS in addressing the challenges of classimbalance. Overall the research is wellstructured insightful and presents a promising approach for enhancing pairwise ranking models in recommendation tasks. I recommend the acceptance of the paper with minor revisions to further enhance the clarity and depth of the discussion.", "q2DCMRTvdZ-": "Summary: The paper presents a novel approach to Differentiable Neural Architecture Search (NAS) by framing supernet NAS as a twostage search process which decouples the training of the supernet from the extraction of a final design. The authors propose metrics to evaluate the quality of the supernets shared weights and the learned sampling distribution independently facilitating a robust evaluation of NAS algorithms. They examine the interaction between performance estimation and search strategy in NAS and offer insights into the impact of supernet training and architecture selection techniques. The study includes a case study on modifications to DARTS and evaluates Stage1 and Stage2 search algorithms using NASBench201. Strengths: 1. The paper addresses the critical issue of evaluating NAS algorithms in a systematic and comprehensive manner. 2. The proposed twostage search framework provides a clear and insightful perspective on the interaction between different components of NAS algorithms. 3. The use of metrics to evaluate the performance of the supernets shared weights and architecture selection methods is a valuable contribution to the field. 4. The case study on modifications to DARTS adds practical relevance and demonstrates the application of the proposed evaluation framework. Weaknesses: 1. The paper could benefit from more detailed explanations of the methodology and experimental setup. 2. It would be helpful to provide a clearer comparison with existing NAS evaluation methods to highlight the novelty and advantages of the proposed framework. 3. The lack of indepth discussion on potential limitations or challenges faced during the study may limit the overall understanding of the research. Questions and Feedback for the Author: 1. Could you elaborate on the rationale behind choosing the specific metrics proposed for evaluating the supernets shared weights and architecture selection methods 2. How do you intend to address potential biases or limitations in the proposed evaluation framework in future studies 3. Have you considered the scalability and generalizability of the twostage search approach to different NAS algorithms and architectures Review Summary: The paper presents a novel twostage search framework for evaluating NAS algorithms providing insights into the interaction between supernet training and architecture selection methods. The proposed metrics and evaluation approach offer a systematic and practical way to analyze the performance of NAS algorithms. While the paper contributes valuable insights to the field addressing the limitations identified and providing more detailed methodological explanations would enhance the overall quality of the research. Overall based on the strengths and potential impact of the study I recommend further revisions to enhance the clarity and comprehensiveness of the paper before publication.", "fVu3o-YUGQK": " Summary of the Paper: This paper explores the development of efficient selfsupervised vision transformers (EsViT) for visual representation learning. Two key techniques are discussed: utilizing multistage architectures with sparse selfattentions to reduce modeling complexity and introducing a new pretraining task of region matching to capture finegrained region dependencies. The combination of these techniques in EsViT demonstrates superior performance on ImageNet linear probe evaluation and outperforms previous methods in downstream classification tasks across various datasets.  Strong Points: 1. Comprehensive Study: The paper conducts a thorough empirical study on different techniques and provides insights into the impact of multistage architectures and region matching pretraining tasks. 2. Novel Contributions: The paper introduces a novel pretraining task of region matching and demonstrates how it improves the learned vision representations. 3. StateoftheArt Results: EsViT achieves remarkable results on ImageNet linear probe evaluation and outperforms supervised counterparts on multiple downstream classification tasks. 4. Efficiency and Scalability: EsViT shows better scaling performance with higher throughput and parameter efficiency compared to previous stateoftheart methods. 5. Open Source: The release of code and pretrained models on GitHub enhances reproducibility and allows for further experimentation and research in the community.  Weak Points: 1. Complexity in Region Matching Task: The paper does not delve deeply into the practical challenges or potential limitations in implementing the proposed region matching pretraining task especially in realworld applications.  Questions to Author: 1. Could you further elaborate on the computational overhead and scalability considerations when implementing the region matching pretraining task on different network architectures 2. How do the insights from your experiments using the proposed EsViT translate to realworld applications or practical scenarios especially in the context of handling diverse and uncurated pretraining datasets  Feedback to Author: The paper presents valuable insights and contributions to the field of selfsupervised vision transformers. It would be beneficial to provide more detailed discussions on the practical implications and potential limitations of the proposed region matching pretraining task. Additionally further elaboration on the transferability of the learned representations to broader vision tasks could strengthen the impact of the study. Overall the paper offers significant advancements in the efficiency and effectiveness of selfsupervised vision transformers and sets a promising direction for future research in this area.  Review Summary: This paper presents a comprehensive study on developing efficient selfsupervised vision transformers (EsViT) for visual representation learning. The incorporation of multistage architectures with sparse selfattentions and a novel region matching pretraining task yields impressive results showcasing superior performance on ImageNet and downstream classification tasks. The opensource release of code and pretrained models enhances reproducibility and further research in the field. While the paper offers valuable insights and contributions further elaboration on practical implementation challenges and realworld applicability would strengthen the impact of the proposed techniques. The work demonstrates significant progress in the efficiency and scalability of selfsupervised vision transformers making it a noteworthy contribution to the research community.", "swiyAeGzFhQ": " 1) Summary of the Paper: The paper explores the ArchitectBuilder Problem (ABP) where an architect must guide a builder towards a specific task without explicit rewards or shared meanings. The proposed solution ArchitectBuilder Iterated Guiding (ABIG) leverages two highlevel interaction priors: shared intent and interaction frames. Through iterative modeling and guiding frames ABIG helps the architectbuilder pair develop a communication protocol to successfully solve tasks in a construction gridworld environment.  2) Evaluation:  Strong Points: 1. Innovative Problem Formulation: The ABP formulation is a unique and intriguing approach to study interaction learning without direct rewards. 2. Theoretical Underpinnings: Drawing inspiration from Experimental Semiotics the paper builds on humanhuman communication dynamics to develop AI interaction models. 3. Comprehensive Methodology: ABIGs iterative approach emphasizing selfimitation learning and modeling guiding interactions offers a novel solution to the ABP. 4. Robust Analysis: Detailed analyses metrics definitions and supplementary results provide a deep understanding of the learning dynamics and algorithm performance.  Weak Points: 1. Data Efficiency Concerns: The paper acknowledges the nondata efficiency aspect of ABIG. Exploring approaches to enhance data efficiency could be a valuable addition. 2. Complexity of Factors: Delving into the impact of hyperparameters and algorithmic settings on ABIG\u2019s performance could further enrich the analysis.  Questions and Feedback to the Author: 1. Data Efficiency: Are there considerations for improving data efficiency in ABIG such as leveraging past experiences or transfer learning mechanisms 2. Hyperparameters Sensitivity: Have you explored how different hyperparameter settings impact ABIGs convergence speed and performance 3. Comparison to Human Performance: How does ABIG\u2019s performance in solving tasks compare to humanguided interaction scenarios  3) Review Summary: The paper offers a unique contribution by formulating the ABP and proposing ABIG as a solution showcasing a novel approach to interaction learning without direct rewards. The research is grounded in compelling theoretical foundations and provides thorough insights into the learning mechanisms of ABIG. While the study investigates a promising avenue for autonomous agent interaction future research could address data efficiency concerns and further analyze hyperparameter sensitivities for enhanced algorithm performance.  Recommendation: Based on the innovative problem formulation robust methodology and detailed analyses presented in the paper I recommend its acceptance. The research addresses a significant gap in understanding agentagent interaction and offers valuable insights into emergent communication protocols in AI systems. With minor enhancements in exploring data efficiency and hyperparameters the paper stands as a valuable contribution to the field.", "i3abvoMoeCZ": " Peer Review of \"Characterizing OOD Data with Covariate and Concept Shift for Improved Detection and Calibration\"  1) Summary: The paper proposes a systematic characterization of outofdistribution (OOD) data based on covariate and concept shift. It introduces the concept of near and far OOD detection and derives two score functions (covariate shift score and concept shift score) based on KLdivergence. The authors then propose a geometricallyinspired method Geometric ODIN to improve OOD detection and calibration. The method achieves stateoftheart performance on both indistribution and OOD data.  2) Strong Points:  Strong Points:  Originality:  The paper introduces a novel approach to characterize OOD data using covariate and concept shift which is a unique and insightful contribution to the field.  Methodological Rigor:  The theoretical derivation of the score functions and the geometricallyinspired method are wellexplained and solid.  Experimental Results:  The experimental results showcase the effectiveness of the proposed approach in both OOD detection and calibration demonstrating its stateoftheart performance.  Comparative Analysis:  The comparison with existing methods and the analysis of the performance under varying types of shifts provide a thorough understanding of the proposed methods strengths.  Weak Points:  Clarity and Accessibility:  The paper presents complex theoretical derivations and concepts that may be challenging for readers with less technical background. Simplifying the presentation could enhance accessibility.  Reproducibility:  More details on the implementation hyperparameters and code availability are crucial for reproducibility in the scientific community.  Discussion on Limitations:  It would be beneficial to discuss the limitations of the proposed method potential failure cases and areas for future improvement to provide a wellrounded evaluation.  Questions and Feedback: 1) Can you provide more insights into the scalability and applicability of this method to larger and more complex datasets beyond the CIFAR datasets 2) Considering the potential impact of distribution shift in realworld applications how does the proposed method address robustness to various types of noise and perturbations 3) Could you discuss the computational efficiency and training time of Geometric ODIN compared to existing methods especially considering the additional network layers for sensitivity improvement  3) Review Summary: The paper presents a novel and wellthoughtout approach to characterize OOD data using covariate and concept shifts leading to the development of a robust OOD detection and calibration method. The theoretical derivations score functions and the Geometric ODIN method are methodologically sound and demonstrate stateoftheart performance in experiments. While the paper could improve clarity and address reproducibility concerns the strong conceptual foundation and promising results support recommending this work for publication in the field given the novelty of its contributions and the potential impact on advancing research in OOD detection and calibration.", "ks_uMcTPyW4": "1) Summary: The paper proposes a modelbased reinforcement learning framework to tackle the explorationexploitation problem in partially observable environments with active feature acquisition. The key component of the framework is a novel sequential variational autoencoder that learns highquality representations from partially observed states. The model aims to minimize the cost of acquiring information while maximizing task rewards. Experimental validation is conducted in control and medical simulation domains demonstrating superior performance compared to conventional baselines. 2) Strong Points:  Innovative Approach: The concept of actively learning feature acquisition strategies in a sequential setting is novel and addresses a crucial challenge in reinforcement learning.  Comprehensive Analysis: The paper provides an indepth discussion on the theoretical aspects and practical implications of the proposed framework including related works methodology and experimental results.  Experimental Validation: Rigorous evaluation in both control and medical simulation tasks showcases the effectiveness of the proposed method in outperforming conventional approaches.  Clear and Structured Writing: The paper is wellstructured easy to follow and effectively communicates complex concepts. Weak Points:  Technical Detail: The paper is highly technical assuming familiarity with concepts in reinforcement learning and variational autoencoders which may limit accessibility for readers from nonspecialized backgrounds.  Baseline Comparison: While the proposed method shows superiority over conventional baselines a comparison with stateoftheart methods in the field could provide a more comprehensive evaluation.  Visualization: Including more visual aids such as diagrams or graphs to illustrate key concepts could enhance understanding for readers. Questions and Feedback to the Author: 1. Can you provide more insights into how the proposed framework performs under noisy or uncertain observations in realworld scenarios 2. How scalable is the framework to handle tasks with higher dimensionality or more complex environments 3. Have you considered the computational efficiency of the model in realtime applications especially in medical settings where rapid decisionmaking is critical 3) Review Summary: The paper presents a wellstructured and innovative framework for learning policies in partially observable environments with active feature acquisition. The novel approach of leveraging a sequential variational autoencoder for representation learning demonstrates significant improvements in task performance and cost efficiency compared to traditional methods. Experimental results across control and medical domains validate the efficacy of the proposed framework. However the paper could benefit from further discussion on handling noisy observations scalability to complex tasks and computational efficiency. Addressing these aspects would enhance the applicability and robustness of the method in realworld settings. Overall the paper makes a substantial contribution to the field of reinforcement learning and active feature acquisition warranting publication with minor revisions to address the mentioned points.", "vqGi8Kp0wM": " Summary: The paper presents a novel method for oneshot domain adaptation using a trained GAN in domain A and a single reference image IB from domain B. The method leverages pretrained StyleGAN generator and CLIP model for encoding images as vectors to optimize the weights of the generator to output images in domain B. The contributions include reducing overfitting issues improving visual quality and providing more control over domain gap attributes for various applications like pose adaptation lighting adaptation and texture adaptation. The method involves finetuning the GAN for domain A to adapt to domain B by preserving semantic and visual qualities.  Strengths: 1. Novel Method: The paper introduces a new approach for oneshot domain adaptation that tackles the overfitting problem and improves visual quality showcasing significant contributions to the field. 2. Deep Technical Details: The paper provides thorough technical details including regularizers loss functions and style mixing which are crucial for understanding the proposed method. 3. Comparative Analysis: Detailed comparisons with existing methodologies user studies and ablation studies add credibility to the effectiveness of the proposed method. 4. Applications: The paper highlights practical applications of the method in various domains like image editing which enhances its potential utility.  Weaknesses: 1. Lack of Experimental Diversity: While the paper focuses on faces for evaluation exploring a broader range of image types could provide a more comprehensive validation of the proposed method. 2. Limitations and Future Directions: The paper identifies limitations but could elaborate more on potential future research directions to address these constraints and expand the scope of applicability. 3. Clarity in Presentation: Some technical terms and components might require further clarification for readers who are not deeply familiar with the concepts presented.  Questions: 1. How does the proposed method compare to stateoftheart approaches in terms of computational efficiency and scalability for larger datasets 2. Can you provide insights into the interpretability of the generated images and how the method ensures fidelity to both the source and target domains 3. Have you considered alternative evaluation metrics beyond user studies to quantify the performance of the method objectively  Feedback: 1. Strengthen the experimental section by including datasets other than faces to evaluate the methods robustness across diverse image categories. 2. Expand on the limitations section to discuss potential strategies for overcoming these constraints in future studies. 3. Consider refining the presentation to make complex technical details more accessible to a broader audience while maintaining the depth of analysis. In conclusion the paper presents a promising method for oneshot domain adaptation with notable contributions in reducing overfitting and enhancing visual quality. Additional clarity in presentation diverse experimentation and further exploration of limitations and future research directions will strengthen the overall impact of the work. Overall I recommend considering this paper for publication given its significance in advancing the field of domain adaptation.", "rhOiUS8KQM9": " Summary: The paper introduces an adaptive tree search algorithm BATS as an alternative to beam search for text generation tasks specifically machine translation. BATS is based on Monte Carlo tree search and aims to find highscoring outputs under translation models without making assumptions about the form or structure of the search objective. The paper argues that beam search commonly used in autoregressive models has limitations such as bias towards translations with high initial scores which may obscure modeling issues and limit longterm scientific progress. BATS is designed to address these shortcomings by providing a more unbiased exploration of the search space.  Strong Points:  Innovative Algorithm: The introduction of BATS as an adaptive tree search algorithm is a novel approach to address the limitations of beam search in translation tasks.  Comprehensive Evaluation: The paper provides a thorough experimental evaluation including comparisons with beam search analysis of different translation model objectives and discussions on when to use BATS over beam search.  Theoretical Background: The paper provides a solid theoretical foundation by discussing Monte Carlo tree search and the rationale behind the design of BATS.  Empirical Results: The empirical results demonstrate that BATS can outperform beam search in certain scenarios highlighting its potential utility in improving translation quality in various model classes.  Weak Points:  Complexity: The paper is technical and may be challenging to understand for readers unfamiliar with advanced concepts in machine translation and search algorithms.  Assumption Explanation: The paper assumes a certain level of expertise in the field and could benefit from further explanations to make the content more accessible to a broader audience.  Need for Visualization: Visual representations or examples of BATS execution on sample translation tasks could enhance the understanding of the algorithm for readers.  Questions and Feedback to the Author: 1. How does the BATS algorithm handle the tradeoff between exploration and exploitation in the search space especially in comparison to traditional beam search 2. Could the paper provide more insights into the computational complexity of BATS compared to beam search particularly in highdimensional search spaces 3. Would it be possible to include illustrative examples or case studies to showcase the practical application of BATS in realworld translation tasks  Review Summary: The paper presents an innovative solution BATS as an adaptive tree search algorithm for text generation tasks particularly in machine translation. The experimental results and theoretical foundations provided support the efficacy of BATS in addressing the limitations of beam search and improving translation quality. The comprehensive evaluation and thoughtful discussions on different model classes and translation objectives contribute to the advancement of research in text generation algorithms. Overall based on the strengths of the paper I recommend acceptance after addressing the minor weaknesses highlighted.", "iim-R8xu0TG": " 1. Summary of the Paper The paper introduces FitVid a novel architecture for video prediction that efficiently uses parameters to fit video prediction benchmarks without overfitting. By addressing underfitting through parameter efficiency FitVid outperforms existing stateoftheart models in video prediction across multiple datasets.  2. Strengths and Weaknesses Questions and Feedback to the Author Strong Points:  Innovative Approach: FitVid introduces a novel approach to video prediction by efficiently using parameters to fit benchmarks without suffering from overfitting.  StateoftheArt Performance: FitVid outperforms existing models on multiple video prediction datasets across various metrics showcasing its effectiveness.  Simplicity and Scalability: The simplicity of FitVids architecture makes it not only effective but also scalable and easy to build upon for further improvements.  Data Augmentation: The use of data augmentation to prevent overfitting is a smart and effective strategy that significantly enhances the models performance. Weak Points:  Generalization to New Domains: FitVids inability to perform well in a zeroshot real robot experiment raises questions about its generalization capabilities to entirely new and unseen domains.  Subject Generalization Issue: The observation that FitVid replaces unseen subjects in Human3.6M with training subjects raises concerns about its ability to generalize to different subjects effectively. Questions to the Author: 1. Can you elaborate on the limitations of FitVid in terms of generalizing to entirely new domains outside the training datasets 2. How does FitVid handle scenarios where the subjects in the test data are significantly different from those in the training data Feedback to the Author:  Provide more insights on the models generalization capabilities to diverse and unseen scenarios to strengthen the papers impact and practical utility.  Consider addressing the subject generalization issue in Human3.6M by proposing strategies to improve subject diversity in predictions without compromising performance.  3. Review Summary The paper presents FitVid a novel video prediction model that efficiently uses parameters to fit benchmarks without overfitting. The innovative approach stateoftheart performance simplicity and the use of data augmentation are notable strengths. However the models challenges in generalizing to new domains and issues with subject generalization raise concerns. Addressing these limitations and providing further insights on the models generalization capabilities would enhance the papers contribution to the field of video prediction. Overall the paper makes a significant contribution to video prediction research with its efficient parameter utilization and superior performance on benchmark datasets. However further investigation into generalization challenges and subject diversity issues is recommended to strengthen the models applicability in realworld scenarios.", "oh4TirnfSem": " Summary of the Paper: The paper proposes a novel approach named Particle Filter Graph Neural Networks (PFGNN) to enhance the expressive power of Message passing Graph Neural Networks (GNNs) on graph structured data. The key idea is to guide the learning process with exact isomorphism solver techniques and approximate the generated search tree of colorings by sampling multiple paths. PFGNN is endtoend differentiable efficient and applicable with any GNN as a backbone. Experimental evaluations show that PFGNN outperforms leading GNN models on isomorphism detection tasks and realworld datasets.  Strengths: 1. Innovative Approach: The paper introduces a unique method PFGNN to enhance the expressive power of GNNs by leveraging isomorphism solver techniques and sampling strategies. 2. EndtoEnd Differentiable: PFGNNs endtoend differentiability allows for efficient training and optimization. 3. Experimental Validation: Extensive experimental evaluations on various datasets demonstrate the effectiveness of PFGNN in outperforming existing models on isomorphism detection and graph property tasks. 4. Efficiency: The linear increase in runtime compared to the base GNN model showcases the efficiency of PFGNN in learning more discriminative representations.  Weaknesses: 1. Complexity: The method involves several intricate steps such as sampling individualization and refinement which might make the implementation and understanding challenging for new users. 2. Theoretical Analysis: While the method is empirically validated a more indepth theoretical analysis discussing the soundness and completeness of the approach could enhance the paper.  Questions and Feedback for the Authors: 1. Scalability: How does the performance of PFGNN scale with the size of the graphs and the number of nodes in realworld datasets 2. Sample Efficiency: Have you considered techniques to improve sample efficiency in PFGNN to reduce the number of paths needed for good embeddings with less variance 3. Comparison Metrics: Besides accuracy and efficiency have you considered evaluating the robustness and stability of PFGNN across different datasets and network configurations  Review Summary: The paper presents a novel approach PFGNN to enhance the expressive power of GNNs on graph structured data by leveraging isomorphism solver techniques and sampling strategies. The method is shown to outperform leading GNN models on isomorphism detection tasks and realworld datasets through extensive experimental validations. The innovative approach efficiency in runtime and promising empirical results make PFGNN a valuable contribution to the field. The authors are recommended to address the potential complexity and scalability of the method for broader applicability. Further theoretical insights and considerations for sample efficiency could enhance the robustness of the approach. Overall the paper demonstrates a strong contribution to advancing GNNs in graph representation learning.", "u2JeVfXIQa": " Peer Review  Summary: The paper proposes a novel attention mechanism Adaptive CrossLayer Attention (ACLA) to address the limitations of conventional attention modules in image restoration tasks. The proposed ACLA module allows each query feature to attend to key features from previous layers in a neural network thereby enhancing representation learning. A neural architecture search method is utilized to find optimal insert positions of ACLA modules for improved performance and efficiency in image restoration tasks such as singleimage superresolution image denoising image compression artifacts reduction and image demosaicing. Extensive experiments validate the efficacy of ACLA in enhancing the quality of restored images.  Strengths: 1. Innovative Contribution: The introduction of ACLA addresses the limitations of conventional attention mechanisms by allowing crosslayer attention which has shown improved performance in various image restoration tasks. 2. Efficiency and Effectiveness: The paper demonstrates the effectiveness and efficiency of ACLA through extensive experiments on different image restoration tasks showcasing its capability to enhance representation learning. 3. Comprehensive Experiments: The paper provides detailed experimental results on multiple benchmark datasets comparing ACLA against stateoftheart methods which bolsters the credibility of the proposed approach. 4. Technical Detail and Clarity: The technical details of CLA and ACLA along with the neural architecture search methodology are explained clearly making it easy to understand the proposed mechanisms. 5. Ablation Studies: The ablation studies conducted to analyze the performance variations with different settings such as varying the number of keys in CLA and ACLA provide valuable insights into the effectiveness of the proposed methods.  Weaknesses: 1. Complexity of the Method: The adoption of a neural architecture search method might pose challenges in practical implementation due to computational complexity and resource requirements. It would be beneficial to discuss potential computational limitations and practical considerations. 2. Comparison with Transformerbased Methods: While the paper discusses the comparison with CNNbased methods comparing ACLA with transformerbased methods in terms of performance and computational efficiency could provide a more comprehensive evaluation. 3. Visual Results: Including visual results or qualitative analysis of image restoration output could enhance the paper by providing a more holistic view of the effectiveness of ACLA in comparison to existing methods.  Questions  Feedback to the Authors: 1. Scalability: How scalable is the proposed ACLA method when applied to larger datasets or more complex image restoration tasks 2. Computational Cost: Could you provide more insights into the computational cost and resource requirements of deploying ACLA in practical scenarios especially in comparison to other stateoftheart methods 3. Transferability: Have you tested the transferability of ACLA across different domains within computer vision tasks apart from image restoration specifically  Review Summary: The paper introduces a novel attention mechanism Adaptive CrossLayer Attention (ACLA) addressing limitations in conventional attention modules for image restoration tasks. The proposed ACLA module demonstrates superior efficiency and effectiveness in enhancing representation learning through extensive experiments. The comprehensive evaluation of ACLA against stateoftheart methods and insightful ablation studies strengthen the credibility of the proposed approach. However further discussions on computational complexity comparisons with transformerbased models and practical considerations could enhance the overall impact of the paper. Based on the innovative contribution thorough analysis and strong experimental validation I recommend accepting this paper for publication pending the authors consideration and incorporation of the feedback provided.", "lNreaMZf9X": " Summary of the Paper: The paper explores the impact of various design choices in learning dynamics models for ModelBased Reinforcement Learning. It focuses on comparing the performance of these design choices through systematic evaluations on five different environments from the DeepMind Control Suite. The design choices include stochastic vs. deterministic models multistep vs. 1step losses network ensemble usage and input noise addition. The paper presents both quantitative and qualitative findings providing insights into the planning performance with learned dynamics models.  Strong Points of the Paper: 1. Comprehensive Evaluation: The paper conducts a detailed and systematic comparison of different design choices for learning dynamics models providing valuable insights into their impact on planning performance. 2. Thorough Experimentation: The experimental setup is welldesigned utilizing diverse trajectories and evaluation criteria to assess model performance effectively. 3. Novel Contributions: The paper offers valuable contributions by disambiguating the effects of various design choices on planning performance through ablation studies best practices derivation and qualitative assessments. 4. Clear Presentation: The paper is wellorganized with clear sections results and conclusions making it easy to follow the research methodology and findings. 5. Insightful Conclusions: The paper\u2019s conclusions regarding the efficacy of different design choices and the limitations in highdimensional systems provide important directions for future research in modelbased reinforcement learning.  Weak Points of the Paper: 1. Humanoid System Failure: The inability of the learned models to predict stable trajectories for the humanoid system signifies a limitation in the current approach warranting further investigation into more complex systems. 2. Model Exploitation Concerns: The observation of model exploitation in certain scenarios raises concerns about the robustness of learned models and suggests the need for strategies to mitigate exploitation.  Question to the Authors:  How do you plan to address the challenge of model exploitation observed in scenarios like ball in cup and what are your thoughts on integrating adversarial model learning techniques to improve model robustness  Feedback to the Authors:  Consider elaborating on the implications of model exploitation and how addressing this issue could enhance the practical applicability of learned dynamics models in more complex and interactive environments.  Overall Review Summary: The paper provides a comprehensive analysis of different design choices in learning dynamics models for modelbased reinforcement learning offering valuable insights and best practices. While the experiments are thorough and welldocumented addressing the challenges of model exploitation and extending the study to more complex systems could enhance the impact of the research. Therefore I recommend the paper for publication pending the authors efforts in addressing the noted weaknesses and incorporating suggestions for future research directions.", "z3Tf4kdOE5D": " Peer Review  1) Summary of the Paper: The paper proposes FEDDISCRETE a probabilistic discretization mechanism in federated learning (FL) to defend against availability poisoning attacks integrity backdoor attacks and inference attacks. The mechanism transforms the clients model weights into a vector with two different values ensuring unbiased estimation at the server. The paper provides theoretical analysis on utility robustness and convergence. Empirical evaluations on image classification datasets show superior robustness against weightbased attacks.  2) Feedback on Strong and Weak Points:  Strong Points:  Innovative approach to address various attacks in FL using a probabilistic discretization mechanism.  Theoretical analysis provides insights into utility robustness and convergence of the proposed mechanism.  Empirical verification on popular image classification datasets demonstrates the effectiveness of FEDDISCRETE against weightbased attacks.  Comprehensive evaluation across different attack scenarios enhances the credibility of the proposed approach.  Weak Points:  Lack of comparison with stateoftheart FL defense mechanisms beyond availability poisoning integrity backdoor and inference attacks.  The paper could benefit from a more detailed explanation of the experimental setup and results interpretation.  The discussion on limitations could be expanded to include potential future research directions for addressing these limitations.  Questions and Feedback to the Author:  Question: How does FEDDISCRETE compare in terms of communication efficiency and computational cost with existing defense mechanisms in FL  Feedback: Consider discussing the scalability of FEDDISCRETE in handling a larger number of clients and the potential impact on training efficiency.  3) Review Summary: The paper presents an innovative approach FEDDISCRETE to defend against attacks in FL using a discretization mechanism. The theoretical analysis and empirical evaluations provide strong evidence of the mechanisms effectiveness in mitigating various weightbased attacks. The paper demonstrates a comprehensive understanding of the FL security landscape and addresses key vulnerabilities. However further comparisons with existing defense mechanisms and scalability considerations could enhance the papers impact. Overall I recommend accepting the paper after addressing the mentioned points.", "iulEMLYh1uR": " Summary: The paper delves into the critical aspect of model efficiency in machine learning emphasizing that inference time latency and training costs have direct financial and environmental impacts. It reviews common cost indicators like the number of trainable parameters floatingpoint operations (FLOPs) and speedthroughput discussing their advantages disadvantages and potential contradictions. The paper highlights the efficiency misnomer cautioning against partial reporting of cost indicators leading to incomplete or misleading conclusions about model efficiency. It suggests that no single cost indicator provides a comprehensive view universally applicable to all contexts and proposes improved reporting practices.  Strengths: 1. Comprehensive Analysis: The paper thoroughly examines various cost indicators and their implications shedding light on the complexity and nuances of assessing model efficiency. 2. Novel Contribution: The paper introduces the concept of the efficiency misnomer and provides insightful experiments and examples to illustrate the challenges in comparing different models using singular cost indicators. 3. Relevance: Given the increasing emphasis on model efficiency and the proliferation of machine learning models the topic addressed in the paper is pertinent and timely.  Weaknesses: 1. Clarity: At times the papers content can be dense and technical potentially limiting accessibility for readers unfamiliar with the subject matter. 2. Potential Repetition: Some sections appear to cover similar ground resulting in possible redundancy and dilution of the central message. 3. Lack of Empirical Validation: While the paper discusses experiments and examples additional empirical validation or case studies could strengthen the papers arguments and conclusions.  Question to Author: 1. Could you elaborate on the specific challenges or limitations in implementing the suggested improvements in reporting efficiency metrics How feasible are these recommendations in realworld scenarios 2. Have you considered exploring the impact of emerging technologies or frameworks (e.g. MLOps) on addressing the efficiency misnomer and enhancing model efficiency assessment  Feedback to Author: 1. Consider refining the organization of the paper to enhance readability and facilitate a clearer flow of ideas. 2. Provide more concrete examples or case studies to bolster the theoretical discussions and make the paper more engaging for a broader audience. 3. Strengthen the conclusion section by succinctly summarizing key findings and emphasizing the practical implications of the efficiency misnomer phenomenon.  Review Summary: The paper provides a comprehensive exploration of model efficiency metrics uncovering the limitations of singular cost indicators and emphasizing the need for a holistic approach to assess model efficiency. It offers valuable insights into the challenges of comparing models based on partial cost reporting and highlights the importance of contextspecific considerations in evaluating efficiency. While the papers analysis is intricate and informative addressing potential weaknesses in clarity and empirical validation could enhance its impact and applicability. Overall the paper contributes significantly to the discourse on model efficiency and underscores the complexities in evaluating machine learning models effectively.", "i4qKmHdq6y8": " Review of the Paper: \"Learning on Noisy Datasets: A Selective Approach\"  1) Summary: The paper addresses the challenging issue of learning on datasets with high noisetosignal ratios in domains like Finance or Public Health. The authors introduce a noisy generative process that delineates between structuredinformative and uninformativenoisy data both during training and inference. They propose a selective learning approach where a predictor and a selector are jointly trained to differentiate learnable data from nonlearnable data minimizing a novel selector loss. The paper offers theoretical guarantees and provides an iterative algorithm for empirical evaluation. Experimental results demonstrate the superiority of the proposed method over existing baselines.  2) Strengths and Weaknesses: Strong Points:  Theoretical Contributions: The paper offers valuable theoretical guarantees for the proposed selector loss and empirical minimizer which enhances the credibility of the method.  Innovative Methodology: Introducing a selector to distinguish informative from uninformative data and optimizing the learning process accordingly is innovative and addresses a critical issue in noisy datasets.  Empirical Evaluation: Extensive experiments on synthetic data illustrate the effectiveness of the proposed approach over existing selective learning methods demonstrating its practical applicability.  Clear Presentation: The writing is clear detailed and wellorganized making it easy to follow the complex methodology and theoretical analysis. Weak Points:  Empirical Validation: While the synthetic experiments provide initial validation the paper could benefit from realworld data experiments to further validate the proposed methods robustness and applicability in practical scenarios.  Algorithm Complexity: The iterative algorithm proposed for joint learning of the predictor and selector may be computationally intensive and a discussion on scalability and efficiency would be valuable.  Future Work: The paper alludes to future work on extending the method to handle continuous transitions between informative and uninformative data which could be discussed in more detail to set a clear direction for future research. Questions and Feedback to the Author: 1. What are the implications of the proposed method in realworld applications within domains like Finance or Public Health 2. Can you provide more insights into the tradeoffs between algorithm complexity and performance in the proposed iterative learning approach 3. How robust is the method to varying levels of noise in datasets and are there specific scenarios where it may be less effective 4. Could you elaborate on the practical considerations for implementing the method particularly in terms of computational resources and scalability  3) Review Summary: The paper \"Learning on Noisy Datasets: A Selective Approach\" presents a novel method to differentiate between structured and uninformative data in high noisetosignal ratio datasets leveraging a selector and predictor for improved learning. The theoretical guarantees innovative methodology and empirical validations highlight the methods efficacy. While there are areas for improvement such as realworld data validation and algorithm complexity discussions the paper makes a significant contribution to selective learning in noisy datasets. Overall based on the thorough review provided I recommend acceptance of the paper with minor revisions to address the identified weak points and potentially explore further practical implications.", "g8NJR6fCCl8": "Peer Review 1) Summary of the paper: The paper introduces a novel approach namely Neural GAM (NODEGAM) and Neural GAM (NODEGA2M) which are deep learning models derived from Generalized Additive Models (GAMs) and GA2Ms. The authors address limitations of traditional GAMs by combining the interpretability of GAMs with the scalability and differentiability of deep learning models. NODEGAM and NODEGA2M demonstrate improved performance on large datasets and limited labeled data scenarios while maintaining interpretability. The paper also discusses the methodology model design results and presents shape graphs to interpret model predictions on Bikeshare and MIMIC2 datasets. 2) Evaluation of strong and weak points Questions and Feedback: Strong Points:  The paper addresses a significant gap by proposing interpretable deep learning models derived from GAMs.  The use of selfsupervised pretraining to improve performance in limited labeled data scenarios is an innovative approach.  The comprehensive comparison with other GAMs GA2Ms and full complexity models provides a clear understanding of the models performance.  Interpretability is highlighted through shape graphs on Bikeshare and MIMIC2 datasets enhancing the papers applicability in realworld settings. Weak Points:  While the paper is detailed and technically sound it may benefit from further discussion on the practical implications and realworld applications of NODEGAM and NODEGA2M.  The interpretation of the shape graphs could be strengthened by providing more context and practical insights into how the patterns influence decisionmaking in highrisk domains such as healthcare. Questions and Feedback to the Author: 1. Could you elaborate on the specific use cases or applications where NODEGAM and NODEGA2M could provide significant value particularly in highrisk domains like healthcare 2. How does the proposed model compare to existing stateoftheart interpretable deep learning models in terms of accuracy scalability and interpretability 3. Have you conducted any user studies or practical implementations to validate the interpretability of NODEGAM and NODEGA2M in realworld scenarios 4. Consider expanding the discussion section to highlight potential future research directions or areas where the model could be further optimized or applied. 3) Review Summary: The paper presents a novel and promising approach in the field of interpretable deep learning with the development of NODEGAM and NODEGA2M models derived from GAMs. The use of selfsupervised pretraining along with the demonstrated performance improvements on large datasets and limited labeled data showcase the practical benefits of the proposed models. The comparison with existing models and the detailed interpretation of shape graphs in healthcare datasets enhance the papers credibility. Overall the paper provides an important contribution to the field but could benefit from further discussions on realworld applications and implications. Thus based on the innovative methodology results and potential impact I recommend this paper for publication with minor revisions to address the weak points and provide additional clarity.", "kEvhVb452CC": "Summary: The paper presents a method called Transformed CNN (TCNN) that reparametrizes pretrained convolutional layers as Gated Positional SelfAttention (GPSA) layers. By initializing GPSA layers from pretrained convolutional layers and finetuning for only 50 epochs the resulting TCNNs show significant performance gains and improved robustness over traditional CNNs. The study reveals insights into the relationship between convolutions and selfattention layers. Strengths: 1. Innovative Methodology: The approach of reparametrizing pretrained CNNs into GPSA layers is innovative and showcases a novel way to blend convolutions and selfattention yielding improved performance. 2. Performance Gains: The TCNNs demonstrate substantial performance improvements and enhanced robustness compared to traditional CNNs showcasing the effectiveness of the proposed method. 3. Insightful Analysis: The paper delves into the inner workings of TCNNs explaining how they learn more robust representations by combining convolutional and attentional heads in a complementary manner. 4. Experimental Rigor: The study provides detailed experimental setups comparisons with existing models and thorough analyses making the results reliable and comprehensive. Weaknesses: 1. Complexity: The paper contains dense technical information and complex mathematical formulations which may be challenging for readers without a strong background in the subject or deep learning expertise. 2. Limited Scope: The study mainly focuses on the performance gains of TCNNs in comparison to traditional CNNs leaving room for further exploration of the full potential and application scenarios of the proposed method. 3. Feature Presentation: While the paper discusses the benefits and applications of TCNNs further discussion on potential limitations or challenges in realworld deployment could provide a more balanced perspective. Questions and Feedback to the Author: 1. Scalability: How does the method proposed scale with larger datasets or more complex architectures Are there any considerations for scaling this approach to industrylevel applications 2. Comparative Analysis: Could the paper provide a more detailed comparison with existing stateoftheart approaches in terms of performance computational efficiency and robustness 3. Practical Implications: How feasible is the implementation of TCNNs in realworld applications Are there any practical challenges or tradeoffs to consider when adopting this method Review Summary: The paper presents a novel approach TCNN which reparametrizes pretrained CNNs as GPSA layers demonstrating significant performance gains and enhanced robustness. While the methodology is innovative and results are promising the dense technical content and limited scope of the study could be challenging for general readers. Further discussion on scalability realworld implications and comparative analysis with stateoftheart methods would enhance the papers impact and applicability. Overall the paper offers valuable insights into the interplay between convolutions and selfattention in deep learning architectures.", "keQjAwuC7j-": "1) Summarize the paper: The paper introduces TransDenoiser a framework designed to achieve both certified robustness against adversarial attacks and differential privacy (DP) in models with pretrained classifiers. By leveraging input perturbation transformation into gradient perturbation TransDenoiser aims to improve privacy guarantees while maintaining model performance. The paper presents detailed algorithms mechanisms and experiments to demonstrate the effectiveness of TransDenoiser compared to existing methods. 2) Strong Points:  Innovative Framework: The proposed TransDenoiser framework addresses the dual challenges of certified robustness and DP in pretrained models demonstrating a novel approach to improving model security.  Detailed Analysis: The paper provides thorough explanations of perturbation transformations mechanisms like Multivariate Gaussian Mechanism and training algorithms enhancing the understanding of the proposed methods.  Experimentation: Extensive experiments on benchmark datasets validate the efficacy of TransDenoiser in achieving high levels of certified robustness and DP while outperforming existing stateoftheart methods in terms of accuracy and robustness.  Privacy Preservation: The incorporation of differential privacy mechanisms ensures a provable privacy guarantee on training data offering enhanced protection against privacy leakage through inference attacks. Weak Points:  Complexity: The paper introduces several novel concepts which may be challenging for readers unfamiliar with differential privacy and certified robustness techniques to grasp immediately.  Assumptions: Some analytical assumptions and transformations may need further validation to ensure their applicability in realworld scenarios.  Scalability: The scalability of the proposed framework to largescale datasets and complex models could be a potential concern that needs further exploration. Questions and Feedback to the Author:  Can you provide more insights into the computational overhead introduced by TransDenoiser compared to existing methods  How robust is the proposed framework to different types of attacks beyond those considered in the experiments  Are there any specific realworld applications or use cases where TransDenoiser has the potential to offer significant advantages over traditional methods  Have you considered the impact of hyperparameters selection on the performance and privacy guarantees of TransDenoiser 3) Review Summary: The paper introduces a novel framework TransDenoiser to simultaneously achieve certified robustness and differential privacy in models with pretrained classifiers. The detailed analysis mechanisms like Multivariate Gaussian Mechanism and extensive experiments on benchmark datasets demonstrate the effectiveness of TransDenoiser in improving model security and privacy while maintaining performance. The innovative approach thorough explanations and superior performance showcased in the experiments support a strong recommendation for the acceptance of this work. Further validation scalability considerations and exploration of realworld applications could enhance the impact and relevance of TransDenoiser in practical settings.", "rGg-Qcyplgq": " Summary of the Paper: The paper introduces a novel approach in distributional reinforcement learning that focuses on exploring with a randomized risk criterion to reach a riskneutral optimal policy. The proposed method Perturbed Quantile Regression (PQR) perturbs the criterion on uncertainty by sampling risk measures from an ambiguity set. This perturbation allows for efficient exploration that avoids being overly pessimistic. The paper provides theoretical results supporting the convergence and optimality of the method and demonstrates its performance superiority over existing distributionbased algorithms in various environments including Atari games.  Strengths: 1. Novelty: The paper addresses a significant challenge in exploration for distributional reinforcement learning by introducing a randomized risk criterion. This novel approach sets it apart from existing methods. 2. Theoretical Foundation: The paper provides detailed theoretical analysis including convergence guarantees and optimization objectives supporting the proposed Perturbed Quantile Regression method. 3. Experimental Validation: The paper conducts thorough experiments on multiple environments including a toy environment LunarLanderv2 and Atari games demonstrating the effectiveness of the proposed method in achieving faster convergence and superior performance compared to existing algorithms. 4. Clarity and Structure: The paper is wellstructured and presents complex concepts in a clear and understandable manner. The methodology experimental setups and results are welldescribed aiding in easy comprehension.  Weaknesses: 1. Assumptions and Generalization: The paper could further elaborate on the assumptions made in the theoretical analysis and discuss the generalizability of the proposed method beyond the specific environments tested. 2. Comparison Metrics: While the experimental results showcase the superiority of PQR over existing methods the paper could benefit from a more indepth comparison of performance metrics and statistical significance tests to strengthen the empirical validation. 3. Implementation Details: Providing more detailed implementation specifics including hyperparameters training configurations and computational complexity would enhance the reproducibility and understanding of the method.  Questions and Feedback to the Author: 1. Could you provide insights into how the proposed Perturbed Quantile Regression method adjusts to highly complex environments with significant intrinsic uncertainties beyond the tested environments 2. Did you consider exploring the impact of different hyperparameters or variations in the ambiguity set construction on the performance of PQR to evaluate its robustness and sensitivity to these factors 3. It would be beneficial to discuss potential realworld applications or scenarios where the randomized risk criterion approach could offer significant advantages in practical reinforcement learning tasks.  Review Summary: The paper introduces a promising exploration method Perturbed Quantile Regression in distributional reinforcement learning showcasing theoretical guarantees and empirical success. Despite strengths in novelty theoretical foundations and experimental validation addressing assumptions enhancing comparison metrics and providing detailed implementation insights would further strengthen the paper. Overall the method shows promise in overcoming exploration challenges warranting further exploration and validation in diverse and complex environments.", "hC474P6AqN-": " Summary: The paper proposes a multitype continuous disentanglement variational autoencoder to address the compatibility issues arising from different labelling systems used in categorical annotations in machine learning datasets. The model aims to identify and disentangle the true generative factors that determine each categorical label enabling the merging of datasets with different categorical models into a unified latent space. Experiments on synthetic datasets show a strong correlation between the disentangled latent values and the true generative factors. The approach shows promise for enhancing explainability in neural network architectures by explicitly decomposing each label into its generative factors.  Strong Points: 1. Innovative Approach: The paper addresses a significant challenge in machine learning datasets related to categorical annotations by proposing a novel multitype continuous disentanglement variational autoencoder. 2. Experimental Validations: The experiments conducted on synthetic datasets demonstrate a strong correlation between the disentangled latent values and the true generative factors showcasing the effectiveness of the proposed model. 3. Clear Explanations: The paper provides clear explanations of the proposed architecture highlighting the modifications made to previous models and the methodology used for disentanglement.  Weak Points: 1. Limited Realworld Datasets: The experiments are conducted on synthetic datasets which may not fully reflect the complexity and diversity of realworld datasets limiting the generalizability of the proposed model. 2. Lack of Comparative Analysis: While the paper discusses the limitations of previous works and presents their methods advantages a more comprehensive comparative analysis with other stateoftheart models would help in showcasing the uniqueness and superiority of their approach. 3. Need for Realworld Validation: The models performance on realworld datasets relevant to emotion recognition or other practical applications should be tested to evaluate its efficacy in handling more complex and diverse data.  Question to the Authors: 1. Have you considered testing the proposed model on realworld datasets in domains such as emotion recognition to validate its effectiveness in handling more complex and varied data 2. How do you plan to address scalability issues when applying the model to largescale datasets with high dimensionality or varied data distributions  Feedback to the Authors: 1. Realworld Validation: Conduct experiments on realworld datasets related to emotion recognition or other domains to validate the models performance and generalizability. 2. Comparative Analysis: Include a more detailed comparative analysis with existing stateoftheart methods to highlight the advantages and uniqueness of the proposed model. 3. Scalability Considerations: Discuss potential scalability issues and strategies to address them when applying the model to larger and more diverse datasets.  Review Summary: The paper presents an innovative approach to address compatibility issues in machine learning datasets with different categorical annotations. While the model shows promise based on experiments on synthetic datasets further validation on realworld datasets and a more indepth comparative analysis with existing methods are necessary to establish the models effectiveness and superiority. Addressing scalability concerns and providing insights into realworld applicability will enhance the practical value of the proposed framework.", "oLYTo-pL0Be": "Summary of the Paper: The paper introduces a novel approach to federated learning in the healthcare domain to train machine learning models using patient data from multiple hospital data centers while ensuring patient privacy is maintained. The proposed approach involves a studentteacher algorithm with a scheduler that manages training by selecting data from different data centers. By utilizing metagradients and reinforcement learning the scheduler directs the teacher to choose appropriate data for training the student model. The method aims to address challenges in federated learning such as diverse dataset distributions and potential data corruption at individual nodes. Strengths: 1. Innovative Approach: The paper introduces a novel methodology combining studentteacher learning and federated learning to train machine learning models on diverse datasets without compromising patient privacy. 2. Robustness to Attacks: The method is designed to defend against data poisoning and adversarial attacks in federated learning setups showing promising results in comparison to stateoftheart methods. 3. Performance Results: The results demonstrate stateoftheart performance in terms of final model performance compared to standard classifiers and federated learning methods. 4. RealWorld Application: The paper presents the potential application of the proposed method in a live hospital environment showcasing the adaptability and reliability of the system. Weaknesses: 1. Lack of Empirical Analysis: While the paper extensively discusses the methodology more detailed experimental results and comparisons with existing techniques could add depth to the evaluation. 2. Complexity: The approach involving multiple agents (student teacher scheduler) may introduce complexity in implementation and training which could require significant computational resources and expertise. 3. Practical Implications: The need for centralised control and continuous communication between nodes may limit the scalability and practicality of the proposed approach in certain contexts such as mobile learning environments. Questions and Feedback to the Author: 1. How does the proposed method handle potential biases in the data collected from different hospital data centers Have you considered methods to mitigate bias during training 2. Can you provide more insights into the scalability of the system and the computational resources required for training the studentteacher setup in a realworld scenario 3. How does the scheduler prioritize data selection in situations where data characteristics or quality vary significantly across different hospitals Review Summary: The paper presents an innovative approach that combines federated learning with studentteacher learning to address privacy concerns and data quality issues in training machine learning models using healthcare data from multiple data centers. The method demonstrates promising results in terms of robustness to attacks and model performance. However concerns about complexity scalability and practical implementation need to be further addressed. Overall the paper provides valuable insights into the intersection of federated learning and healthcare applications warranting further research and empirical validation.", "v-f7ifhKYps": "Peer Review:  1) Summary of the Paper: The paper introduces a novel methodology called Maximum Entropy Populationbased training (MEP) for training Reinforcement Learning (RL) agents to collaborate effectively with unseen human partners in a zeroshot setting. The key objective is to train a diverse population of RL agents using entropy maximization and centralized population entropy. The proposed method is evaluated in the Overcooked game environment and online experiments with real humans. The results demonstrate that MEP outperforms baseline methods such as selfplay PPO and PopulationBased Training (PBT) in terms of humanAI coordination. 2) Strengths and Weaknesses: Strengths:  Innovation: The paper introduces a novel approach MEP that addresses the challenging problem of training RL agents to collaborate effectively with unseen human partners.  Theoretical Foundation: The methodology is wellgrounded in maximum entropy RL and diversity concepts providing a strong theoretical foundation for the proposed approach.  Experimental Validation: The methodology is rigorously evaluated through experiments in both simulated environments and realworld scenarios demonstrating its effectiveness compared to existing methods.  Comprehensive Explanations: The paper provides detailed explanations of the theoretical framework algorithms and experimental procedures enhancing the understanding of the proposed method. Weaknesses:  Complexity: The paper involves complex theoretical concepts and algorithms which may be challenging for readers with limited background knowledge in RL and related fields.  Lack of Visualization: While the paper mentions video clips for demonstrations integrating more visual aids such as figures or diagrams could enhance the clarity of the presented concepts and results.  HumanAI Coordination Evaluation: Although the paper presents promising results in humanAI coordination further analysis on the generalizability of the MEPtrained agents to a wider range of human behaviors could strengthen the conclusions. Questions and Feedback to the Author: 1. Question: Can you elaborate on the computational efficiency of the proposed MEP framework compared to existing methods particularly in terms of training time and resource requirements 2. Feedback: Including more visualization aids such as figures or diagrams illustrating key concepts and experimental setups would greatly enhance the readability and understanding of the paper for a broader audience. 3. Question: How sensitive is the performance of MEP to hyperparameters such as the entropy weight \u03b1 and the prioritized sampling parameter \u03b2 Have you conducted sensitivity analyses to explore their impact on the results 4. Feedback: Providing deeper insights into the interpretability of the learned policies and how the MEPtrained agents adapt to diverse human strategies would offer valuable insights for the readers. 3) Review Summary: The paper presents a novel methodology Maximum Entropy Populationbased training (MEP) for training RL agents to collaborate effectively with unseen human partners. The proposed approach is theoretically sound innovative and demonstrates superior performance in comparison to baseline methods in both simulated and realworld scenarios. While the paper is comprehensive in its explanations and experimental validation further discussions on computational efficiency sensitivity analysis of hyperparameters and enhanced visualization aids could improve the clarity and applicability of the proposed method. Overall the paper provides valuable contributions to the field of humanAI coordination and reinforcement learning.  Note: This peerreview provides a detailed analysis of the strengths weaknesses questions feedback and an overall evaluation of the paper aiming to provide constructive feedback for the authors. Feel free to incorporate any additional points or details based on the specific context of the paper.", "fUhxuop_Q1r": " Summary The paper presents a rigorous evaluation of generalization in Reinforcement Learning (RL) by proposing a method that combines a tabular environment with observations from a supervised learning dataset. The study investigates generalization of Deep QNetwork (DQN) and Quantile RegressionDQN (QRDQN) in state observation and action spaces across various gridworld environments combined with the MNIST dataset. The results indicate differences in generalization capabilities between DQN and QRDQN highlighting the importance of evaluating generalization in RL.  Strengths  The paper provides a clear problem formulation using Contextual Decision Processes (CDPs) and supervised learning datasets to disentangle generalization across states observations and actions in RL.  Rigorous experiments are conducted on simple gridworld environments combined with the MNIST dataset to investigate generalization capabilities of DQN and QRDQN.  The paper brings new insights by evaluating different mechanisms such as dropout and regularization on generalization in RL providing valuable findings in both online and offline learning settings.  The discussion on observations states and actions generalization in RL and the comparison between DQN and QRDQN offer valuable insights for future research in this area.  The demonstration of the limitations of current evaluation protocols and the proposed improvements in measuring generalization in RL are valuable contributions to the field.  Weaknesses  The paper could benefit from a more structured presentation of results potentially using tables or figures to summarize key findings more clearly.  Descriptions of the experiment setups and methodologies could be more detailed to enhance reproducibility and understanding.  The paper would benefit from a more indepth discussion of the implications of the findings for practical RL applications and potential areas for future research.  Questions 1. Have you considered exploring other regularization techniques beyond L2 regularization and dropout to further enhance generalization in RL 2. How do you plan to extend this research to more complex environments or realworld applications to validate the findings further 3. Can you elaborate on the computational efficiency implications of the proposed method for evaluating generalization in RL compared to traditional approaches  Feedback The paper offers valuable insights into evaluating generalization in RL using a combination of tabular environments and supervised learning datasets. The detailed experiments and comparisons between DQN and QRDQN provide a solid foundation for future research in this area. Strengthening the presentation of results providing additional experimental details and discussing the broader implications of the findings could further enhance the impact of this work. Additionally considering further exploration of regularization methods and extending the research to more complex environments would be beneficial for advancing the field of RL.", "rX3rZYP8zZF": " Summary The paper presents a novel approach named \"CareGraph\" which leverages a knowledge graph and deep neural networks to provide personalized health nudges for users managing chronic diseases on a digital health platform. The study addresses the challenge of coldstart problems in recommendation systems by proposing a method that integrates user attributes and knowledge graph embeddings. Multiple experiments are conducted to evaluate the models performance showing improvements in clickthrough rate prediction and handling coldstart situations.  Strengths 1. Innovative Approach: The paper introduces a unique approach combining knowledge graphs and deep neural networks for personalized health nudges addressing coldstart problems. 2. Thorough Literature Review: The paper provides a comprehensive background on recommender systems in digital health and knowledge graph embeddings establishing the research context effectively. 3. Experimental Evaluation: The study includes extensive experiments to compare the proposed model with baseline methods evaluating performance in both general and coldstart scenarios. 4. Clear Methodology: The methodology is welldefined explaining the construction of the knowledge graph user and nudge embeddings and model prediction steps in a detailed manner. 5. Results Interpretation: The paper not only presents the results but also provides insightful analysis on the implications of the findings especially in relation to coldstart scenarios.  Weaknesses 1. Lack of RealWorld Deployment: While the experimental results are promising there is no discussion on realworld deployment challenges or insights from user feedback. 2. Text Embedding Integration: The paper briefly mentions the integration of text embeddings but does not delve into the rationale behind the results showing no improvement in AUC with textenabled treatments. 3. Data Description: More information on the characteristics of the dataset such as diversity and distribution of user profiles and nudge attributes could enhance the understanding of the results.  Questions to the Author 1. How does the proposed model handle scalability issues when deployed in a realworld setting with a large number of users and a continuously growing content library 2. Can you elaborate on the reasons why the text embedding integration did not lead to performance improvements and discuss potential pathways for optimizing this aspect in future studies 3. How do you plan to assess the models interpretability and ensure that the recommendations align with user expectations especially in a sensitive domain like healthcare 4. What are the key challenges you foresee in transitioning this research into practical applications within digital health platforms and how do you plan to address them  Feedback The paper provides a strong foundation for leveraging knowledge graph embeddings in personalized health recommendation systems. To enhance the study further considerations for realworld implementation challenges deeper analysis of text embedding integration and addressing usercentric concerns would add valuable insights. Overall the manuscript showcases a promising approach that bridges advanced techniques with practical healthcare applications. With some refinements and addressing the identified areas the study could significantly contribute to the field of personalized digital health management.", "psQ6wcNXjS1": " Summary: The paper presents novel strategies for learning an EnergyBased Model (EBM) with varying lengths of Markov Chain Monte Carlo (MCMC) sampling trajectories. The experiments cover three different trajectory magnitudes and learning outcomes: shortrun sampling for image generation midrun sampling for adversarial defense and longrun sampling for principled modeling of image probability densities. The authors introduce three innovative methods of MCMC initialization for negative samples used in Maximum Likelihood (ML) learning. With standard network architectures and an unaltered ML objective the MCMC initialization methods alone enable significant performance improvements across the investigated applications achieving stateoftheart results for image synthesis adversarial defense and density modeling on datasets like CIFAR10 and ImageNet.  Strengths: 1. Comprehensive Experimentation: The paper covers a wide range of applications and provides experimental evidence to support the effectiveness of the proposed methods in different scenarios. 2. Innovative MCMC Initialization: The introduction of novel methods for MCMC initialization tailored to different trajectory lengths is a significant contribution enabling improved performance across different tasks. 3. StateoftheArt Results: The results including surpassing stateoftheart FID scores for unnormalized image densities and achieving top performance in adversarial defense showcase the effectiveness of the proposed techniques.  Weaknesses: 1. Complexity of Concepts: The paper contains dense technical content and may be challenging for readers less familiar with the subject matter to grasp the details and implications of the proposed methods fully. Efforts could be made to simplify the presentation for a broader audience.  Questions and Feedback: 1. Clarification on Implementation: It would be beneficial to provide more details on the implementation aspects of the proposed methods such as code availability hyperparameters used and computational resources required to facilitate reproducibility and further experimentation by the research community. 2. Comparison with Existing Methods: It would be insightful to include a more detailed comparison with existing MCMC initialization methods and discuss the advantages and limitations of the proposed approaches in relation to prior work. 3. Further Insights into Limitations: Exploring the limitations and potential challenges faced in realworld scenarios can enhance the practical relevance of the research. Providing insights into potential failure modes or edge cases can offer a more holistic view of the proposed techniques.  Review Summary: The paper presents innovative strategies for learning EBMs with different lengths of MCMC sampling trajectories showcasing significant performance gains in image synthesis adversarial defense and density modeling tasks. The experimental results demonstrate stateoftheart performance across various benchmarks. The proposed MCMC initialization methods are effective in improving model stability and quality. However the paper could benefit from clearer explanations and comparisons with existing methods. Overall the research introduces valuable contributions to the field and warrants further investigation and potential adoption in future generative modeling and defense applications.", "lycl1GD7fVP": "Summary: The paper proposes a new theory predicting the generalization of kernel regression by shedding light on the learning behavior of wide neural networks. The main contributions include proving a conservation law for the inductive bias of kernel regression deriving a theory of generalization predicting first and secondorder statistics of the learned function and proving a new \"nofreelunch\" theorem for kernel regression which highlights a fundamental tradeoff in inductive bias. The paper also presents experimental verification of the theorys predictions on synthetic datasets and wide finite networks. Strengths: 1. Innovative Theory: The paper introduces a novel theory predicting generalization performance in kernel regression offering insights into wide neural networks. 2. Comprehensive Analysis: The authors derive expressions for various statistics of learned functions providing a detailed understanding of the learning behavior. 3. Empirical Validation: Experimental results confirm theoretical predictions on synthetic datasets and wide finite networks demonstrating the theorys accuracy. Weaknesses: 1. Complexity: The paper contains intricate mathematical derivations and technical language that may pose challenges for readers without a strong background in machine learning theory. 2. Limited Application: The theory is primarily focused on kernel regression and may not directly address broader issues in deep learning models beyond the NTK regime. 3. Lack of RealWorld Examples: The experiments are conducted on synthetic datasets and the practical implications for realworld applications may require further exploration. Questions for the Authors: 1. How do you envision extending this theory to more practical scenarios with complex data sets and varied model architectures beyond those considered in this work 2. Can the proposed theory be applied to other machine learning models beyond kernel regression and wide neural networks If so what adaptations or modifications would be necessary 3. How might the insights gained from this research impact the design and optimization of neural networks for realworld applications particularly in scenarios with limited data availability Feedback: 1. Consider providing additional insights or examples to illustrate how the theory could be applied in realworld settings to enhance the papers practical relevance. 2. Aiming for a more accessible presentation of the mathematical derivations could improve the papers readability for a broader audience interested in machine learning theory. 3. Discussing the scalability and generalizability of the theory to more diverse data types and model architectures could strengthen the papers impact and significance. Review Summary: The paper presents an innovative theory predicting generalization behavior in kernel regression offering valuable insights into the learning behavior of wide neural networks. While the paper demonstrates strong analytical rigor and empirical validation there are areas for improvement in terms of readability and practical applicability. Overall the paper contributes significantly to the understanding of inductive bias in machine learning models warranting further exploration and potential realworld applications.", "xbx7Hxjbd79": " Summary The paper analyzes the Learning with OpponentLearning Awareness (LOLA) algorithm and its limitations particularly its inconsistency due to assuming opponents as naive learners. The study introduces highorder LOLA (HOLA) and Competitive Gradient Descent (CGD) and showcases their inconsistencies and lack of recovery of highorder LOLA. To address these issues the authors propose a new method called Consistent LOLA (COLA) which aims to learn update functions that are consistent under mutual opponent shaping. The paper provides theoretical analysis proof and experimental comparisons on various games to evaluate the performance and consistency of LOLA HOLA CGD and COLA.  Strengths 1. Thorough Analysis: The paper provides a comprehensive analysis of LOLA HOLA CGD and introduces a new method COLA while addressing the inconsistencies in existing algorithms. 2. Theoretical Grounding: The authors offer detailed theoretical underpinnings with formal definitions propositions and proofs to demonstrate the problems and solutions presented in the paper. 3. Experimental Validation: The study conducts empirical experiments on different games to compare the performance and consistency of the proposed COLA method against existing algorithms providing concrete evidence to support their claims.  Weaknesses 1. Complexity: The paper delves into intricate technical details and formal proofs making it challenging for readers without a strong background in the subject to follow the content easily. 2. Lack of Intuitive Explanations: The concepts and results presented could benefit from more intuitive explanations to aid comprehension for readers with varied expertise levels. 3. Limited RealWorld Applications: The paper focuses heavily on theoretical and algorithmic aspects without discussing practical applications or realworld implications of the proposed methods.  Questions 1. How does the proposed Consistent LOLA (COLA) approach compare to other stateoftheart methods beyond LOLA HOLA and CGD in terms of convergence properties and robustness 2. Are there any considerations for scaling the COLA method to more complex settings such as larger games multiagent systems or highdimensional spaces 3. Could you provide more insights into the computational complexity and efficiency of the COLA method compared to existing algorithms like LOLA and HOLA  Feedback 1. Consider adding more practical examples or case studies to better illustrate the implications and benefits of the proposed COLA method in realworld scenarios. 2. Address the readability and accessibility of the paper by explaining complex concepts in simpler terms to enhance understanding for a broader audience. 3. Extend the discussion on future research directions potential applications and implications of the proposed COLA method to broaden the impact of the study. Overall the paper presents a solid groundwork in addressing the consistency issues in existing opponentlearning algorithms and introduces a novel approach in COLA. It provides strong theoretical foundations and empirical results to support the efficacy of the proposed method but could benefit from further clarity in explanations and expanding discussions on practical applications and scalability.", "uPv9Y3gmAI5": " Summary: The paper introduces the concept of FisherWeighted Singular Value Decomposition (FWSVD) as a novel approach to address the issue of mismatched optimization objectives in model compression. It highlights that standard Singular Value Decomposition (SVD) minimizes reconstruction error without considering the importance of parameters for task accuracy. FWSVD incorporates Fisher information to weigh parameter importance resulting in improved task accuracy compared to traditional SVD. The methodology is applied and evaluated on transformerbased language models across various natural language tasks demonstrating superior model compression without sacrificing performance.  Strengths: 1. Novelty and Contribution: The paper addresses an important issue in model compression by introducing a novel approach FWSVD which provides a fresh perspective and significant improvements over traditional SVD. 2. Empirical Analysis: The paper provides extensive empirical evidence to support the proposed solution showcasing the effectiveness of FWSVD in compressing models while maintaining task accuracy across diverse language tasks. 3. Experimental Rigor: The experiments are welldesigned and thorough comparing FWSVD with SVD across different compression paths and tasks. The results highlight the superiority of FWSVD in retaining model performance. 4. Clear Explanation: The paper provides a detailed explanation of the proposed method the underlying problem and the experimental setup making it accessible for readers to understand the concepts and findings.  Weaknesses: 1. Limitation in Applicability: The authors acknowledge that FWSVD is more suitable for compressing taskspecific models compared to pretrained generic models limiting its generalizability. Future work could focus on improving applicability to various model types. 2. Simplification of Importance Matrix: The paper mentions a limitation of using a simplified importance matrix that assigns equal importance to parameters in the same row which might not fully utilize the Fisher information. Further refinement in this aspect could enhance the methods performance.  Questions to the Authors: 1. How sensitive is the performance of FWSVD to variations in task complexity or dataset characteristics 2. Have you considered exploring FWSVDs performance on other types of neural network architectures beyond transformerbased models 3. How does the computational efficiency of FWSVD compare to traditional SVD especially in largescale model compression tasks  Feedback to the Authors: The paper is wellstructured clearly explaining the problem statement methodology experiments and results. Strengthening the discussion on the limitations and future directions as well as addressing the questions raised above could enhance the papers impact. Overall the novel concept of FWSVD and the promising experimental results make this work a valuable contribution to the field of model compression.  Recommendation: I recommend accepting this paper for publication based on its substantial contributions welldesigned experiments and clear presentation of findings. The novel approach of FisherWeighted SVD addresses a significant problem in model compression and demonstrates superior performance compared to traditional methods. The empirical evidence provided supports the efficacy of FWSVD in maintaining task accuracy while achieving efficient model compression. Further clarifications on the limitations and future work could strengthen the papers impact and relevance in the research community.", "mKsMcL8FfsV": " Summary: The paper introduces a novel method for selfsupervised model ensembling aiming to maximize representation quality. The proposed framework involves learning representations directly through gradient descent at inference time which has shown improvements in knearest neighbors performance in both indomain and transfer settings. The approach is demonstrated on various datasets using ensembles of selfsupervised models like SimCLR SwAV Barlow Twins PIRL and RotNet showcasing superior performance over baseline methods.  Strengths: 1. Innovative Approach: The paper introduces a unique method for selfsupervised model ensembling through gradient descent showing significant improvements in representation quality. 2. Comprehensive Experiments: The paper includes thorough experiments on different datasets with varied ensembles of selfsupervised models validating the effectiveness of the proposed method. 3. Detailed Description: The paper provides detailed pseudocode and explains the methodology clearly making it easy to understand and replicate. 4. Comparison with Baselines: The comparison with baselines like concatenation and averaging provides a clear understanding of the proposed methods advantages.  Weaknesses: 1. Evaluation Limitation: The paper heavily relies on knearest neighbor evaluation which may not fully capture the models performance in all scenarios. The method could benefit from additional evaluation metrics for a more comprehensive assessment. 2. Lack of Theoretical Analysis: Although the paper presents empirical results a more indepth theoretical analysis explaining the underlying principles of the proposed method could strengthen the findings. 3. Generalization and Robustness: While the method shows promising results generalization to various models and datasets may need further investigation to ensure robust performance in diverse settings. 4. Writing Clarity: Some sections appear dense and might benefit from improved clarity in language and structure to enhance readability.  Questions  Feedback: 1. How does the proposed method perform in scenarios with noisy or limited data where the models may produce suboptimal embeddings Are there strategies to address such challenges 2. Could you provide insights into potential extensions or modifications of the method for handling more complex tasks or incorporating additional information during training 3. Have you explored the impact of varying hyperparameters in the model or the ensembling strategy for different datasets to assess robustness across different settings 4. It would be beneficial to discuss potential realworld applications or practical implications of the proposed method highlighting its advantages in practical implementations.  Review Summary: Overall the paper presents an innovative approach for selfsupervised model ensembling showing improvements in representation quality through gradient descent learning at inference time. While the methodology is welldescribed and the experiments are comprehensive the paper would benefit from additional theoretical analysis and exploration of the methods generalization capabilities. Addressing these aspects and considering the feedback provided can further enhance the papers contribution to the field of selfsupervised learning and model ensembling. Therefore based on the presented strengths and weaknesses I recommend considering revisions and additional experiments to strengthen the papers impact and theoretical underpinning.", "fuaHYhuYIDm": "Summary of the Paper: The paper introduces MAGNEx a global modelagnostic algorithm that utilizes neuralnetwork based explainers to produce rationales for any blackbox decision model. The algorithm aims to address the lack of interpretability in modern AI systems by providing explanations for decisions made by blackbox models. MAGNEx creates featurebased explanations using a neural network explainer operating in a global rather than local manner which leads to more faithful explanations and improved computational efficiency during inference. The paper presents various experiments across different tasks and modalities to showcase the effectiveness of MAGNEx compared to popular explainability algorithms like LIME and Integrated Gradients. Strengths: 1. Innovative Approach: MAGNEx introduces a novel global algorithm that provides explanations for blackbox decision models addressing the critical need for interpretability in AI systems. 2. ModelAgnostic: The algorithm is designed to be applicable to a wide range of models and domains ensuring its versatility and generalizability. 3. Superior Performance: The experiments conducted demonstrate that MAGNEx outperforms existing explainability algorithms in terms of explanation quality and computational efficiency. 4. Rigorous Methodology: The paper provides a comprehensive explanation of the methodology used including the formulation learning process and evaluation metrics enhancing the reproducibility of the results. 5. Implications for Hindsight Bias: The global nature of MAGNEx is highlighted as a key advantage that may mitigate the hindsight bias issue observed in local perturbationbased methods. Weaknesses: 1. Complexity: The technical details and mathematical formulations might be challenging for readers unfamiliar with advanced machine learning concepts potentially limiting the accessibility of the paper. 2. Evaluation: While the paper compares MAGNEx against popular algorithms human evaluation for the quality of explanations from a users perspective could further strengthen the study. 3. Scalability: Further exploration on the scalability of MAGNEx to larger and more complex models or datasets could enhance the understanding of its practical utility in realworld applications. Questions for the Authors: 1. How does MAGNEx handle scenarios where complex features interact in nonlinear ways within the input data 2. Can MAGNEx be applied to realworld applications with extremely large feature spaces Have such scenarios been explored in the experiments 3. How does the algorithm perform in scenarios where the interpretability of decisions needs to be explained to nontechnical users or stakeholders 4. Are there plans to opensource the implementation of MAGNEx for broader adoption and reproducibility in the research community Feedback: The paper demonstrates a significant contribution to the field of explainable AI by introducing a novel modelagnostic algorithm that offers more faithful explanations and improved computational efficiency. To enhance the paper further consider providing more insights into the practical implications of MAGNEx in realworld applications and potentially addressing the implications of its global approach on diverse datasets and models. Review Summary: Overall the paper presents a wellexecuted study introducing MAGNEx a promising algorithm for explainability in AI models. With its innovative approach strong experimental results and potential to mitigate hindsight bias MAGNEx is recommended for publication pending minor revisions in readability scalability considerations and further exploration of practical implications.", "rFJWoYoxrDB": " Summary: The paper conducts an indepth analysis of popular cellbased search spaces in Neural Architecture Search (NAS) focusing on identifying redundant complexities and exploring the performance impact of various architectural features. The authors analyze highperforming architectures discovered by diverse search methods emphasizing the key features that drive architecture performance. Through operationlevel analysis and subgraphlevel mining the study reveals that many topperforming architectures share similar traits often characterized by a ResNetstyle residual link and specific operations like skip connections and separable convolutions. The findings suggest that simple constraints like incorporating residual links and specific operations can produce architectures that rival stateoftheart results without the complexity of advanced search algorithms.  Strengths: 1. Comprehensive Analysis: The paper offers a detailed and comprehensive analysis of cellbased search spaces in NAS exploring both operationlevel and subgraphlevel insights. 2. Novelty and Contribution: The study sheds light on the redundancy within the search space offering valuable insights into the limitations and opportunities for improving NAS architectures. 3. Experimentation and Validation: The authors validate their findings through empirical experiments ensuring the reliability and applicability of their conclusions. 4. Relevance and Impact: The insights presented have significant implications for the NAS community challenging conventional beliefs and providing clear directions for future research.  Weaknesses: 1. Generalizability: While the findings are insightful there could be a limitation in generalizing the results beyond the specific cellbased search spaces analyzed in this study. 2. Response to Alternative Search Spaces: The paper focuses predominantly on cellbased search spaces potentially neglecting insights that may arise from alternative NAS paradigms.  Questions and Feedback to the Author: 1. Generalizability: Have you considered how the findings from cellbased search spaces translate to other NAS paradigms such as macrobased approaches or tasks beyond image classification 2. Further Analysis: Are there plans to extend the analysis to include a comparison with alternative search spaces or explore the impact of the identified constraints on different types of architectures 3. Practical Implications: How do you suggest implementing the identified constraints in practical NAS applications to improve efficiency and performance 4. Future Work: What are the next steps in your research agenda following these insightful findings and how do you plan to address the challenges and opportunities identified in the study  Review Summary: The paper provides a valuable analysis of cellbased search spaces in NAS highlighting important insights into architecture performance and redundancy. The comprehensive empirical and analytical approach strengthens the credibility of the findings. The studys implications for rethinking current search space design and enhancing the efficiency of NAS methodologies are significant and warrant attention from the research community. Therefore considering the depth of analysis reliability of findings and relevance to the NAS field I recommend accepting this paper for publication with the suggestion to address questions on generalizability and implications for future research directions.", "gICys3ITSmj": " Summary The paper explores the intersection between contrastive learning and metalearning in the realm of selfsupervised visual representation learning (SSL). It demonstrates that established metalearning methods can achieve comparable performance to recent contrastive learning algorithms when paired with the same data sampling strategy. The study introduces a metaspecific task augmentation strategy that enhances selfsupervised learning performance significantly. The paper offers a detailed comparison and evaluation of the proposed methods on CIFAR10 and ImageNet datasets focusing on linear evaluation semisupervised learning transfer learning and downstream classification tasks.  Strengths 1. Clear Problem Statement: The paper effectively addresses the relationship between contrastive learning and metalearning within the context of selfsupervised learning providing a clear problem statement. 2. Novel Framework: The development of a metalearning framework for SSL is innovative enabling the comparison and optimization of feature extractors trained with metalearning against contrastive learning methods. 3. Comprehensive Evaluation: The study conducts a thorough evaluation on CIFAR10 and ImageNet datasets considering various evaluation scenarios such as linear evaluation semisupervised learning and transfer learning providing robust insights into the proposed methods capabilities.  Weaknesses 1. Technical Depth: The paper delves into intricate technical details and methodologies which may limit accessibility for readers with less expertise in the field. 2. Missing Clarity in Methodology: While the paper presents a detailed methodology certain parts could benefit from clearer explanations to enhance understanding especially regarding the implementation of the proposed tasks and data augmentation strategies.  Questions for the Author 1. Can you elaborate on the computational complexity implications of incorporating the metaspecific task augmentation strategy compared to traditional contrastive learning methods 2. How does the proposed framework address potential limitations in scalability when dealing with larger datasets beyond CIFAR10 and ImageNet 3. Have you considered the impact of hyperparameter tuning on the performance of the metalearning methods and how it may influence the comparative results against contrastive learning  Feedback to the Author  Enhance the readability by breaking down complex technical aspects into simpler terms for broader comprehension.  Consider providing more detailed insights into the scalability and adaptability of the proposed metalearning framework across diverse datasets and domains.  Address the potential hyperparameter sensitivity and generalizability of the methods considering variations in learning environments and tasks.  Review Summary Overall the paper presents a compelling investigation into the synergy between contrastive learning and metalearning in the context of selfsupervised visual representation learning. The innovative framework and metaspecific task augmentation strategy showcase promising results offering a potential avenue for enhancing selfsupervised learning performance. However the paper could benefit from clearer methodological descriptions and considerations for scalability hyperparameter tuning and generalizability. Further clarification and potential adjustments in these areas could significantly strengthen the studys impact and applicability within the research community.", "tBIQEvApZK5": " Summary: The paper explores the relevance of the feature kernel in Knowledge Distillation (KD) for Neural Networks (NNs) by understanding the NNs optimization process. It extends previous theoretical analysis to show the importance of the feature kernel indicating that KD using only pairwise feature kernel comparisons can improve NN test accuracy. The paper introduces Feature Kernel Distillation (FKD) as a novel approach and experimentally validates its effectiveness in image classification settings across various architectures and datasets.  Strengths: 1. Theoretical Foundation: The paper provides a solid theoretical basis for Feature Kernel Distillation linking the feature kernel to knowledge distillation. 2. Novel Approach: The introduction of FKD as a unique method to improve student generalization through feature kernel distillation is a novel contribution to the field. 3. Experimental Validation: The experimental results on CIFAR100 ImageNet and dataset transfer tasks demonstrate the efficacy of FKD outperforming vanilla KD and other featurebased KD methods. 4. Practical Considerations: The practical considerations for implementing FKD such as utilizing feature correlation kernels and feature regularisation enhance the applicability and performance of the proposed method.  Weaknesses: 1. Complexity: The detailed theoretical analysis while informative might be challenging for readers less familiar with the subject matter. 2. Experimental Setup: While the experiments are comprehensive additional insights on the computational complexity and scalability of FKD could further strengthen the paper. 3. Generalization: While the results are promising the paper could benefit from a deeper exploration of the limitations and potential scenarios where FKD may not perform optimally.  Questions: 1. Can you elaborate on how FKD performs under scenarios where the dataset is highly unbalanced or noisy 2. How sensitive is FKD to hyperparameters and how were the hyperparameters optimized in the experiments 3. What computational resources are required for implementing FKD and are there any scalability issues to consider  Feedback: 1. Clarity and Accessibility: Enhancing the clarity of the theoretical analysis and providing more intuitive explanations could make the paper more accessible to a broader audience. 2. Comparative Analysis: Comparing the computational efficiency and scalability of FKD against other KD methods would provide additional insights into its practical utility.  Review Summary: The paper presents a wellsupported theoretical framework for Feature Kernel Distillation (FKD) and demonstrates its effectiveness through rigorous experiments. While the theoretical foundation is strong the paper could benefit from further clarification on practical implementation and potential scalability issues. Overall the novel approach and empirical results showcase the potential of FKD in enhancing NN training and generalization making it a valuable contribution to the field. Based on the strengths in theory and experimental outcomes I recommend acceptance of the paper with minor revisions addressing the outlined weaknesses and questions.", "pIjvdJ_QUYv": " Summary: The paper introduces PrivHFL a privacypreserving Heterogeneous Federated Learning (HFL) framework that addresses the limitations of auxiliary datasets and privacy risks associated with model sharing. It bypasses the need for auxiliary datasets by using a dataset expansion method and implements lightweight additive secret sharing for secure prediction. Extensive evaluations demonstrate PrivHFLs efficiency and accuracy gains compared to existing methods.  Strengths: 1. Innovative Approach: Introducing a practical framework for privacypreserving HFL without the need for auxiliary datasets is novel and addresses a critical limitation in the field. 2. Efficiency Improvements: The paper demonstrates substantial gains in efficiency up to two orders of magnitude and improved accuracy making significant contributions to improving federated learning protocols. 3. Comprehensive Evaluation: Extensive experiments conducted on various datasets and heterogeneous models validate the effectiveness of PrivHFL providing robust evidence to support the proposed framework.  Weaknesses: 1. Scalability Concerns: The paper acknowledges scalability challenges inherent in the system due to the O(n2) complexity of secure predictions which could limit the applicability of PrivHFL to a large number of clients or models. 2. Communication Overhead: While the paper focuses on efficiency gains there is a need for a more detailed analysis of communication overhead in practical scenarios to understand the full implications of deploying PrivHFL.  Questions and Feedback: 1. Scalable Implementation: How does PrivHFL address the scalability concerns with O(n2) overhead for secure predictions Are there plans to optimize the system for larger client numbers 2. Realworld Applicability: How does PrivHFL perform in realworld scenarios with diverse client devices and network environments where direct communication between clients is limited 3. Security Concerns: Can the paper elaborate on potential security threats or vulnerabilities that PrivHFL may be susceptible to in practical deployment scenarios  Review Summary: The paper presents PrivHFL a novel framework for privacypreserving HFL that overcomes the limitations of auxiliary datasets and privacy risks. The papers innovative approach efficiency improvements and thorough experimental evaluations demonstrate the effectiveness of PrivHFL. However scalability concerns and the need for a more detailed analysis of communication overhead warrant further investigation. Overall based on the strengths in innovation and efficiency gains and considering the identified weaknesses I recommend acceptance with minor revisions for addressing scalability and communication concerns.", "w_drCosT76": " Summary The paper introduces Differentiable Scaffolding Tree (DST) to address the challenges in molecular optimization by making molecular graph structures locally differentiable. The model utilizes a learned knowledge network to convert discrete chemical structures into locally differentiable ones enabling gradientbased optimization. DST shows promising results in terms of efficiency interpretability and effectiveness in de novo molecular optimization.  Strengths  Novel Contribution: The proposed DST approach offers a unique solution to the challenges of modeling discrete chemical structures by introducing differentiability.  Efficiency: DSTs sample efficiency and reduced oracle calls compared to baseline methods demonstrate its advantage in optimizing molecular structures.  Interpretability: The ability of DST to provide interpretable results based on chemical intuition enhances the understanding of the model output.  Empirical Evidence: The empirical studies presented in the paper validate the effectiveness of DST in achieving high objective scores and demonstrating diversity in molecule generation.  Weaknesses  Evaluation Metrics: While the paper provides a detailed analysis of optimization performance and oracle efficiency the discussion on limitations or failure cases of DST could enhance the completeness of the evaluation.  Comparative Analysis: While comparison with baseline methods is conducted more detailed discussions on how DST outperforms specific methods and why could strengthen the analysis.  Interpretability Analysis: Although the interpretability analysis is mentioned it could be expanded to provide more concrete examples and insights into the local gradients significance.  Questions and Feedback to the Author 1. Could you elaborate on any scenarios or conditions where DST may not perform optimally or encounter challenges 2. How does the DST approach compare with other stateoftheart methods in terms of scalability to larger datasets or more complex molecular structures 3. Considering the interpretability aspect of DST could you provide further examples or case studies illustrating how the local gradients enhance the understanding of chemical structureproperty relationships  Review Summary The paper presents a novel approach in molecular optimization introducing DST to address the challenges of discrete and nondifferentiable molecular structures. The method shows significant improvements in efficiency interpretability and optimization performance compared to existing baselines. The thorough empirical studies and evaluation metrics provide strong evidence supporting the effectiveness of DST. Overall the paper demonstrates a highquality contribution to the field and is recommended for publication based on its novelty empirical validation and potential impact on the domain of molecular optimization.", "nZOUYEN6Wvy": " Summary: The paper introduces GrIDNet a framework based on graph neural networks for Granger causal inference in systems represented as directed acyclic graphs (DAGs). The motivation behind GrIDNet is to enable causal analysis in scenarios where traditional Granger causality is limited such as nonlinear systems with partial ordering. The framework is applied to singlecell multimodal data to infer regulatory relationships between genomic loci and gene expression. GrIDNet outperforms existing methods in identifying peakgene links achieving greater agreement with independent estimates based on population genetics.  Strengths: 1. Innovative Approach: Introducing GrIDNet for Granger causal inference on DAGstructured systems is a novel and innovative approach expanding the application of causal analysis to complex systems. 2. Application in SingleCell Biology: The application of GrIDNet in the context of singlecell multimodal data analysis demonstrates practical utility in uncovering gene regulatory interactions. 3. Performance: The results showcase the superior performance of GrIDNet over existing methods in predicting regulatory relationships evidenced by metrics like AUPRC and AUROC.  Weaknesses: 1. Complexity: The detailed technical description and methodological aspects may be challenging for readers not wellversed in graph neural networks and Granger causality potentially limiting accessibility to a broader audience. 2. Evaluation: While the paper provides thorough evaluation against eQTL and chromatin interaction datasets a more extensive discussion on potential limitations or biases in these evaluation methods could enhance the robustness of the findings. 3. Practical Implementation: The discussion on runtime and memory usage complexities highlights potential challenges in applying GrIDNet to large datasets suggesting a need for further optimization for scalability.  Questions for the Authors: 1. Can you elaborate on any potential biases or limitations in the evaluation against eQTL and chromatin interaction datasets that could impact the generalizability of the results 2. How does the complexity of GrIDNet impact practical implementation and realworld applications especially in handling largescale or diverse biological datasets 3. Have you considered any extensions or modifications to GrIDNet to address potential limitations like hidden confounders or scalability issues in analyzing even larger datasets  Feedback: The paper presents a significant advancement in causal inference methods particularly in the context of analyzing complex biological systems like singlecell multimodal data. Strengthening the discussion on the practical implications potential biases in evaluation and considerations for scalability would enhance the comprehensiveness and impact of the research findings. Overall the innovative approach and promising results of GrIDNet make it a valuable contribution to the field of causal analysis in dynamical systems.", "l431c_2eGO2": " Summary The paper introduces a simple regularization approach called MixMaxEnt aimed at improving accuracy and obtaining reliable uncertainty estimates in deep neural networks. By combining crossentropy loss with an entropy maximization regularizer on synthetically generated samples between class clusters the method guides the model to be uncertain in regions of the data manifold unknown during training. Extensive experiments on CIFAR10 and CIFAR100 datasets using ResNet and WideResNet architectures demonstrate superior performance compared to existing approaches in terms of classification accuracy calibration and uncertainty estimates especially under domainshift and outofdistribution scenarios.  Strengths  Innovation: The proposed MixMaxEnt method introduces a novel way to improve the reliability of uncertainty estimates in neural networks without compromising accuracy.  Effectiveness: Extensive experiments showcase that MixMaxEnt outperforms existing singlemodel baselines providing superior accuracy calibration and uncertainty estimates.  Simplicity: A significant strength of the approach is its simplicity as it does not require architectural modifications and does not sacrifice accuracy for improved uncertainty estimates.  Comprehensive Experiments: The paper conducts thorough experiments across different datasets and architectures demonstrating the methods robustness and effectiveness under various scenarios.  Reproducibility: Detailed descriptions of the methodology experiments and code availability enhance the reproducibility and transparency of the research work.  Weaknesses  Evaluation Metrics: The paper extensively evaluates various metrics which may lead to complexity in result interpretation. Streamlining the metrics used could enhance clarity.  Generalization: While the method performs well in the presented experiments further validation on a broader range of datasets and neural network architectures would strengthen the generalization of the proposed approach.  Interpretability: Providing additional insights into the interpretability of the trained models and the decisionmaking process could enhance the papers impact.  Questions and Feedback to the Author 1. Can you elaborate on how MixMaxEnts performance varies with different neural network architectures other than ResNet and WideResNet 2. How would you address potential tradeoffs between accuracy and uncertainty estimates when scaling MixMaxEnt to larger and more complex datasets 3. Could you explain further the practical implications of the MixMaxEnt approach in realworld applications such as in medical imaging or autonomous systems  Review Summary The paper introduces a novel and effective method MixMaxEnt for improving accuracy and reliability of uncertainty estimates in deep neural networks. The approachs simplicity coupled with its superior performance in extensive experiments positions it as a notable advancement in the field. The clear presentation of methodology detailed experiments and transparency in code availability enhance the credibility and reproducibility of the research. Overall I recommend acceptance of the paper for publication based on its innovative approach strong empirical results and potential impact in enhancing the trustworthiness of deep learning models.", "ziRLU3Y2PN_": " Review of \"A waveletbased model for texture synthesis\"  1) Summary: The paper introduces a novel family of statistics for texture synthesis based on wavelet transforms and a generalized rectifier nonlinearity. These statistics significantly improve the visual quality of classical waveletbased models for texture synthesis. By changing the number of statistics in the proposed models the paper examines the tradeoff between quality and diversity. The results show that the proposed model can produce syntheses of similar quality to stateoftheart models both for grayscale and color textures.  2) Evaluation: Strong Points:  Innovation: The paper introduces a novel approach utilizing wavelet transforms and a generalized rectifier nonlinearity for texture synthesis improving on classical waveletbased models.  Performance Comparison: Comprehensive performance comparison with stateoftheart models like PS RF and VGG on various texture examples enhances the credibility of the proposed model.  Methodology: Detailed explanation of the model parameters and algorithmic setup enhances reproducibility and transparency.  Analysis: Indepth discussion on the tradeoff between quality and diversity impact of wavelet family choice scale parameter and comparison of grayscale and color models add valuable insights. Weak Points:  Memorization Effects: The paper mentions memorization effects in the models but could provide more extensive analysis on mitigating these effects which could enhance model robustness.  Quantitative Evaluation: While the VGG score is mentioned as a quantitative evaluation metric the paper could elaborate more on other potential metrics to further evaluate the models performance objectively.  Wavelet Family Consideration: The impact of choosing the wavelet family could be elaborated further providing insights on potential future research directions or recommendations.  Generalization: As the proposed model is evaluated across various examples discussing limitations or scenarios where the model might not perform well would add clarity and completeness to the analysis. Questions and Feedback to the Authors: 1. Could you provide a more indepth analysis or potential strategies to address the memorization effects observed in the models as this can impact the models generalizability 2. Have you considered additional quantitative metrics beyond the VGG score to evaluate the performance of the proposed model comprehensively 3. Given the discussed limitations with the wavelet family choice could you suggest areas for future research or potential directions to address such limitations This could provide valuable insights for researchers exploring similar approaches.  3) Review Summary: The paper presents a novel waveletbased model for texture synthesis introducing a family of statistics that significantly improve visual quality. The comprehensive performance comparison with existing models and detailed methodology make the paper a valuable contribution to the field of texture synthesis. Despite some limitations in memorization effects and the choice of wavelet family the papers indepth analysis and clarity make it a strong candidate for acceptance. Considering potential enhancements in addressing memorization effects and exploring additional evaluation metrics can further strengthen the paper.  Recommendation: Based on the innovative approach thorough analysis and comparison with stateoftheart models I recommend acceptance with minor revisions to address the weak points highlighted and further enhance the discussion on addressing memorization effects and potential future research directions.", "lvM693mon8q": "Summary: The paper proposes Compressed Vertical Federated Learning (CVFL) for training on vertically partitioned data. CVFL utilizes compressed intermediate results shared periodically to achieve communicationefficient training. The theoretical analysis shows convergence to a fixed point at a rate of O(1\\xe2\\x88\\x9aT) when the compression error is bounded during training. The paper also evaluates the performance of CVFL on the MIMICIII and ModelNet10 datasets demonstrating significant communication reduction without compromising accuracy. Strengths: 1. Theoretical Contributions: The paper offers novel theoretical insights into the convergence of VFL algorithms employing embedding compression filling a crucial gap in the existing literature. 2. Experimental Validation: The empirical evaluation on realworld datasets substantiates the theoretical claims showcasing the practical effectiveness of CVFL in communicationefficient training. 3. Comprehensive Analysis: The paper provides detailed analyses of common compressors like quantization and sparsification along with convergence bounds on compression parameters enhancing the understanding of compression techniques in VFL. 4. Privacy Considerations: Discussion on privacypreserving mechanisms and potential vulnerabilities coupled with suggested solutions adds depth to the papers relevance in practical applications. 5. Clear and Structured Presentation: The paper is logically organized with a systematic outline of problems assumptions algorithms analyses and experimental results aiding in clarity and readability. Weaknesses: 1. Complexity: The technical depth and mathematical rigor might make the paper less accessible to readers without a strong background in federated learning and optimization theory. 2. Relevance to Broad Audience: While the paper caters to a niche audience focused on distributed machine learning and algorithmic efficiency some parts may be too specialized for general machine learning researchers. 3. Limited Discussion on Robustness: There could be more emphasis on robustness aspects such as sensitivity to hyperparameters or model architecture changes to provide a more comprehensive understanding of CVFLs performance under different conditions. Questions and Feedback: 1. Could you elaborate on the performance impact of different compressors like scalar quantization lattice vector quantization and topk sparsification on convergence rates and accuracy levels in VFL settings 2. How does the communication cost scale with the number of parties in the CVFL algorithm especially in scenarios with varying levels of network latency or geographical distances 3. Have you considered exploring adaptive compression techniques that can adjust compression levels dynamically during training based on the data characteristics or convergence behavior Review Summary: The paper on Compressed Vertical Federated Learning (CVFL) presents a significant advancement in communicationefficient training over vertically partitioned data. Its strong theoretical foundation detailed empirical validations and practical implications demonstrate the effectiveness and potential of CVFL. Although the complex nature of the topic may limit its broader accessibility the paper contributes substantial value to the field of federated learning. Enhanced discussions on robustness aspects and potential future directions could further elevate the papers impact in the research community. Based on the comprehensive analysis and the papers innovative contributions I recommend acceptance after addressing the mentioned weaknesses and considering the posed questions for further clarity and extension of the research.", "iLHOIDsPv1P": " Summary: The paper introduces a novel algorithm for the efficient approximation of information stored in weights (IIW) within neural networks (NNs). The authors propose the PACBayes Information Bottleneck (PIB) and derive an approximation method for IIW leading to a new information measure capable of explaining NN behavior across various scenarios such as activation functions architecture types noise ratios and batch sizes. Furthermore the paper introduces a Bayesian inference algorithm based on stochastic gradient Langevin dynamics to sample from the optimal weight posterior determined by PIB. Experimental results confirm the effectiveness of the proposed approach in understanding the generalization ability of NNs.  Strong Points: 1. Innovation: The paper introduces a novel approach focused on understanding the generalization ability of NNs through information stored in weights offering a fresh perspective on the NN behavior. 2. Theoretical Contribution: The derivation of the PACBayes Information Bottleneck and the approximation of IIW provide a new theoretical framework for exploring the generalization capabilities of neural networks. 3. Empirical Validation: Extensive experiments demonstrate the effectiveness of the proposed PIB algorithm in explaining NN behavior across activation functions architecture types noise ratios and batch sizes. 4. Methodological Rigor: The detailed explanation of the algorithms and the clear presentation of experimental results enhance the credibility of the proposed approach.  Weak Points: 1. Clarity in Implementation: The algorithm descriptions could be further detailed to facilitate reproducibility especially in complex methodologies like Bayesian inference with SGLD. 2. Discussion on Limitations: The paper could benefit from a discussion on potential limitations of the proposed approach or areas where further development is needed.  Questions and Feedback:  Could you elaborate on the computational complexity of the proposed algorithms Understanding the scalability of the approach would be beneficial.  Consider discussing potential limitations or challenges in deploying the PIB method in realworld NN applications. How does the approach handle highdimensional data or larger NN architectures  It would be insightful to include a discussion on the interpretability of IIW and how it could aid in model diagnosis or improvement strategies.  Review Summary: The paper presents an innovative algorithm the PACBayes Information Bottleneck for approximating information stored in weights within neural networks. The experimental results showcase the effectiveness of the method in understanding NN behavior across various scenarios. The clarity in presenting the theory rigorous methodological approach and comprehensive experiments support the credibility of the proposed approach. However enhancing the clarity in implementation details and addressing potential limitations would further strengthen the paper. Overall the paper presents a significant contribution to the field of machine learning and warrants consideration for publication pending minor revisions.", "morSrUyWG26": " Summary: The paper introduces AutoOED an Automated Optimal Experimental Design platform powered by machine learning to efficiently tackle multiobjective optimization problems with limited resources. It leverages stateoftheart multiobjective Bayesian optimization algorithms and introduces a novel strategy called BelieverPenalizer for timeefficient batch experiments. The platform includes a userfriendly GUI modular structure for algorithm extensions and supports fully automated realworld hardware experiments.  Strong Points: 1. Innovative Approach: The introduction of BelieverPenalizer strategy for asynchronous batch optimization is a notable contribution to the field of multiobjective optimization. 2. Modular Framework: The platforms modular design allows for easy extension and replacement of MOBO algorithm components making it a versatile tool for researchers. 3. UserFriendly Interface: The graphical user interface (GUI) provides an intuitive way for users with no coding experience to visualize and guide the optimization process effectively. 4. RealWorld Applicability: The demonstration of AutoOEDs effectiveness in optimizing a PID heater controller showcases its practical application in realworld experimental design.  Weak Points: 1. Lack of Comparative Analysis: While the platforms performance is compared with other BO platforms a detailed comparative analysis with a broader range of metrics could enrich the evaluation. 2. Theoretical Grounding: While the BelieverPenalizer strategy shows promising results empirically a more indepth theoretical analysis or proof of its effectiveness could enhance the papers academic rigor.  Questions and Feedback: 1. Clarity on Algorithm Selection: Could the authors elaborate on the process of selecting specific MOBO algorithms to be implemented in AutoOED and justify their choices based on performance or applicability 2. Potential Enhancements: Are there plans to incorporate additional features like constraint handling or noise handling in future iterations of AutoOED to enhance its capabilities further 3. Generalization of BelieverPenalizer: How generalizable is the BelieverPenalizer strategy across different types of optimization problems beyond the ones tested in the paper  Review Summary: The paper presents a novel and practical Automated Optimal Experimental Design platform AutoOED with a strong emphasis on data and timeefficient multiobjective optimization. The platforms innovative features such as the BelieverPenalizer strategy and userfriendly GUI make it a valuable tool for both researchers and practitioners in experimental design. While the paper showcases robust experimental results and realworld applications additional theoretical grounding and comparative analysis could further strengthen the papers academic contribution. Overall AutoOED demonstrates significant potential and fills a gap in the field of automated experimental design warranting its acceptance for publication.", "sEIl_stzQyB": " 1) Summary of the Paper: The paper proposes a new criterion called optimal consistency to evaluate the optimality of coordination in fully cooperative multiagent reinforcement learning (MARL) tasks. It addresses the limitation of linear or monotonic value decomposition methods in ensuring optimal coordination due to relative overgeneralization. The authors introduce the TrueGlobalMax (TGM) condition and propose the greedybased value representation (GVR) algorithm to achieve optimal stable points and eliminate nonoptimal stable points through inferior target shaping and superior experience replay. Experimental results demonstrate that GVR outperforms existing baselines on various benchmarks.  2) Strong Points:  a) Contribution:  The paper introduces a novel criterion optimal consistency and proposes a new method (GVR) to address the issue of relative overgeneralization in MARL.  It provides theoretical analysis and practical solutions to ensure optimal coordination in value decomposition methods.  Experimental results show the superiority of GVR over stateoftheart baselines on different tasks.  b) Methodological Strengths:  Derivation of the joint Q value function for linear and monotonic value decomposition.  Introduction of inferior target shaping and superior experience replay to ensure optimal stable points and eliminate nonoptimal stable points.  Theoretical proofs provided to support the effectiveness of the proposed method.  c) Experimental Validation:  Extensive experiments conducted on various benchmarks including matrix games predatorprey games and StarCraft multiagent challenge (SMAC) demonstrating the performance of GVR.  Comparison with existing methods to showcase the efficacy of GVR in improving coordination and stability.  2) Weak Points:  a) Complexity:  The paper involves extensive theoretical derivations and explanations which might be challenging to follow for readers not wellversed in the topic.  Some sections are highly technical making it difficult for readers to grasp the core concepts without a deep understanding of MARL and value decomposition methods.  3) Question and Feedback to the Author:  Could you provide more insights into the computational complexity and scalability of the proposed GVR algorithm  How do you plan to address the limitations of GVR in tasks with hard exploration where obtaining superior samples is challenging  4) Review Summary: The paper introduces a novel criterion optimal consistency to evaluate coordination in MARL and proposes the GVR algorithm to achieve optimal stable points and eliminate nonoptimal stable points. The methodological strength lies in the theoretical foundation experimental validation and comparison with existing methods. While the paper is comprehensive and innovative the complexity of the content could hinder understanding for some readers. Addressing scalability and overcoming challenges in tasks with hard exploration could enhance the applicability of GVR. Overall the paper presents a significant contribution to the field of MARL and provides a solid foundation for further research and development. I recommend the paper for publication after addressing the mentioned points.", "saNgDizIODl": "Peer Review  1) Summary of the Paper: The paper introduces a novel method Nonparametric Uncertainty Quantification (NUQ) for uncertainty estimation in machine learning models predictions. The method is based on a principled approach that distinguishes between aleatoric and epistemic uncertainties. It works directly in the feature space and can be applied to any deterministic neural network model. The paper provides a detailed theoretical background presents experimental results on various realworld image datasets and compares the performance of NUQ with other uncertainty estimation methods.  2) Strong Points: a) Theoretical Grounding: The paper is wellstructured and provides a thorough theoretical background to support the proposed method. b) Novel Methodology: NUQ introduces a unique approach to uncertainty quantification that explicitly separates aleatoric and epistemic uncertainties. c) Experimental Validation: The method is evaluated on multiple datasets showing promising results in misclassification and OOD detection tasks. d) Practical Applicability: NUQ is scalable and applicable to large datasets like ImageNet demonstrating its potential for realworld applications. Weak Points: a) Comparison with Existing Methods: While the paper discusses the performance of NUQ against other methods a more detailed comparison could enhance the evaluation of NUQs effectiveness. b) Hyperparameter Sensitivity: The paper could address the sensitivity of the method to hyperparameters and provide guidance on selecting optimal values. c) Theoretical Limitations: The paper could discuss potential limitations or assumptions of the proposed method to provide a more comprehensive understanding of its applicability. Questions and Feedback to the Author: 1. Can you provide insights into the computational efficiency of NUQ compared to other uncertainty estimation methods especially in largescale datasets like ImageNet 2. How generalizable is the proposed method to different types of neural network architectures Have you considered the adaptability of NUQ to diverse model structures and complexities 3. Could you elaborate on the robustness of NUQ to noisy or lowquality data inputs and discuss how the method handles data uncertainty in such scenarios  3) Review Summary: The paper presents a solid contribution with the introduction of NUQ a novel method for uncertainty quantification in machine learning models. By explicitly addressing aleatoric and epistemic uncertainties NUQ offers a principled approach that demonstrates strong performance in various experimental settings. The theoretical grounding practical applicability and scalable nature of the method highlight its potential for realworld applications. While the paper could benefit from more detailed comparisons with existing methods and addressing sensitivity to hyperparameters overall NUQ presents a valuable addition to uncertainty estimation research.  Based on the presented analysis I recommend acceptance of the paper with minor revisions to address the weak points identified above. The methodological novelty experimental validation and potential impact on uncertainty quantification make the paper a valuable contribution to the field. With the suggested improvements the paper can further strengthen its theoretical underpinnings and applicability in practical scenarios.", "qNcedShvOs4": " PeerReview of \"EinSteinVI: A Probabilistic Programming Library for Stein Variational Inference\"  1) Summary: The paper introduces EinSteinVI a lightweight library that integrates the latest Stein variational inference (VI) methods with the NumPyro probabilistic programming language. It presents a novel algorithm called ELBOwithinStein for inference of Stein mixtures which supports custom guide programs. The paper demonstrates EinSteinVI using toy examples and realworld problems such as Bayesian neural networks and a Steinmixture deep Markov model showing promising results. Various kernels nonlinear optimization and graphical models are also integrated into EinSteinVI.  2) Strengths:  Innovative Approach: The paper introduces a novel algorithm ELBOwithinStein and integrates various advanced Stein VI methods into a probabilistic programming library.  Comprehensive Coverage: The library includes a wide range of kernels nonlinear scaling and supports for large models demonstrating versatility.  Performance Comparison: The results show that EinSteinVI outperforms existing methods on realworld problems indicating the effectiveness of the proposed approach.  Detailed Description: The paper provides detailed explanations of the algorithms methodologies and their integration with NumPyro making it accessible for readers.  Weaknesses:  Lack of Comparative Analysis: While the paper illustrates the advantages of EinSteinVI some direct comparisons with existing libraries like PyMC3 could reinforce the significance of the proposed library.  Incomplete Evaluation: While the results are promising a more extensive evaluation on diverse datasets and comparisons with more baseline methods could strengthen the paper.  Complexity: The technical details provided might be overwhelming for readers unfamiliar with the topic suggesting the need for clearer explanations and illustrations.  Question and Feedback to the Author: 1. Could you provide a more indepth evaluation comparing EinSteinVI with PyMC3 particularly highlighting the strengths and limitations of both libraries in different scenarios 2. Have you considered incorporating additional realworld case studies or benchmarks to further validate the performance and scalability of EinSteinVI 3. Clarify the complexity or learning curve associated with implementing custom guide programs in EinSteinVI especially for users with limited experience in probabilistic programming.  3) Review Summary: The paper presents EinSteinVI a novel probabilistic programming library for Stein variational inference that integrates cuttingedge techniques into the NumPyro platform. While the proposed library demonstrates promising results and showcases advancements in the field further comparative analyses and detailed evaluations are warranted to establish its superiority over existing libraries. The papers comprehensive description innovative algorithms and integration with NumPyro make it a valuable contribution to the Bayesian inference community. However addressing the mentioned weaknesses and providing additional validation would enhance the papers impact and credibility in the field.", "ngjR4Gw9oAp": " Summary: The paper introduces a novel approach SACI that leverages inhibitory networks to address the challenge of efficiently retraining reinforcement learning agents to acquire new skills in conflicting environments. By utilizing separate value functions and automatic entropy tuning the method aims to balance exploitation of previous skills with exploration of new ones. Experimental validation is provided with tests in modified OpenAI Gym environments.  Strengths: 1. Innovative Approach: Introducing inhibitory networks inspired by neuroscience for efficient retraining in deep reinforcement learning is a significant contribution. 2. Comprehensive Background: The paper provides a detailed background on related work foundational concepts like maximum entropy RL and the SAC algorithm. 3. Empirical Validation: The method is validated through experiments in OpenAI Gym environments showcasing the effectiveness of the proposed SACI architecture for accelerated retraining. 4. Clear Presentation: The paper is wellstructured with a clear explanation of the proposed method background information and experimental results.  Weaknesses: 1. Clarity in Experiments: While the experiments are described in detail some sections especially the mathematics behind the proposed approach may be challenging for readers not familiar with the subject matter. Providing simpler explanations or visual aids could enhance understanding. 2. Comparative Analysis: While the experiments demonstrate the effectiveness of SACI a direct comparison with existing stateoftheart methods could strengthen the evaluation. 3. Discussion of Limitations: Limited discussion on potential limitations or challenges of the proposed approach could be addressed to provide a more comprehensive understanding.  Question  Feedback to the Author: 1. Question: How does the proposed SACI method compare with other existing approaches for retraining RL agents in conflicting environments 2. Feedback: It would be beneficial to discuss the computational complexity and scalability of the SACI method especially when applied to complex RL tasks or largescale environments.  Review Summary: The paper presents a novel approach SACI that utilizes inhibitory networks to enhance retraining of RL agents in conflicting environments. Despite its innovative nature and successful validation in experiments the paper could benefit from further clarity in explanations comparative analysis and addressing potential limitations. Overall considering the methods novelty and promising experimental results I recommend the paper for acceptance pending minor revisions for clarity and deeper analysis.", "tQ2yZj4sCnk": "Summary: The paper investigates divergence regularization in cooperative multiagent reinforcement learning (MARL) and proposes a new offpolicy cooperative MARL framework called divergenceregularized multiagent actorcritic (DMAC). The study derives the update rule of DMAC and demonstrates that it guarantees a monotonic policy improvement and is not biased by the regularization. The framework is evaluated in a stochastic game and StarCraft MultiAgent Challenge showing substantial performance improvements over existing MARL algorithms. Strengths: 1. Innovative Approach: The introduction of divergence regularization in cooperative MARL is innovative and addresses the limitations of entropy regularization. 2. Theoretical Analysis: The theoretical derivation of DMACs update rules and the guarantees of monotonic policy improvement are wellsupported and add credibility to the framework. 3. Empirical Evaluation: The empirical evaluation in a stochastic game and SMAC tasks demonstrates the effectiveness of DMAC in improving performance and convergence speed providing practical evidence of the proposed frameworks benefits. 4. Flexibility: DMAC is depicted as a flexible framework that can be integrated with existing MARL algorithms thus enhancing its applicability and generalizability. Weaknesses: 1. Complexity: The papers complexity might make it inaccessible to readers who are not wellversed in reinforcement learning and mathematical derivations. 2. Limited Comparisons: While comparisons with existing algorithms and entropy regularization are made a broader comparison with a wider range of approaches could provide a more comprehensive evaluation. 3. Experimental Setup: The paper could benefit from providing more details on the experimental setup hyperparameters baseline algorithms and statistical significance testing methods used. 4. Clarity: Some sections especially those involving mathematical derivations may be challenging to follow for readers without an advanced background in the subject matter. Question and Feedback to the Author: 1. Question: How does the proposed DMAC framework compare with other stateoftheart MARL algorithms beyond those discussed in the paper such as MADDPG or MATD3 2. Question: Can you provide more insights into the design choices for the hyperparameters used in the experiments and how sensitive the frameworks performance is to these choices 3. Feedback: It would be beneficial to include further realworld applications or scenarios where DMAC could be effectively applied to showcase the relevance and potential impact of the proposed framework. 4. Feedback: Consider expanding on the discussion surrounding the scalability and efficiency of DMAC compared to existing MARL algorithms to strengthen the practical implications of the findings. Review Summary: The paper presents a novel divergenceregularized multiagent actorcritic (DMAC) framework for cooperative MARL offering theoretical insights experimental validation and flexibility for integration with existing algorithms. While the approach is innovative and the empirical results are promising the paper could benefit from more straightforward explanations expanded experimental details additional comparisons with various MARL methods and further discussions on the practical implications and scalability of the proposed framework. Overall the study makes a valuable contribution to the field of MARL with potential for significant impact if the mentioned areas are addressed.", "lf0W6tcWmh-": " Summary of the Paper: The paper investigates the impact of momentum in stochastic gradient descent (SGD) on the generalization of deep learning models. While prior work focused on the faster convergence of momentum in training neural networks this paper empirically demonstrates that gradient descent with momentum (GDM) can significantly improve generalization compared to vanilla gradient descent (GD) in various deep learning tasks. The key insight is that momentum can help learn features in datasets where examples share certain features but differ in their margin. The analysis is based on a binary classification problem and a twolayer convolutional neural network setting. The paper provides theoretical insights and empirical evidence supporting the idea that momentum aids in better generalization by amplifying historical gradients and facilitating learning from shared features.  Strong Points: 1. Novel Contribution: The paper offers a unique perspective on the impact of momentum in deep learning focusing on the mechanism behind its enhancement of generalization. 2. Theoretical and Empirical Analysis: The paper combines theoretical analysis and empirical verification to support its claims providing a comprehensive evaluation of the phenomenon. 3. Clear Structure: The paper is wellorganized with detailed sections covering the introduction methodology results and additional experiments enhancing readability and understanding. 4. Comprehensive Explanations: The paper effectively explains the theoretical models algorithms and training processes making it accessible to a wide audience in the deep learning field.  Weak Points: 1. Complexity: The technical depth of the paper may complicate its accessibility to readers not wellversed in deep learning theory and optimization algorithms. 2. Lack of Comparative Studies: While the paper highlights the benefits of GDM over GD it could benefit from comparing its outcomes with other optimization techniques used in deep learning. 3. Empirical Validation Scope: The empirical validation seems limited to specific binary classification scenarios and extending the experiments to a wider range of tasks could provide more robust evidence.  Question to the Author:  Have you considered exploring the applicability of your findings in realworld scenarios beyond binary classification problems such as image recognition or natural language processing tasks  How do you plan to address the challenge of generalizing the benefits of momentum to diverse neural network architectures and datasets  Feedback to the Author:  Providing additional insights on the practical implications and potential applications of your research findings could enhance the relevance and impact of your work.  Broadening the scope of empirical validation to encompass various deep learning tasks could strengthen the generalizability of your conclusions.  Review Summary: The paper presents a compelling investigation into the role of momentum in improving generalization in deep learning models. With a strong theoretical foundation and empirical evidence the analysis sheds light on the potential benefits of gradient descent with momentum compared to traditional gradient descent. While the paper showcases a thorough exploration of the subject matter reinforcing the empirical validation and extending the practical implications could further enhance the significance and broader applicability of the research. Overall the paper serves as a valuable contribution to the understanding of optimization algorithms in deep learning and warrants attention from researchers in the field.", "famc03Gg231": " Summary: The paper proposes a novel hybrid training approach that combines higherorder optimization methods with machine learning techniques to solve inverse problems involving physical processes. The approach involves embedding physical gradients into the gradientdescentbased learning pipeline which significantly improves optimization directions and accuracy on a variety of physical systems. The experiments cover scenarios like NavierStokes equations Poissons equation and the heat equation demonstrating the effectiveness of the proposed method in comparison to traditional training schemes.  Strengths: 1. Innovative Approach: The paper introduces a novel method that integrates higherorder solvers with deep learning pipelines addressing the limitations of traditional optimization schemes. 2. Demonstrated Performance: Extensive experiments on various physical systems show significant improvements in accuracy and convergence speed using physical gradients. 3. Thorough Explanations: The paper provides detailed explanations of the methodology algorithms and experimental setup enhancing the understanding of the proposed approach.  Weaknesses: 1. Complexity: The paper contains technical terminology and detailed mathematical formulations that may be challenging for readers without a strong background in optimization and deep learning. 2. Limited Comparison: While the experiments demonstrate the superiority of the proposed method over traditional training schemes a comparison with other stateoftheart methods could provide more comprehensive insights. 3. Computational Cost: The increased computational cost associated with some of the proposed techniques could be a limiting factor for practical implementation especially in realtime applications.  Questions: 1. Scalability: How does the computational complexity scale with the size and complexity of the physical system being analyzed Have you considered any strategies to mitigate potential scalability issues  Feedback to the Author:  Clarity: Enhance the clarity of the paper by providing intuitive examples or visual aids to explain complex concepts.  Robustness Evaluation: Consider including additional experiments or comparisons with other advanced optimization techniques to further validate the effectiveness of the proposed approach.  Review Summary: The paper presents a novel hybrid training approach that integrates physical gradients into deep learning pipelines for solving inverse problems involving physical systems. The experimental results demonstrate a significant improvement in accuracy and convergence speed compared to traditional training methods. Although the paper is technically comprehensive improvements in clarity scalability considerations and additional comparisons with stateoftheart methods would strengthen its impact. Overall the innovative methodology demonstrated performance and detailed explanations make the paper a valuable contribution to the field of optimization in physical systems.", "rbFPSQHlllm": "1) Summary of the Paper: The paper introduces a new automated multiobjective Mixer (AutoMOMixer) model for medical image diagnosis. It addresses the challenge of balancing sensitivity and specificity in patient status identification by considering them as simultaneous objective functions during model training. The model combines a multilayer perception architecture with Bayesian optimization to optimize hyperparameters and incorporates an evidence reasoning approach to fuse output probabilities. Experimental studies on brain tumor and COVID19 datasets demonstrate the superior performance of AutoMOMixer over Mixer and CNN models. 2) Evaluation: Strong Points:  The paper addresses a crucial issue in medical image diagnosis by proposing a novel model that balances sensitivity and specificity.  Introducing the AutoMOMixer model with multiobjective optimization and Bayesian hyperparameter tuning showcases innovation.  The inclusion of experimental studies on real datasets strengthens the credibility of the proposed models performance.  Detailed methodology description and framework illustrations enhance the understanding of the proposed approach.  Clear presentation of results through tables and figures facilitates comparison and interpretation. Weak Points:  The paper lacks a discussion on potential limitations or challenges faced during development and implementation of the AutoMOMixer model.  While the methodology is described in detail more insights into the decisionmaking process for selecting specific hyperparameters and optimization strategies could enhance the readers understanding.  The paper could benefit from a more indepth comparison with existing multiobjective optimization approaches for medical image diagnosis highlighting the unique contributions of the AutoMOMixer model. Questions and Feedback to the Author: 1. Have you considered exploring the robustness and generalizability of the AutoMOMixer model on a wider range of medical image datasets to ascertain its performance across various clinical scenarios 2. Could you provide more insights into the computational complexity of the AutoMOMixer model compared to traditional CNN models and discuss any efficiency gains achieved through the proposed approach 3. How do you plan to address potential challenges related to model interpretability and explainability given the complexity of the AutoMOMixer architecture and hyperparameter optimization process 3) Review Summary: The paper presents an innovative AutoMOMixer model for medical image diagnosis addressing the crucial aspect of balancing sensitivity and specificity in patient status identification. The thorough methodology description incorporation of advanced techniques like Bayesian optimization and evidence reasoning and the experimental validation on real datasets demonstrate the models superior performance over traditional CNN and Mixer models. While the paper showcases significant strengths in innovation and experimental validation aspects such as discussing limitations providing deeper insights into hyperparameter selection and potential comparisons with existing approaches could further enhance the papers impact. Overall based on the outlined strong points and potential improvements I recommend this paper for publication with minor revisions and additional insights provided.", "irARV_2VFs4": "1) Summary of the Paper: The paper addresses the problem of training classification models with groupannotated training data where models trained using standard empirical risk minimization (ERM) suffer from poor performance on minority groups due to distribution shift. The paper introduces a new algorithm called Common Gradient Descent (CGD) that focuses on learning shared features among different groups to enhance minority group performance. CGD outperforms existing baselines like ERM and GroupDRO on various benchmarks by encouraging learning of features that contribute to the common good across all groups. 2) Strong Points: Strong Points:  Novel Algorithm: The paper introduces a novel algorithm CGD which explicitly encourages learning shared features across groups leading to improved performance on both minority and majority groups.  Theoretical Grounding: The paper provides theoretical analyses showing that CGD is a descent method and converges to firstorder stationary points of smooth nonconvex functions.  Empirical Validation: Empirical results demonstrate that CGD matches or outperforms existing baselines on standard benchmarks for both minority and majority groups.  Qualitative Insights: The paper provides qualitative insights through simple scenarios that highlight the efficacy of CGD in handling label noise uneven intergroup similarity and spurious correlations. Weak Points:  Narrow Applicability: The algorithms dependency on groupannotated training data limits its applicability in scenarios where such annotations are noisy or unavailable.  Hyperparameter Sensitivity: The paper discusses the challenge of tuning hyperparameters like C for group adjustment which could be tedious and potentially impact algorithm performance.  Limited Domain Diversity: The empirical evaluation focuses primarily on image and text datasets limiting generalizability across diverse domains. Questions: 1. Have you considered the potential challenges of implementing CGD in scenarios where group annotations are noisy or unavailable How could the algorithm be adapted to address such scenarios 2. How sensitive is CGD to hyperparameters such as C for group adjustment Have you explored techniques to automate or optimize hyperparameter tuning to enhance algorithm performance 3. Could CGD be extended to handle domain diversity beyond image and text datasets considering the algorithms effectiveness in these domains Feedback:  The papers strength lies in its innovative algorithm and thorough empirical evaluation showcasing substantial improvements over existing baselines. To enhance the papers impact consider addressing the limitations related to narrow applicability and hyperparameter sensitivity.  Providing insights on potential extensions of CGD to handle noisy or unavailable group annotations could broaden the algorithms utility and relevance in realworld applications.  Exploring techniques for automated hyperparameter tuning and conducting experiments across a wider range of domains could strengthen the papers contributions and applicability. 3) Review Summary: The paper presents a novel algorithm CGD for training classification models with groupannotated data demonstrating superior performance over existing baselines on standard benchmarks. While the algorithm shows promise in addressing distribution shift and improving minority group performance its applicability in scenarios with noisy annotations and sensitivity to hyperparameters may raise challenges. By addressing these limitations and exploring broader domain diversity the papers contributions can be further enhanced potentially advancing the stateoftheart in training models for diverse groups with shared features.", "yKIAXjkJc2F": " Summary of the Paper: The paper introduces a new class of deep neural networks called Invariant Imbedding Networks (InImNets) which focus on the concept of depth in neural networks. The authors propose a new method that reduces nonlinear vectorvalued optimal control problems to a system of forwardfacing initial value problems. They demonstrate the potential of this approach through experiments on supervised learning and time series prediction tasks showcasing competitive performance compared to existing techniques.  Strengths of the Paper: 1. Innovative Conceptual Framework: The paper introduces a novel approach to understanding network depth providing a fresh perspective on the structure and functioning of deep neural networks. 2. Theoretical Contribution: The introduction of InImNets offers a significant theoretical advancement in the field of deep learning by relating network depth to optimal control problems expanding the understanding of how neural networks learn. 3. Experimental Validation: The authors back their theoretical claims with empirical results from benchmark experiments demonstrating the effectiveness of InImNets in practical applications like highdimensional rotating MNIST and bouncing balls prediction tasks. 4. Detailed Methodology: The paper provides a comprehensive overview of the proposed method including algorithms theorems and experiments enabling a thorough understanding of the new InImNet architectures.  Weaknesses of the Paper: 1. Complexity and Accessibility: The paper delves into intricate mathematical derivations and concepts which may be challenging for readers with limited background in optimal control theory or machine learning. 2. Error Analysis and Robustness: While the experiments demonstrate promising results a deeper analysis of error metrics robustness to hyperparameters and scalability to larger datasets could provide additional insights into the performance of InImNets.  Questions for the Authors: 1. Can you provide more insight into the computational complexity of implementing InImNets compared to traditional deep learning architectures 2. How does the performance of InImNets scale with increasingly complex datasets such as those with higher dimensions or larger sample sizes  Feedback to the Authors: 1. Consider providing a more simplified explanation of key concepts and methodologies for better readability by a broader audience. 2. Enhance the discussion section by addressing potential future research directions such as the adaptation of InImNets to specific domains or exploring the interpretability of models generated with this new approach.  Review Summary: The paper presents a novel approach to understanding depth in neural networks through Invariant Imbedding Networks (InImNets) offering a fresh perspective on deep learning architectures. The theoretical foundation detailed methodologies and experimental results demonstrate the potential of this new framework. However simplification of complex concepts and deeper analysis of model robustness would enhance the papers accessibility and completeness. Overall the paper makes a significant contribution to the field of deep learning and warrants publication pending minor revisions.", "sWbXSWzHPa": " Summary of the Paper: The paper proposes an algorithm Worstoff DRO to address the challenge of training machine learning models that are invariant to group membership using partially labeled data. The work focuses on situations where group labels are partially available or completely missing due to privacy concerns or difficulty in data labeling efforts. By constructing a constraint set and optimizing for the worstoff group assignments the algorithm aims to improve minority group performance while maintaining overall accuracy across all groups. The method is evaluated on image and tabular datasets showcasing improvements compared to existing baselines.  Strengths of the Paper:  Innovative Approach: The paper presents a novel algorithm Worstoff DRO to address the realworld challenge of training models with partially labeled group information.  Gap Filling: The work fills a gap in the literature by focusing on scenarios where group labels are not fully available which is a common practical scenario in sensitive data applications.  Theoretical Grounding: The paper provides theoretical guarantees and explanations for the proposed algorithm enhancing the transparency and robustness of the method.  Comprehensive Experiments: The algorithm is evaluated on multiple datasets showcasing improvements in minority group performance while maintaining overall accuracy compared to existing baseline methods.  Weaknesses of the Paper:  Complexity: The algorithm involves various optimizations and constraints making it potentially challenging to implement and understand for individuals with limited experience in this specific domain.  Assumption Dependency: The effectiveness of the method relies on strong assumptions such as missing data being completely at random which might not always hold true in practical scenarios.  Discussion Clarity: Some parts of the paper especially in the methodology section might be difficult for readers unfamiliar with the specific technical terminology to follow potentially hindering the broader accessibility of the work.  Questions and Feedback for the Author: 1. Can you provide more insights into the scalability and computational efficiency of the proposed algorithm especially when dealing with large datasets 2. Have you considered scenarios where the assumption of missing data being completely at random may not hold true and how your algorithm would perform in such circumstances 3. Could you elaborate on potential realworld applications or use cases where Worstoff DRO could have a significant impact  Review Summary: The paper introduces an innovative algorithm Worstoff DRO to address the challenge of training machine learning models invariant to group membership using partially labeled data. The proposed method demonstrates improvements in minority group performance while maintaining overall accuracy across groups through comprehensive experiments. Despite some complexities in the approach and assumptions made the work contributes significantly by bridging a gap in the literature and offering a practical solution for scenarios with limited group label availability. Overall the paper presents a valuable contribution to the field of biased algorithm mitigation and fairnessaware machine learning.", "xw04RdwI2kS": " Summary of the Paper The paper introduces the concept of Inverse Contextual Bandits (ICB) to understand how decisionmakers behavior evolves over time using liver transplantations as a core application. It proposes two algorithms Bayesian ICB (BICB) and Nonparametric Bayesian ICB (NBICB) focusing on interpretability in policy learning to capture nonstationary knowledge and operate offline.  Strengths of the Paper: 1. Innovative Concept: The paper introduces a novel concept of ICB to address the critical issue of understanding evolving decisionmakers behavior which has practical applications in various fields. 2. Methodological Rigor: The paper provides a wellstructured methodology formalizes the problem and proposes algorithms that address the objectives effectively. 3. Applicability: The paper demonstrates the applicability of ICB using real and simulated data for liver transplantations showcasing the utility of the proposed algorithms in understanding how policies evolve over time. 4. Investigative Device: The paper effectively highlights ICB as an investigative device for auditing and quantifying behaviors over time providing interpretable representations of policy evolution.  Weaknesses of the Paper: 1. Complexity: The paper delves into technical details and complex algorithms which may make it challenging for readers not wellversed in the subject matter to comprehend fully. 2. Lack of Comparative Analysis: While the paper presents the performance of the proposed algorithms more thorough comparative analysis against existing methods could reinforce the superiority of ICB.  Questions and Feedback to the Authors: 1. Clarifications: Provide more detail on the implications of the results in practical healthcare decisionmaking scenarios. How can the findings be practically applied 2. Generalization: How applicable and scalable are the proposed algorithms to different decisionmaking scenarios beyond organ transplantations 3. Future Directions: What are the potential future research directions or improvements for refining the proposed algorithms or applying ICB to other domains  Review Summary: The paper presents a novel concept of Inverse Contextual Bandits to investigate the evolution of decisionmakers behavior over time. It introduces effective algorithms like BICB and NBICB for interpretative policy learning. The research is wellstructured methodologically sound and demonstrates practical applicability. However it could benefit from more detailed explanations for wider comprehension and comparative analysis against existing methods. Overall the paper significantly contributes to the field of policy learning and understanding behavior evolution warranting publication.  Recommendation: Based on the innovative concept methodological rigor applicability and investigative value of ICB I recommend accepting this paper for publication with minor revisions to enhance clarity and provide more depth in the practical implications section.", "jJJWwrMrEsx": " Summary of the Paper: The paper introduces a new architecture called Truth Table Deep Convolutional Neural Networks (TTDCNNs) that can be encoded into SAT formulas and provide realvalued weights and intermediate values. This architecture aims to address the lack of interpretability and formal verification scalability in conventional Binary Neural Networks (BNNs). The TTDCNNs are designed to enable human postprocessing enhance robustness verification and decrease the complexity of SAT formulas to make formal verification more tractable. The paper provides detailed explanations of the architecture encoding processes posttuning properties tractability features and experimental results comparing TTDCNNs with BNNs and realvalued networks in robustness verification tasks.  Strong Points: 1. Innovative Contribution: The paper introduces a novel architecture TTDCNNs that bridges the gap between eXplainable AI (XAI) and formal verification by enabling interpretability and humanfriendly postprocessing of neural networks. 2. Interpretability: The use of truth tables allows for a global and exact interpretation of the logic rules learned by the network enabling users to understand the models behavior and reasoning. 3. Scalability and Tractability: The paper addresses the scalability issues of formal verification by reducing the complexity of SAT formulas making it more tractable for general SAT solvers and model counting solvers. 4. Experimental Validation: The experimental results showcase the superior performance of TTDCNNs in terms of verifiable accuracy robustness and scalability compared to BNNs and realvalued networks.  Weak Points: 1. Complexity: The paper provides a detailed and complex explanation of the architecture encoding procedures and experimental results which may be challenging for readers not wellversed in the subject matter. 2. Limited Discussion: The paper could benefit from more detailed discussions on the limitations and practical implications of using TTDCNNs in realworld scenarios. 3. Evaluation Extent: While the experimental results are comprehensive more insights on the generalizability and applicability of TTDCNNs to other datasets or applications would enhance the papers impact.  Questions and Feedback to the Author: 1. Can you provide more insights into the potential challenges or limitations of implementing TTDCNNs in realworld applications outside of the experimental setting 2. How does the interpretability of TTDCNNs contribute to enhancing user trust and understanding of neural network decisionmaking processes 3. Have you considered the computational overhead and training time required for implementing TTDCNNs in comparison to traditional neural networks 4. Is there a plan to explore the applicability of TTDCNNs to other domains beyond image classification such as natural language processing or reinforcement learning  Review Summary: The paper introduces a novel architecture TTDCNNs that offers a promising solution to the challenges of interpretability and formal verification scalability in neural networks. With innovative features like truth tables for global interpretation and tractability in SAT encoding TTDCNNs showcase superior performance in robustness verification tasks. While the paper is comprehensive and provides valuable insights addressing the complexity of the content and expanding the discussion on practical implications would strengthen its impact. Overall the innovative approach experimental validation and potential applications of TTDCNNs make this paper a significant contribution to the field of explainable AI and formal verification.", "s5lIqsrOu3Z": " Summary: The paper proposes a new computational framework for learning a closedloop transcription for multiclass multidimensional data. The framework aims to unify the benefits of AutoEncoding and Generative Adversarial Networks (GAN) for learning discriminative and generative representations. The proposed method introduces the concept of linear discriminative representation (LDR) and utilizes a twoplayer minimax game approach to optimize the encoderdecoder mappings. Experimental results on various datasets demonstrate the potential and competitiveness of the proposed framework against existing methods based on GAN VAE or a combination of both.  Strengths:  Conceptual Contribution: The paper introduces a novel framework that unifies AutoEncoding and GAN benefits for learning discriminative and generative representations.  Theoretical Rigor: The paper provides a detailed theoretical background to support the proposed methodology including the concept of LDR transcription and the minimax game formulation.  Experimental Validation: Extensive experiments on various datasets showcase the effectiveness and potential of the framework compared to existing methods like GAN and VAE.  Empirical Results: The comparison with existing generative methods visualization of feature correlations and qualitative analysis of decoded images provide thorough empirical justification for the proposed approach.  Practical Implications: The proposed framework shows promise in both generative and discriminative tasks offering potential applications in realworld data analysis.  Weaknesses:  Mathematical Rigidity: The paper delves into complex mathematical formulations making it challenging for readers unfamiliar with the field to grasp the methodology fully.  Lack of Realworld Applications: While the proposed framework is theoretically sound the paper could benefit from more practical examples or case studies to demonstrate realworld applications and impact.  Clarification Needed: The theoretical discussions and optimization approaches may require further clarification to improve the accessibility and understanding for readers.  Question and Feedback: 1. Could you provide more insights into how the proposed framework can be practically implemented and integrated into realworld applications for data analysis and processing 2. How does the proposed method address challenges related to scalability and computational efficiency especially when dealing with large datasets 3. Have you considered exploring potential collaborations or partnerships to validate the framework in specific industry applications or domains such as image processing or natural language processing 4. Can you elaborate on any potential limitations or constraints of the proposed method especially in scenarios where dataset diversity and complexity are extremely high  Review Summary: Overall the paper presents a novel computational framework for learning discriminative and generative representations showcasing competitive performance compared to existing methods. The empirical validation theoretical underpinnings and the proposed minimax game approach provide a strong basis for the proposed methodology. Clarifications on practical implementations addressing scalability challenges and potential realworld applications would further enhance the impact and relevance of this work. Consequently based on the conceptual contribution theoretical rigor and promising experimental results I recommend acceptance of the paper with minor revisions for clarity and practical implications.", "jFfRcKVut98": " 1) Summary of the Paper: The paper introduces a novel approach called Partial Group equivariant Convolutional Neural Networks (Partial GCNNs) which enhance the traditional group equivariant networks by allowing them to learn partial or full equivariances from data at every layer. The key idea is to adjust the level of equivariance during training by defining a learnable probability distribution over group elements enabling models to restrict equivariance whenever necessary. The paper demonstrates the effectiveness of Partial GCNNs in cases where full equivariance may be harmful outperforming traditional GCNNs. Experimental results on both toy tasks and benchmark datasets validate the approach by showcasing improved performance in scenarios requiring partial equivariance.  2) Strengths and Weaknesses of the Paper:  Strengths:  Innovative Concept: Introduces a novel concept of learning partial equivariances at each layer in convolutional neural networks addressing situations where full equivariance may be detrimental.  Comprehensive Experiments: Thorough experimental validation on both toy tasks and benchmark datasets to showcase the efficiency and effectiveness of Partial GCNNs compared to traditional GCNNs.  Theoretical Foundation: The paper provides a strong theoretical foundation in defining partial group equivariance and enables model adaptability based on learnable probability distributions.  Practical Implementation: It offers a practical implementation of learning partial equivariances directly from data in an endtoend manner emphasizing realworld applicability.  Weaknesses:  Complexity: The method may introduce additional complexity compared to traditional equivariant networks especially in the learning of probability distributions.  Stable Training on Discrete Groups: The paper acknowledges stability issues in training with discrete groups pointing towards the need for more robust training techniques.  Scalability: The scalability of the proposed approach to larger and more complex datasets or different modalities might be a concern that needs to be addressed.  Questions and Feedback to the Author:  Scalability: How scalable is the proposed approach to larger datasets or different modalities beyond the ones experimented on Have you explored the potential limitations in scalability  Interpretability: How do you interpret the impact of partial equivariance on the interpretability of the models especially in complex scenarios where multiple symmetries play a role  Future Work: What are the potential avenues for extending this work to deal with more complex group representations and how could the approach be further generalized for broader applicability in neural network architectures  3) Review Summary: The paper introduces an innovative concept of Partial Group equivariant Convolutional Neural Networks demonstrating the significance of learning partial equivariances from data at each layer to optimize model performance. The comprehensive experiments validate the approach on various tasks and datasets showcasing improved performance in scenarios where traditional group equivariance may lead to suboptimal outcomes. While the paper converges theory and practical implementation effectively scalability and interpretability aspects could be further explored. Overall the innovative approach and experimental validation make a strong case for the adoption of Partial GCNNs in scenarios where full equivariance is not always optimal.", "yOBqNg-CqB0": " 1) Summary of the Paper: The paper reevaluates the performance of the word movers distance (WMD) technique a method for measuring document similarity and compares it with classical baselines such as bagofwords (BOW) and TFIDF. It challenges the original findings that WMD significantly outperforms the baselines and highlights that proper normalization is crucial. The study introduces an analogy between WMD and L1normalized BOW demonstrating similarities in performance and distance values in highdimensional spaces.  2) Strengths and Weaknesses:  Strong Points:  Thorough Reevaluation: The paper conducts a detailed reevaluation of WMD against classical baselines providing valuable insights into the importance of normalization.  Novel Analogy: The analogy between WMD and L1normalized BOW sheds light on the underlying mechanisms and similarities between the two methods.  Methodological Rigor: The study includes experiments comparisons and analyses offering a comprehensive exploration of the topic.  Weak Points:  Misleading Original Evaluations: The paper points out confusing aspects in the original study on WMD indicating potential biases or oversights.  Limited Impact Assessment: While the reevaluation provides critical insights the impact on future studies or applications of WMD may need more discussion.  Need for Further Validation: The findings should be further validated with additional datasets or by other research groups to strengthen the claims made in the paper.  Questions and Feedback:  Clarity on Normalization: Could the paper elaborate more on the implications of different normalization methods for WMD performance  Generalizability: How applicable are the findings to other datasets or document types beyond the ones used in the study  Recommendation for Future Research: What are the potential implications of these findings on the broader field of NLP and document similarity measurement  3) Review Summary: The paper provides a valuable reevaluation of the word movers distance and its performance against classical baselines. The detailed analyses and insightful analogies contribute significantly to the understanding of document similarity measurement techniques. However further validation clarity on the normalization impact and exploration of broader implications in the NLP field are recommended to enhance the studys impact. Overall the rigorous approach and critical findings make this paper a valuable contribution to the evaluation of document similarity methods warranting attention from researchers in the NLP domain.", "r5qumLiYwf9": " Summary: The paper introduces MaGNET a novel method for achieving uniform sampling on the learned manifold of Deep Generative Networks (DGNs) without the need for retraining. By leveraging the concept of affine splines and continuous piecewise affine mappings MaGNET can be applied to any pretrained DGN with CPA nonlinearities. The method is capable of producing samples uniformly distributed on the manifold enhancing distribution precision and decreasing biases as demonstrated through experiments on various datasets and DGN architectures.  Strengths: 1. Novel Contribution: MaGNET introduces a unique approach to address the issue of nonuniform sample distribution in DGNs providing a theoretical foundation and practical implementation for achieving uniform sampling. 2. Theoretical Justification: The paper provides a solid theoretical framework supported by Lemmas and Theorems to explain the methodology behind MaGNET and how it ensures uniform sampling on the learned manifold. 3. Experimental Validation: Extensive experiments conducted on different DGN architectures and datasets showcase the effectiveness of MaGNET in improving distribution precision recall and decreasing biases without the need for retraining. 4. Practical Applications: The paper demonstrates the practical implications of MaGNET including its utility in reducing variance in MonteCarlo estimations rebalancing attributes and improving statistical measures like Fre\u0301chet Inception Distance (FID). 5. Comprehensive Discussion: The paper includes thorough sections discussing the limitations of MaGNET scenarios where uniform sampling may not be suitable and potential directions for future research enhancing the overall quality of the study.  Weaknesses: 1. Clarity of Presentation: The paper although detailed may benefit from improved clarity in presenting complex technical concepts especially for readers less familiar with the specific domain of DGNs and affine splines. 2. Evaluation Metrics: While the paper discusses the impact of MaGNET on various DGN architectures the evaluation could be further enhanced by including additional quantitative measures and comparisons with existing methods to highlight the superiority of MaGNET more explicitly.  Questions and Feedback to the Authors: 1. Could you provide more insight into the computational complexity and efficiency of MaGNET in comparison to other methods for achieving uniform sampling on DGN manifolds 2. How does MaGNET perform in scenarios where the true data manifold is not continuous and how does it handle potential discontinuities in the learned manifold 3. Have you considered applying MaGNET to realworld datasets with inherent biases to study its impact on fairness and data augmentation in practical applications 4. Can you elaborate on the scalability of MaGNET for largescale datasets and its potential for deployment in realtime generative applications 5. In future work how do you plan to address the challenges of ensuring that a trained DGN accurately captures the true underlying data manifold to maximize the effectiveness of MaGNET sampling  Review Summary: The paper presents a compelling methodology MaGNET for achieving uniform sampling on the learned manifold of DGNs offering theoretical foundations and practical implications. The extensive experiments provide strong evidence of the methods efficacy in enhancing distribution precision reducing biases and improving statistical measures. The papers detailed discussion on limitations and future directions adds depth to the study. Overall based on the innovative approach thorough theoretical underpinnings and promising experimental results I recommend the paper for acceptance pending minor revisions to enhance clarity and potentially expand the evaluation metrics for further validation.", "vJb4I2ANmy": "1. Summary: The paper introduces a new data augmentation method called Noisy Feature Mixup (NFM) that combines mixup and noise injection techniques to improve model robustness and generalization. NFM leverages noiseperturbed convex combinations of pairs of data points in input and feature space to enhance the smoothing of decision boundaries and increase model robustness. The paper provides theoretical analysis on NFMs implicit regularization effects and demonstrates its advantages through empirical results showing improved performance on various computer vision benchmark datasets compared to mixup and manifold mixup. 2. Evaluation: Strong Points:  Theoretical Analysis: The paper provides a solid theoretical foundation for NFM showcasing its implicit regularization and robustness benefits through mathematical analysis.  Empirical Results: The empirical evaluation demonstrates the effectiveness of NFM across a range of computer vision tasks showcasing improved performance on clean data and enhanced robustness to various perturbations.  Novelty: NFM combines mixup and noise injection methods in a novel way providing a potentially powerful technique for improving model robustness. Weak Points:  Complexity: The paper contains intricate mathematical formulations and technical details which might be challenging for readers with limited mathematical background to follow.  Reproducibility: Detailed hyperparameters and implementation specifics for training with NFM are not emphasized making reproducibility and practical implementation more challenging. Question and Feedback to the Author:  Question: Could you provide more insights into how NFM compares to other stateoftheart data augmentation techniques such as cutmix puzzle mixup and noisy mixup in terms of model performance and robustness  Feedback: Adding a comparison with more recent data augmentation techniques and considering realworld applications beyond computer vision tasks could strengthen the practical relevance and impact of NFM. 3. Review Summary: The paper introduces an innovative data augmentation method Noisy Feature Mixup (NFM) that combines mixup and noise injection techniques for improving model robustness and generalization in computer vision tasks. The theoretical analysis elucidates the implicit regularization effects of NFM while empirical results demonstrate its superiority over existing methods. Despite the complexity of the theoretical framework the unique combination of mixup and noise injection in NFM presents a promising approach to enhancing model robustness. Addressing reproducibility concerns and expanding comparisons with stateoftheart techniques will further enhance the impact and applicability of NFM. Overall given the strong theoretical basis and promising empirical results the paper merits publication. LocalizedMessage: The paper presents a novel data augmentation technique Noisy Feature Mixup (NFM) combining mixup and noise injection for enhanced model robustness and generalization in computer vision tasks. Despite theoretical complexity the methodology shows promise necessitating additional comparisons and practical considerations for wider impact.", "q4tZR1Y-UIs": "Summary: The paper proposes a novel method called Curriculum Self Play (CuSP) for automatic curriculum generation in reinforcement learning specifically focusing on training generalpurpose reinforcement learning agents that can solve a wide variety of goals efficiently. CuSP is a multiagent system with two offpolicy learners (Alice and Bob) and two regretmaximizing goal generators (GA and GB). The method aims to balance progressive exploration and anticatastrophic exploitation by generating challenging yet feasible goals for the learners. CuSP leverages a regret objective offpolicy entropyregularized optimization inspired by Soft Actor Critic (SAC) and dynamic regret updates to track the learners current abilities and induce a curriculum that aids in sample efficiency and generalization to novel outofdistribution goals. Strengths: 1. Innovative Approach: CuSP introduces a unique method for automatic curriculum generation in reinforcement learning focusing on progressive exploration and anticatastrophic exploitation. 2. Balanced Design: The symmetric setup of CuSP with two offpolicy learners and two regretmaximizing goal generators provides an effective balance between cooperation and competition. 3. Theoretical Foundation: The paper is wellsupported with a detailed background clear problem setup and theoretical justification for the proposed method. 4. Empirical Validation: The method is empirically validated across a suite of continuous control tasks demonstrating improved sample efficiency and generalization to outofdistribution goals. 5. Comprehensive Analysis: The paper conducts thorough experiments ablations and comparisons with existing methods providing a strong basis for evaluating the effectiveness of CuSP. Weaknesses: 1. Complexity: The method description may be challenging to follow for readers unfamiliar with reinforcement learning concepts and terminology. 2. Evaluation Metrics: While the paper demonstrates the effectiveness of CuSP in terms of sample efficiency and generalization additional metrics related to convergence speed and scalability could provide a more comprehensive evaluation. 3. Limitation Acknowledgment: The paper briefly mentions limitations and future work but a more indepth discussion on potential challenges or shortcomings of the method could enhance the papers credibility. Questions: 1. How does CuSP address potential issues related to scalability and convergence in largerscale or more complex environments 2. Can you provide insights into the computational requirements and training time associated with implementing CuSP in practical RL applications 3. Have you considered any realworld applications or scenarios where CuSP could be particularly beneficial and if so how does the method generalize to such domains Feedback: Overall the paper presents a wellstructured and innovative approach to automatic curriculum generation in reinforcement learning. The thorough theoretical background detailed explanation of CuSP and extensive experimental validation contribute to the papers strength. Addressing potential scalability challenges incorporating additional evaluation metrics and providing more detailed limitations could further enhance the paper. Consider providing practical insights and realworld applications to demonstrate the broader impact and applicability of CuSP. Great work overall Recommendation: I recommend acceptance with minor revisions to address the highlighted weaknesses and incorporate the suggested feedback for further strengthening the paper. The innovative nature of CuSP and the comprehensive evaluation make it a valuable contribution to the reinforcement learning community.", "wClmeg9u7G": " Summary: The paper presents new distributed methods MASHA1 and MASHA2 for solving variational inequalities and saddle point problems using compressed communication. These methods address the issue of communication bottlenecks in distributed training specifically for highdimensional and overparameterized models. The theoretical foundations of the methods are grounded in the use of unbiased and contractive compressors. The paper empirically validates the methods through experimental setups involving a standard bilinear minmax problem and largescale distributed adversarial training of transformers.  Strengths: 1. Novel Contribution: Introduces theoretically grounded distributed methods for solving variational inequalities and saddle point problems with compressed communication addressing a critical issue in distributed training. 2. Theoretical Analysis: Provides detailed theoretical complexity results convergence guarantees and algorithmic designs for the proposed methods showcasing a comprehensive understanding of the problem. 3. Experimental Validation: Empirically validates the conclusions through experiments including a comparison with existing methods enhancing the credibility of the proposed approaches. 4. Detailed Methodology: Wellstructured algorithmic descriptions and theoretical analysis provide clarity on the proposed methods facilitating reproducibility and implementation. 5. RealWorld Applications: Demonstrates the practical relevance of the methods through application in adversarial training of largescale models showcasing potential impact in machine learning scenarios.  Weaknesses: 1. Complexity: The paper delves into advanced mathematical and algorithmic concepts which may pose challenges for readers not deeply familiar with the subject matter. 2. Limited Detail: Some important methodological details are provided only in the appendix potentially hindering a full understanding of the proposed methods without referencing supplementary materials. 3. Sparse Explanation: The experimental section particularly in the transformer training scenario could benefit from a more detailed explanation of the setup and results for better comprehension.  Questions and Feedback to the Author: 1. Can you elaborate on the specific challenges faced during the practical implementation of MASHA1 and MASHA2 in the experimental setups Understanding the practical considerations can enhance the applicability of your methods. 2. Could you provide insights into potential avenues for further research or extensions of your proposed methods in addressing other machine learning optimization problems or scenarios 3. Consider providing additional insights into the relevance of the proposed methods in current trends in distributed computing and machine learning highlighting the potential impact on future research directions.  Review Summary: Overall the paper presents a significant contribution in the domain of distributed optimization for solving variational inequalities and saddle point problems. The theoretical foundations algorithmic designs and empirical validations collectively support the effectiveness and potential impact of the proposed methods. While the paper showcases strong methodology and results addressing certain aspects like accessibility of detailed methodology and practical implementation challenges would further enhance its value. Given the relevance of the topic in modern machine learning applications I recommend this paper for publication following revisions to address the identified weaknesses and queries.", "wYqLTy4wkor": " Paper Summary The paper introduces the problem of curriculuminduced covariate shift in reinforcement learning (RL) and proposes a solution through the development of a method called SampleMatched PLR (SAMPLR). It addresses the issue of suboptimal policies that arise when adaptive curricula in RL lead to a mismatch between the training and testing distributions particularly in settings featuring aleatoric uncertainty. The method corrects bias by aligning advantage estimates to the groundtruth distribution over aleatoric parameters resulting in a minimaxregret UED method with Bayesoptimal guarantees.  Strengths of the Paper 1. Innovative Problem Formulation: The paper introduces a novel problem definition of curriculuminduced covariate shift in RL shedding light on the challenges imposed by adaptive curricula. 2. Thorough Background: The paper provides a comprehensive background on Unsupervised Environment Design (UED) methods and the problem of curriculum learning in RL setting a strong foundation for the proposed solution. 3. Formalization and Solution: The authors present a clear and formalized problem statement around CICS and propose a method SAMPLR to address this issue in a principled manner. 4. Experimental Evaluation: The paper includes detailed experiments demonstrating the effectiveness of SAMPLR in challenging environments showcasing significant improvements over domain randomization and PLR methods. 5. Theoretical Analysis: The paper includes theoretical analysis and proofs to support the efficacy and robustness of SAMPLR in achieving Bayesoptimal policies with respect to the groundtruth distribution.  Weaknesses of the Paper 1. Complexity: The paper contains intricate technical details and formalisms that may be challenging for readers less familiar with reinforcement learning concepts. 2. Limited RealWorld Applications: While the proposed method shows promise in experimental environments the direct applicability to realworld settings might need further exploration and validation. 3. Lack of Comparative Analysis: While experimental results are provided a more detailed comparison with existing methods in terms of computational efficiency and scalability could enhance the papers impact. 4. Clarity in Presentation: Some sections of the paper could benefit from clearer explanations and better organization to improve readability and understanding for a wider audience.  Questions for the Authors 1. How does the proposed SAMPLR method compare to other stateoftheart methods in terms of computational efficiency and scalability 2. Have you considered applying SAMPLR to other RL domains beyond the ones tested in your experiments If so what are your expectations for performance in different types of environments 3. Can you provide insights into the potential limitations or challenges of deploying SAMPLR in practical RL applications with more complex and dynamic environments  Feedback for the Authors 1. Consider discussing potential realworld applications and implications of SAMPLR to enhance the practical relevance of the work. 2. Clarify and simplify technical aspects where possible to make the paper more accessible to a broader audience. 3. Extend the discussion on scalability and generalizability of SAMPLR to diverse RL settings to provide a more comprehensive evaluation of the method. Overall the paper presents innovative research on addressing curriculuminduced covariate shift in RL through the SAMPLR method backed by theoretical analysis and experimental results. With some enhancements in presentation and broader applicability considerations this work has the potential to make a significant contribution to the field of reinforcement learning.", "rhDaUTtfsqs": " PeerReview of \"Curriculum Learning for Stable and Efficient Training of LargeScale Autoregressive Language Models\"  Summary: The paper investigates the training instability issue faced by autoregressive language models focusing on GPT2 models with 117M and 1.5B parameters. The study explores the impact of model sizes sequence lengths batch sizes and learning rates on training stability and efficiency. The authors propose a curriculum learningbased approach to enhance pretraining convergence speed demonstrating that it enables stable training with larger batch sizes and learning rates. The study shows that curriculum learning reduces required training tokens and wall clock time significantly while maintaining or improving validation perplexity and zeroshot evaluation results.  Strong Points: 1. Comprehensive Investigation: The paper provides a thorough analysis of the training stabilityefficiency dilemma in autoregressive models. 2. Novel Approach: The use of curriculum learning as a regularization method to address training instability and improve efficiency in largescale autoregressive language models is innovative. 3. Experimental Rigor: The evaluations are wellstructured with clear comparisons between baseline and curriculum learning approaches providing detailed insights into convergence speed required tokenstime and zeroshot evaluation results. 4. Impactful Results: The findings indicate significant improvements in training stability and efficiency with substantial reductions in training tokens and wall clock time while achieving better validation perplexity and evaluation results.  Weak Points: 1. Clarity in Methodology: Some sections particularly on the design and implementation of curriculum learning could benefit from a clearer explanation of the proposed approach and the reasoning behind specific choices. 2. Comparison with Related Works: While the comparison with related works is valuable providing more detailed insights into the limitations of existing approaches and how the proposed method overcomes them would strengthen the discussion. 3. Data Visualization: While the figures and tables presented are informative clearer labeling and additional analysis on specific trends or patterns observed in the data would enhance the visual representation.  Feedback and Questions for the Authors: 1. Validation and Robustness: Have you considered testing the proposed curriculum learning approach on other autoregressive models or evaluating its performance under different hyperparameter configurations to ensure its robustness and generalizability 2. Scalability: How scalable is the curriculum learning approach in terms of accommodating even larger model sizes or diverse training datasets Are there any limitations or challenges when applying this method to extremely largescale language models 3. Future Work: In the conclusion you mention the potential applications of curriculum learning in other domains such as computer vision. Could you elaborate on potential research directions or extensions of this work in different domains beyond autoregressive language models  Review Summary: The paper presents a wellstructured investigation into training instability challenges faced by autoregressive language models proposing a novel curriculum learningbased approach to enhance training stability and efficiency. The experimental results demonstrate significant improvements in convergence speed reduced training tokenstime and better evaluation results. While the paper provides valuable insights and a promising solution some sections could benefit from further clarification and indepth analysis. Overall the paper offers important contributions to the field of largescale language model training and paves the way for future research in the application of curriculum learning as a regularization method. Based on the strengths of the study and the impactful results obtained I recommend acceptance of the paper pending minor revisions to enhance clarity and detailed analysis in certain sections.", "rJvY_5OzoI": " Summary of the paper: The paper introduces MultiCritic Actor Learning (MultiCriticAL) as a solution to the negative interference between tasks in ActorCritic multitask reinforcement learning (MTRL). Unlike using a single shared critic for multiple tasks MultiCriticAL maintains separate critics for each task while training a single multitask actor. This approach explicitly distinguishes between tasks and eliminates the need for critics to learn to do so thus mitigating interference between taskvalue estimates. The authors test MultiCriticAL in the context of multistyle learning where agents are trained to exhibit different behavior styles and report significant performance gains over singlecritic baselines. The method is evaluated in simulated environments and realworld applications such as adapting behavior styles in EA\u2019s UFC game.  Strengths: 1. Innovation: The paper introduces a novel MultiCriticAL framework which addresses the negative interference between tasks in MTRL and achieves substantial performance gains. 2. Comprehensive Evaluation: The authors evaluate the proposed method across various multistyle RL problems providing a thorough analysis of its effectiveness. 3. Realworld Application: By showcasing the use of MultiCriticAL in a practical scenario like the UFC game the paper demonstrates the applicability of the proposed approach.  Weaknesses: 1. Baseline comparison: While the paper compares MultiCriticAL with MTSAC and MTPPO which are common baselines it would be beneficial to compare with stateoftheart MTRL methods to strengthen the argument for the proposed approach. 2. Computational Cost: The authors mention that MultiCriticAL requires increased computational resources during training which could be seen as a limitation in resourceconstrained environments. Further exploration on computational efficiency would be valuable.  Questions: 1. How does MultiCriticAL handle scenarios where the number of tasks or styles becomes very large Have you encountered any scalability issues in your experiments 2. Can you provide insights into the generalizability of MultiCriticAL across diverse environments and tasks beyond the ones evaluated in the paper 3. How sensitive is MultiCriticAL to hyperparameter tuning and are there any guidelines or best practices recommended for applying this method to new RL problems  Feedback to the Authors:  Strengthen the comparison: Consider comparing MultiCriticAL with additional stateoftheart MTRL methods to provide a more comprehensive analysis.  Discuss computational efficiency: Provide a more detailed analysis or potential strategies to improve the computational efficiency of MultiCriticAL.  Generalizability and scalability: Discuss the generalizability and scalability of MultiCriticAL to diverse environments and larger numbers of tasks in future works.  Review Summary: The paper introduces MultiCritic Actor Learning (MultiCriticAL) as a solution to negative interference in multitask reinforcement learning. The proposed framework is innovative and demonstrates significant performance gains in multistyle learning scenarios. However further evaluation against stateoftheart methods exploration on computational efficiency and discussions on scalability and generalizability would enhance the papers impact. Overall considering the strengths and the potential for advancements I recommend this paper for acceptance pending the requested revisions.", "kWuBTQmkO8_": " Summary: The paper introduces MixRL a data augmentation metalearning framework tailored for regression tasks that aims to improve model performance by selectively mixing examples based on certain data or label distances. The framework utilizes Monte Carlo policy gradient reinforcement learning to determine for each example how many nearest neighbors it should be mixed with for optimal performance. MixRL is designed to address the limitations of existing Mixup techniques for regression due to the continuous nature of the label space. Experimental results on synthetic and real datasets demonstrate that MixRL outperforms stateoftheart data augmentation baselines for regression tasks.  Strengths:  Innovative Approach: MixRL proposes a novel approach by combining Mixup techniques with reinforcement learning specifically tailored for regression tasks addressing a gap in the current literature.  Empirical Validation: The paper provides clear experimental results on both synthetic and real datasets to support the effectiveness of MixRL in improving regression performance.  Comprehensive Explanations: The paper offers detailed explanations of the methodology including the assumptions made theoretical underpinnings and algorithmic details which enhance the readers understanding of the proposed framework.  Weaknesses:  Complexity: The paper dives deep into technical details potentially making it challenging for readers with limited expertise in reinforcement learning or regression methodologies to grasp the core concepts.  Evaluation Metrics: While the paper discusses RMSE and R2 as evaluation metrics additional metrics or comparisons with other regression techniques could provide a more comprehensive assessment of MixRLs performance.  Questions for the Authors: 1. Could you provide more insights into the computational complexity of the MixRL framework in terms of training time and resource requirements especially when scaling to larger datasets 2. How does the choice of the Mixup ratio (\u03bb) impact the performance of MixRL and are there scenarios where tuning this parameter leads to better results 3. Have you considered the potential ethical implications of data augmentation particularly in terms of bias introduction and how does MixRL address or mitigate these concerns  Feedback:  Clarity: To improve readability consider breaking down complex concepts into more digestible sections for a broader audience while maintaining the technical depth.  Comparison: Expand the comparison with existing regressionspecific data augmentation techniques to highlight the unique contributions and advantages of MixRL.  Future Work: Including discussions on potential applications extensions or further research directions stemming from MixRLs framework could enhance the papers impact and relevance.  Review Summary: The paper presents a pioneering framework MixRL that effectively addresses the challenges of data augmentation for regression tasks through a novel combination of Mixup techniques and reinforcement learning. The empirical results demonstrate the superior performance of MixRL relative to existing baselines providing strong evidence of its efficacy. While the technical depth is commendable addressing potential ethical considerations and refining the presentation for broader accessibility would strengthen the papers impact and appeal to a wider audience. Overall I recommend accepting the paper for publication pending minor revisions and enhancements in presentation and discussion aspects.", "vXGcHthY6v": " Peer Review of \"Invariance through Inference: Improving Testtime Performance of Agents in Unknown Perceptual Environments\" 1. Summary: The paper presents a novel approach called \"Invariance through Inference\" for enhancing the testtime performance of agents in deployment environments with unknown perceptual variations. The method refrains from producing invariant visual features through interpolation and instead focuses on unsupervised learning to adapt at deploymenttime. Through experiments in a robotics environment the paper demonstrates the effectiveness of the approach in scenarios involving changes in camera poses and lighting conditions. 2. Strengths:  Innovative Approach: The concept of Invariance through Inference is novel and addresses an important challenge in reinforcement learning.  Comprehensive Experiments: The paper provides detailed experiments on the proposed method comparing it with existing techniques and showcasing its performance in various scenarios.  Clear Problem Formulation: The problem formulation and methodology are wellstructured making it easy to understand and replicate the experiments.  Indepth Analysis: The paper includes indepth discussions on related works problem formulation and the significance of the proposed approach. 3. Weaknesses:  Clarity in Algorithm Description: The detailed description of the algorithm and its implementation could be broken down into more digestible parts for better understanding especially for readers less familiar with the subject.  Limited Comparative Analysis: While the paper compared the proposed method with existing techniques a more detailed analysis of the performance metrics tradeoffs and limitations in different scenarios would enhance the discussion.  Extension Beyond Visual RL: The paper focuses largely on imagebased observations in robotic environments expanding the discussion to other domains would broaden the applicability of the proposed method.  Question and Feedback to the Author: 1. Can you provide a more approachable explanation of the Invariance through Inference algorithm for readers less experienced in reinforcement learning 2. How does the proposed method perform in environments with highdimensional observation spaces beyond imagebased scenarios 3. What further experiments or analyses could strengthen the validation of the approach especially in more complex or diverse environments  Review Summary: The paper introduces a promising method Invariance through Inference that demonstrates significant improvements in adaptation scenarios without access to deploymenttime rewards. The detailed experiments and comparisons with existing techniques highlight the potential of the proposed approach. Strengthening the algorithms explanation expanding the discussion to diverse environments and conducting additional experiments could further solidify the validation of the method. Overall I recommend acceptance with minor revisions to improve clarity and depth in the presentation.", "s-b95PMK4E6": " Summary of the Paper: The paper introduces a hierarchical modular approach named HACR (Hierarchical Approach for Compositional Reasoning) for robotic agents to perform complex tasks involving navigating an environment and interacting with objects based on natural language directives. The proposed agent operates at three hierarchical levels  Policy Composition Controller (PCC) Master Policy (MP) and Interaction Policies (IP) to efficiently manage subgoals and achieve stateoftheart performance on the ALFRED benchmark.  Strengths of the Paper: 1. Hierarchical Modular Approach: The hierarchical structure allows for the disentanglement of highlevel subgoals and precise handling of navigation and interaction tasks which enhances performance. 2. Clear Justification: The paper provides a clear justification for each component of the proposed method detailing how it contributes to tackling the challenges of the task. 3. Novel Contributions: Introducing the Object Encoding Module Loop Escape Module and the hierarchical policy composition controller adds novelty to the existing research in the field. 4. StateoftheArt Performance: The agent achieves stateoftheart results on the ALFRED benchmark indicating the effectiveness of the proposed approach. 5. Comprehensive Experimental Analysis: The paper includes thorough experimental analysis comparisons with prior works and ablation studies to showcase the significance of each module.  Weaknesses of the Paper: 1. Complexity: The proposed hierarchical model may be complex to implement and require significant computational resources which could limit its practical adoption. 2. Limited Discussion on Failure Cases: While the success rates and performance metrics are highlighted discussing failure cases or situations where the model struggles could provide a more comprehensive analysis. 3. Limited RealWorld Validation: It would be beneficial to validate the proposed approach in realworld scenarios to assess its practical applicability outside of benchmark settings.  Questions and Feedback to the Author: 1. Could you provide more insights into the scalability of the proposed hierarchical modular approach in realworld scenarios with varying environments and tasks 2. How does the proposed Object Encoding Module handle cases where the visual information may not align perfectly with the language instructions provided 3. Have you considered scenarios with dynamic or changing environments and how the HACR model adapts to such situations  Review Summary: The paper introduces a novel hierarchical modular approach for robotic agents addressing complex tasks involving navigation and object interaction based on natural language directives. The proposed HACR model demonstrates significant performance improvements and achieves stateoftheart results on the ALFRED benchmark. The paper presents a clear structure justifications for each component and comprehensive experimental analysis. While the complexity and practical applicability of the model could be further explored the papers detailed approach and impressive results make it a valuable contribution to the field of robotic agents performing household tasks. I recommend acceptance after minor revisions to address the identified weaknesses and questions.", "ydopy-e6Dg": " Summary: The paper introduces a novel framework named iBOT which focuses on Masked Image Modeling (MIM) for training Vision Transformers. The framework utilizes a selfdistillation approach with an online tokenizer to achieve stateoftheart performance on various downstream tasks including image classification object detection instance segmentation and semantic segmentation. iBOT demonstrates a significant improvement in feature representation achieving high accuracy on the ImageNet1K dataset and showing robustness against common image corruptions.  Strengths: 1. Innovative Contribution: The introduction of MIM for Vision Transformers and the development of iBOT framework represent a novel and innovative approach to improving image modeling tasks. 2. Performance: The results presented indicate that iBOT outperforms previous methods in various downstream tasks demonstrating its effectiveness in enhancing feature representation and achieving high accuracy. 3. Technical Rigor: The paper is technically sound using a selfsupervised methodology with selfdistillation and an online tokenizer to enhance visual semantics leveraging existing knowledge distillation techniques. 4. Comprehensive Evaluation: The comprehensive evaluation on various downstream tasks including classification object detection instance segmentation and semantic segmentation provides a thorough understanding of the effectiveness of iBOT. 5. OpenSource: Availability of code and models on GitHub promotes reproducibility and transparency in research.  Weaknesses: 1. Lack of Comparison: While the paper compares iBOT with DINO and BEiT in some instances a broader comparison with a wider range of existing methods would enhance the understanding of iBOTs performance.  Questions and Feedback: 1. Validation: Can you provide more insights into the experimental setup for validation and how the performance metrics were selected 2. Scalability: How scalable is the iBOT framework to larger datasets beyond ImageNet1K or to larger Vision Transformer models 3. Generalization: How does iBOT perform when applied to datasets with different characteristics or domains compared to ImageNet1K 4. Impact of Hyperparameters: Could you elaborate on the sensitivity of the framework to hyperparameters and the process used to tune them effectively  Review Summary: The paper presents a novel framework iBOT for Masked Image Modeling that demonstrates significant advancements in Vision Transformers performance. The innovative approach coupled with the selfsupervised methodology and online tokenizer enhances feature representation and robustness in various downstream tasks. While the paper shows strong technical rigor and a comprehensive evaluation further comparison with a wider range of existing methods could strengthen the analysis. Considering the innovative contributions and promising results I recommend this paper for publication pending minor revisions based on the feedback provided.", "psNSQsmd4JI": " Summary of the Paper: The paper proposes a containerized learning framework for distributed valuebased multiagent reinforcement learning (MARL). The framework addresses challenges such as demanding data transfer interprocess communication and efficient exploration in multiagent settings. The container architecture consists of multiple containers each interacting with environment instances a local learner and a buffer. A multiqueue manager is used to avoid blocking and prioritize data transfer to a centralized learner. The framework encourages diverse behavior among containers to improve exploration efficiency. Experimental results show superior performance on Google Research Football and StarCraft II micromanagement benchmarks compared to stateoftheart nondistributed MARL algorithms.  Strong Points: 1. Innovative Framework: The paper introduces a novel containerized architecture for distributed MARL addressing key challenges efficiently. 2. Empirical Results: The experimental results demonstrate the effectiveness of the proposed framework on challenging benchmarks showcasing significant performance improvements. 3. Effective Data Transfer: Prioritizing data transfer based on highpriority trajectories enhances learning efficiency and reduces computational overhead. 4. Diversity Encouragement: Encouraging diverse exploration among containers leads to improved performance and efficient learning in large search spaces.  Weak Points: 1. Complexity: The paper introduces multiple components and mechanisms which may be difficult to implement and replicate without clear detailed guidance. 2. Limited Comparison: While comparisons with baselines are provided a more extensive comparison with other distributed MARL methods could enhance the papers credibility. 3. Ablation Studies Interpretation: Further explanation and insights into the results of the ablation studies could strengthen the understanding of the contributions of the container architecture and diversity encouragement.  Questions  Feedback to the Author: 1. Could you provide additional insights into the scalability of the proposed framework in terms of largescale applications and increased computational resources 2. How does the diversity encouragement mechanism impact the convergence and stability of training across different environments and scenarios 3. Have you considered the generalizability of the proposed framework to different types of multiagent tasks beyond the benchmarks used in the experiments  Review Summary: The paper presents a wellstructured and innovative containerized learning framework for distributed MARL addressing critical challenges in data transfer communication and exploration. Empirical results showcase significant performance improvements over existing methods on challenging benchmarks. While the paper demonstrates the effectiveness of the proposed framework clearer explanations of complex components and additional insights into the scalability and generalizability of the approach could further enhance its impact. Overall the paper contributes a valuable advancement in the field of distributed MARL and merits publication with minor revisions and clarifications.", "xVlPHwnNKv": "1) Paper Summary: The paper presents a novel approach called Fast Stackelberg Deep Deterministic Policy Gradient (FSDDPG) aimed at improving the efficiency of Stackelberg ActorCritic learning schemes. The proposed method removes timeconsuming indirect gradient terms and adopts a block diagonal approximation technique to enhance computational speed. Experimental results demonstrate that FSDDPG outperforms stateoftheart methods in terms of average returns within acceptable training times. 2) Strong Points: Strengths:  Innovation: The paper introduces a novel approach FSDDPG that addresses the limitations of current Stackelberg actorcritic methods by removing timeconsuming terms and adopting approximation techniques.  Theoretical Analysis: The theoretical analysis provided in the paper showcases the convergence properties of FSDDPG demonstrating its advantages over existing methods.  Experimental Results: The experiments reveal that FSDDPG outperforms other methods in various environments demonstrating the effectiveness of the proposed approach.  Efficiency: The paper emphasizes the computational efficiency of FSDDPG by reducing time complexities improving convergence rates and balancing computational speed with performance.  Comparison with Baselines: The comparison with other baselines including DDPG and TD3 showcases the superior performance of FSDDPG in terms of average returns. Weak Points: Weaknesses:  Complex Terminology: The paper includes complex technical terms that may be challenging for readers to understand without a strong background in the field.  Mathematical Proofs: Some mathematical proofs provided in the paper may require additional clarification or detailed explanations for readers to comprehend.  Experimental Details: While the experimental results are promising additional details regarding hyperparameters initialization settings and specific conditions of the experiments could enhance the comprehensibility of the paper.  Visualization: More visual aids such as diagrams or graphs could assist in explaining the theoretical concepts and experimental outcomes more effectively.  Applicability: The paper focuses on specific environments and scenarios limiting the generalizability of the proposed approach to broader applications. Question and Feedback to the Author: 1. Can you provide more insights into the implementation details of FSDDPG including any specific considerations or challenges faced during the experimental setup 2. How does FSDDPG perform in more complex or challenging environments and what are the potential limitations or drawbacks of the proposed approach in such scenarios 3. Further discussions on the potential realworld applications and practical implications of FSDDPG would provide valuable insights into the broader impact of the research. 4. Could you elaborate on any future directions for research or potential enhancements to FSDDPG to further improve its performance or address specific shortcomings identified in the study 3) Review Summary: The paper presents a novel Fast Stackelberg Deep Deterministic Policy Gradient (FSDDPG) approach to improve the efficiency of Stackelberg actorcritic learning schemes. The proposed method showcases promising results in terms of average returns and computational speed outperforming stateoftheart methods in various environments. While the paper demonstrates innovation and provides theoretical analyses supporting the effectiveness of FSDDPG some aspects such as complex terminology and limited experimental details could be further elaborated to enhance understanding and applicability. Overall the paper makes a significant contribution to the field of actorcritic reinforcement learning highlighting the potential of FSDDPG as a promising approach for future research and applications.", "pfNyExj7z2": "Peer Review of \"Vectorquantized Image Modeling for Unsupervised Learning Image Generation and Understanding\" 1. Summary: The paper introduces a novel approach called Vectorquantized Image Modeling (VIM) for image synthesis and understanding tasks. It involves two stages: Image Quantization using ViTVQGAN and Vectorquantized Image Modeling using Transformer. The proposed improvements in VQGAN architecture and codebook learning enhance efficiency and reconstruction quality. The results on ImageNet demonstrate significant improvements in IS and FID scores over existing methods. Furthermore the unsupervised learning evaluation shows competitive linearprobe accuracy with stateoftheart models. 2. Strengths:  Innovative Approach: Introducing VIM for image tasks based on language model pretraining is innovative and shows promising results.  Improvements Over Existing Methods: The enhancements in VQGAN architecture and codebook learning effectively improve efficiency and reconstruction fidelity.  Comprehensive Evaluation: The paper provides a detailed evaluation of the proposed approach on various datasets and tasks showcasing the effectiveness of VIM.  Comparative Analysis: Comparison with existing models like iGPT and VQGAN demonstrates the superiority of VIM in terms of performance and efficiency. 3. Weaknesses:  Ethical Considerations: While the paper briefly discusses potential biases in datasets and model correlations a more indepth analysis of ethical implications and proposed mitigations would be beneficial.  Model Interpretability: Further insights into the interpretability of the pretrained VIM models could add value to understanding how they encode and process information. 4. Questions and Feedback to the Author:  Clarification on Implementation: Providing more insights into the implementation details of model training hyperparameters tuning and codebook learning process would be beneficial for reproducibility.  Generalization and Robustness: How does the proposed VIM approach perform on other datasets beyond ImageNet and how does it handle unseen data or domain shifts 5. Review Summary: The paper presents a novel Vectorquantized Image Modeling approach VIM for image tasks showcasing significant improvements in efficiency reconstruction quality and task performance. The detailed evaluation and comparison with existing models highlight the superiority of VIM. Despite the commendable results addressing ethical considerations and providing more insights into model interpretability would strengthen the paper. Overall I recommend acceptance with minor revisions to augment the ethical considerations and implementation details. (Note: The review is based on the provided abstract. For a complete review the full paper needs to be considered.)", "o_HsiMPYh_x": " Summary of the Paper: This paper investigates methods for predicting target domain accuracy using only labeled source data and unlabeled target data. The proposed method Average Thresholded Confidence (ATC) calculates a threshold on the models confidence using labeled source data and predicts accuracy on the target domain as the fraction of unlabeled target points that exceed that threshold. The paper shows that ATC outperforms existing methods across various model architectures distribution shifts and datasets including WILDS ImageNet BREEDS CIFAR and MNIST. Theoretical analysis reveals the challenges in accurately estimating accuracy without additional assumptions. The paper also presents empirical evaluations on synthetic and natural shifts demonstrating the effectiveness of ATC. Additionally the paper introduces a toy model to illustrate the underlying principles behind ATCs performance.  Strengths: 1. Innovative Methodology: Proposing a novel method ATC for estimating target domain accuracy using unlabeled target data sets this paper apart. 2. Thorough Evaluation: The paper conducts comprehensive experiments across multiple datasets model architectures and distribution shifts to demonstrate the superiority of ATC. 3. Theoretical Analysis: The theoretical foundation of the problem including the impossibility results and the conditions under which accuracy estimation can be achieved adds depth to the research. 4. Reproducibility Emphasis: Providing detailed information for reproducibility enhances the credibility and transparency of the study.  Weaknesses: 1. Lack of Code Availability: While the paper promises to release the code availability at the time of publication can enhance reproducibility. 2. Needs Further Analysis: The paper mentions that ATC fails on certain types of distribution shifts but does not provide indepth exploration of these failure cases. 3. Complexity: The paper covers a wide range of topics making it dense and potentially challenging for readers to grasp all aspects fully.  Questions for the Authors: 1. How does the ATC method handle scenarios where the shift in distributions is extreme or the target dataset drastically differs from the source dataset 2. Are there realworld applications where ATC may not be suitable and if so how can the method be adapted to perform better in such scenarios 3. Have you considered exploring the impact of various hyperparameters or model tweaks on the performance of ATC to provide insights into its robustness  Feedback for the Authors:  To enhance the practical applicability of the paper further exploration of scenarios where ATC may not perform optimally could be valuable.  Providing a deeper dive into the failure cases can improve the understanding of the limitations of the proposed method.  Consider highlighting the implications of the theoretical findings on the practical implementation of the method in realworld machine learning deployments.  Review Summary: This paper presents a novel method ATC for estimating target domain accuracy in machine learning settings using only labeled source data and unlabeled target data. The papers strengths lie in its innovative methodology thorough evaluation theoretical analysis and emphasis on reproducibility. However further exploration of potential failure scenarios and indepth analysis of the methods limitations are suggested areas for improvement. Overall the paper contributes significantly to the field of accurate model performance estimation in the presence of distribution shifts and with some enhancements it can make a significant impact in realworld applications.", "vdKncX1WclT": " Summary of the Paper The paper introduces a novel attack technique Neuronlevel Backdoor Attack (NeuBA) which exploits the vulnerability of pretrained models to malicious behavior on triggerembedded instances without prior knowledge of downstream tasks. The study demonstrates the effectiveness of NeuBA on both Natural Language Processing (NLP) and Computer Vision (CV) pretrained models highlighting the potential security threat posed by backdoor attacks in the widely adopted pretrainingthenfinetuning paradigm.  Strong Points  Innovative Approach: The paper introduces a novel attack technique NeuBA which extends the scope of backdoor attacks to a more general setting without prior task information.  Comprehensive Experiments: The study evaluates the vulnerability of various NLP and CV pretrained models across multiple tasks demonstrating the efficacy of NeuBA against finetuned models.  Thorough Analysis: The paper provides detailed analysis on various factors affecting NeuBA such as trigger selection number of triggers and batch normalization enhancing the understanding of the attack method.  Effective Defense Methods: The study explores diverse defense methods including FinePruning Neural Cleanse and Neural Attention Distillation to resist NeuBA providing actionable strategies for improving model robustness.  Weak Points  Theoretical Clarity: The paper could benefit from clearer explanations regarding the technical aspects of NeuBA particularly in terms of the attack mechanism and its implications for different types of pretrained models.  Limited Generalizability: While the study offers valuable insights into the vulnerability of pretrained models to backdoor attacks more emphasis on the generalizability of the findings across diverse datasets and model architectures would enhance the papers impact.  Experiment Visualization: Visual representations or examples of trigger designs and attack scenarios could help readers better understand the attack mechanism and its practical implications.  Questions and Feedback to the Author 1. Can you provide more insights into the practical implications of NeuBA in realworld scenarios especially with regards to potential security risks and mitigation strategies for practitioners 2. How do you envision future research leveraging NeuBA to enhance the security and robustness of pretrained models particularly in the context of emerging deep learning applications and architectures 3. Could you elaborate on the scalability of NeuBA to largescale models and datasets and its adaptability to different types of pretrained models beyond those evaluated in the paper  Review Summary The paper presents a significant contribution to the understanding of backdoor attacks in deep learning systems introducing a novel attack method NeuBA that showcases the susceptibility of pretrained models to malicious manipulations without prior task knowledge. The extensive experiments on NLP and CV tasks demonstrate the effectiveness of NeuBA and highlight the urgent need for robust defense mechanisms to mitigate such security threats. Despite some areas for improvement in theoretical clarity and generalizability the papers innovative approach thorough analysis and insightful defense strategies make it a valuable addition to the field of adversarial machine learning. I recommend acceptance of the paper with minor revisions to enhance clarity and generalizability.", "lyzRAErG6Kv": " Summary of the Paper The paper introduces a novel approach named selfsupervised spatial representations (S3R) to capture local spatial structures efficiently in consecutive frames for reinforcement learning methods. The proposed method learns spatial latent volumes which are used to compute flow maps depicting spatial deformations among frames. These flow maps characterize the local spatial structures and are used for selfsupervised learning. The paper further integrates the spatial representations with the selfpredictive representations method to predict future representations using an actionconditioned transition model. Experimental results on complex tasks in Atari Games and DeepMind Control Suite demonstrate significant performance improvements with the proposed selfsupervised learning of spatial representations.  Strong Points 1. Innovative Approach: Introducing a novel method S3R to encode local spatial structures in a selfsupervised manner is a significant contribution. 2. Thorough Evaluation: Conducting experiments on challenging environments in Atari Games and DeepMind Control Suite showcases the effectiveness of the proposed method. 3. Clear Presentation: The paper is wellstructured explaining the methodology related work and experimental results in a detailed and organized manner. 4. Comparison with StateoftheArt: Comparative analysis with existing stateoftheart reinforcement learning methods provides a comprehensive view of the proposed methods performance.  Weak Points 1. Limited Clarity on Results: While the experimental results are strong providing more detailed insights into how the proposed method improves sample efficiency and performance would be beneficial. 2. Complexity of Method: The methods complexity especially in terms of implementation and computational resources required may pose challenges for adoption in practical applications. 3. Lack of Benchmarking Metrics: Providing additional benchmarks and comparisons with other comparable methods could strengthen the validation of the proposed approach. 4. Practical Implementation Details: More practical implementation guidelines such as computational requirements training time and potential scalability issues would be helpful for reproducibility.  Questions and Feedback to the Author 1. Can you provide more insights into the scalability and generalizability of the proposed method beyond the specific environments tested in Atari and DeepMind Control Suite 2. How does the proposed S3R method perform with varying levels of complexity in tasks or environments Can it adapt to different scales of spatial structures effectively 3. Could you elaborate on the computational resource requirements training time and potential challenges in implementing the proposed approach in realworld scenarios  Review Summary Overall the paper presents an innovative approach S3R for selfsupervised learning of spatial representations in reinforcement learning tasks. The thorough evaluation on challenging environments demonstrates notable performance improvements. While the method shows promise further clarification on the practical implementation details scalability and adaptability to different environments would enhance the impact of the research. Considering the strong experimental results and the unique contribution of the proposed method with some enhancements in clarity and practical applicability details I recommend acceptance of the paper.", "qfaNCudAnji": "1) Summarize the paper: The paper introduces a novel algorithm DQN with Proximal Iteration (DQNPro) which incorporates Proximal Iteration into the Deep QNetwork (DQN) to optimize the value function in reinforcement learning. Proximal Iteration aims to stabilize the optimization procedure by biasing the online network towards the target network. The proposed algorithm is tested on Atari benchmarks and shows significant improvements over the original DQN. 2) Strengths and Weaknesses: Strong Points:  The paper introduces a novel algorithm DQNPro which combines Proximal Iteration with DQN showing significant performance improvements over the original DQN.  Provides a detailed theoretical background on reinforcement learning Proximal Iteration and the proposed algorithm.  Experimental results demonstrate the effectiveness of DQNPro on Atari benchmarks showcasing its potential in deep reinforcement learning.  The paper includes ablation experiments and further analyses to validate the benefits of the proximal term and robustness of the optimization procedure. Weak Points:  While the theoretical underpinnings are thorough the practical implications for realworld applications beyond the Atari benchmark are not fully explored.  Limited discussion on the generalizability of the proposed framework to different types of environments or tasks.  The paper could benefit from a more detailed comparison with other stateoftheart algorithms in deep reinforcement learning. Questions and Feedback to the author:  Could you elaborate on the scalability and generalizability of the DQNPro algorithm to other environments or tasks beyond Atari benchmarks  Have you considered conducting experiments on more diverse and challenging environments to evaluate the performance and effectiveness of DQNPro in different settings  How does the computational complexity of DQNPro compare to other existing algorithms in deep reinforcement learning and are there any areas for potential optimization or efficiency improvements  Further insights into the practical implications and potential applications of DQNPro in realworld scenarios would enhance the papers impact. 3) Review Summary: The paper presents a novel algorithm DQN with Proximal Iteration (DQNPro) that combines Proximal Iteration with the Deep QNetwork (DQN) to optimize the value function in reinforcement learning. Theoretical foundations are robust and experimental results on Atari benchmarks demonstrate significant performance improvements over the original DQN. However the paper lacks discussion on the algorithms scalability to diverse environments and could benefit from a more comprehensive comparison with other stateoftheart algorithms. Overall the innovative approach and promising results make the paper a valuable contribution to the field of deep reinforcement learning. More insights into realworld applications and potential optimizations would further strengthen the paper.", "uEBrNNEfceE": " 1) Summary: The paper introduces a novel approach to the dual control problem in the context of datadriven control and reinforcement learning. The focus is on the linearquadratic dual control problem where system parameters need to be identified and control objectives need to be optimized simultaneously. Contrary to existing works the proposed online algorithm guarantees the asymptotic optimality of the controller in the almost sure sense. The dual control strategy involves a switched controller with timedecaying exploration noise and Markov parameter inference based on crosscorrelation between the exploration noise and the system output. The key contribution is a safe switched control strategy that ensures stability and guarantees convergence to the optimal policy almost surely. The paper provides theoretical analysis and simulation results to showcase the effectiveness of the proposed strategy.  2) Strengths and Weaknesses: Strong Points:  Introduction of a novel approach to the dual control problem in the context of datadriven control and reinforcement learning.  Guarantees asymptotic optimality of the controller in the almost sure sense which is a significant advancement compared to existing works.  The concept of boundedcost safety is introduced ensuring that no destabilizing control policies are applied during the learning process.  Theoretical analysis provides rigorous proofs and convergence rates for parameter inference error and suboptimality gap.  Simulation results on an industrial process example illustrate the effectiveness of the proposed strategy. Weak Points:  The paper is highly technical and may be challenging for readers not wellversed in the field of control theory and reinforcement learning.  Theoretical proofs are deferred to the appendix which might make it harder to follow the details for some readers.  The evaluation of the proposed algorithm mainly relies on simulation results realworld validation could further strengthen the claims.  More discussion on the computational complexity and practical aspects of implementing the proposed algorithm could be beneficial. Questions and Feedback to the author:  Could you provide more insights into the practical implications and computational complexity of implementing the proposed algorithm in realworld scenarios  How does the proposed approach compare to existing stateoftheart methods in terms of computational efficiency and performance in realistic control applications  It would be interesting to discuss potential limitations or assumptions made in the theoretical analysis and how they could impact realworld implementation.  3) Review Summary: The paper presents a novel approach to the dual control problem in datadriven control and reinforcement learning offering a significant advancement by ensuring asymptotic optimality in the almost sure sense. The concept of boundedcost safety and the rigorous theoretical analysis provide a strong foundation for the proposed strategy. While the paper is technical and may require domain expertise to fully grasp the simulation results provide evidence of the effectiveness of the approach. Realworld validation and further discussions on practical implications would enhance the papers impact. Overall the paper is valuable in advancing the field of online LQR with its innovative approach and theoretical contributions.", "nRj0NcmSuxb": " Peer Review:  1) Summary: The paper introduces the Fairness Calibration (FairCal) method a posttraining approach for reducing bias in face recognition models without sacrificing accuracy. By calibrating probabilities fairly across sensitive subgroups FairCal aims to address the challenges of bias mitigation in existing models. The method achieves stateoftheart accuracy fairness calibration and predictive equality without the need for retraining or knowledge of sensitive attributes. Through comprehensive experiments on two datasets the paper demonstrates the effectiveness of FairCal in improving fairness in face verification tasks. The study highlights the importance of fairness in face recognition models particularly in law enforcement applications.  2) Strengths and Weaknesses: Strong Points:  Innovative Method: FairCal introduces a novel posttraining approach to mitigate bias in face recognition models achieving superior accuracy fairness calibration and predictive equality.  StateoftheArt Results: The method outperforms existing techniques in terms of accuracy fairness calibration and predictive equality offering a significant contribution to the field.  Comprehensive Experiments: The paper presents detailed experiments on two datasets showcasing the effectiveness of FairCal in improving fairness without the need for sensitive attributes or retraining.  Ethical Considerations: The inclusion of an ethics statement contextualizes the research in addressing biases and potential misuse of facial recognition systems emphasizing the socially responsible aspects of the study. Weak Points:  Limited Generalizability: The study focuses on face verification tasks and may not directly translate to other domains within face recognition or broader machine learning applications.  Complexity: The method involves clustering and posthoc calibration which may introduce complexity and additional computational overhead in practical implementations.  Deployment Challenges: The requirement for a calibration dataset may pose challenges in deploying the method on new or diverse datasets impacting the reproducibility of results.  Incomplete Discussion: While the paper extensively covers the technical aspects of FairCal it could benefit from a more detailed discussion on the implications of the findings potential limitations and future research directions. Question to the Authors:  How do you envision overcoming the challenges related to deploying FairCal on different datasets that may not have suitable calibration sets  Have you considered potential implications or unintended consequences of implementing FairCal in realworld face recognition systems Feedback to the Authors:  Consider discussing the scalability and generalizability of FairCal in different face recognition scenarios or machine learning applications.  Include a more detailed discussion on the practical implications limitations and ethical considerations of implementing FairCal in realworld contexts.  Provide insights on the potential impact of FairCal on addressing biases in face recognition models used in diverse applications.  3) Review Summary: The paper presents a promising methodology FairCal for mitigating bias in face recognition models showcasing significant improvements in accuracy fairness calibration and predictive equality without the need for sensitive attributes or retraining. The innovative approach and comprehensive experiments underscore the potential of FairCal in addressing ethical concerns and improving fairness in face verification tasks. While the study excels in technical rigor and experimental validation there is room for further discussion on practical implications scalability and potential implications of deploying FairCal. Overall the paper makes a valuable contribution to the field of machine learning by offering a novel method to enhance fairness in facial recognition systems.  Recommendation: Based on the innovative methodology superior results and significant contribution to mitigating bias in face recognition models I recommend the paper for publication pending minor revisions to address the highlighted points for clarity and completeness.", "gtvM-nBZEbc": " Summary: The paper introduces a novel object captioning model VLAF2 for learning VisualLinguistic Adequacy Fidelity and Fluency. The model leverages the linguistic knowledge from pretrained models like BERT and CLIP to produce captions for images with novel objects. Extensive experiments are conducted on the nocaps dataset demonstrating superior performance in terms of fluency fidelity and adequacy metrics compared to stateoftheart models.  Strong Points: 1. Innovative Approach: The paper addresses the challenges of novel object captioning by integrating linguistic fluency visual fidelity and adequacy aspects using BERT and CLIP offering a novel and effective solution. 2. Comprehensive Evaluation: The extensive experiments on the nocaps dataset showcase the effectiveness of the proposed model against benchmarks demonstrating improvements in caption metrics like CIDEr and SPICE as well as fluency fidelity and adequacy measures. 3. Technical Depth: The technical details provided in Sections 2 and 3 elucidate the methodology clearly particularly in explaining the relationships between fluency fidelity and adequacy with BERT and CLIP models enhancing the papers technical rigor.  Weak Points: 1. Robustness Analysis: While the paper performs well in the experiments a more indepth analysis on the robustness of the model to different scenarios or challenging image types could strengthen the validation of the proposed methodology. 2. Experimental Comparison: While the comparison with existing models is thorough additional comparisons with different datasets or realworld scenarios can provide a broader perspective on the models generalizability and applicability. 3. Clarity in Repetition Penalty: The concept of the repetition penalty is crucial for maintaining linguistic fluency. Further elaboration or examples can clarify its importance and impact on the models output.  Questions and Feedback: 1. Can you provide insights on the scalability of the proposed model to larger datasets or different modalities beyond images 2. How would the model adapt to handling noisy or ambiguous visual data as well as addressing challenges in multiobject scenarios within an image  Review Summary: The paper presents a novel object captioning model VLAF2 that effectively addresses the fluency fidelity and adequacy aspects in image captioning using pretrained models like BERT and CLIP. The comprehensive experiments validate the models performance superiority across metrics compared to stateoftheart models. However further robustness analysis broader experimental comparisons and clarity enhancement on certain concepts can enhance the papers impact and applicability. Overall the innovative approach technical depth and strong experimental results support the recommendation for acceptance after addressing the minor weak points and questions for further clarification.", "tBtoZYKd9n": "Summary: The paper critically evaluates the use of Maximum Mean Discrepancy (MMD) as a metric for comparing graph generative models. It addresses the challenges and pitfalls researchers face when using MMD particularly in the context of evaluating graph generative models. The authors systematically analyze the behavior of MMD on perturbed graphs and various graph generative models. They provide recommendations for researchers to mitigate these challenges and pitfalls suggesting improvements in the selection of kernels and hyperparameters when using MMD for model evaluation. Strengths: 1. Thorough Analysis: The paper provides a comprehensive examination of the current practice of using MMD for evaluating graph generative models. 2. Clear Presentation: The paper is wellstructured making it easy for readers to follow the arguments and understand the issues discussed. 3. Detailed Experiments: The empirical experiments conducted to highlight the sensitivity to kernel and parameter choices provide valuable insights into the challenges faced in practice. Weaknesses: 1. Complexity: The topic discussed in the paper is highly technical and specialized which might make it less accessible to a broader audience. 2. Limited Impact: While the paper identifies important issues with using MMD for model evaluation it lacks empirical evidence of the impact of these issues on the validity and reliability of comparing graph generative models. 3. Recommendation Ambiguity: The paper suggests practical solutions but more clarity on how these solutions can be practically implemented and their effectiveness in improving model evaluation would enhance the papers utility. Questions  Feedback: 1. Clarification Needed: Can the authors provide more insights into the potential consequences of the issues identified in their study on the reliability and validity of comparing graph generative models 2. Experimental Validation: Have the authors considered conducting experiments to demonstrate the impact of their proposed recommendations on improving the reliability of model evaluation using MMD 3. Generalizability: How applicable are the findings and recommendations of this study to other domains or machine learning applications beyond graph generative models Review Summary: The paper offers a critical examination of the challenges associated with using MMD for evaluating graph generative models. While it provides valuable insights into the limitations of current practices the paper would benefit from further empirical validation of the proposed recommendations to enhance the reliability and validity of model comparisons. Overall with its detailed analysis the paper contributes to the ongoing discussions on improving evaluation metrics for graph generative models. It is recommended that the authors address the weaknesses identified particularly by providing clearer implementation guidelines for their proposed solutions.", "size4UxXVCY": "Summary: The paper proposes Graph Tree Neural Networks (GTNNs) as an innovative framework to address limitations in existing neural network architectures. Utilizing a data structure called a graph tree GTNNs aim to capture hierarchical and relational information similar to the human brain. The paper introduces two GTNN models namely Graph Tree Convolutional Networks (GTC) and Graph Tree Recursive Networks (GTR) which adapt the depth of network layers based on data complexity. The proposed models leverage depthfirst convolution and deconvolution methods to encode and decode information flow within the GT. The paper also presents experiments demonstrating the efficacy of GTNNs in simultaneously learning diverse datasets and leveraging transfer learning and finetuning for improved performance. Strengths: 1. Innovative Approach: The paper introduces a novel concept of GTNNs which aim to mimic human neural networks and capture complex hierarchical relationships effectively. 2. Adaptive Network Depth: The flexibility of adjusting network depth based on data complexity is a significant strength allowing for efficient learning across various datasets. 3. Experimental Validation: The paper provides detailed experimental results demonstrating the capabilities of GTNNs in handling multiple datasets and showcasing potential improvements through transferlearning and finetuning. 4. Attention Mechanism Integration: The incorporation of attention models like GTRAs and GTAs enhances the networks ability to learn important features and improve performance. Weaknesses: 1. Complexity and Technical Jargon: The paper contains intricate technical details and algorithms which may pose a challenge for readers without a deep understanding of neural network architectures. 2. Dataset Sampling Methodology: The use of sampling to handle the combination of multiple datasets may raise concerns about representativeness and potential biases in the experimental results. 3. Performance Evaluation: While the experiments demonstrate promising results further comparative analysis with stateoftheart models or additional benchmark datasets could provide a more comprehensive evaluation. Questions and Feedback: 1. Could you elaborate on the impact of the dataset sampling method on the generalizability and scalability of the proposed GTNN framework 2. How does the computational complexity of GTNNs compare to traditional neural network architectures especially with the adaptive depth feature 3. Considering the integration of attention mechanisms how did the introduction of attention models impact the interpretability and feature extraction capabilities of GTNNs Review Summary: The paper presents a cuttingedge approach with Graph Tree Neural Networks (GTNNs) to overcome limitations in existing neural network architectures. The adaptability of network depth integration of attention mechanisms and experimental validation highlight the potential of GTNNs in handling diverse datasets and achieving efficient learning. While the technical complexity and experimental methodology warrant further discussion the innovative nature of GTNNs and the demonstrated performance improvements through transfer learning and finetuning make a strong case for the papers contribution to the deep learning field. Overall the study provides valuable insights for researchers exploring novel neural network architectures and their applications across various domains.", "oTQNAU_g_AZ": " Summary: The paper proposes a novel technique called Disentangled Attention Intrinsic Regularization (DAIR) to solve the challenges of domination and conflict in training reinforcement learning agents for safe bimanual robot manipulation tasks. DAIR encourages two robots to collaboratively and efficiently focus on different subtasks by utilizing attention mechanisms in policy and value networks. The regularization is aimed at improving safety and efficiency by reducing conflicts and avoiding domination during collaboration. Experimental results on various bimanual manipulation tasks demonstrate the effectiveness of DAIR in improving task completion speed sample efficiency and reducing conflicts between robots.  Strengths: 1. Innovative Approach: The introduction of DAIR to address domination and conflict issues in bimanual manipulation tasks is a novel and promising technique. 2. Clear Problem Statement: The paper effectively highlights the challenges of domination and conflict in collaborative robotics tasks providing a solid motivation for the proposed technique. 3. Comprehensive Experiments: The paper provides a thorough evaluation of the proposed approach on diverse manipulation tasks showcasing improvements in efficiency safety and generalization capability. 4. Theoretical Basis: The incorporation of attention mechanisms and regularization techniques adds a strong theoretical foundation to the proposed solution.  Weaknesses: 1. Limited Comparative Analysis: While the paper compares DAIR with baselines a more extensive comparison with a wider range of stateoftheart methods in bimanual manipulation tasks could enhance the evaluation. 2. Complexity of Mechanism: The detailed technical description of DAIR while informative may be challenging for readers not deeply familiar with reinforcement learning and attention mechanisms. 3. Lack of Realworld Validation: The evaluation is limited to simulation environments and realworld validation would further strengthen the practical utility of DAIR.  Questions for the Authors: 1. How does DAIR perform in scenarios with dynamic or changing environments that might introduce uncertainties 2. Are there specific scenarios or tasks in realworld bimanual manipulation where DAIR would particularly excel 3. How scalable is DAIR to handle more than two robots collaborating on complex tasks  Feedback:  The experimental validation is robust however extending the evaluation to realworld scenarios or physical robot platforms would enhance the practical relevance of the proposed technique.  Providing a more detailed discussion on the computational complexity and scalability of implementing DAIR in realworld robotic systems would be beneficial.  Consider expanding the discussion on potential limitations or challenges that might arise when deploying DAIR in practical applications.  Review Summary: The paper presents a wellmotivated and innovative solution DAIR to address domination and conflict issues in collaborative bimanual manipulation tasks. The comprehensive experimental evaluation demonstrates significant improvements in efficiency safety and generalization capabilities. While the paper is technically detailed it provides a strong theoretical basis and practical insights for advancing safe collaboration in robotic manipulation tasks. Overall I recommend accepting this paper with minor revisions and further realworld validation.", "gCmCiclZV6Q": " Summary: The paper investigates the utilization of pretrained largescale vision models specifically the Contrastive LanguageImage Pretrained model (CLIP) to automatically curate offensive content in image datasets. The authors argue that large datasets scraped from the web may contain offensive terms and images leading to models with undesirable behavior. They propose a methodology to leverage pretrained models to automatically identify and curate offensive images within datasets. The study demonstrates that CLIP contains implicit knowledge that can infer offensiveness in images assisting in the curation process. The experiments are conducted on the ImageNetILSVRC2012 dataset showcasing the ability of the proposed method to identify additional inappropriate content not detected by previous curation processes.  Strengths: 1. Innovative Approach: The paper presents a novel approach utilizing pretrained vision models to automatically curate offensive content in largescale image datasets addressing a critical issue in data curation. 2. Relevance: The topic is highly relevant in todays context where concerns about offensive and inappropriate content in AI models are gaining increased attention. 3. Empirical Evidence: The authors provide a detailed experimental setup and results demonstrating the effectiveness of their proposed method on the ImageNetILSVRC2012 dataset. 4. Detailed Explanations: The paper explains the methodology experimental setup and results comprehensively making it easier for readers to understand the research.  Weaknesses: 1. Offensive Content: The paper contains offensive content within the examples which may be inappropriate for some readers. It is crucial to handle such content sensitively to maintain professionalism. 2. Ethical Considerations: The issue of ethical considerations surrounding the use and curation of offensive content is briefly mentioned but could be explored further. How the authors address potential biases and ethical dilemmas in their methodology could be elaborated. 3. Validation: While the methodology shows promise further validation on different datasets beyond ImageNetILSVRC2012 could enhance the robustness and generalizability of the proposed approach. 4. Interpretability: The blackbox nature of deep learning models is acknowledged but more emphasis on interpretability methods to explain the models decisionmaking process would strengthen the paper.  Questions to the Authors: 1. How do you plan to address the ethical considerations and potential biases associated with automatically curating offensive content in datasets 2. Have you considered validating the proposed method on other largescale vision datasets to assess its generalizability and performance consistency 3. Could you provide more insights into how interpretability methods could be integrated to enhance the transparency of the models decisionmaking in identifying offensive content 4. How do you envision the scalability of your proposed approach in terms of handling even larger datasets with diverse offensive content categories  Feedback: The paper presents a pioneering approach to automatically curate offensive content in image datasets using pretrained vision models. To strengthen the research consider refining the methodology for handling offensive content sensitively exploring ethical implications in more depth validating the method on diverse datasets emphasizing interpretability methods and discussing the scalability of the approach for future applications. Overall the paper contributes significantly to the important area of mitigating offensive content in AI datasets.", "q4HaTeMO--y": " Summary of the Paper: The paper presents a theoretical analysis connecting Deep Equilibrium Models (DEQs) and Deep Declarative Networks (DDNs) in the context of solving optimisation problems. The authors demonstrate that by incorporating a kernelised regularised maximum likelihood estimate as an inner problem in a DDN a large class of DEQ architectures can be obtained. They provide a closedform expression for the DEQ parameters in terms of the kernel showcasing the equivalence between these two models. The paper also introduces an initialisation scheme for DEQs derived from this theoretical result which improves training stability and performance over random initialisation. The analytical framework is supported by empirical demonstrations in image inpainting and image denoising tasks.  Strong Points: 1. Theoretical Contribution: The paper introduces a novel theoretical equivalence between DEQs and DDNs providing a rigorous analytical framework for understanding their relationship. 2. Empirical Validation: The theoretical results are backed by empirical evidence showcasing the practical utility of the proposed initialisation scheme in improving the performance and stability of DEQ models. 3. Clarity of Presentation: The paper is wellstructured presenting the problem contributions theoretical derivations and experiments in a logical sequence aiding in understanding the concepts.  Weak Points: 1. Assumptions and Conditions: The paper relies on specific assumptions and conditions such as the existence and uniqueness of fixed points which might limit the applicability of the proposed framework in more general settings. Authors could discuss the implications of these assumptions further. 2. Generalization: The empirical evaluation focuses on specific tasks and scenarios. Extending the analysis to a broader range of problems or datasets could strengthen the generalizability of the findings. 3. Practical Implementation: While the theoretical connection is insightful practical implementation details of the proposed initialisation scheme including computational complexity or scalability could be further elaborated.  Questions for the Author: 1. How do the assumptions made in the paper impact the practical applicability of the proposed framework Are there scenarios where these assumptions might not hold and how would that affect the theoretical results  Feedback for the Author: The paper provides valuable insights into the relationship between DEQs and DDNs and offers a promising initialisation scheme for DEQ models. Strengthening the discussion on the implications of assumptions expanding the empirical evaluation to diverse tasks and elaborating on the practical aspects of implementation would enhance the impact and applicability of the presented work.  Review Summary: The paper presents a rigorous theoretical analysis establishing the equivalence between DEQs and DDNs complemented by empirical validation in imagerelated tasks. The proposed initialisation scheme showcases improved training stability and performance validating the practical implications of the theoretical findings. Strengthening the discussion on assumptions broadening empirical evaluations and providing more insights into practical implementation would further enhance the impact and clarity of the paper. Overall the paper makes a significant contribution to understanding and improving the training of deep equilibrium models based on their connection with declarative networks.", "pN1JOdrSY9": "Summary: The paper introduces a novel approach called LanguageAgnostic Constraint for SwAV loss (LAgSwAV) to mine pseudoparallel data from unlabeled monolingual corpora for unsupervised machine translation. The method finetunes pretrained models to extract pseudoparallel data and utilizes it for UMT tasks achieving stateoftheart results in various bilingual and lowresource language pairs. The methodology involves enhancing models with semantic contrastiveness and languageagnostic properties employing a filtering suite for data quality and utilizing a rankbased crossentropy loss for UMT integration. Experimental results demonstrate the effectiveness of the proposed approach in improving UMT performance and mining highquality pseudoparallel data. Strengths: 1. Innovative Methodology: Introduces a novel approach to mine pseudoparallel data from monolingual corpora for UMT tasks showcasing significant improvements over existing baselines. 2. StateoftheArt Results: Achieves stateoftheart performance in WMT bilingual UMT and FLoRes lowresource tasks indicating the effectiveness of the proposed method. 3. Comprehensive Analysis: Conducts a thorough analysis of the methods performance through various metrics including Tatoeba Accuracy Global Accuracy and Cluster Word Distribution providing detailed insights into the models behavior and efficacy. Weaknesses: 1. Complexity: The methodology presented is intricate and may be challenging for readers less familiar with the subject matter necessitating a clear explanation and possibly simpler illustrations. 2. Limited Evaluation: While the paper provides extensive experimental results additional comparisons with more diverse baseline methods could further strengthen the evaluation section. 3. Contextual Information: Some technical details and background information could have been explained more comprehensively to aid readers in understanding the nuances of the proposed approach. Questions for the Author: 1. How does the proposed methodology in LAgSwAV address the challenges posed by noisy and lowquality synthetic data typically generated by unsupervised translation models 2. Can you provide insights on scalability and generalizability of the proposed approach to other language pairs or datasets beyond the ones tested in the experiments 3. How do you plan to address potential issues related to varying text lengths vocabulary differences or style variations between different languages while mining pseudoparallel data for UMT tasks Feedback for the Author: 1. Consider providing a more detailed explanation of the implementation and training process of LAgSwAV for readers seeking a deeper understanding of the methodology. 2. Enhance the clarity of the paper by including illustrative examples or figures to elucidate key concepts or processes involved in the pseudoparallel data mining process. 3. Extend the comparison section to include broader sets of baseline methods to strengthen the discussion on the novelty and effectiveness of the proposed approach. Review Summary: The paper presents a compelling and innovative approach LAgSwAV to mine pseudoparallel data for unsupervised machine translation tasks. It demonstrates significant improvements in UMT performance and data quality evaluation metrics. While the methodology is robust and the experimental results are promising additional clarification in certain sections and elaboration on evaluation comparisons could enhance the readability and depth of the paper. Overall the proposed approach shows strong potential in advancing the field of unsupervised machine translation.", "z-5BjnU3-OQ": " Summary of the Paper: The paper introduces HyperCGAN a new approach for texttoimage synthesis using hypernetworks to condition a GAN model on text. The method explores modulating weight parameters based on textual input query achieving high flexibility and demonstrating stateoftheart performance in texttoimage evaluations on COCO 256 and ArtEmis 256 datasets. HyperCGAN can generate discrete and continuous images from text descriptions outperforming existing methods.  Strong Points: 1. Innovative Approach: The use of hypernetworks for conditioning GAN on textual information is a novel and effective approach leading to improved image generation quality. 2. Flexibility: The method is shown to be versatile working with both traditional and continuous image generation models showcasing its adaptability. 3. Stateoftheart Performance: Achieving superior results compared to existing texttoimage synthesis methods in terms of modern evaluation metrics and human studies is a significant strength.  Weak Points: 1. Complexity: The extreme modulating tensor factorization method can be memoryintensive and may limit scalability for larger architectures. 2. Evaluation Metrics: The reliance on automated metrics like FID and human studies might have limitations in capturing the full spectrum of imagetext alignment and quality. 3. Ablation Studies: While some ablation studies were conducted further analysis on different aspects of the hypernetwork modulation could provide deeper insights into model performance.  Questions for the Authors: 1. Have you considered the computational overhead of the extreme modulating tensor factorization technique How would this impact model scalability to larger datasets 2. How generalizable is the HyperCGAN approach across different domains beyond texttoimage synthesis 3. Can you elaborate on the potential drawbacks or limitations of the hypernetworkbased conditioning method in specific scenarios or applications  Feedback for the Authors: 1. Consider providing more insight into the computational efficiency and scalability of the proposed approach. 2. Further exploration of the limitations and potential extensions of HyperCGAN in different contexts could enhance the papers impact. 3. Including more detailed discussions on the practical implications of the findings could enrich the papers contribution to the field of texttoimage synthesis.  Review Summary: The paper presents an innovative and effective method HyperCGAN for texttoimage synthesis using hypernetworks. The approach demonstrates high performance flexibility and stateoftheart results on texttoimage evaluation metrics. While the method shows promise addressing scalability implications and conducting more indepth ablation studies could provide a more comprehensive understanding of the models capabilities. Overall the paper makes a significant contribution to the field and warrants further exploration and refinement.", "gciJWCp3z1s": "Peer Review 1) Summary: The paper addresses the equitable and optimal transport (EOT) problem introducing a projected alternating maximization algorithm (PAM) to solve the dual of the entropy regularized EOT. The study provides the first convergence analysis of PAM and proposes a novel rounding procedure. Additionally a variant of PAM incorporating the extrapolation technique is introduced. 2) Strengths and Weaknesses: Strengths:  The paper fills a gap in the literature by providing the first convergence analysis of PAM.  The proposed rounding procedure for constructing the primal solution is novel and addresses a crucial aspect of the problem.  Analysis of the variation with extrapolation methodology provides valuable insights and promising numerical results.  The connections to block coordinate descent (BCD) and block coordinate gradient descent (BCGD) methods are discussed enhancing the understanding of the proposed algorithm. Weaknesses:  The complexity analysis of PAME did not yield a proven improvement over PAM raising questions about the theoretical justification for choosing PAME over the standard PAM algorithm.  The significance of the algorithm improvements in practical applications needs to be elaborated upon further.  The numerical experiments presented are limited to synthetic datasets realworld application examples could strengthen the validity of the proposed methods. Questions and Feedback to the Author: 1. Could you elaborate on the practical implications of the proposed algorithm improvements in solving realworld optimization problems where EOT is relevant 2. The paper mentions potential future work on developing provably better algorithms. Could you outline specific directions for enhancing the algorithms or addressing any limitations identified in the study 3. How do you plan to validate the effectiveness of the proposed algorithms in scenarios with larger and more complex datasets or in practical applications beyond synthetic datasets 3) Review Summary: The paper makes a significant contribution to the optimization field by addressing the EOT problem and proposing novel algorithms. The analysis of PAM the introduction of PAME and the discussions on extrapolation techniques provide valuable insights for researchers in the area of optimization algorithms. While the theoretical advancements are commendable further clarification on the practical relevance and potential future improvements would enhance the impact of the study. Overall the paper presents a methodologically sound approach backed by comprehensive analysis and results that hold promise for advancing optimization techniques. Recommendation: Based on the strengths of the study in addressing a niche problem providing novel algorithmic approaches and conducting a thorough analysis I recommend acceptance of the paper pending minor revisions to address the weaknesses and provide additional insights into practical applications and future directions.", "ijygjHyhcFp": " Summary of the Paper The paper introduces a novel federated learning paradigm named \"Anarchic Federated Learning\" (AFL) which allows workers complete freedom in choosing when to participate and the number of local steps to perform in each round based on their own situation. The paper proposes two Anarchic Federated Averaging (AFA) algorithms for AFL in crossdevice (CD) and crosssilo (CS) settings. The theoretical convergence analysis shows that the AFL algorithms achieve competitive convergence rates and retain the \"linear speedup effect.\"  Strong Points: 1. Innovative Approach: The paper introduces a novel paradigm AFL that addresses the limitations of traditional servercentric federated learning providing insights into workercentric federated learning with flexible participation. 2. Theoretical Foundations: The paper provides a robust theoretical foundation by establishing lower bounds on convergence error and proving the convergence rates of the proposed AFA algorithms under various scenarios. 3. Algorithm Development: The development of two AFA algorithms tailored for crossdevice (CD) and crosssilo (CS) settings demonstrates the versatility and adaptability of AFL in diverse federated learning scenarios. 4. Numerical Experiments: The paper presents extensive experimental results on realworld datasets validating the proposed algorithms and demonstrating their effectiveness in various scenarios.  Weak Points: 1. Limited Discussion on Implementation: The paper could benefit from a more detailed discussion on the practical implementation aspects of AFL and AFA algorithms such as computation overhead communication costs and implementation challenges in realworld edge computing environments. 2. Further Comparative Analysis: While the paper compares the performance of AFA algorithms against FedAvg it could provide a more indepth comparison with other stateoftheart federated learning algorithms to highlight the unique advantages of AFL.  Questions to the Authors: 1. Can you elaborate on any potential challenges or limitations in implementing the proposed AFL paradigm in realworld edge computing systems 2. How do the proposed AFA algorithms compare with existing federated learning algorithms in terms of computational efficiency and communication overhead 3. Have you considered scenarios where the number of workers dynamically changes in AFL If so how does the system adapt to such scenarios  Feedback to the Authors: The paper presents a novel and wellstructured approach in Anarchic Federated Learning supported by robust theoretical frameworks and experimental validations. Enhancing the discussion on practical implementation aspects and extending comparative analyses with existing algorithms would further strengthen the paper. Addressing the questions raised and considering dynamic worker scenarios could provide a comprehensive insight into the scalability and adaptability of the proposed AFL paradigm. Overall the paper contributes significantly to the evolving field of federated learning and showcases promising results for further exploration and research.", "xNO7OEIcJc6": " Summary of the Paper The paper investigates languagebiased image recognition in artificial models through a benchmark task using wordembedded images. It introduces a method to assess bias in artificial models based on pictureword interference akin to human cognitive processes. The study evaluates the CLIP model by testing the effects of presenting words on image classification across different category levels. Results show that the CLIP model is influenced by the presence of words but the effect is not dependent on semantic relationships between images and embedded words. The study provides insights into how language biases affect image classification in artificial models trained on joint language and vision tasks.  Strengths of the Paper 1. Methodological Innovation: The paper introduces a novel benchmark task to evaluate language biases in artificial models and integrates tools from cognitive science literature. 2. Relevance: The investigation of languagebiased image classification in artificial models fills a gap in existing research on joint learning of language and vision. 3. Thorough Analysis: The paper conducts comprehensive experiments including label switching rates semantic similarity analysis spelling similarity evaluation and representational similarity analysis (RSA) providing a thorough assessment of language biases in the CLIP model.  Weaknesses of the Paper 1. Limited Comparison: The paper primarily focuses on evaluating the CLIP model and lacks comparative analysis with other models trained on joint language and vision tasks. 2. Interpretational Limitation: While the paper uncovers interesting findings on language biases in the CLIP model the interpretation of results could be further elaborated to explain the implications of the observed biases clearly. 3. Future Directions: The paper could benefit from discussing potential future research directions based on the findings such as exploring ways to mitigate language biases in artificial models.  Questions for the Author 1. Have you considered comparing the language biases observed in the CLIP model with other models trained on similar tasks to gain a broader perspective on languagebiased image classification  Feedback for the Author 1. Clarifications: Enhance the discussion section to provide a more indepth interpretation of the observed biases and their implications for artificial models joint learning of language and vision. 2. Future Directions: Consider discussing potential future research avenues based on the studys findings such as exploring methods to mitigate language biases in artificial models effectively.  Review Summary The paper presents a detailed investigation into languagebiased image recognition in artificial models focusing on the CLIP model through a novel benchmark task. While the study showcases methodological innovation and provides valuable insights into language biases in image classification further analyses comparisons with other models and clearer interpretation of results could enhance the papers impact. Overall the paper contributes to understanding biases in artificial models trained on joint language and vision tasks but could benefit from additional discussions on implications and potential future research directions.", "kHkWgqOysk_": " Quality PeerReview:  1) Summary of the Paper: The paper focuses on an empirical analysis of PseudoLabeling (PL) in a classmismatched SemiSupervised Learning (SSL) setting. It aims to address how OutOfDistribution (OOD) data affects PL and what are the better pseudolabeling strategies for OOD data. The authors propose a twocomponent model called \\xce\\xa5Model comprising Rebalanced PseudoLabeling (RPL) and Semantic Exploration Clustering (SEC) to handle imbalanced pseudolabels and optimize performance. The experimental results demonstrate the superiority of their method over traditional SSL and existing classmismatched SSL approaches across multiple benchmarks.  2) Evaluation of Strong and Weak Points Questions and Feedback:  Strong Points:  Comprehensive Analysis: The paper provides a thorough investigation of PL in a classmismatched SSL setting.  Novel Contribution: The proposed \\xce\\xa5Model with RPL and SEC presents a novel solution to address imbalanced pseudolabels and optimize SSL performance.  Experimental Validation: Extensive experiments on different datasets validate the effectiveness of the proposed method.  Weak Points:  Clarifications Needed: The description and implementation of the \\xce\\xa5Models components (RPL and SEC) could be elaborated for better understanding.  Comparison Methods: More detailed insights into how the proposed \\xce\\xa5Model compares with traditional SSL methods and existing classmismatched SSL approaches could further enhance the papers contributions.  Questions to the Author: 1. Can you explain in more detail the computational complexity of the \\xce\\xa5Model compared to traditional SSL methods 2. How does the performance of \\xce\\xa5Model vary when the class mismatch ratio is altered in the experiments  Feedback to the Author:  I recommend further emphasizing the evaluation of \\xce\\xa5Model compared to a broader range of SSL methods to provide a more comprehensive analysis.  Providing a detailed explanation of the \\xce\\xa5Models implementation and how it mitigates the issues faced by PL in classmismatched SSL settings would enhance the papers clarity and impact.  3) Review Summary: The paper presents a valuable study on the performance of PseudoLabeling in classmismatched SSL and proposes an innovative \\xce\\xa5Model to overcome challenges associated with imbalanced pseudolabels. The thorough experimental evaluation demonstrates the effectiveness of the proposed model. However further clarifications on the models implementation and comparisons with existing methods would strengthen the papers contributions.  Recommendation: Based on the thorough analysis and experimental validation presented in the paper I recommend its acceptance for publication. With minor revisions to enhance clarity and deepen comparisons the paper can make a significant contribution to the field of SemiSupervised Learning.", "w60btE_8T2m": " Summary of the Paper: The paper introduces a novel framework called Spanning TreeBased Graph Generation (STGG) for generating molecules using deep neural networks. The framework formulates molecular graph generation as the construction of a spanning tree and residual edges exploiting the sparsity of molecular graphs. The authors design a Transformer architecture with treebased relative positional encodings for the tree construction process. Experiments on QM9 ZINC250k and MOSES benchmarks demonstrate the effectiveness of STGG in metrics like validity Fr\u00e9chet ChemNet distance and fragment similarity as well as in optimizing penalized LogP values of molecules.  Strengths of the Paper: 1. Novel Framework: The introduction of the STGG framework is innovative and addresses the limitations of existing graph generative models for molecules by utilizing spanning trees. 2. Transformer Architecture: The utilization of a Transformer architecture with treebased relative positional encodings enhances the efficiency and accuracy of the model. 3. Experimental Results: The paper provides comprehensive experimental results on popular benchmarks showcasing the superiority of STGG over existing methods in metrics like validity uniqueness novelty and specific optimization tasks. 4. Ablation Studies: Ablation studies demonstrate the importance of each component of the proposed method in achieving high validity of generated molecules.  Weaknesses of the Paper: 1. Limited Discussion on Drawbacks: The paper does not extensively discuss potential drawbacks or challenges associated with the proposed framework offering a more balanced perspective. 2. Evaluation Metrics: While the paper presents a variety of metrics for evaluation a deeper analysis on the shortcomings or limitations of these metrics could add more depth to the evaluation section. 3. Comparison with Existing Models: While the paper compares STGG with existing models a more detailed comparison with different kinds of graph generative models could provide a more comprehensive understanding of the frameworks performance.  Questions and Feedback for the Authors: 1. Scalability Concerns: How does the proposed STGG framework perform in terms of scalability for larger datasets or more complex molecular structures 2. General Applicability: Are there plans to extend the STGG framework for other applications beyond molecule generation given its generalizability to sparse graphs 3. Further Work: What are the next steps in the research agenda to enhance the performance or versatility of the STGG framework  Review Summary: The paper presents a compelling framework STGG for molecule generation using a Transformer architecture with treebased encodings. The experimental results showcase the efficacy of STGG in achieving high validity and performance metrics on popular benchmarks. The innovative nature of the framework and the thorough analysis of results indicate that STGG is a valuable contribution to the field of molecule generation. Addressing minor concerns and further exploring the generalizability of STGG would enhance the papers impact. Therefore I recommend accepting the paper with minor revisions based on the feedback provided.", "qESp3gXBm2g": " Summary: The paper introduces a reservoirbased tool TRAKR for accurately distinguishing complex nonlinear timeseries data particularly neural recordings. TRAKR offers high accuracy similar to deep supervised methods while maintaining computational benefits similar to simple distance metrics. It achieves exceptional performance in classifying timeseries patterns outperforming other approaches on a benchmark dataset (sequential MNIST) and in recognizing behaviorally relevant epochs in neural recordings from the macaque orbitofrontal cortex. The approach involves training the reservoir with a single time series obtaining an error signal for classification and utilizing support vector machines as classifiers.  Strengths: 1. Innovation and Contribution: TRAKR offers a novel approach to timeseries classification using reservoir computing to achieve high accuracy with low computational cost. 2. Performance: The paper demonstrates high accuracy and robustness of TRAKR in classifying both synthetic and realworld data compared to existing methods. 3. Methodological Rigor: The methodology is wellstructured with clear explanations of the model details training process error signal calculation and classification methods. 4. Realworld Application: The application of TRAKR to differentiate behaviorally relevant epochs in neural recordings highlights its potential for practical use in neuroscience research and clinical settings. 5. Comparative Analysis: Comparisons with other classification methods such as deep supervised networks and distance metrics provide a comprehensive evaluation of TRAKRs performance.  Weaknesses: 1. Dataset Limitation: The evaluation primarily focuses on benchmark datasets and neural recordings from the macaque OFC potentially limiting the generalizability of TRAKRs performance across a wider range of datasets. 2. Training Process Complexity: While the training time is relatively lower than deep supervised networks the oneshot training approach may raise questions about the stability and generalizability of the model across different datasets. 3. Limited Discussion on Error Signal: The paper could provide more discussion on the significance and interpretation of the error signal generated by the reservoir for classification purposes.  Questions: 1. Can you elaborate on the scalability of TRAKR when handling larger and more diverse datasets beyond the ones mentioned in the paper 2. How does TRAKR handle potential overfitting concerns especially when dealing with complex neural timeseries data with noise and variability  Feedback:  Enhance the discussion on the interpretation and practical implications of the error signal generated by TRAKR in realtime applications.  Include a section on future directions or potential extensions of TRAKR such as handling temporal dependencies or integrating with different neural network architectures.  Review Summary: The paper presents an innovative reservoirbased tool TRAKR for classifying complex nonlinear timeseries data showcasing superior performance compared to existing methods. While the methodology is robust and wellstructured further exploration into the scalability generalizability and interpretability of TRAKR could enhance the papers contribution. Overall the paper offers a valuable tool for realtime applications in neuroscience research and clinical settings warranting further investigation and refinement.", "jeLW-Fh9bV": " 1) Summary of the Paper: The paper proposes a method called Skillbased MetaPolicy Learning (SiMPL) that aims to enable metalearning on longhorizon sparsereward tasks by combining metareinforcement learning algorithms with taskagnostic offline datasets. The approach involves three main phases: skill extraction skillbased metatraining and target task learning. Through extracting reusable skills and a skill prior from offline datasets and leveraging metatraining tasks SiMPL efficiently solves unseen target tasks with orders of magnitude fewer environment interactions compared to prior works. Experimental results on maze navigation and kitchen manipulation tasks demonstrate the effectiveness of the proposed method in achieving improved sample efficiency.  2) Evaluation of the Paper:  Strong Points: 1. Innovative Approach: The paper introduces an innovative method that combines metareinforcement learning with offline datasets to efficiently solve longhorizon sparsereward tasks. 2. Comprehensive Experiments: The paper includes detailed experiments on maze navigation and kitchen manipulation tasks comparing SiMPL with various baselines demonstrating its superior sample efficiency. 3. Clear Presentation: The paper is wellorganized with clear explanations of the proposed method related work problem formulations and experimental results.  Weak Points: 1. Lack of Visual Aids: While the paper includes figures to illustrate the architectures and environments visual aids could be enhanced to improve the understanding of the approach. 2. Complex Technical Jargon: The paper includes technical terms and concepts that may be challenging for readers unfamiliar with reinforcement learning and metalearning to grasp easily.  Questions and Feedback to the Authors: 1. How did you ensure the generalizability of SiMPL across various tasks and environments 2. Can you provide more insights into the scalability of SiMPL for realworld applications with higherdimensional control problems 3. Have you considered evaluating SiMPL on tasks with varying complexities or noise levels to assess its robustness  3) Review Summary: The paper presents an innovative approach SiMPL that combines metareinforcement learning with offline datasets to efficiently solve longhorizon sparsereward tasks. Through comprehensive experiments and comparisons with baselines the authors demonstrate the superior sample efficiency and effectiveness of SiMPL. The clear presentation of the methodology and results adds to the papers strength. However improving visual aids and simplifying technical explanations could enhance the accessibility of the paper to a broader audience. Overall based on the strong theoretical foundation thorough experiments and clear presentation I recommend this paper for acceptance in its current form.", "oxwsctgY5da": " Summary: The paper presents a novel approach BaBAttack for generating strong adversarial attacks on deep neural networks by searching for adversarial examples in the activation space of ReLU networks. The proposed method uses a branch and bound strategy coupled with specialized GPUbased bound propagation methods to efficiently search for adversarial examples in hard instances where existing attacks fail. The paper demonstrates the effectiveness of BaBAttack in generating stronger adversarial examples compared to existing attacks and outperforming offtheshelf MIP solver based attacks in terms of success rates and efficiency.  Strengths: 1. Innovative Approach: The paper introduces a novel way of searching for adversarial examples in the activation space addressing the limitations of existing approaches that search in the input space. 2. Efficiency Improvements: By utilizing specialized GPUbased bound propagation methods and a topdown beam search strategy the proposed BaBAttack is able to efficiently search for adversarial examples even in hard instances where existing attacks fail. 3. Contributions to Closing the Gap: The paper contributes to bridging the gap between the upper bounds of robust accuracy obtained by attacks and the lower bound obtained by verification introducing a new perspective to the adversarial attack problem. 4. Experimental Results: The experimental results show that BaBAttack can successfully generate adversarial examples for hard input instances where existing strong attacks fail showcasing the effectiveness of the proposed method.  Weaknesses: 1. Complexity: The approach relies on mixed integer programming (MIP) and is limited to small networks and easy problem instances. Scalability and practicality may be a concern in larger realworld scenarios. 2. Speed Considerations: The BaBAttack method is not as fast as conventional gradientbased adversarial attacks which could limit its practical use in realtime applications.  Questions and Feedback: 1. Scalability: How does the proposed BaBAttack method handle scalability concerns in larger and more complex neural networks Have scalability experiments been conducted on larger datasets beyond MNIST and CIFAR10 2. Comparison: How does BaBAttack compare to other stateoftheart adversarial attack methods in terms of scalability efficiency and success rates on larger datasets and more complex models 3. Applications: Can the BaBAttack method be adapted or extended for use in realworld applications beyond academic benchmarks  Review Summary: The paper introduces a novel approach BaBAttack for generating strong adversarial attacks by searching in the activation space of ReLU networks. The method shows promise in improving the robustness evaluation of deep neural networks and demonstrates superior performance over existing attacks in challenging instances. However scalability and speed considerations could be further addressed to enhance the practical applicability of the proposed method. Overall the innovative approach and promising experimental results make this paper a significant contribution to the adversarial attack research domain.", "i2baoZMYZ3": "1. Summary: The paper proposes a novel method called Action Quantization from Demonstrations (AQuaDem) to learn a discretization of continuous action spaces based on human demonstrations. By leveraging prior human data AQuaDem aims to reduce the complexity of exploration in continuous control tasks and allows the application of discreteaction deep RL algorithms. The method is evaluated on various setups including Reinforcement Learning with demonstrations RL with play data and Imitation Learning demonstrating improved performance and sample efficiency compared to stateoftheart methods. 2. Strengths: Strong Points:  The paper addresses a significant challenge in reinforcement learning by proposing a novel method that leverages human demonstrations to simplify exploration in continuous control tasks.  Provides a comprehensive overview of existing literature clearly establishing the groundwork and necessity for the proposed method.  The method is wellexplained and supported by theoretical foundations making it reproducible and understandable to the reader.  Detailed experimental setups comparisons with stateoftheart methods and visualizations add depth and clarity to the results.  Empirical results demonstrate consistent improvement in performance and sample efficiency across different experimental scenarios.  Analysis of the methods implications and potential future research avenues provide valuable insights for further investigation. Weak Points: Weak Points:  While the method shows promising results a more extensive comparison with a broader range of baseline methods could strengthen the analysis.  Detailed information on the computational and time complexity of the proposed method should be provided for a more thorough understanding of its practical feasibility.  The paper could benefit from further discussion on the limitations of the proposed approach and potential areas of improvement or refinement. Questions and Feedback to the Author: 1. How does the AQuaDem method compare to other discretization strategies such as those based on countbased exploration or planning 2. Can you provide insights into the scalability of the proposed method to higherdimensional action spaces and larger datasets 3. How robust is the AQuaDem framework to noise or variability in human demonstrations 4. Considering the success of AQuaDem in combination with Munchausen DQN have you explored alternative RL algorithms in conjunction with the proposed discretization technique 3. Review Summary: The paper introduces a novel method AQuaDem for learning discretizations of continuous action spaces from human demonstrations in reinforcement learning tasks. The method demonstrates improved performance and sample efficiency compared to stateoftheart continuous control methods across various experimental setups. The paper is wellconceived supported by a strong theoretical foundation and offers valuable insights into addressing the challenges posed by continuous action spaces. The comprehensive evaluation and discussion of results along with potential research directions make this paper a significant contribution to the field of reinforcement learning. Therefore I recommend this paper for publication based on its innovative approach thorough experimental results and potential impact on the field.", "v-27phh2c8O": " 1) Summary: The paper introduces Automated Auxiliary loss for Reinforcement Learning (AARL) a method that automatically searches for the optimal auxiliary loss function for Reinforcement Learning (RL). AARL aims to improve sample efficiency and decision performance in RL by designing an evolutionary search strategy to explore a large search space of auxiliary loss functions. The proposed method is evaluated on the DeepMind Control Suite demonstrating significant improvements in RL performance for both pixelbased and statebased environments.  2) Strengths:  Strong Points: 1. Innovative Approach: The automated search for auxiliary loss functions in RL is a novel and principled approach that addresses the challenge of handcrafting auxiliary losses. 2. Experimental Results: Extensive experiments validate the effectiveness of AARL demonstrating notable performance gains over stateoftheart methods in both pixelbased and statebased environments. 3. Generalization Ability: AARL shows strong generalization ability to unseen environments highlighting its robustness and effectiveness. 4. Comprehensive Analysis: The paper provides insightful analyses on the relationships between auxiliary loss patterns and RL performance enriching the understanding of the role of auxiliary losses in RL.  Weak Points: 1. Complexity of Search Space: The paper acknowledges the substantial size of the search space raising concerns about scalability and computational costs. 2. Comparison with Baselines: The paper could benefit from a more comprehensive comparison with existing methods especially in terms of scalability computational efficiency and robustness. 3. Algorithm Details: Certain algorithmic details such as the evolutionary search strategy and the evaluation metrics used could be further elaborated for better understanding.  Questions and Feedback to the Author: 1. Scalability: How do you plan to address the scalability issues associated with the large search space in realworld applications 2. Comparative Analysis: Could you provide more insights into the computational efficiency and computational costs of the proposed method compared to existing approaches 3. Generalization: How do you ensure that the automatically searched auxiliary loss functions generalize well to unseen environments in practice 4. Codebase: When will the codebase be made publicly available for the research community to reproduce and extend your findings  3) Review Summary: The paper presents a significant contribution to the field of reinforcement learning by introducing an automated approach for designing optimal auxiliary loss functions. The robust experimental results and detailed analysis demonstrate the effectiveness and generalizability of the proposed method. Despite some concerns regarding the scalability and detailed algorithmic explanations the papers innovative approach strong empirical evidence and insightful analyses make it a valuable addition to the RL research community. I recommend this paper for publication based on its technological advancement experimental validation and potential to deepen the understanding of auxiliary losses in RL applications.", "zIUyj55nXR": "PeerReview of \"Frame Averaging: A General Method for Building Expressive InvariantEquivariant Neural Networks\" 1) Paper Summary: The paper introduces Frame Averaging (FA) a systematic framework for adapting existing neural network architectures to become invariant or equivariant to specific symmetries in input data such as graph or point cloud data. FA is based on the concept of employing averaging operations over carefully selected subsets of group elements called frames to ensure exact invariance or equivariance while maintaining expressive power. The paper demonstrates the effectiveness of FA across various applications including point cloud normal estimation graph separation and nbody dynamics prediction achieving stateoftheart results in these benchmarks. 2) Strengths and Weaknesses: Strong Points:  The paper addresses a critical challenge in machine learning by proposing a general method FA for incorporating symmetries into neural network architectures.  The theoretical framework presented for frame averaging is welldefined and provides clear guidelines for ensuring invariance or equivariance.  Comprehensive experimental evaluations across various tasks showcase the practical effectiveness and superiority of FA achieving stateoftheart results.  Employs rigorous mathematical formulations and theoretical proofs to support the claims and benefits of the proposed FA framework. Weak Points:  While the theoretical constructs and experimental results are robust the paper could further delve into the computational complexity and scalability aspects of the FA framework.  More discussion on the limitations or potential challenges of applying FA to realworld datasets or complex symmetries could enhance the comprehensiveness of the study. Questions and Feedback to the Authors: 1. Have you considered exploring the impact of different frame choices on the learning process and model performance 2. Can the FA framework be extended to handle more complex symmetries or nonEuclidean domains beyond the cases studied in this paper 3. How scalable is the FA framework for largescale datasets or highdimensional input spaces and are there any considerations for optimizing computational efficiency further 3) Review Summary: The paper presents a novel and impactful contribution in the field of machine learning by introducing Frame Averaging a method for adapting neural network architectures to respect symmetries in input data. The theoretical foundations and experimental evaluations demonstrate the effectiveness and superiority of the proposed FA framework in achieving stateoftheart results across various tasks. The clear articulation of the FA methodology along with the sound mathematical formulations and empirical validations underscores the importance and potential of this approach. Overall the paper is wellstructured with strong theoretical underpinnings and comprehensive experimental analyses supporting the claims. Further exploration of computational efficiency scalability and applicability to diverse symmetries could enhance the impact and applicability of the FA framework. Recommendation: Based on the robust theoretical framework compelling experimental results and the potential impact of the proposed FA framework I recommend accepting this paper for publication. The innovative nature of the method along with the rigorous treatment of the topic makes it a valuable contribution to the field of machine learning and neural network research. Further discussions on the scalability and broader applicability of the FA framework can strengthen the papers contributions.", "t1QXzSGwr9": "Summary: The paper proposes a novel framework for classifying larger realistic images using quantum systems. The approach involves encoding images in quantum states using a novel encoding mechanism that requires fewer qubits than previous methods. By using the Flexible Representation of Quantum Images method to embed input images in fewerqubit systems the framework enables classification of larger images up to 16x16 resolution achieving accuracy comparable to classical neural networks with the same number of parameters. The work also introduces a technique for further reducing the number of qubits needed to represent images. The paper outlines the theoretical background of quantum computation presents related work and provides detailed descriptions of dataset preprocessing image encoding in wavefunctions implementation details network architecture and experimental results. Strengths: 1. Innovative Approach: The paper introduces a novel framework for classifying larger realistic images using quantum systems contributing to the field of quantum machine learning. 2. Efficient Encoding Scheme: The proposed encoding mechanism requires fewer qubits than previous methods making it more suitable for practical implementation. 3. Good Performance: The framework achieves accuracy comparable to classical neural networks on larger images demonstrating the effectiveness of the proposed approach. 4. Comprehensive Experiments: The paper provides detailed results of experiments comparing the quantum neural network with a classical neural network offering a clear evaluation of the proposed framework. Weaknesses: 1. Performance Impact of Qubit Reduction: The paper acknowledges a degradation in test accuracy when reducing the number of required qubits which may limit the applicability of the compression technique in practical scenarios. 2. Unstable Learning Behavior: Attempting to lower the number of required qubits further resulted in unstable learning behavior indicating potential challenges in optimizing the network architecture for reduced feature dimensionality. Questions and Feedback: 1. Could you elaborate on the specific factors that led to the performance degradation when reducing the number of required qubits further Understanding these factors could help in refining the approach for future applications. 2. How does the proposed method compare in terms of computational efficiency and resource requirements compared to classical neural networks on larger realistic image datasets beyond MNIST Review Summary: The paper presents an innovative framework for quantum classification of larger realistic images using a novel encoding mechanism that reduces the required number of qubits. While achieving competitive accuracy with classical neural networks on MNIST images the paper highlights challenges related to qubit reduction and unstable learning behavior. Overall the paper makes significant contributions to quantum machine learning but further investigation is needed to address the limitations observed in reducing qubits and optimizing network architecture for reduced feature dimensionality. The comprehensive experiments and detailed explanations enhance the credibility of the proposed framework. Therefore I recommend acceptance with minor revisions to address the raised concerns and clarify the factors influencing performance degradation.", "qhkFX-HLuHV": " Summary of the Paper: The paper proposes a novel perspective on video understanding by treating the video recognition problem as an image recognition task. The approach involves rearranging input video frames into super images enabling training an image classifier directly for action recognition. This methodology is termed SIFAR (Super Image for Action Recognition). By utilizing transformerbased image classifiers especially the Swin Transformer the proposed approach achieves strong performance on various benchmark datasets like Kinetics400 Moments In Time SomethingSomething V2 (SSV2) Jester and Diving48. The paper also explores the use of prevailing ResNet image classifiers to further validate the proposed concept.  Strengths of the Paper: 1. Novel Perspective: The paper introduces a fresh approach to video understanding by reframing action recognition as an image classification task simplifying the process and achieving competitive results. 2. Strong Experimental Results: The methodology shows promising performance across multiple public datasets outperforming stateoftheart methods in some cases. 3. Efficiency: SIFAR is computationally efficient and can leverage existing image classifiers without the need for dedicated temporal modeling modules. 4. Swin Transformer Implementation: The use of Swin Transformer adds credibility to the proposed approach and modifications made to the architecture enhance its effectiveness.  Weaknesses of the Paper: 1. Lack of Comparison on Larger Scale Datasets: While the paper presents results on various benchmark datasets the performance on largerscale datasets could provide a more comprehensive evaluation. 2. Limited Theory and Technical Details: The paper lacks indepth discussion on the theoretical underpinnings and technical implementation aspects of the proposed approach limiting the clarity for readers looking to replicate the methodology.  Questions for the Author: 1. How does the computational efficiency of SIFAR compare to traditional spatiotemporal modeling methods in terms of training time and resource utilization 2. Can the proposed method be extended to incorporate realtime video analysis applications and what challenges might arise in such contexts 3. How sensitive is the SIFAR approach to hyperparameters such as the layout of the super image or the size of the sliding window and how were these parameters selected in the experiments  Feedback to the Author: 1. Provide more insights into the theoretical background driving the proposed methodology to enhance the understanding of the readers. 2. Include additional experiments or analysis to demonstrate the scalability and robustness of SIFAR on diverse datasets.  Review Summary: The paper presents a novel approach to video understanding by reframing action recognition as an image classification task using super images. The proposed SIFAR methodology especially with the use of transformerbased image classifiers like Swin Transformer shows strong performance on various benchmark datasets. While the paper lacks indepth technical details and comparisons on largerscale datasets the efficiency and competitiveness of the approach make it a promising direction for future video understanding research. Overall the paper provides valuable insights into simplifying action recognition with innovative imagebased strategies and warrants further exploration and validation.", "xaTensJtCP5": "Summarize the paper: The paper introduces a new approach for optimizing neural Markov Chain Monte Carlo (MCMC) proposal distributions using Ab Initio objective functions. These objective functions are designed to be applicable to arbitrarily parameterized proposal distributions defined using deep generative models. The authors propose a semiempirical procedure to determine these objective functions through the weighted combination of functions following specific constraints and transformation invariances. The experimental results demonstrate that the Ab Initio objective functions outperform existing ones for neural MCMC optimization enabling the confident optimization of highly expressive neural proposal distributions. Strengths: 1. Novel Approach: The introduction of Ab Initio objective functions for MCMC proposal optimization is novel and can overcome limitations associated with existing objective functions. 2. Theoretical Foundation: The paper provides a solid theoretical foundation for the proposed approach by outlining the first principles properties of the ground truth objective function and the constraints for constructing the Ab Initio objective functions. 3. Experimental Results: The experimental results support the efficacy of Ab Initio objective functions by demonstrating their advantages in optimizing arbitrarily parameterized MCMC proposal distributions. 4. Comparative Analysis: The comparison with existing objective functions and the demonstration of the advantages of Ab Initio objectives through various experiments enhance the credibility and relevance of the proposed approach. Weaknesses: 1. Complexity: The paper is highly technical and complex which may make it challenging for readers without a strong background in MCMC optimization and neural networks to fully grasp the content. 2. Evaluation Metrics: While the paper extensively discusses the experimental results it could benefit from a more indepth analysis and discussion of the limitations or potential biases of the evaluation metrics used. 3. Reproducibility: The paper could provide more detailed information on the methodology and experimental setup to facilitate reproducibility by other researchers. 4. Generalizability: The generalizability of the proposed Ab Initio objective functions to a broader range of optimization tasks and domains could be further explored and discussed. Questions and Feedback to the author: 1. Could you elaborate on potential challenges or limitations in applying the Ab Initio objective functions to realworld MCMC optimization problems outside the experimental setup 2. How do you envision the practical implementation of these Ab Initio objective functions in existing MCMC algorithms and neural network architectures 3. Have you considered incorporating qualitative feedback or insights from domain experts in MCMC optimization to validate the efficiency and effectiveness of the Ab Initio objective functions 4. In future research how do you plan to address the scalability of the proposed approach to handle higherdimensional data or more complex target distributions Review Summary: The paper introduces a promising approach for optimizing neural MCMC proposal distributions using Ab Initio objective functions. The theoretical foundation comparative analysis with existing objective functions and experimental results showcase the potential advantages of this new methodology. The paper could be further strengthened by addressing the complexity for nonexperts providing more transparency in the experimental methodology and discussing the broader applicability and realworld implications of the proposed Ab Initio objective functions. Overall provided the authors address these aspects the paper has the potential to make a significant contribution to the field of MCMC optimization and neural network applications.", "xS8AMYiEav3": "Summary: The paper introduces a novel methodology called REASSURE for repairing ReLU neural networks with a focus on maintaining soundness completeness and minimal modification to the network. The approach involves creating patch networks tailored to the buggy input regions ensuring correct behavior for those inputs while minimizing changes to the overall functionality of the network. The methodology is demonstrated to outperform existing methods in terms of local repairs and limiting negative side effects on several benchmarks. Strengths: 1. Innovative Methodology: The approach of using patch networks tailored to buggy input regions to ensure soundness and completeness is innovative and addresses the limitations of existing repair methods. 2. Strong Theoretical Guarantees: The paper provides strong theoretical guarantees for the proposed methodology ensuring that repairs are effective local and minimal in function space changes. 3. Comparative Analysis: The comparison with existing methods showcases the superiority of REASSURE in terms of efficacy locality minimality and performance preservation. 4. Experimental Validation: The experiments conducted on MNIST and HCAS datasets demonstrate the effectiveness of REASSURE in pointwise and area repairs providing empirical evidence to support the proposed methodology. Weaknesses: 1. Complexity in Computation: The paper mentions the challenge of computation for large DNNs with high input dimensions. Further exploration on handling this computational complexity could be beneficial. 2. Tool Dependency for Evaluation: The reliance on external tools like pycddlib for vertex enumeration might introduce variability in experimental results and can be a limitation. 3. Generalizability: While the methodology is demonstrated on ReLU networks the generalizability to networks using other activation functions could be discussed for a broader application. Question to the Authors: 1. How does the proposed methodology of REASSURE handle scenarios where the ReLU network structure is more complex such as with deeper architectures or different activation functions 2. Have you considered extending the approach to incorporate repair strategies for networks trained on nonstandard datasets or tasks that may exhibit different types of bugs or errors Feedback to the Authors: 1. To enhance the paper consider providing insights into the scalability of the methodology for larger and more complex neural networks to ensure practical applicability. 2. It would be beneficial to discuss the robustness of REASSURE to various types of buggy inputs and how the methodology adapts to different types of correctness specifications. Review Summary: The paper presents a novel and promising approach for repairing ReLU neural networks with strong theoretical guarantees and superior performance compared to existing methods. The rigor in theoretical analysis coupled with empirical validation supports the credibility of the proposed methodology. However addressing potential limitations related to computation complexity and generalizability would further strengthen the papers contribution. Overall the innovative nature of REASSURE and its demonstrated effectiveness make it a strong candidate for publication.", "tx4qfdJSFvG": " Summarize the Paper The paper introduces a novel approach called Feature Propagation for handling missing features in graph machine learning tasks. The method is based on minimizing the Dirichlet energy leading to a diffusiontype differential equation on the graph. Discretization of this equation results in an efficient iterative algorithm that outperforms existing methods in node classification tasks even when a high percentage of features are missing.  Strong Points: 1. Innovative Approach: The paper introduces a novel method Feature Propagation for handling missing features in graph machine learning based on energy minimization and diffusion. 2. Experimental Validation: The method is evaluated on various datasets and outperforms existing stateoftheart methods showcasing its effectiveness in node classification tasks even with high rates of missing features. 3. Scalability: Feature Propagation is shown to scale well to large graphs making it practical for realworld applications. 4. Comprehensive Analysis: The paper provides a thorough discussion on related work experimental setup runtime analysis and insights into when Feature Propagation works best.  Weak Points: 1. Limited Comparison: While the paper compares Feature Propagation with several methods it could benefit from a more extensive comparison with a wider range of missing data handling techniques. 2. Theoretical Insights: While the methods practical implications are wellexplained more theoretical insights into the convergence properties and computational complexity could enhance the papers depth. 3. Generalization: The discussion on extending Feature Propagation to heterogeneous graphs and dependencies among feature channels is brief and could be expanded upon to improve the methods applicability.  Questions and Feedback to the Author: 1. Can you elaborate on any further insights into the theoretical properties of Feature Propagation such as convergence guarantees or complexity analysis 2. How do you envision extending Feature Propagation to handle heterogeneous graphs and dependencies among feature channels in future research 3. Could more details be provided on the experimental setup such as hyperparameters trainvalidationtest splits and reproducibility to enhance the transparency of the results  Review Summary The paper introduces a novel method Feature Propagation for handling missing features in graph machine learning tasks based on energy minimization and diffusion. The experimental results demonstrate the methods effectiveness in node classification tasks even with high rates of missing features outperforming existing stateoftheart methods. While the paper showcases innovation scalability and practical application of Feature Propagation further theoretical insights more extensive comparisons and detailed experimental setups would enhance the overall contribution and impact of the work. Therefore I recommend this paper for publication after addressing the suggested improvements.", "zAyZFRptzvh": " Review of \"AuditAI: Bridging the Gap Between SemanticallyAligned Formal Verification and Scalability\" Summary: The paper introduces AuditAI a framework for auditing deep learning models prior to deployment through semanticallyaligned unit tests based on controlled variations in a generative models latent space. By conducting certified training and connecting generative and DL models through a shared latent space AuditAI enables verification of specifications in a meaningful interpretable manner. The framework is evaluated across diverse datasets demonstrating its effectiveness in providing robustness and scalability in auditing deep learning models. Strengths: 1. Innovative Approach: The paper proposes a novel methodology that bridges the gap between interpretability and scalability in auditing deep learning models offering a unique contribution to the field. 2. Comprehensive Evaluation: The authors conduct experiments on various datasets to validate the effectiveness of AuditAI showcasing its applicability across different domains. 3. Theoretical Foundation: The paper provides a solid theoretical foundation to support the verification process using the proposed framework demonstrating the rigor of the approach. 4. Detailed Explanation: The authors provide a thorough description of the AuditAI framework unit test definitions verification methods and deployment strategies enhancing the clarity and reproducibility of the proposed methodology. Weaknesses: 1. Limited Human Involvement: While the framework is technically robust there is a lack of direct integration of human domain expertise in the auditing process which could enhance the interpretability and adaptability of the framework in realworld scenarios. 2. Scalability Challenges: Though the scalability of the framework is addressed the practical implications of applying AuditAI to extremely large models with billions of parameters or complex architectures are not extensively discussed. Question to the Authors: 1. How do you envision incorporating domain experts directly into the AuditAI framework to enhance interpretability and address potential biases in the auditing process Feedback to the Authors: 1. Consider discussing potential strategies to scale up AuditAI for use with largescale models and complex architectures to make the framework accessible for a wider range of applications. 2. Provide insights into the computational resources and time required for implementing AuditAI on various datasets emphasizing its practical feasibility in realworld scenarios.  Review Summary: The paper introduces a novel audit framework AuditAI which bridges interpretability and scalability in verifying deep learning models. With comprehensive evaluations across diverse datasets the authors validate the effectiveness of the proposed framework. While the framework shows promise in enhancing the audit process further exploration of human involvement scalability challenges and practical implications would enhance the completeness and applicability of AuditAI. Considering the innovative methodology and thorough experimental validation I recommend accepting the paper with minor revisions to address the highlighted weaknesses and questions.", "gmxgG6_BL_N": "Summary: The paper introduces the supervised Variational Component Decoder (sVCD) as a method dedicated to extracting a single source from a nonlinear mixture. It leverages a sequencetosequence translation architecture and variational inference to approximate the inverse of the mixture process. The proposed sVCD model is validated on diverse datasets and outperforms existing methods. The paper presents the mathematical framework model architecture loss functions and evaluation results on artificially generated RFPPG and EEGEOG datasets. Strengths: 1. Novel Contribution: The paper introduces a novel sVCD model aimed at source extraction from nonlinear mixtures providing a unique and valuable contribution to the signal processing domain. 2. Theoretical Foundation: The paper presents a solid theoretical foundation by framing signal extraction as a variational inference problem and providing detailed derivations to support the proposed sVCD model. 3. Model Architecture: The sVCD architecture combining sequencetosequence translation and variational inference is welldesigned and tailored for efficient source extraction from nonlinear mixtures. 4. Experimental Validation: The paper conducts extensive evaluation on multiple datasets demonstrating superior performance of sVCD compared to baseline methods through quantitative analysis and visual representation of results. 5. Ablation Study: The ablation study provides insights into the components of sVCD showing the importance of variational inference mean regularization and attention mechanism in achieving highquality source extraction. Weaknesses: 1. Nonlinear Mixture Realworld Application: While the proposed sVCD model shows promising results on synthetic and controlled datasets the realworld applicability of source extraction from nonlinear mixtures such as in the RFPPG dataset needs further exploration. 2. Reproducibility Concern: As the RFPPG dataset is not made public due to privacy concerns replicating the experiments on this specific dataset might be challenging for researchers. 3. Model Complexity: The complexity of the sVCD model including variational inference and attention mechanism might hinder its practical implementation in certain resourceconstrained environments. Question and Feedback to the Authors: 1. How does the sVCD model handle different levels of noise and variability in realworld datasets especially in scenarios like RFPPG where signals may be affected by various external factors 2. Could you provide more insights into the interpretability of the extracted source signals by the sVCD model especially in scenarios where prior knowledge might be limited or noisy 3. Considering the complexity of the sVCD architecture are there any plans to optimize the model for faster inference or realtime applications Review Summary: The paper introduces a novel supervised Variational Component Decoder (sVCD) model for signal extraction from nonlinear mixtures. With a strong theoretical foundation welldesigned architecture and robust experimental validation the sVCD outperforms existing methods on diverse datasets. Despite some limitations in realworld applicability and model complexity the thorough evaluation and ablation study provide valuable insights into the efficacy of sVCD. Therefore based on the comprehensive analysis and strong experimental results I recommend the acceptance of this paper for publication. Further refinements and applications of the sVCD model could enhance its impact in signal processing research.", "fRb9LBWUo56": " Summary of the Paper: The paper investigates the effectiveness of deep reinforcement learning (RL) sampling policies in accelerating magnetic resonance imaging (MRI) reconstruction compared to fixed sampling masks. The authors carefully reproduce and compare stateoftheart RL methods in MRI sampling. Surprisingly they find that a simple greedily trained fixed policy can match or even outperform deep RL methods. The results suggest that the added value of current RL approaches over fixed masks in MRI sampling may be limited emphasizing the importance of strong fixed baselines standardized reporting and careful experimental design.  Strengths of the Paper: 1. Rigorous Analysis: The paper demonstrates a thorough reproduction and comparison of stateoftheart RL methods in MRI sampling providing a comprehensive evaluation. 2. Clear Presentation: The paper is wellstructured providing a detailed background on MRI fundamentals sampling optimization and data processing pipeline enhancing understanding for readers with varying levels of expertise. 3. Reproducibility: The authors plan to release their code upon acceptance promoting reproducibility and transparency in research. 4. Insights on Baselines: The paper emphasizes the importance of using strong fixed baselines such as Learningbased Compressive Sampling (LBCS) for fair evaluations contributing to best practices in the field. 5. Discussion on Metrics: The paper discusses the importance of using multiple metrics such as SSIM PSNR MSE and AUC and the implications of different ways of reporting results promoting a comprehensive analysis approach.  Weaknesses of the Paper: 1. Limited Scope: The paper focuses mainly on the comparison between RL approaches and fixed sampling masks possibly missing insights from other sampling optimization techniques. 2. Possible Overgeneralization: The paper primarily evaluates on MRI datasets and the generalizability of the findings to other medical imaging modalities or domains may need further validation. 3. Implications for RL Methods: While the findings are important the paper does not extensively delve into why RL methods did not offer significant benefits over fixed masks leaving some questions unanswered.  Questions and Feedback: 1. Question: Could the performance of RL methods be influenced by the specific hyperparameters chosen in the experiments and how sensitive are the results to variations in these parameters 2. Feedback: It would be beneficial to discuss the limitations and constraints of the experimental setup in more detail to provide a clearer understanding of the implications of the findings.  Review Summary: The paper provides a comprehensive investigation into the performance of deep RL methods compared to fixed sampling masks in accelerating MRI reconstruction. While the study shows that fixed policies can perform on par or better than RL approaches it raises questions about the significant benefits of current RL methods in this context. The papers strength lies in its rigorous analysis clear presentation and emphasis on the importance of strong baselines and metrics in evaluating MRI sampling policies. Overall the paper contributes valuable insights to the field but further elaboration on the limitations and practical implications of the findings could enhance its impact. Based on the strengths and the potential impact of highlighting the importance of standardized reporting and strong baselines I recommend acceptance of the paper provided the authors address the weaknesses identified and incorporate additional clarity in the discussion.", "mRF387I4Wl": "1) Summary of the paper The paper introduces FlowX a novel method to improve the explainability of graph neural networks (GNNs) by focusing on message flows. The paper argues that understanding message flows is crucial for explaining the inner workings of GNNs and proposes a systematic framework based on the philosophy of Shapley values to quantify the importance of different message flows. The proposed method involves initial approximation with Monte Carlo sampling and a learningbased algorithm to enhance explainability. Experimental studies on synthetic and realworld datasets demonstrate that FlowX outperforms existing methods in terms of explainability. 2) Evaluation of Strong and Weak Points Questions and Feedback Strong Points:  Innovative Approach: The paper presents a novel approach to explain the working mechanisms of GNNs by focusing on message flows which is fundamental and intuitive.  Theoretical Foundation: By utilizing the Shapley values from cooperative game theory the paper establishes a sound theoretical foundation for explaining GNNs.  Comprehensive Experimental Validation: Extensive experiments on both synthetic and realworld datasets demonstrate the effectiveness of FlowX in improving explainability compared to existing methods.  Clear Presentation: The paper is wellstructured and the explanations are detailed and easy to follow. Weak Points:  Complexity: The methodology involving Shapley values and Monte Carlo sampling might be challenging for readers unfamiliar with these concepts. Simplifying the explanations or providing more intuitive examples could help improve clarity.  Comparison with Baselines: While the paper mentions comparisons with existing methods a more detailed analysis of how FlowX outperforms these baselines and the reasons for superior performance could enhance the evaluation section. Questions and Feedback:  Clarification on Shapley Values: Could the paper provide a more detailed explanation or a simple example illustrating how Shapley values are computed and utilized in the context of GNN explainability  Scalability: How does the proposed method scale with larger and more complex graph structures Is there any impact on performance when dealing with significantly larger datasets  Generalization: Can the insights and lessons learned from explaining GNNs using FlowX be applied to other types of deep learning models beyond graph neural networks 3) Review Summary The paper presents an innovative method FlowX to enhance the explainability of graph neural networks by focusing on message flows and utilizing Shapley values. While the approach is theoretically sound and supported by comprehensive experiments the complex nature of the methodology and the need for more detailed comparisons with existing methods could be addressed. This work makes a significant contribution towards understanding the inner workings of GNNs and provides valuable insights for improving explainability in deep learning models. Overall I recommend the paper for acceptance with minor revisions to enhance clarity and strengthen the comparison with existing methods.", "hniLRD_XCA": " PeerReview of \"Deep Stochastic Koopman Operator for Robust Learning Control\"  1) Summary: The paper introduces the concept of the deep stochastic Koopman operator (DeSKO) model for handling uncertainties in dynamical systems to enhance robustness. The DeSKO model infers a distribution of observables to capture system uncertainty and designs a robust control framework based on this distribution. The paper showcases experiments on various advanced control benchmarks demonstrating that DeSKO outperforms existing methods in terms of modeling control and robustness.  2) Strengths:  Innovative Concept: Introducing the DeSKO model to handle system uncertainties is a novel approach that can potentially advance the field of learning control for complex nonlinear systems.  Robust Control Framework: The proposed robust control framework based on the DeSKO model showcases promising results in terms of stability and robustness as demonstrated in the experiments.  Comprehensive Experiments: The paper conducts extensive experiments on multiple control benchmarks comparing the DeSKO model with stateoftheart methods showcasing its superiority in various scenarios.  Theoretical Underpinning: The paper provides a solid theoretical foundation deriving the DeSKO model and presenting stability guarantees for both exact and approximated Koopman operators which strengthens the credibility of the proposed approach.  3) Weaknesses:  Complexity Concerns: The paper involves advanced mathematical concepts and algorithms potentially making it challenging for readers less familiar with control theory and deep learning techniques to grasp the full depth of the methodology.  Evaluation Limitation: While the experiments are comprehensive more detailed discussion on the limitations or scenarios where the DeSKO model might not perform as well would provide a more holistic perspective on its applicability.  Practical Implementation: It would be beneficial to include insights into the practical implementation aspects of the DeSKO model such as computational efficiency and realworld deployability.  Questions and Feedback to the Author: 1. Can you elaborate on how the DeSKO model handles scalability to highdimensional systems particularly in terms of computational efficiency and training time 2. How does the proposed entropy constraint impact the overall performance and robustness of the DeSKO model Is there a tradeoff between robustness and model accuracy with varying entropy thresholds 3. Have you considered realworld applications or scenarios where the DeSKO model could be applied and what are the potential challenges in implementing this approach in practical systems  Review Summary: The paper presents a cuttingedge methodology with the DeSKO model for robust learning control of nonlinear stochastic systems addressing uncertainties in dynamical systems. The comprehensive theoretical foundation coupled with extensive experiments showcases the effectiveness and superiority of the DeSKO model compared to existing methods. The innovative concept robust control framework and theoretical rigour support a strong recommendation for acceptance provided that the authors address the complexity concerns elaborate on practical implementation aspects and offer insights into realworld applications and challenges. Overall the paper contributes significantly to the field of learning control for highdimensional nonlinear systems.", "vLz0e9S-iF3": " Summary of the Paper: The paper introduces a quantitative theory on the escape problem of a stochastic gradient descent (SGD) algorithm and explores the impact of loss surface sharpness on escaping efficiency. The authors utilize a quasipotential theory from large deviation theory to formalize the escape problem. They investigate mean exit times to quantify escaping focusing on the effects of batch size learning rate and geometric parameters of loss surfaces. The theory findings suggest that sharp minima can slow down escaping but the SGD\u2019s noise can counteract this effect leading to exponentially fast escape from sharp minima.  Strong Points: 1. Novel Approach: Introducing quasipotential theory to analyze SGD escaping is a significant contribution providing a novel perspective on understanding SGD dynamics. 2. Comprehensive Analysis: The paper covers a detailed theoretical analysis of SGD escaping mechanisms considering important factors like sharpness of loss surfaces batch size and learning rate. 3. Experimental Validation: Conducting experiments to empirically validate the theoretical findings using neural networks and real data enhances the credibility of the research. 4. Theoretical Rigor: Assumptions and basic formulations are clearly stated and the theoretical framework is systematically developed to support the main findings. 5. Extension to Practical Setups: The theorys flexibility in applying to practical setups including SGD with discrete update and statedependent gradient noise enhances its practical relevance.  Weak Points: 1. Complexity: The papers technical nature and mathematical depth might pose challenges for readers not wellversed in large deviation theory and stochastic dynamical systems. 2. Clarity in Results Presentation: The presentation of theoretical results and equations could be improved for better comprehension particularly for readers outside the specialized field. 3. Empirical Study Depth: While the empirical validation is a strong point a more detailed discussion on the experiments methodologies and results interpretation could enhance the papers impact.  Questions and Feedback to the Author:  Can you elaborate on how the findings of your study can be practically applied in optimizing deep learning models using SGD in realworld scenarios  How does the quasipotential theory contribute to addressing the open question of why SGD can find parameters that generalize well over nonconvex loss surfaces  Considering the complexities of the theoretical framework have you considered developing simplified models or visual aids to make the theory more accessible to a broader audience  Review Summary: The paper presents a novel and rigorous theory utilizing quasipotential to analyze the escape problem of SGD shedding light on the dynamics of SGD escaping from sharp minima. The comprehensive theoretical analysis and empirical validation enhance the credibility of the research. However improvements in clarity of presentation and practical implications could strengthen the impact of the paper. Overall the paper provides valuable insights into understanding the escaping mechanisms of SGD and merits consideration for publication pending minor revisions and clarifications.", "uxgg9o7bI_3": " Summary of the Paper: The paper introduces a new approach to design powerful Graph Neural Networks (GNNs) by injecting structural properties of graphs into a messagepassing aggregation scheme. The proposed model GraphSNN is theoretically proven to be more expressive than the WeisfeilerLehman (WL) test in distinguishing graph structures. The paper provides a theoretical basis for the proposed model discusses a new hierarchy of local isomorphism on neighborhood subgraphs and empirically validates the models strength on different graph learning tasks. The results show that GraphSNN outperforms stateoftheart methods without compromising computational simplicity and efficiency.  Strong Points of the Paper: 1. Theoretical Foundation: The paper establishes a solid theoretical foundation by introducing a new hierarchy of local isomorphism and demonstrating how GNNs can be designed to be more expressive than the WL test which is a significant contribution. 2. Novel Neural Model: The proposed GraphSNN model is innovative and proven to be strictly more expressive than the WL test in distinguishing graph structures. 3. Empirical Verification: The paper conducts experiments on benchmark tasks to validate the proposed models effectiveness showcasing consistent improvements over existing methods. 4. Clear Exposition: The paper is wellstructured clearly explaining the problem methodology contributions and experimental results. 5. Comparative Analysis: The paper provides detailed comparisons with existing models in terms of expressive power computational complexity and performance on various datasets.  Weak Points of the Paper: 1. Lack of Realworld Applications: The paper could benefit from providing insights into realworld applications where the proposed GraphSNN model can be applied and its practical implications. 2. Overemphasis on Theoretical Aspect: While the theoretical foundation is essential the paper could further elaborate on practical implications and scenarios where the proposed model may be advantageous. 3. Complexity and Scalability: The paper should discuss the scalability and computational complexity of the proposed model in dealing with largescale graphs and datasets.  Question to the Author: 1. Can you provide more insights into the practical applications where the proposed GraphSNN model can demonstrate its effectiveness in realworld scenarios 2. How does the proposed model address challenges related to scalability and computational complexity when applied to largescale graphs 3. Have you considered any interpretability techniques to understand the decisionmaking process of the GraphSNN model in distinguishing graph structures  Feedback to the Author: 1. Clarity of Presentation: The paper is wellstructured and effectively communicates the proposed methodology. Maintaining this clarity will enhance the readability and understanding of the research. 2. Detailed Explanations: Provide more detailed explanations and examples to help readers grasp the significance and implications of the new hierarchy of local isomorphism and the practical benefits of the proposed GraphSNN model. 3. Future Directions: Consider discussing future research directions potential enhancements and extensions of the proposed model to contribute further to the field of graph neural networks.  Review Summary: Overall the paper presents a novel approach to designing powerful GNNs with a focus on injecting structural properties into the aggregation scheme. The theoretical foundation empirical validation and comparisons with stateoftheart methods demonstrate the effectiveness of the proposed GraphSNN model. While the paper excels in theoretical rigor and innovation addressing practical applications scalability and interpretability aspects would further strengthen the contribution of the research. Given the strong theoretical basis and promising experimental results I recommend the paper for acceptance with minor revisions to address the weak points highlighted above.", "onwTC5W0XJ": " Summary: The paper proposes a method called Causally Focused Convolutional Networks (CFCN) to address the issue of Convolutional Neural Networks (CNNs) learning spurious correlations between features and labels. The authors propose using minimal human guidance in the form of activation masks to guide CNNs to focus on extracting causal features from images. By modifying the learning process to prioritize causal features the proposed method aims to improve learning efficiency with less data and enhance the interpretability of CNN models. The effectiveness of the method is demonstrated across various datasets showing promising results in learning causal features and outperforming baselines in terms of accuracy and robustness.  Strong Points: 1. Innovative Approach: The paper addresses a critical issue in CNNs by proposing a method to focus on learning causal features which has significant implications in domains where interpretability and justification of model decisions are crucial. 2. Experimental Validation: The authors provide a detailed experimental setup including multiple datasets and baseline comparisons to demonstrate the effectiveness of their proposed method. 3. Comprehensive Analysis: The paper includes a thorough analysis of the proposed method discussing the significance of causal features the impact on learning efficiency and the comparison with existing techniques.  Weak Points: 1. Clarification on Implementation: A detailed explanation of the practical implementation of the proposed method including the process of creating activation masks and integrating them into the CNN training process would enhance the clarity of the methodology. 2. Evaluation Metrics: While the paper mentions the use of quantitative metrics like accuracy macro f1score and AUROC more indepth analysis and comparison with existing interpretability techniques would strengthen the evaluation of the proposed method.  Questions to the Author: 1. How scalable is the proposed method in terms of incorporating human guidance for larger datasets Have you tested its performance with datasets of varying sizes 2. Can you elaborate on the computational overhead involved in training CNNs with activation masks and how it compares to traditional CNN training processes  Feedback to the Author: 1. Consider expanding the discussion on the generalizability of the proposed method to different image classification tasks and datasets. 2. Provide insights into the interpretability aspects of the model how easily can the decisions made by the CFCNs be explained to endusers or domain experts  Review Summary: The paper presents a compelling approach Causally Focused Convolutional Networks (CFCN) to enhance the interpretability and efficiency of CNNs by focusing on learning causal features. The method shows promising results in improving learning efficiency with reduced data requirements and enhancing the robustness of feature extraction. The experimental validation and detailed analysis provided offer strong support for the effectiveness of the proposed method. Addressing the weak points related to implementation clarity and indepth evaluation would further solidify the papers contributions. Overall I recommend accepting the paper for publication with minor revisions for clarification and additional analysis.", "tUMr0Iox8XW": " Summary of the Paper: The paper introduces a novel feature learning limit called the \u03c0limit which aims to overcome computational difficulties associated with the existing \u00b5limit in deep multilayer perceptrons. The \u03c0limit is proposed as a limit of a form of projected gradient descent enabling feature learning capabilities while simplifying computations. The authors evaluate the \u03c0limit on CIFAR10 and Omniglot datasets comparing it with Neural Tangent Kernel (NTK) and finitewidth networks. Results show that the \u03c0limit outperforms finitewidth models and closes the performance gap between finite and infinitewidth neural networks previously left by NTK. Experimental results on transfer learning from ImageNet to CIFAR10 and fewshot learning on Omniglot showcase the effectiveness of the \u03c0limit in practical scenarios.  Strengths of the Paper: 1. Innovative Contribution: Introducing the \u03c0limit as a solution to enable feature learning in deep neural networks without the computational complexities of existing limits is a significant contribution. 2. Experimental Validation: Extensive experiments on standard datasets (CIFAR10 Omniglot) demonstrate the superiority of the \u03c0limit over existing models showcasing its practical utility. 3. Theoretical Insight: The paper provides theoretical insights into the working mechanisms of the \u03c0limit offering a systematic and rigorous analysis of the proposed method. 4. Transfer Learning and FewShot Learning Results: The successful application of the \u03c0limit in transfer learning scenarios and fewshot learning tasks underscores its versatility and effectiveness across different scenarios. 5. Comprehensive Experiments Analysis: The paper provides detailed analyses of the effects of width depth and other hyperparameters on model performance giving insights into the behavior of the proposed \u03c0limit.  Weaknesses of the Paper: 1. Complexity of Concepts: The paper involves intricate mathematical formulations and theoretical explanations that may be challenging for readers without a strong background in neural networks and optimization. 2. Experimental Complexity: The experimental setup especially for fewshot learning and transfer learning may be complex and involve numerous hyperparameters potentially making the reproducibility of results challenging for researchers. 3. Limited RealWorld Datasets: While the experiments on CIFAR10 and Omniglot are popular in the research community the generalizability of the proposed method to a wider range of realworld datasets could be further explored.  Questions and Feedback to the Authors: 1. Clarification: Can the authors provide more insights into the computational complexity of the \u03c0limit compared to other existing limits in practical scenarios 2. Generalization: How do the findings of the paper extend to more complex architectures beyond MLPs such as convolutional neural networks or transformers 3. Reproducibility: In what ways can the authors simplify their experimental setup to enhance reproducibility and scalability for other researchers in the field 4. Impact: How does the proposed \u03c0limit contribute to advancing the understanding of neural network behaviors and their applications in realworld scenarios beyond the datasets considered in the paper  Review Summary: The paper introduces a novel feature learning limit the \u03c0limit to address computational difficulties while enabling feature learning in deep neural networks. The experimental results showcase the superior performance of the \u03c0limit over existing models closing the performance gap between finite and infinitewidth networks. The theoretical analysis and comprehensive experiments strengthen the papers credibility. Overall I recommend acceptance of the paper due to its innovative contribution rigorous analysis and promising results showcased in diverse experimental scenarios. Further clarifications on computational complexity applicability to varied architectures and practical implications will enhance the papers impact and relevance in the field.", "mKDtUtxIGJ": " Summary The paper introduces a deep point cloud reconstruction network that addresses the challenges of sparsity noise irregularity and outliers in 3D point clouds obtained from scanning devices. The proposed network consists of two stages: a voxel generation network for densification and outlier removal and a voxel relocalization network for converting discrete voxels into 3D points using transformers. A novel module called amplified positional encoding is incorporated to enhance the performance of the transformer. Extensive experiments on various datasets demonstrate that the network achieves stateoftheart performance and generalizes well to realworld scenarios.  Strengths 1. Innovative Approach: The paper proposes a novel joint framework for point cloud reconstruction integrating densification denoising and outlier removal in a single network. 2. Novel Module: The use of amplified positional encoding to enhance the performance of transformers is a novel contribution that significantly improves the quality of point cloud reconstruction. 3. Extensive Experiments: The paper provides detailed experimental results on multiple datasets comparing the proposed method with stateoftheart techniques and showcasing its superior performance.  Weaknesses 1. Lack of Detailed Analysis: While the experiments demonstrate the effectiveness of the proposed network a more indepth analysis of the networks limitations or failure cases could provide valuable insights for further improvements. 2. Generalization Claims: The paper claims good generalization to realworld and unmet scenes but a more thorough evaluation on a diverse set of realworld datasets could strengthen this claim.  Questions and Feedback to the Author 1. Scalability: How does the proposed network scale with larger and more complex point cloud datasets Have you evaluated its performance on extremely largescale point clouds 2. Training Efficiency: What is the training time and computational complexity of the proposed network Are there any computational bottlenecks that need to be addressed for practical deployment 3. Robustness: How robust is the network to variations in input data quality or characteristics Have you explored its performance under different levels of noise or outliers  Review Summary The paper presents a comprehensive study on point cloud reconstruction proposing an innovative deep network that jointly addresses densification denoising and outlier removal. The incorporation of amplified positional encoding and transformerbased voxel relocalization enriches the networks performance leading to stateoftheart results across various datasets. While the paper showcases significant advancements in point cloud processing further analysis on network scalability efficiency and robustness would enhance the overall credibility of the proposed approach. Overall the paper is wellstructured and the experimental results provide sufficient evidence to support the claims made. Therefore I recommend this paper for publication pending minor revisions to address the mentioned weaknesses.", "k7-s5HSSPE5": "Summary: The paper explores the effectiveness of deep multilingual language models in crosslingual transfer learning and proposes an unsupervised transfer method called importanceweighted domain alignment (IWDA). The authors empirically analyze the invariance of feature representations across languages the impact of distributional shifts in class priors on transfer performance and the effectiveness of IWDA in addressing these challenges. Through experiments on sentiment analysis namedentity recognition and textual entailment tasks the authors demonstrate the superiority of IWDA under large prior shifts and show additional performance gains when combined with semisupervised learning techniques. Strengths: 1. Empirical Analysis: The paper provides a thorough empirical analysis of key factors influencing crosslingual transfer learning offering valuable insights into shared feature representations and class prior shifts. 2. Novel Methodology: The proposed method IWDA addresses an overlooked issue of class prior shifts in crosslingual transfer learning and demonstrates superior performance under large prior shifts. This introduces a novel approach to unsupervised transfer learning. 3. Experimental Validation: The experiments conducted on multilingual sentiment analysis namedentity recognition and textual entailment tasks provide concrete evidence of the effectiveness of IWDA and its compatibility with semisupervised learning techniques. Weaknesses: 1. Optimization Instabilities: The paper acknowledges instabilities in the importance weight estimation process of IWDA which could raise questions about the stability and reliability of the method in practical applications. 2. Noise in Results: The authors highlight potential noise from the optimization and adversarial training aspects of IWDA which may affect the performance compared to semisupervised learning methods and could raise concerns about the robustness of the proposed approach. Questions and Feedback: 1. Clarity on Optimization: Can the authors provide more insight into the specific reasons behind the optimization instabilities observed in the importance weight estimation procedure of IWDA Are there plans to address these instabilities in future work 2. Robustness Evaluation: Have the authors considered conducting additional experiments to test the robustness of IWDA under different noise levels or perturbations in the data to assess its generalizability across various scenarios Review Summary: The paper presents a comprehensive analysis of factors influencing crosslingual transfer learning and introduces a novel unsupervised transfer method IWDA to address the issue of class prior shifts. While the empirical results demonstrate the superiority of IWDA under large prior shifts and its compatibility with semisupervised learning techniques the paper should address concerns regarding optimization instabilities and noise in results for better clarity and robustness. Overall the paper makes a significant contribution to the field of crosslingual transfer learning and warrants further investigation and refinement to enhance its applicability and reliability.", "iedYJm92o0a": "Summary: The paper explores how Transformers despite struggling with multistep computation tasks can be enhanced by introducing a \"scratchpad\" mechanism. By allowing models to emit intermediate computation steps into a scratchpad the researchers demonstrate improved performance on tasks such as long addition polynomial evaluation and executing Python programs. The scratchpad approach helps models adapt computation time store intermediate states and reduce error propagation. The experiments showcase significant enhancements using the scratchpad technique especially in tasks involving algorithmic reasoning. Strengths: 1. Innovative Approach: Introducing the scratchpad concept to enhance Transformer models for multistep computation tasks is novel and demonstrates tangible performance improvements. 2. Comprehensive Experiments: The paper conducts a series of experiments on various tasks providing detailed results and comparisons to showcase the efficacy of the scratchpad technique. 3. Clear Presentation: The paper is wellstructured with a comprehensive introduction detailed methodology results analysis related work discussion and future research directions. 4. Theoretical Insights: Discussing the limitations of traditional Transformer architectures for advanced computational tasks and proposing a simple but effective modification is valuable for the research community. Weaknesses: 1. Evaluation Metrics Clarity: While the results are presented comprehensively it would be beneficial to provide a more detailed explanation of how performance metrics were calculated and evaluated. 2. Reproducibility: Although the finetuning and evaluation datasets are opensource or synthetic the dependency on generations from the pretrained transformer model for MBPPAug dataset could hinder full reproducibility. Question and Feedback to the Authors: 1. Can you provide more insights into the exact training procedures used including hyperparameters and training epochs 2. Have you considered exploring the generalizability of the scratchpad technique to other types of tasks beyond the ones mentioned in the paper 3. Further investigation into whether the scratchpad technique can be extended to architectures beyond Transformers could be intriguing have you explored this possibility Review Summary: The paper presents an innovative scratchpad technique to enhance Transformers for multistep computation tasks demonstrating remarkable improvements in performing long addition polynomial evaluation and executing Python programs. The clear presentation comprehensive experiments and theoretical insights add significant value to the research domain. While the paper excels in innovation and methodological clarity addressing metrics clarity and reproducibility concerns would further strengthen its impact. Therefore based on the strong contributions and potential for advancing the field I recommend this paper for publication with minor revisions to address the outlined weaknesses.", "mMiKHj7Pobj": " Summary The paper introduces the concept of Autoinduced Distributional Shift (ADS) highlighting that machine learning algorithms can inadvertently change the distribution of their inputs leading to unexpected behaviors. It proposes unit tests for incentives to diagnose whether algorithms reveal incentives for ADS and introduces the notion of managing learners incentives to guide algorithmic behavior. Experiments are conducted to demonstrate how metalearning can reveal hidden incentives impacting algorithm behavior. Moreover a toy environment simulating realworld content recommendation issues is developed to illustrate the effects of ADS.  Strong Points 1. Innovative Concept: The introduction of ADS and unit tests for incentives is novel and provides a fresh perspective on algorithm behavior. 2. Clear Methodology: The paper systematically details the approach for diagnosing hidden incentives conducting experiments and proposing a mitigation strategy. 3. Practical Applications: The experiments on content recommendation showcase the relevance and impact of ADS in realworld scenarios adding practical value to the research.  Weak Points 1. Complexity: The paper delves into technical details and terminology that may be challenging for readers unfamiliar with the field. 2. Experimental Setup: While the experiments are insightful there could be more emphasis on discussing the implications of the results on practical applications. 3. Limitations: The paper could elaborate on the limitations of the proposed methodology and potential challenges in implementing these strategies in realworld systems.  Questions and Feedback 1. Clarity on Unit Tests: Could you elaborate on the specific parameters and conditions set for the unit tests to ascertain whether incentives for ADS are revealed 2. Practical Implementation: How feasible would it be to incorporate the proposed unit tests and incentive management strategies in existing machine learning systems 3. Comparison with Existing Approaches: How does this work align with or differentiate from other studies that address similar issues related to incentives and algorithmic behavior manipulation  Review Summary The paper presents a thoughtprovoking concept of ADS and provides a structured methodology for identifying incentives that drive algorithmic behavior changes. It effectively combines theoretical discussions with practical experiments to demonstrate the implications of hidden incentives in machine learning systems. While the paper contributes valuable insights into the management of learners incentives further clarity on the implementation and realworld applicability of the proposed strategies would enhance its impact. Overall the paper makes a significant contribution to the understanding of algorithmic decisionmaking and highlights the importance of considering incentives in ensuring ethical and desired behaviors in machine learning systems.", "rHMaBYbkkRJ": "Summary of the Paper: The paper introduces the Continual Learning EValuation Assessment Compass (CLEVACompass) a visual tool designed to facilitate transparent assessment comparison and interpretation of empirical setups and results in the field of continual machine learning. The authors argue that the diverse applications of continual learning necessitate distinct evaluation scenarios challenging prevalent algorithmcentric perspectives and evaluation protocols. They highlight the complexity of continual learning evaluation which involves factors like catastrophic interference evolving datasets and various influences from related machine learning paradigms. The CLEVACompass comprises two levels: an inner level representing methodological influences and an outer level focusing on practical reporting of evaluation metrics. Strong Points: 1. Innovative Tool: The introduction of the CLEVACompass is a significant contribution to the field of continual machine learning providing a novel visual representation for evaluating methods. 2. Thorough Examination: The paper extensively examines the challenges and nuances of continual learning evaluation shedding light on the complexities that arise in comparison studies. 3. Contextualization: The paper effectively contextualizes the need for a tool like CLEVACompass in light of related machine learning paradigms and the evolving landscape of continual learning research. 4. Comprehensive Framework: The tools inner and outer levels offer a holistic framework for evaluating continual learning methods considering both methodological influences and practical reporting aspects. Weak Points: 1. Complexity: The detailed and technical nature of the paper may make it challenging for readers unfamiliar with continual learning concepts to grasp the full scope and implications of the CLEVACompass. 2. Limited Practical Examples: While the paper discusses five example continual learning methods more practical demonstrations or case studies could enhance the readers understanding of how the CLEVACompass is applied in real scenarios. 3. Future Directions: The paper hints at future updates and improvements to the CLEVACompass to incorporate additional elements indicating that the tool may still be evolving and may require ongoing refinement. Questions and Feedback for the Author: 1. Can the authors provide more insights into how the CLEVACompass could address challenges related to reproducibility and comparability in continual learning research 2. Considering the evolving nature of machine learning paradigms how adaptable is the CLEVACompass to accommodate new influences and evaluation metrics 3. Could the authors elaborate on any practical considerations or challenges in implementing and utilizing the CLEVACompass within research studies Review Summary: The paper presents an innovative solution the CLEVACompass to address the challenges of evaluating continual machine learning methods. While the paper offers a comprehensive analysis of the complexities in continual learning evaluation providing a new tool for contextualizing and comparing research works there is a need for further clarification on practical implementation and adaptability. Overall the papers contribution to enhancing transparency and comparability in continual learning research is commendable and I recommend its publication pending minor revisions to address the identified weak points and clarifications to the practical application of the CLEVACompass.", "vnENCLwVBET": " Paper Summary The paper introduces OUMG a novel universal evaluation metric for text generation tasks that do not have fixed reference standards such as story generation and poetry generation. OUMG utilizes a discriminator trained to differentiate between humangenerated and machinegenerated text to score the generated sentences objectively. The discriminators scores reflect the similarity of the generated text to humangenerated text and can guide the text generation process to enhance model performance. Experiments in poetry generation demonstrate OUMGs effectiveness in evaluating models without reference standards and improving the quality of generated text.  Strengths  Innovative Concept: The paper presents a novel approach OUMG to evaluate text generation tasks without relying on fixed reference standards addressing a critical limitation in existing evaluation metrics.  Objectivity: OUMG leverages a discriminator to score sentences objectively based on their similarity to humangenerated text eliminating the subjectivity often present in human judgments.  Versatility: The proposed metric is applicable to a wide range of text generation tasks not limited to poetry generation showcasing its universality.  Guidance for Model Improvement: The discriminator can guide the text generation process to enhance model performance providing practical utility beyond evaluation.  Weaknesses  Lack of Comparison: The paper could benefit from comparing OUMG with existing stateoftheart metrics to demonstrate its superiority.  Clarity of Experiments: While the experiments on poetry generation showcase the effectiveness of OUMG additional experiments on diverse text generation tasks would strengthen the papers empirical validation.  Theoretical Depth: Some sections especially related work could be further expanded to provide a more comprehensive literature review and theoretical foundation for OUMG.  Questions  Feedback to the Author  Question: How does OUMG perform in comparison to established metrics like BLEURT or Bertscore in evaluating text generation tasks especially in scenarios with available reference standards  Feedback: Consider elaborating on the computational efficiency and scalability of OUMG particularly in largescale text generation tasks to provide a complete picture of its practical applicability.  Review Summary The paper introduces a novel evaluation metric OUMG for text generation tasks without fixed reference standards demonstrating its objectivity and usefulness in guiding model improvement. While showcasing promising results in poetry generation experiments the paper could benefit from broader empirical validation and deeper theoretical exploration. Overall the innovative nature of OUMG and its potential to address key limitations in text generation evaluation make it a valuable contribution to the field. Further validations and comparisons with existing metrics would enhance the papers impact and credibility.", "rqolQhuq6Hs": "Summary: The paper explores the relationship between the noise in stochastic gradient descent (SGD) and the escape from local minima in deep learning models. The study introduces a novel formalism using the logarithmized loss gradient to derive a stochastic differential equation (SDE) with simpler additive noise leading to insights into the escape dynamics from local minima and the implicit biases of SGD. The main contributions include the derivation of an equation approximating the SGD noise a novel SDE formulation and the escaperate formula influenced by the logarithmized loss barrier height. Strengths: 1. Theoretical Insights: The paper provides theoretical insights into the dynamics of SGD in deep learning linking SGD noise to escape from local minima and implicit biases enhancing the understanding of optimization processes. 2. Novel Methodology: The introduction of the logarithmized loss gradient and the resulting SDE formulation with simpler additive noise is a novel approach that sheds light on the stochastic aspects of gradient descent in machine learning. 3. Experimental Validation: The experimental verification of theoretical predictions using linear regression and neural network models strengthens the papers theoretical contributions by demonstrating practical applicability. Weaknesses: 1. Exclusion of NonGaussian Effects: While the paper focuses on Gaussian noise considering nonGaussian effects as mentioned in the conclusion could further enrich the findings and provide a more comprehensive understanding. 2. Limited Empirical Exploration: While the experiments validate the theoretical predictions a more extensive exploration with diverse datasets and architectures could enhance the generalizability of the findings. 3. Clarity in Presentation: The paper contains dense technical content that may be challenging for readers not deeply familiar with the subject matter. Improving clarity in presenting complex ideas could enhance accessibility. Question to the Author:  How do you envision extending your research to incorporate nonGaussian effects in SGD and its impact on escape dynamics from local minima  Could you elaborate more on the implications of your findings on practical deep learning applications and potential future research directions stemming from this work Feedback:  Enhancing the clarity of technical content and providing concise summaries of results and implications could benefit readers unfamiliar with the specific domain.  Expanding empirical validation with a wider range of models and datasets could strengthen the practical relevance of the findings. Review Summary: The paper introduces a novel approach utilizing the logarithmized loss gradient to analyze stochastic gradient descent dynamics in deep learning models providing valuable theoretical insights and practical implications. The experimental validation showcases the applicability of the theoretical framework. While the paper is strong in theoretical contributions and experimental verification improvements in clarity and empirical exploration could enhance its impact. Therefore based on the presented strengths and weaknesses I recommend acceptance after addressing the minor concerns raised.", "yV4_fWe4nM": " Peer Review  1. Summary The paper introduces a novel deep fair clustering framework aimed at incorporating grouplevel fairness into deep clustering algorithms. The authors tackle the challenge of ensuring fairness in clustering applications where sensitive attribute variables must be considered. They propose a formulation based on Integer Linear Programming (ILP) to guarantee grouplevel fairness and demonstrate the effectiveness of their approach through experiments on realworld datasets. The main contributions include optimizing fairness for multistate Protected Status Variables (PSVs) formulating fairness assignments as an ILP problem proposing an endtoend refinement learning algorithm and achieving competitive clustering performance with guaranteed fairness.  2. Strengths and Weaknesses  Strong Points:  Novel Concept: The paper addresses an important and challenging problem of ensuring fairness in deep clustering algorithms by introducing grouplevel fairness through ILP.  Theoretical Foundation: The use of ILP for fairness assignment formulation and optimization is theoretically sound and makes the approach efficient.  Experimental Validation: Extensive experiments on realworld datasets demonstrate the superiority of the proposed method in terms of clustering performance and fairness compared to existing fair clustering algorithms.  Flexibility: The approach supports multistate PSVs and flexible fairness constraints providing a versatile solution for various clustering tasks.  Weak Points:  Complexity: The incorporation of ILP may increase the complexity of the deep clustering framework potentially affecting scalability.  Interpretability: The paper could benefit from further discussion on the interpretability of the fairness results especially in cases of tradeoffs between fairness and accuracy.  Comparison: While comparisons with existing methods are made more detailed comparisons with a broader range of fair clustering algorithms could enhance the depth of the evaluation.  Questions and Feedback to the Authors: 1. Scalability: How does the ILP formulation impact the scalability of the proposed deep fair clustering approach especially for large datasets 2. Interpretability: Could you provide insights into how the model maintains the tradeoff between fairness and accuracy especially when optimizing the fairness loss `Fair` alongside clustering objectives 3. Generalization: How well does the model generalize to different types of data beyond the ones used in the experiments and how sensitive is it to variations in data distributions  3. Review Summary The paper presents a novel deep fair clustering framework that addresses grouplevel fairness in clustering applications. By formulating fairness as an ILP problem the authors demonstrate improved clustering performance while ensuring fairness on realworld datasets. The theoretical foundation experimental validation and flexibility in supporting multistate PSVs and flexible fairness constraints contribute to the significance of the proposed approach. While the paper addresses a critical area in clustering algorithms future work could focus on scalability interpretabilty of fairness results and further generalization to different data types. Overall given the papers theoretical soundness and promising experimental results I recommend its acceptance with minor revisions addressing the aforementioned points.", "wfRZkDvxOqj": " Summary of the Paper: The paper introduces MultiTask Neural Processes (MTNPs) a variant of neural processes designed for multitask learning. The method explores transferable knowledge from related tasks in the function space by incorporating shared knowledge into individual tasks through hierarchical Bayesian inference. MTNPs are demonstrated to be effective in improving task performance with limited labeled data across multitask regression classification and brain image segmentation tasks.  Strong Points: 1. Theoretical Foundation: The paper provides a clear theoretical background by integrating neural processes with hierarchical Bayesian modeling for multitask learning. 2. Model Innovation: MTNPs offer a novel approach to learning multiple tasks simultaneously by leveraging shared knowledge among related tasks. 3. Experimental Validation: Extensive experiments on various benchmarks showcase the effectiveness of MTNPs in transferring knowledge across tasks and improving task performance in different domains consistently. 4. Comparative Analysis: The paper thoroughly compares MTNPs with existing methods demonstrating superior performance in multitask classification regression and segmentation tasks.  Weak Points: 1. Complexity of Model: The hierarchical context modeling and variational inference could make the approach challenging to implement and understand for those unfamiliar with advanced Bayesian methods. 2. Limited Practical Validation: While the theoretical foundation and experimental results are strong more realworld applications could further validate the capabilities and potential drawbacks of the proposed MTNPs.  Questions for the Author: 1. Can you discuss any potential limitations when scaling up MTNPs to more complex and diverse multitask learning scenarios 2. How sensitive is the performance of MTNPs to hyperparameters and what strategies are in place to optimize hyperparameter selection 3. In practical applications how does the computational efficiency and scalability of MTNPs compare to other stateoftheart multitask learning methods  Feedback for the Author: Overall the paper presents a wellstructured and significant contribution to the field of multitask learning. Enhancing the discussion on scalability hyperparameter tuning and practical considerations could further strengthen the impact and applicability of MTNPs in realworld scenarios.  Review Summary: The paper introduces MultiTask Neural Processes as a methodological extension of neural processes for multitask learning offering a principled approach to leverage shared knowledge among tasks. With solid theoretical underpinnings and promising experimental results across regression classification and segmentation tasks MTNPs demonstrate superior performance and potential for practical applications. While the complex nature of the model and scalability to diverse scenarios pose challenges addressing hyperparameter optimization and computational efficiency could enhance the methods applicability and impact in the field of multitask learning. Overall the paper is wellstructured informative and makes a valuable contribution to the research community.", "kcwyXtt7yDJ": " PeerReview:  1) Paper Summary: The paper introduces a novel domain adaptation method GraphRelational Domain Adaptation (GRDA) which leverages domain graphs to perform adaptation across domains with varying relationships. It extends traditional uniform alignment methods by introducing a graph discriminator to adapt flexibly based on the graph structure. The theoretical analysis shows that GRDA achieves classic domain adaptation for clique graphs and nontrivial alignment for other types of graphs. Empirical results demonstrate the effectiveness of GRDA on both synthetic and realworld datasets.  2) Strengths and Weaknesses:  Strong Points:  Innovative Approach: The paper addresses a crucial issue in domain adaptation by considering domain relationships described by graphs introducing a novel method to adapt across graphrelational domains.  Theoretical Analysis: The theoretical analysis provides insights into how GRDA aligns domains based on different types of domain graphs enhancing the understanding of the methods effectiveness.  Empirical Results: The experimental evaluation on synthetic and realworld datasets demonstrates the superiority of GRDA over stateoftheart domain adaptation methods.  Comprehensive Discussion: The paper provides a thorough discussion on related works methodology theoretical guarantees and practical implications offering a holistic view of the proposed approach.  Weak Points:  Complexity: The method may be challenging to implement and deploy especially for users not familiar with graphbased domain adaptation techniques.  Clarity of Presentation: Some parts of the paper especially the theoretical analysis could benefit from more intuitive explanations or examples to enhance understanding.  Comparison: While the paper compares GRDA with existing methods a more extensive comparison with a broader range of domain adaptation techniques and realworld tasks could further validate the methods efficacy.  Questions and Feedback to the Author: 1. Can you provide more insights into the computational complexity of the proposed GRDA method compared to traditional domain adaptation techniques 2. How robust is GRDA to noisy or incomplete domain graphs and are there ways to enhance its performance under such conditions 3. Could you elaborate on potential realworld applications where GRDA could offer significant benefits beyond the scenarios considered in the paper 4. Have you considered exploring the interpretability of the graph discriminator in understanding the alignment decisions made by GRDA  3) Review Summary: The paper presents a novel approach GRDA to address domain adaptation across graphrelational domains. The comprehensive theoretical analysis and empirical results demonstrate the methods effectiveness in adapting domains flexibly based on the domain graph structure. While the paper excels in introducing an innovative method and providing indepth analysis there is room for improvement in terms of clarity of presentation and wider comparison with existing domain adaptation techniques. Overall the paper makes a significant contribution to the domain adaptation field and is recommended for publication pending minor revisions to enhance clarity and further validate the methods applicability. This review provides a comprehensive assessment that highlights the strengths and weaknesses of the paper offers critical questions for the author and concludes with a concise summary to support the recommendation for minor revisions before publication.", "xtZXWpXVbiK": "Abstract: The paper introduces the FlOwbased Recurrent BElief State model (FORBES) which leverages normalizing flows in variational inference to learn general continuous belief states for Partially Observable Markov Decision Processes (POMDPs). The goal is to address the limitations of existing methods that approximate belief states with diagonal Gaussians restricting the ability to accurately capture the true belief states in highdimensional continuous spaces. FORBES aims to overcome these restrictions and improve downstream Reinforcement Learning (RL) performance on visualmotor control tasks. Review: Strong Points: 1. Innovative Approach: The integration of normalizing flows into variational inference for learning general continuous belief states is a novel and promising approach to address the limitations of existing methods. 2. Theoretical Clarity: The paper provides a comprehensive theoretical foundation for the proposed model supporting the claims with detailed derivations and theorems. This enhances the credibility of the proposed method. 3. Experimental Validation: The experiments conducted on digit writing tasks and visualmotor control tasks demonstrate the effectiveness of FORBES in capturing complex belief states and improving performance and sample efficiency. 4. Comparative Analysis: The paper effectively compares FORBES with existing methods and showcases its superior performance providing a clear understanding of the benefits of the proposed approach. Weak Points: 1. Evaluation Metrics: The paper could benefit from more detailed explanations on the specific metrics used for evaluating the performance of FORBES both in belief state learning and downstream RL tasks. 2. Generalization: While the experiments show promising results the paper could further discuss the generalizability of FORBES across different POMDP scenarios or environments to emphasize its broader applicability. 3. Implementation Details: More detailed information on the implementation specifics such as hyperparameters network architectures and training procedures would enhance reproducibility and understanding. Question and Feedback: 1. Question: How does the choice of hyperparameters and network architectures impact the performance of FORBES in learning general continuous belief states Could you provide more insights into the design decisions behind these choices 2. Feedback: It would be valuable to include discussions on the computational efficiency of FORBES especially in terms of training time and inference speed to provide a holistic assessment of its practical feasibility. Review Summary: The paper presents an innovative approach FORBES for learning general continuous belief states in POMDPs using normalizing flows. The theoretical foundation comparative analysis with existing methods and experimental validation highlight the effectiveness of FORBES in capturing complex belief states and improving RL performance on visualmotor control tasks. The paper is wellstructured informative and provides a solid contribution to the field. Addressing minor gaps in evaluation metrics generalization and implementation details would further enhance the clarity and impact of the work. Overall I recommend accepting this paper for its significant theoretical and practical contributions to the domain of POMDPs and RL.", "yCS5dckx_vj": " Peer Review: 1. Summary of the Paper: The paper introduces a novel Generalized version of DirectPred called DirectSet(\u03b1) and presents a thorough theoretical analysis of DirectSet(\u03b1) in a linear network setting. The analysis demonstrates that DirectSet(\u03b1) can learn a desirable projection matrix onto invariant features thereby reducing sample complexity on downstream tasks. The paper also introduces DirectCopy a simplified and more computationally efficient algorithm based on the theoretical findings. Empirical evaluations on CIFAR10 CIFAR100 STL10 and ImageNet datasets demonstrate that DirectCopy outperforms or rivals DirectPred in terms of performance. 2. Strengths:  Theoretical Rigor: The paper provides a comprehensive theoretical analysis of DirectSet(\u03b1) in a linear network setting offering insights into the mechanism of representation learning in noncontrastive selfsupervised learning.  Empirical Evaluation: The empirical evaluations on various datasets showcase the effectiveness of DirectCopy demonstrating its competitive or superior performance compared to DirectPred.  Algorithm Simplicity: DirectCopy is highlighted as a simpler and more computationally efficient alternative to DirectPred making it more practical for realworld applications.  Novel Contribution: The introduction of DirectSet(\u03b1) and DirectCopy presents novel approaches to tackle challenges in selfsupervised learning providing valuable contributions. 3. Weaknesses:  Limitation to Linear Models: The paper primarily focuses on linear models for the theoretical analysis and there is mention of limitations in capturing empirical phenomena that may require nonlinear network analysis.  Incomplete Explanation: While the paper offers insights into the representation learning mechanism certain mysteries beyond the current theory are highlighted as areas for future research. Providing more clarity on potential future directions could enhance the conclusions drawn from the study.  Questions and Feedback to the Authors:  Question: Have you considered further experiments or extensions to validate the effectiveness and scalability of DirectCopy beyond the datasets mentioned in the paper  Feedback: Additionally providing more insights into the practical implications of the theoretical findings and empirical results could enhance the impact of the study. Consider elaborating on the implications for realworld applications and potential future research directions.  Review Summary: The paper presents a novel contribution in the field of selfsupervised learning by introducing DirectSet(\u03b1) and demonstrating the efficacy of DirectCopy as a simplified and efficient algorithm. The theoretical analysis provides a deep understanding of the representation learning process in noncontrastive selfsupervised learning. The empirical evaluations underscore the superiority of DirectCopy over DirectPred. However further experiments or extensions beyond linear models and additional insights into practical implications could enhance the papers impact. Overall the paper is wellstructured rigorous and presents significant contributions to the field warranting potential publication.", "sX3XaHwotOg": " Summary: The paper introduces a new framework called AMOS for pretraining text encoders with an Adversarial Mixture Of Signals approach using multiple auxiliary generators. The framework is designed as an improvement over the ELECTRAstyle pretraining by jointly training multiple masked language models (MLMs) of different sizes to provide training signals at various levels of difficulty. AMOS outperforms ELECTRA and recent stateoftheart pretrained models by approximately 1 point on the GLUE benchmark for BERT basesized models. The paper goes on to explain the methodology experimental setups and results in detail.  Strengths: 1. Novel Approach: The paper introduces a novel approach AMOS which addresses the challenges in the pretraining dynamic of ELECTRAstyle frameworks. 2. Effectiveness: AMOS outperforms existing stateoftheart models on the GLUE and SQuAD benchmark tasks demonstrating its effectiveness. 3. Comprehensive Experiments: The paper provides thorough experiments ablation studies and analysis to validate the effectiveness of AMOS. It also includes probing experiments to study the impact of diverse pretraining signals and adversarial training. 4. Reproducibility: The paper emphasizes reproducibility by providing detailed information about datasets used hyperparameters and releasing pretrained models. This enhances the credibility of the study.  Weaknesses: 1. Complexity: The framework and methodology can be quite complex and may require a good understanding of transformer models and pretraining methods to comprehend fully. 2. Lack of Comparison: While the paper compares AMOS with existing models it could benefit from more comparisons with a wider range of baseline models to further validate its superiority. 3. Technical Jargon: The paper contains technical terms and concepts that may be difficult for readers unfamiliar with transformer models or pretraining methods to understand.  Questions and Feedback to the Author: 1. How does the proposed AMOS framework handle scenarios where the pretraining data is limited or of lower quality 2. Could the paper provide more insights into the computational requirements and practical implications of using AMOS in realworld applications 3. How does the performance of AMOS scale with different model sizes and text corpora for pretraining  Review Summary: The paper presents a new framework AMOS for pretraining text encoders with an Adversarial Mixture Of Signals approach. The thorough experiments ablation studies and analysis demonstrate the effectiveness of AMOS in outperforming existing stateoftheart models on benchmark tasks. The papers emphasis on reproducibility adds credibility to the research. However the complexity of the framework and technical jargon could pose challenges for readers unfamiliar with the topic. Further comparisons with a broader range of baseline models and more insights into practical implications would enhance the papers value. Overall based on the strengths demonstrated in the results and methodology I recommend this paper for publication after addressing the minor weaknesses and providing additional clarity where needed.", "lD8qAOTu5FJ": " Summary: The paper titled \"KnowledgeAware continual learner: Balancing between forward transfer and mitigating forgetting in classincremental learning\" investigates the stabilityplasticity dilemma in continual learning (CL) focusing on classincremental learning scenarios. The study proposes a KnowledgeAware continual learner (KAN) model that considers the semantic similarity between old and new classes to achieve a balance between forward transfer reducing forgetting and resource efficiency. The model allocates new connections based on relevance to previous similar knowledge and protects dissimilar knowledge. Several experiments are conducted using CIFAR10 and CIFAR100 datasets demonstrating the efficacy of KAN in various scenarios.  Strong points: 1. Innovative Approach: The paper introduces the novel KAN model addressing the stabilityplasticity dilemma by considering semantic relations between old and new classes for effective continual learning. 2. Comprehensive Experiments: The paper conducts thorough experiments on different datasets and scenarios to evaluate the performance of KAN against baselines demonstrating its superiority in balancing forward transfer and mitigating forgetting. 3. Clear Problem Statement: The paper effectively outlines the challenges in classincremental learning emphasizing the significance of balancing between mitigating forgetting and exploiting previous knowledge. 4. Detailed Algorithm Description: The proposed KAN models algorithm is welldefined providing insights into the connections allocation process for each class layer and task.  Weak points: 1. Lack of Discussion on Limitations: The paper could benefit from a more detailed discussion on the limitations of the proposed model particularly in scenarios where semantic similarities between old and new classes are ambiguous. 2. Baseline Comparison: While the paper compares KAN against several baselines a more indepth discussion on why certain baselines were selected and how they contribute to the overall understanding of the problem would add value.  Questions to the Authors: 1. How does KAN perform in scenarios with varying degrees of class similarities and dissimilarities Can you provide more insight into the models adaptability in handling such variations 2. Have you considered the scalability of KAN to larger datasets or more complex task structures What challenges do you foresee in scaling the model to handle more diverse and extensive datasets 3. How sensitive is the performance of KAN to hyperparameters such as sparsity level layer reuse or candidate neuron selection How did you determine the optimal values for these parameters in your experiments  Feedback: The paper presents a novel approach to continual learning with the KAN model showcasing promising results in balancing the tradeoff between forward transfer and mitigating forgetting. To strengthen the paper provide more discussions on the limitations of the model offer a clear comparison rationale for the baselines used and consider implications of scalability and parameter sensitivity in future work. Overall the paper contributes significantly to the field of CL and presents a valuable framework for addressing the stabilityplasticity dilemma in classincremental learning.  Recommendation: With its innovative approach comprehensive experiments and significant contributions to the field I recommend that this paper be accepted for publication after addressing the minor feedback and questions raised. This research adds valuable insights to the continual learning domain and demonstrates the effectiveness of the KAN model in tackling the challenges of classincremental learning.", "vaRCHVj0uGI": "Summary: The paper presents an unsupervised technique for solving inverse problems in medical imaging by leveraging scorebased generative models. The proposed method focuses on reconstructing medical images from partial measurements in CT and MRI without relying on paired datasets for training. By training a scorebased generative model on medical images the authors introduce a sampling method to reconstruct images consistent with observed measurements and estimated data prior allowing for better generalization to unknown measurement processes. Empirical evaluations on tasks in CT and MRI show comparable or better performance compared to supervised learning techniques even when tested with varying measurement processes. Strengths: 1. Innovative Approach: The paper introduces a novel unsupervised method utilizing scorebased generative models for solving inverse problems in medical imaging offering a refreshing alternative to the supervised learning techniques commonly used in the field. 2. Generalization Capability: One of the key strengths of the proposed method is its ability to generalize effectively to new or unknown measurement processes which addresses a significant limitation in existing supervised learning approaches. 3. Empirical Results: The empirical evaluation demonstrates competitive performance compared to supervised techniques even when tested with different measurement processes showcasing the effectiveness and versatility of the proposed approach. 4. Comprehensive Experimental Evaluation: The paper includes a thorough evaluation on various tasks in CT and MRI comparing against standard supervised techniques and other unsupervised methods providing a comprehensive analysis of the proposed methods performance and advantages. Weaknesses: 1. Complexity of Proposed Method: The methodology proposed in the paper while innovative may be complex for practitioners without a strong background in generative models or medical imaging potentially limiting the practical implementation of the approach. 2. Lack of RealWorld Implementation Details: The paper lacks discussion on potential challenges or limitations in realworld implementation such as computational resource requirements or scalability issues that may arise when applying the method to large datasets or clinical settings. Questions and Feedback to the Author: 1. Scalability: How does the proposed technique scale when dealing with largescale medical imaging datasets or when deployed in clinical settings 2. Robustness: Have there been experiments conducted to assess the robustness of the method against noise or variations in input measurements that could potentially impact reconstruction accuracy 3. Ethical Considerations: Are there any ethical considerations related to the use of unsupervised techniques in medical imaging especially in clinical applications where accuracy and reliability are crucial Review Summary: The paper presents an innovative unsupervised technique leveraging scorebased generative models for solving inverse problems in medical imaging showcasing impressive generalization to different measurement processes and competitive performance compared to supervised learning methods. The methods versatility and effectiveness demonstrated through empirical evaluations warrant its consideration as a promising approach in medical image reconstruction. However the complexity of the proposed method and potential scalability concerns should be addressed to facilitate wider adoption and practical implementation in realworld clinical settings. Overall based on the strengths of the proposed approach and its promising empirical results I recommend the acceptance of this paper with a suggestion to address clarity scalability and realworld applicability in the final manuscript.", "hcoswsDHNAW": " Summary of the Paper: The paper introduces Fast AdvProp as a modification to Adversarial Propagation (AdvProp) to reduce the high training cost associated with leveraging adversarial examples to improve recognition models. Fast AdvProp addresses the costly training components of AdvProp by decoupling clean training samples from their adversarial counterparts integrating techniques for accelerating adversarial training and synchronizing parameter updating speed. Experimental results demonstrate that Fast AdvProp improves model performance without incurring extra training costs outperforming vanilla training and even the original AdvProp in terms of accuracy and robustness on various visual benchmarks.  Strong Points: 1. Clear Problem Statement: The paper clearly articulates the problem of high training costs in AdvProp and convincingly presents Fast AdvProp as a solution. 2. Innovative Approach: The proposal to decouple training data integrate fast adversarial training techniques and synchronize parameter updating speed is novel and promising. 3. Empirical Validation: The authors provide thorough empirical results demonstrating the effectiveness of Fast AdvProp in improving recognition models on various visual benchmarks. 4. Scalability: It is noted that Fast AdvProp scales better with larger models and datasets illustrating its potential for broader applications. 5. Comprehensive Ablation Study: The ablation study provides a detailed analysis of various components of Fast AdvProp aiding in understanding the impact of different modifications.  Weak Points: 1. Complexity of Algorithm: The complexity of the Fast AdvProp algorithm may pose challenges in practical implementation without a userfriendly interface or clear guidelines. 2. Generalization Concerns: While the empirical results show improvements the generalization of Fast AdvProp to different datasets and tasks may need further validation. 3. Comparative Analysis: While the performance of Fast AdvProp is compared with vanilla training and AdvProp a broader comparison with other stateoftheart methods could provide more context.  Question and Feedback to the Author: 1. Could you provide insights on the computational overhead introduced by Fast AdvProp in terms of memory and processing requirements 2. How does Fast AdvProp compare to other lowcost learning algorithms mentioned in the paper in terms of performance and efficiency 3. Have you considered conducting experiments on additional visual benchmarks or domains to further validate the general applicability of Fast AdvProp  Review Summary: The paper presents an innovative approach Fast AdvProp to reduce the training cost associated with AdvProp for improving recognition models using adversarial examples. The proposed modifications show promise in achieving better performance without additional training costs. The comprehensive empirical results and ablation study strengthen the credibility of Fast AdvProp. With improvements in accuracy and robustness on visual benchmarks Fast AdvProp is a potential gamechanger in the field of adversarial learning. However the study could benefit from further analyses on computational overhead generalization capabilities and comparisons with other learning algorithms. Overall based on the presented evidence I recommend accepting this paper for publication.", "uYLFoz1vlAC": "1) Summary of the Paper The paper introduces a new sequence model called Structured State Space (S4) designed to efficiently handle longrange dependencies (LRDs) in sequence data. The S4 model is based on a reparameterized version of the state space model (SSM) and aims to overcome the computational limitations of previous approaches like Linear State Space Layer (LSSL). The S4 model achieves strong empirical results across various benchmarks demonstrating its efficiency in handling LRDs compared to existing models. 2) Strengths and Weaknesses: Strengths:  The S4 model addresses a critical challenge in sequence modeling by efficiently handling longrange dependencies.  The paper provides a comprehensive theoretical foundation for the S4 model with clear explanations of the technical innovations and contributions.  Empirical results demonstrate the effectiveness of S4 across diverse benchmarks showcasing its competitive performance against existing models.  The proposed algorithms for computing the SSM convolution kernel and addressing longrange dependencies are novel and theoretically sound. Weaknesses:  The paper lacks a detailed comparison with more established models like Transformers in certain experiments limiting the scope of contextual performance evaluation.  While the S4 model is described in detail more insights into the generalizability of the model and its potential limitations could enhance the discussion.  The presentation of experimental results could be more structured and focused making it easier for readers to interpret the significance of the findings. Questions and Feedback to the Author: 1. Can you provide more insights into the scalability of the S4 model in terms of handling larger datasets or more complex sequence tasks 2. How does the S4 model compare to stateoftheart Transformer models in terms of generalization across different domains and tasks 3. Have you considered analyzing the interpretability of the S4 model in handling LRD tasks and providing insights into its internal representations 3) Review Summary: The paper presents a novel sequence model S4 designed to address longrange dependencies efficiently. The technical innovations and theoretical foundations of the S4 model are robust and supported by empirical results demonstrating its effectiveness across various benchmarks. While the paper has strengths in its technical depth and empirical validation further discussion on model generalizability and interpretability could enhance the overall impact and understanding of the S4 model. Overall I recommend acceptance of the paper with minor revisions to address the suggested points for enhancement.", "jJOjjiZHy3h": " Summary of the Paper: The paper introduces a new technique called AdversarialAugment (AdA) that combines the strategies of AugMix and DeepAugment to improve the robustness of neural networks against common image corruptions and `pnorm adversarial perturbations. AdA leverages the parameters of imagetoimage models to generate adversarially corrupted augmented images. The authors provide a theoretical foundation for AdA and show that it outperforms existing methods on benchmarks including CIFAR10C and IMAGENET demonstrating improved performance against common image corruptions and `pnorm perturbations.  Strengths: 1. Novel Approach: Introducing a new method AdversarialAugment that combines both the strategies of AugMix and DeepAugment for better robustness against common image corruptions and adversarial perturbations. 2. Theoretical Foundation: Providing theoretical considerations and performance guarantees showcasing the potential advantages of AdA over previous works. 3. Empirical Results: Demonstrating stateoftheart performance in terms of mean corruption error on CIFAR10C and improved robustness to `pnorm perturbations on both CIFAR10 and IMAGENET. 4. Comprehensive Analysis: Detailed exploration of empirical results comparing AdA with existing methods across various benchmarks and evaluating performance against different types of corruptions and perturbations.  Weaknesses: 1. Dependency on Other Methods: AdA performs best when used in conjunction with existing data augmentation methods such as AugMix and DeepAugment. It does not exhibit notable improvements when employed in isolation. 2. Computational Complexity: The methods computational demands are higher compared to counterparts due to the optimization over parameters of imagetoimage models increasing the complexity and memory requirements.  Questions and Feedback to the Author: 1. How does AdA perform when used in isolation without additional data augmentation methods Are there strategies to enhance its standalone performance 2. Could the authors provide insights into the computational scalability and feasibility of AdA for realworld largescale applications 3. Are there potential extensions or future research directions based on the findings presented in the paper particularly in addressing the identified limitations of AdA  Review Summary: The paper presents a novel method AdversarialAugment (AdA) to enhance the robustness of neural networks against common image corruptions and adversarial perturbations showcasing improved performance over existing approaches. While AdA demonstrates stateoftheart results its dependency on other methods and higher computational complexity are notable limitations. Addressing these issues and exploring its standalone efficacy could further strengthen the methods applicability. Overall the theoretical foundation empirical results and comparative analysis make the paper a valuable contribution to the field of image corruption defenses.", "qCBmozgVr9r": "1) Summary of the Paper: The paper introduces a new paradigm called FewShot Attribute Learning (FSAL) that focuses on learning new attributes with few examples in a fewshot learning setup. The authors emphasize the importance of attributebased representations in facilitating efficient learning of novel classes with zero or few examples. The work explores the challenge of rapid learning of new attributes that were not previously labeled leveraging selfsupervised pretraining to improve generalization performance. Benchmark datasets including CelebA Zappos50K and ImageNetwithAttributes were introduced to evaluate FSAL tasks. The proposed methodology involves unsupervised representation pretraining representation finetuning using attribute or episode information and fewshot learning using approaches like MatchingNet ProtoNet and TADAM. Experiments and comparisons with standard fewshot learning methods demonstrate the effectiveness of the proposed approaches in improving generalization to novel attributes. 2) Strengths and Weaknesses and Questions for the Authors: Strengths:  Novel Concept: Introducing FewShot Attribute Learning as a novel paradigm is a significant contribution to the field of machine learning.  Comprehensive Evaluation: The paper provides a detailed methodology and experimental setup to evaluate the proposed FSAL approach on multiple benchmark datasets showcasing its effectiveness.  Insightful Analysis: The paper conducts an indepth analysis of the generalization performance on new attributes shedding light on the challenges and opportunities in attribute learning. Weaknesses:  Complexity: The methodology presented is intricate and may be challenging for readers unfamiliar with the topic to grasp fully. Simplifying the presentation could enhance accessibility.  Evaluation Metrics: The paper could provide more insights on how different evaluation metrics were chosen and their implications on measuring performance accurately. Questions for the Authors:  Experimental Settings: How were the hyperparameters for different proposed approaches determined and were they optimized across all datasets consistently  Comparison with SOTA: How does the proposed FSAL approach compare with stateoftheart methods on similar benchmarks or tasks in fewshot learning scenarios  Future Directions: What are the potential future directions or applications of FewShot Attribute Learning that you envision based on the findings of this study 3) Review Summary: The paper introduces an innovative paradigm of FewShot Attribute Learning (FSAL) to address the challenge of learning new attributes with limited labeled data. The experimental results and analyses presented in the paper demonstrate the effectiveness of the proposed approach in improving generalization performance to novel attributes. The comprehensive evaluation and insightful analysis provide valuable contributions to the field of machine learning. Overall the paper is wellwritten and provides compelling evidence for the relevance and significance of FewShot Attribute Learning. Based on the strengths highlighted and the potential for further discussions I recommend acceptance of the paper with minor revisions focusing on clarity of presentation and additional insights on experimental settings and future directions.", "hOaYDFpQk3g": "1) Summary: The paper introduces LightWaveS a distributed solution for multivariate time series classification (MTSC) that aims to address the limitations of the ROCKET model family. LightWaveS leverages wavelet scattering transformation and distributed feature selection to create a solution that achieves accuracy comparable to deep learning solutions while being more efficient during both training and inference. The authors present three variants of LightWaveS and evaluate their performance in terms of training time accuracy inference speedup and scalability. 2) Strengths and Weaknesses: Strengths:  Innovative Approach: LightWaveS introduces a novel framework that combines wavelet theory with distributed feature selection for fast and accurate multivariate time series classification.  Efficiency: The solution offers significant improvements in inference speed compared to ROCKET making it a practical choice for deployment on edge devices.  Scalability: LightWaveS demonstrates linear scalability with the number of channels showcasing its potential for handling large datasets and distributed computing.  Comparative Analysis: The paper thoroughly compares the performance of LightWaveS with (MINI)ROCKET and other recent methods providing valuable insights into the strengths and weaknesses of each approach. Weaknesses:  Interpretability: While LightWaveS offers faster and accurate classification results the paper lacks indepth discussion on the interpretability of the model and the insights gained from feature selection.  Experimental Setup: The paper could benefit from a more detailed discussion on the experimental setup including hyperparameter tuning and model evaluation to provide a more comprehensive understanding of the methods performance.  Limited Discussion on Tradeoffs: Although the paper mentions a tradeoff between speed and accuracy further discussion on the implications of this tradeoff for different applications would enrich the analysis. Questions and Feedback: 1. Can you provide more insights into the interpretability of LightWaveS and how the feature selection process contributes to understanding the underlying patterns in multivariate time series data 2. How does LightWaveS handle noisy or irrelevant channels in the multivariate time series data and are there strategies in place to mitigate their impact on classification accuracy 3. Could you elaborate on the potential realworld applications where LightWaveS could outperform existing methods due to its efficiency and accuracy tradeoffs 4. Considering the scalability of LightWaveS are there specific scenarios or industries where its distributed nature could offer significant advantages over traditional models 3) Review Summary: The paper presents an innovative solution LightWaveS for efficient multivariate time series classification by leveraging wavelet scattering and distributed feature selection. While the approach demonstrates significant improvements in inference speed and scalability further discussions on interpretability and implications of speedaccuracy tradeoffs would enhance the papers impact. Overall the paper presents a valuable contribution to the field of MTSC offering a practical and efficient framework for deployment on edge devices and largescale datasets. This work is recommended for acceptance with minor revisions to address the identified weaknesses.", "xDIvIqQ3DXD": " Summary The paper investigates the approximation properties of recurrent encoderdecoder architectures in a linear setting. It focuses on providing a mathematical understanding of the working principles of these architectures and how they generalize traditional RNNs by lifting the timehomogeneity requirement. The key findings include proving a universal approximation result demonstrating the capability to learn timeinhomogeneous relationships and identifying a specific \"temporal product structure\" that characterizes efficient approximation. The study explores the differences between encoderdecoder architectures and traditional RNNs shedding light on their unique characteristics.  Strengths 1. Theoretical Depth: The paper provides a comprehensive theoretical analysis of recurrent encoderdecoder architectures offering insights into their approximation capabilities and distinguishing features. 2. Generalization Capabilities: By demonstrating how these architectures can handle timeinhomogeneous relationships the paper highlights their potential to tackle a broader class of sequencetosequence problems. 3. Clear Structure: The paper is wellstructured with a detailed introduction welldefined problem formulation and organized sections discussing related work theoretical results and numerical illustrations. 4. Relevance and Importance: Understanding the differences between encoderdecoder architectures and traditional RNNs is crucial for guiding architectural choices in sequence modeling tasks.  Weaknesses 1. Complexity: The papers mathematical rigor and technical terminology may be challenging for readers without a strong background in sequence modeling and deep learning. 2. Applicability: While the theoretical insights are valuable for understanding the fundamental principles the direct practical implications for realworld applications could be further explored.  Questions for the Authors 1. How do the findings in this paper contribute to the broader research landscape on sequence modeling and neural network architectures 2. Are there plans to empirically validate the theoretical findings presented in this paper using realworld datasets and applications to showcase the practical benefits of understanding the approximation properties of these architectures  Feedback  The paper could benefit from more practical examples or case studies to illustrate how the theoretical results translate into tangible improvements in realworld applications.  It would be valuable to discuss potential extensions or implications of the identified \"temporal product structure\" for designing novel neural network architectures or optimizing existing ones.  Review Summary This paper offers a rigorous investigation into the approximation properties of recurrent encoderdecoder architectures shedding light on their theoretical foundations and how they differ from traditional RNNs. The indepth analysis and mathematical insights presented make a significant contribution to the understanding of these architectures. While the complexity of the material may pose a challenge for some readers the clear structure and relevance of the study warrant its consideration for publication. Further empirical validations and discussions on practical implications could enhance the papers impact in the field. Overall I recommend acceptance after addressing the aforementioned points.", "gKWxifgJVP": "Summary: The paper proposes a factdriven logical reasoning model named FOCAL REASONER to enhance machine reading comprehension by incorporating both global and local knowledge pieces necessary for logic reasoning. The model leverages syntactic processing to extract fact units from text constructs a supergraph to capture connections between these fact units and utilizes relational graph convolution networks for reasoning over the supergraph. Experimental results on logic reasoning benchmarks show significant performance improvements over baseline models. Strong Points: 1. Innovative Approach: The paper introduces a novel factdriven approach that fills the gap in existing methods by incorporating both global and local logical units leading to superior results. 2. Detailed Methodology: The paper provides a comprehensive description of the model architecture including fact unit extraction supergraph construction reasoning process and training objectives facilitating reproducibility. 3. Experimental Performance: The results demonstrate substantial improvements in logic reasoning benchmarks showcasing the effectiveness of the proposed FOCAL REASONER model over baseline approaches. 4. Ablation Studies: The ablation studies conducted to analyze various components of the proposed model offer valuable insights enhancing the credibility of the findings. Weak Points: 1. Complexity: The model architecture involving multiple steps and components may introduce a high level of complexity that could hinder the ease of implementation and understanding for researchers unfamiliar with the field. 2. Evaluation: While the results indicate a notable performance boost in logic reasoning tasks a more indepth analysis of the models generalizability across a wider range of datasets and tasks could strengthen the papers claims. 3. Interpretability: While a case study is provided for interpretability further analysis on how the models reasoning process aligns with human intuition or expert systems would enhance the discussion on the models decisionmaking process. Question to Author: 1. Can you elaborate on the computational complexity of the proposed factdriven approach compared to existing methods especially concerning scalability and efficiency Feedback to Author: 1. Consider discussing the limitations of the proposed model such as potential performance degradation on datasets with highly complex logical structures. 2. Enhance the discussion on the implications of the factdriven approach in advancing the field of logic reasoning within machine reading comprehension potentially comparing it to other emerging methods or trends. Review Summary: The paper introduces an innovative factdriven logical reasoning model FOCAL REASONER that addresses the limitations of existing methods by incorporating both global and local logical units. The detailed methodology strong experimental results and insightful ablation studies support the efficacy of the proposed approach. However the complexity of the model and the need for further evaluation on diverse datasets are points to consider. Overall the paper presents a significant contribution to advancing logic reasoning in machine reading comprehension and is recommended for publication pending minor revisions and clarifications on the scalability and broader implications of the proposed model.", "ljCoTzUsdS": " Review of \"Investigating ExemplarvsRule Bias in Machine Learning Systems\"  1) Summary: The paper investigates two inductive biases in machine learning systems: featurelevel bias and exemplarvsrule bias. Drawing inspiration from cognitive psychology the study presents a protocol to probe these biases and provides empirical results across various models in both synthetic and realworld image and language domains. The paper highlights the importance of understanding how models generalize and extrapolate beyond the data they have seen shedding light on potential implications for research areas like data augmentation fairness and systematic generalization.  2) Strengths:  Strong Points:  Innovative Approach: The paper offers a novel approach by applying methods from cognitive psychology to study inductive biases in machine learning providing a fresh perspective.  Comprehensive Study: The research covers both synthetic and realworld domains indicating a thorough investigation of the proposed protocol across a range of models and scenarios.  Clear Definition of Biases: The distinction between featurelevel bias and exemplarvsrule bias is clear aiding in understanding the nuances of how machine learning systems make decisions.  Applicability: The proposed protocol can be easily applied to various classification tasks demonstrating its practical utility and potential for broader adoption in the field.  Relevance: The studys findings on how different inductive biases impact the generalization of models can have crucial implications for model design interpretability fairness considerations and data augmentation strategies.  Weak Points:  Need for Formalization: While the behavioral manifestations of bias are explored formalizing the properties that lead to exemplarvsrule bias is suggested for enhancing the theoretical foundation.  Limitations of Model Factors: The paper acknowledges the need to investigate further the factors that influence the exemplarvsrule propensity across different model families providing avenues for future research.  Application Scenarios: While the practical implications are touched upon more detailed exploration of how understanding biases can impact realworld scenarios like fairness in AI could be beneficial.  Questions and Feedback for the Author: 1. Clarification on Formalization: Could the authors elaborate on potential directions for formalizing the properties that result in exemplar vs. rule bias as mentioned in the limitations section 2. Generalization to Practical Applications: How do the findings on featurelevel bias and exemplarvsrule bias relate to practical situations especially in critical areas such as fairness in AI models 3. LongTerm Implications: Considering the impact of inductive biases on model behavior how might the insights from this study guide the development of more robust and reliable machine learning systems in the future  3) Review Summary: The paper \"Investigating ExemplarvsRule Bias in Machine Learning Systems\" presents a valuable contribution to the field of machine learning by exploring distinct inductive biases using a protocol inspired by cognitive psychology. The studys innovative approach comprehensive analysis across different domains and relevance towards practical applications make it a significant research endeavor. The research opens up new pathways for understanding how machine learning systems generalize and extrapolate emphasizing the need for investigating biases in diverse model families. The suggested refinements such as formalizing bias properties and exploring deeper implications for realworld applications can further enhance the impact and relevance of this study.  Recommendation: Based on the strengths of the paper its potential to advance understanding of inductive biases in machine learning systems and the relevance of the findings for practical applications I recommend acceptance for publication pending revisions to address the suggested points for further clarity and exploration.", "wwDg3bbYBIq": " Summarize the paper: The paper proposes a new approach for traffic forecasting by converting the forecasting problem into a patternmatching task. The proposed model PatternMatching Memory Networks (PMMemNet) learns to match input data to representative traffic patterns to improve forecasting accuracy. The model utilizes a novel memory architecture GCMem which integrates attention and graph convolution to capture the spatiotemporal correlation of traffic data. Experimental results demonstrate that PMMemNet outperforms existing models especially in longterm prediction by effectively managing representative traffic patterns and achieving higher responsiveness to abrupt speed changes.  Strong Points: 1. Novel Approach: The paper introduces a unique perspective on traffic forecasting by transforming it into a patternmatching task showcasing innovation in the field. 2. StateoftheArt Performance: PMMemNet demonstrates superior forecasting accuracy compared to existing models particularly in longterm prediction scenarios. 3. Memory Architecture (GCMem): The integration of attention and graph convolution in the memory architecture GCMem is highlighted as a strong point in capturing spatiotemporal dependencies efficiently. 4. Experimental Evaluation: The paper provides detailed experimental results comparing PMMemNet to baseline models demonstrating the models effectiveness across different datasets and prediction intervals. 5. Qualitative Analysis: The qualitative analysis provides valuable insights into how PMMemNet works and its strengths in managing abrupt speed changes for accurate traffic forecasting.  Weak Points: 1. Memory Space Optimization: The paper acknowledges the need to optimize the number of memory slots and find important patterns suggesting further research in this area. 2. Training Imbalance: The imbalance in training representations among memory slots due to patterns not referred to is identified as a limitation pointing towards the need for a solution to reduce this representation gap. 3. Distance Measurement Optimization: The use of cosine similarity for pattern matching is highlighted as a potential limitation suggesting the consideration of alternative methods for similarity computation especially in noisy traffic data scenarios.  Questions and Feedback to the Author: 1. How did you select the threshold values for clustering patterns based on cosine similarity and did you consider any alternatives to cosine similarity for pattern matching 2. Can you provide insights into how PMMemNet could adapt to dynamic traffic conditions and how it could potentially handle rare traffic patterns effectively  Review Summary: The paper presents a novel approach PMMemNet for traffic forecasting by converting the problem into a patternmatching task and utilizing a graph convolutional memory architecture. The experimental results support the efficacy of PMMemNet showcasing its superior performance over existing models especially in longterm prediction scenarios. While the paper acknowledges limitations and suggests future research directions the innovative approach and compelling results make a strong case for the adoption of PMMemNet in the field of traffic forecasting. Overall I recommend this paper for publication given its significant contributions and promising performance in traffic forecasting tasks.", "ht61oVsaya": " Paper Summary: The paper introduces a novel approach Distributive Exploration Safety Training Algorithm (DESTA) in the realm of reinforcement learning (RL) to address safe exploration challenges. DESTA uses a twoplayer framework with a Safety Agent and a Task Agent to minimize safety violations while maximizing task rewards. It leverages Markov games to enable decoupled objectives between safety and task performance allowing for selective interventions by the Safety Agent and deterministic planning. The methodology is validated on challenging tasks in Safety Gym Benchmarks and OpenAIs Lunar Lander showcasing its effectiveness in safe learning scenarios.  Strong Points: 1. Innovative Framework: The introduction of the twoplayer DESTA framework is a novel and promising approach to balancing safety and task performance in RL. 2. Decoupled Objectives: The idea of decoupling safety and task rewards allows each agent to focus on its specific objective efficiently. 3. Selective Interventions: Empowering Safety Agent with the ability to intervene selectively enhances the systems safety planning. 4. Flexibility and Efficiency: DESTA demonstrates flexibility in accommodating various safety notions and shows promising performance in challenging benchmarks. 5. Theoretical Foundation: The use of Markov games and formal definitions enrich the theoretical grounding of the proposed framework.  Weak Points: 1. Complexity: The paper delves into intricate technical details making it challenging for novice readers to grasp the content easily. 2. Experimental Setup: It would be beneficial to provide clearer details regarding the experimental design hyperparameter settings and reproducibility aspects. 3. Comparative Analysis: While the experiments demonstrate DESTAs superiority a deeper comparative analysis with existing methods could add value to the paper. 4. Clarity and Readability: Some sections especially the technical proofs could be further clarified for better readability.  Questions to the Author: 1. How does DESTA perform when faced with more dynamic and complex realworld scenarios compared to the presented benchmarks 2. Could you elaborate on the offpolicy learning approach used for each policy and how it affects the overall training stability 3. Are there any specific constraints or limitations in the current framework that could hinder its practical application in more diverse environments  Feedback to the Author:  Enhance the clarity of technical sections to ensure a broader audience can grasp the concept easily.  Include a section on computational complexity and scalability to discuss the practical feasibility of DESTA in largescale applications.  Provide a more detailed discussion on the implications of decoupling objectives and its impact on overall learning efficiency.  Review Summary: The paper introduces an innovative framework DESTA for safe exploration in reinforcement learning showcasing promising results in complex scenarios. While the methodology presents several strengths such as decoupled objectives and selective interventions enhancing readability comparative analysis and discussing practical constraints would further enrich the paper. With its theoretical foundation and empirical validation DESTA shows potential for addressing challenges in ensuring safety while optimizing task performance in RL warranting further exploration and advancements.", "naoQDOYsHnS": " Review Summary  1) Summary of the Paper: The paper introduces an action representation learning framework called Behavioral Metrics of Actions (BMA) for offline reinforcement learning (RL) tasks with large discrete action spaces. The framework aims to address the issue of inaccurate value estimation for outofdistribution (o.o.d.) actions which is a major challenge in offline RL in environments like recommender systems and dialogue systems. BMA uses a pseudometric to measure the behavioral and datadistributional relations between actions and theoretically proves the effectiveness of learned action representations in improving offline RL performance.  2) Strong Points: a) Theoretical Foundation: The paper provides a strong theoretical foundation for the proposed action representation learning framework discussing the importance of behavioral and datadistributional relations between actions. b) Novel Framework: Introducing the BMA framework to address the limitations of existing offline RL methods in environments with large and discrete action spaces is a novel contribution. c) Experimental Results: The empirical evaluation of the BMA framework on simulated and realworld tasks demonstrates significant performance improvements over existing methods showcasing the practical effectiveness of the proposed approach.  Weak Points: a) Complexity: The paper is highly technical and might be challenging for readers not wellversed in reinforcement learning and related mathematical concepts to understand fully. b) Empirical Evaluation: While the experimental results are promising a deeper analysis of specific scenarios or edge cases where the method might not perform well could be beneficial for a more comprehensive evaluation.  Questions to the Author: 1. Can you provide more insights into how the proposed action representations generalize over different action spaces and how they can be adapted to various realworld applications 2. How scalable is the proposed framework in terms of computational resources and training time especially when dealing with largescale environments and datasets  Feedback to the Author: 1. Consider clarifying the complexity of the proposed method in the introductory sections to help readers grasp the key ideas more easily. 2. Providing additional visual aids or intuitive explanations of key concepts could enhance the accessibility of the paper for a wider audience.  Recommendation: Based on the strong theoretical foundation novel framework and significant performance improvements demonstrated in the empirical evaluation this paper is recommended for publication. However addressing the complexity of the content and providing more detailed insights into practical implications and scalability aspects would enhance the overall value of the paper.", "xxU6qGx-2ew": "1) Summary of the Paper: The paper explores the concept of Gaussian differential privacy (GDP) as a singleparameter family of privacy notions that offer efficient privacy guarantees for machine learning models. It begins by examining traditional (  \u03b4)differential privacy (DP) and then introduces the Gaussian differential privacy transformation (GDPT) as a tool to characterize and identify GDP algorithms efficiently. The authors develop criteria to verify GDP mechanisms and propose methods to narrow down the optimal privacy parameter \u03bc with minimal margin of error. The paper also revisits established DP algorithms to enhance their utility compares DP and \u03bcGDP notions and evaluates the impact of subsampling under the GDP framework. 2) Strong and Weak Points Question and Feedback: Strong Points:  The paper provides a comprehensive overview of GDP and its advantages over traditional DP.  Introduces the novel concept of GDPT for characterizing and identifying GDP algorithms efficiently.  Offers clear and precise definitions and theorems enhancing the understanding of the GDP framework.  Delivers practical applications of newly developed tools including improving DP algorithms and examining the effect of subsampling under the GDP framework. Weak Points:  Some sections are highly technical making it challenging for readers without a strong mathematical background to grasp the intricacies fully.  While the paper offers theoretical advancements practical implementations and realworld applications could be further explored to enhance the relevance and impact of the research.  The complexity of certain mathematical proofs and derivations may limit accessibility for a broader audience. Question to Author:  How do you envision the practical implementation of the proposed GDPT in realworld scenarios particularly in enhancing privacy protection in machine learning applications Feedback to Author:  Consider providing more intuitive explanations or illustrative examples to make the technical concepts more accessible to a wider audience.  Discuss the potential scalability and computational efficiency of the proposed GDPT tool for largescale datasets and complex machine learning models.  Include case studies or experiments to showcase the practical utility and effectiveness of the developed tools in realworld applications. 3) Review Summary: The paper presents a significant contribution to the field of privacypreserving machine learning by introducing the concept of Gaussian differential privacy (GDP) and the Gaussian differential privacy transformation (GDPT) tool for characterizing and identifying GDP mechanisms efficiently. The research offers valuable insights into the theoretical framework of GDP and provides practical applications for enhancing privacy protection in machine learning algorithms. While the paper exhibits strong technical rigor and theoretical advancements further emphasis on practical implementations realworld applications and accessibility to a broader audience could enhance the overall impact and relevance of the research. The development of the GDPT tool opens up new avenues for improving privacy guarantees in machine learning models and future work could focus on refining these tools for broader adoption and practical application in the industry.", "hbGV3vzMPzG": " Peer Review:  1. Summary: The paper investigates the phenomenon of adversarial overfitting in deep learning models trained with adversarial inputs. It introduces a quantitative metric to measure instance difficulty and analyzes the models behavior on instances of varying difficulty levels. The study includes both empirical and theoretical analyses on linear and nonlinear models demonstrating that training on harder instances leads to poorer generalization performance. The paper proposes solutions to mitigate adversarial overfitting in scenarios like fast adversarial training and adversarial finetuning with additional data showing improvements in model robustness.  2. Points of Strength:  Comprehensive Investigation: The paper offers a detailed investigation into the adversarial overfitting phenomenon providing both empirical evidence and theoretical analysis.  Theoretical Analysis: The inclusion of theoretical analyses on linear and nonlinear models adds depth to the study providing insights into the relationship between instance difficulty and model behavior.  Proposed Solutions: The proposed methods to mitigate adversarial overfitting in fast adversarial training and adversarial finetuning with additional data show promising results.  Experimental Results: The experimental results demonstrate the effectiveness of the proposed methods in improving model robustness and reducing adversarial overfitting.  3. Points of Weakness:  Complexity: The paper involves complex theoretical analyses and technical details which might make it challenging for readers with less background in adversarial attacks and deep learning to grasp the findings easily.  Clarity: Some sections could benefit from clearer explanations and illustrations to help readers understand the concepts more intuitively.  Reproducibility: While the paper mentions code submission more details on the implementation and reproducibility of the experiments could enhance the credibility of the findings.  Questions and Feedback to the Authors: 1. Could you provide more insights into the practical implications of your findings on mitigating adversarial overfitting How can the proposed methods be applied in realworld scenarios 2. Can you elaborate on the computational complexity and scalability of the proposed solutions in mitigating adversarial overfitting particularly in scenarios like fast adversarial training with large datasets 3. How do you plan to validate the theoretical findings in realworld applications or more diverse datasets beyond the experiments conducted in this study  Review Summary: The paper presents a thorough investigation into adversarial overfitting offering valuable insights through empirical evidence and theoretical analyses. The proposed solutions to mitigate adversarial overfitting show promising results indicating advancements in improving model robustness against adversarial attacks. While the paper could benefit from enhancing clarity and simplifying complex technical details for broader readership the research provides a significant contribution to understanding and addressing adversarial overfitting in deep learning models. Therefore based on the depth of analysis quality of findings and potential impact on improving model robustness I recommend this paper for acceptance after addressing minor clarity and reproducibility issues.", "iC4UHbQ01Mp": " Summary of the Paper: The paper explores the vulnerability of multimodal contrastive learning models to poisoning and backdoor attacks when trained on noisy and uncurated datasets. The study demonstrates that malicious actors can compromise the models integrity by injecting a tiny fraction of poisoned examples into the training dataset. By poisoning just a small percentage of the dataset the attackers can cause the model to misclassify specific test images or introduce a backdoor trigger to misclassify any input containing a particular pattern. The research provides empirical evidence showing the practicality and effectiveness of these attacks on multimodal contrastive learning models.  Strong Points: 1. Important Contribution: The paper highlights a critical vulnerability in multimodal contrastive learning models shedding light on the risks associated with training on unfiltered datasets. 2. Practical Relevance: Demonstrating the effectiveness of the poisoning and backdoor attacks on real datasets adds practical relevance to the study. 3. Comprehensive Analysis: The research provides a detailed analysis of the attacks including varying parameters such as poisoned fraction patch size and model scalability enhancing the depth of understanding. 4. Innovative Metrics: The introduction of the backdoor zscore as a stable metric to measure the efficacy of backdoor attacks is a valuable contribution to the field. 5. Clear Presentation: The paper is wellstructured with sections covering background methodology experiments and ablation studies making it easy to follow the research methodology and outcomes.  Weak Points: 1. Limited Generalization: The study focuses on specific datasets and model configurations potentially limiting the generalizability of the findings. 2. Assumption of Adversary Capabilities: While assuming realistic adversary capabilities is common in security research the practicality of the described attacks in realworld scenarios may depend on several factors not explicitly mentioned. 3. Lack of Mitigation Strategies: The paper lacks a discussion on potential mitigation strategies or defenses against such attacks which could limit the practical utility of the research for practitioners.  Questions to the Authors: 1. Have you considered how the proposed attacks could be mitigated or defended against in realworld applications of multimodal contrastive learning models 2. How do the findings of this study impact the adoption of contrastive learning techniques in practical applications given the identified security risks 3. Are there any plans to extend this research to explore defenses against poisoning and backdoor attacks on multimodal contrastive learning models  Feedback to the Authors: 1. Consider discussing potential mitigation strategies or defense mechanisms that could help protect multimodal contrastive models from poisoning and backdoor attacks. 2. Address the limitations of the study in terms of generalizability and practical applicability to realworld scenarios to provide a more comprehensive understanding of the research implications. 3. Potentially explore how the identified vulnerabilities could be leveraged to improve model robustness or inform the design of more secure contrastive learning algorithms in the future. Overall the paper presents a significant contribution by exposing the security vulnerabilities of multimodal contrastive learning models but further exploration of mitigation strategies and realworld implications would enhance the impact of the research.", "tDirSp3pczB": "Summary: The paper discusses the contrastive unsupervised representation learning (CURL) and its theoretical analysis. It introduces upper and lower bounds of the downstream classification loss in relation to the negative sample size (K). The study explores the impact of the negative sample size on downstream classification performance and aims to clarify the relationship between them. The theoretical results suggest that larger negative samples reduce the estimation gap of the downstream classification loss. Experiments on synthetic vision and language datasets are conducted to validate the theoretical findings. Strong Points: 1. Theoretical Contribution: The paper contributes to the theoretical understanding of CURL by deriving upper and lower bounds of the downstream classification loss with respect to the negative sample size. This provides a novel perspective on how negative sample size impacts downstream classification performance. 2. Connection Between Theory and Experiments: The theoretical results are supported by experiments on synthetic vision and language datasets. The comparison of the theoretical bounds with actual performance on CIFAR10 dataset shows a consistent pattern validating the theoretical findings. 3. Sharp Bounds: The derived bounds are sharp and provide insightful explanations for the positive impact of larger negative samples on downstream classification accuracy. This enhances the understanding of the empirical observations related to CURL. 4. Discussion of Experimental Results: The paper discusses experimental results indepth highlighting the effects of different negative sample sizes on classification accuracy. The analysis of results on synthetic and realworld datasets offers a comprehensive view of the theoretical findings in practice. Weak Points: 1. Complexity of Notations: The paper utilizes complex mathematical notations and formulations which may make it challenging for readers to grasp the concepts easily. Simplifying the presentation of theoretical derivations could improve accessibility. 2. Discussion Clarity: In some sections the discussion of theoretical results and their implications could be further elaborated to provide clearer insights for readers. Providing more intuitive examples or visuals could enhance understanding. Questions and Feedback: 1. Question: How do the theoretical bounds derived in the study contribute to the practical implementation of contrastive unsupervised representation learning algorithms Can these bounds be utilized to optimize model performance in realworld applications 2. Feedback: Consider enhancing the readability of the paper by providing more intuitive examples or visual aids to illustrate complex theoretical concepts. This could help readers better understand the implications of the theoretical findings in practical scenarios. Review Summary: The paper presents a significant contribution to the theoretical understanding of contrastive unsupervised representation learning particularly focusing on the impact of negative sample size on downstream classification performance. The derived sharp bounds and experimental validations provide valuable insights into the relationship between negative sample size and classification accuracy. Despite some complexities in notations and discussion clarity the studys connection between theory and experiments showcases a strong foundation for further research in this domain. Therefore based on the strong theoretical contributions and empirical validations I recommend this paper for publication after minor revisions to enhance clarity for a wider audience.", "wQStfB93RZZ": "1) Summary The paper introduces a novel approach to address the challenge of multiagent decisionmaking with asynchronous behavior using MacroAction Decentralized Partially Observable Markov Decision Processes (MacDecPOMDPs). The existing policy gradient methods which assume synchronous reasoning about action selection are not suitable for agents making decisions asynchronously. The paper proposes a set of asynchronous multiagent actorcritic methods that allow agents to optimize asynchronous policies using macroactions in decentralized learning centralized learning and centralized training for decentralized execution. The methods are evaluated in various domains showing that highquality solutions can be learned for large domains using the proposed approach. 2) Strengths and Weaknesses Questions and Feedback Strong Points:  The paper addresses an important and underexplored issue of asynchronous multiagent decisionmaking using macroactions.  The proposed asynchronous multiagent actorcritic methods offer a principled way to optimize macroactionbased policies for solving multiagent problems with asynchronous decisionmaking.  The empirical evaluation in diverse domains demonstrates the effectiveness of the proposed methods in learning highquality solutions. Weak Points:  The paper provides thorough theoretical background and methodology explanations but it could benefit from more detailed explanations of the empirical results and comparisons with existing methods.  While the proposed methods show promising results a deeper analysis of the limitations scalability and generalizability of the approach would add value to the paper. Questions: 1. How do the proposed methods compare with stateoftheart multiagent reinforcement learning methods in terms of scalability sample efficiency and robustness in handling complex multiagent scenarios 2. What are the computational complexity and training time of the proposed methods compared to existing approaches 3. How sensitive are the proposed methods to hyperparameter settings and are there any practical guidelines for selecting optimal hyperparameters Feedback: 1. Strengthen the discussion on the practical implications and realworld applications of the proposed methods to highlight their potential impact. 2. Consider including additional experiments and comparisons with other stateoftheart methods to further validate the effectiveness and superiority of the proposed approach. 3) Review Summary The paper successfully addresses the challenge of multiagent decisionmaking with asynchronous behavior using MacroAction Decentralized Partially Observable Markov Decision Processes. The proposed asynchronous multiagent actorcritic methods demonstrate highquality solutions in diverse domains showcasing the effectiveness of the approach. While the paper provides a solid theoretical foundation and empirical evaluation further analysis of limitations scalability and comparisons with existing methods would enhance its robustness and practical significance. Overall the paper presents a novel and promising approach to tackle complex multiagent scenarios laying a strong foundation for future research in this domain.", "mL07kYPn3E": " Summary of the Paper: The paper proposes a novel approach called big prototypes for enhancing the expressivity of classlevel information in fewshot learning scenarios. Prototypes are typically mean output embeddings over instances for each class but they may lack expressivity due to biased sampling. Big prototypes model prototypes as hyperspheres with dynamic sizes represented by two sets of learnable parameters: center and radius of the hypersphere. The paper demonstrates the effectiveness of big prototypes in fewshot learning tasks in NLP and CV through extensive experiments.  Strong Points: 1. Innovative Approach: The paper introduces a novel concept of using hyperspheres to represent prototypes enhancing their expressivity compared to traditional prototypes. 2. Theoretical Grounding: The paper provides a thorough theoretical basis for the proposed method explaining the advantages of using hyperspheres for classification in fewshot learning. 3. Experimental Rigor: The paper conducts extensive experiments across NLP and CV domains to evaluate the effectiveness of big prototypes showcasing significant performance improvements over baseline methods. 4. Comparison with Baselines: The paper appropriately compares the performance of big prototypes with existing prototypebased methods and demonstrates consistent improvements across various fewshot learning tasks. 5. Clear Presentation: The paper is wellstructured with detailed explanations of the problem setup metric calculation initialization and learning processes of big prototypes.  Weak Points: 1. Evaluation Metrics: While the paper demonstrates superior performance of big prototypes compared to baseline methods it could benefit from including additional evaluation metrics or analysis to further validate the effectiveness of the proposed approach. 2. Scalability and Efficiency: The paper does not delve into the scalability and efficiency aspects of using big prototypes especially in largerscale datasets or realworld applications.  Questions for the Authors: 1. How do you envision the scalability of the proposed big prototypes method in handling larger datasets or more complex tasks 2. Have you considered exploring the generalizability and robustness of big prototypes across diverse domains beyond NLP and CV applications  Feedback for the Authors: 1. Discuss the scalability and efficiency aspects of big prototypes especially regarding computational complexity and performance in largerscale scenarios. 2. Consider including additional evaluation metrics or analysis to provide a more comprehensive understanding of the strengths and limitations of the proposed approach.  Review Summary: The paper introduces a novel approach big prototypes for enhancing the expressivity of classlevel information in fewshot learning scenarios. It provides a solid theoretical foundation conducts extensive experiments across NLP and CV domains and showcases improvements over baseline methods. The clear presentation and innovative approach make this paper a valuable contribution to the field of representation learning. However addressing scalability efficiency and incorporating additional evaluation metrics would further strengthen the papers impact and contribute to a deeper understanding of the proposed method. Overall I recommend acceptance with minor revisions to address the aforementioned points.", "zbZL1s-pBF": " Summary of the Paper: The paper proposes a trainingfree robust latefusion method for multimodal learning by leveraging Jacobian regularization and the conditional independence assumption. The key idea is to minimize the Frobenius norm of a Jacobian matrix to stabilize multimodal predictions. The method is applied to different datasets and perturbations demonstrating improved robustness under adversarial attacks and random corruptions.  Strengths: 1. Original Contribution: The paper introduces a novel trainingfree robust latefusion method filling a gap in the existing literature. The approach is wellmotivated and backed by theoretical analysis. 2. Robustness: Through experiments on various datasets and perturbations the proposed method is shown to outperform other fusion methods in terms of robustness against adversarial attacks and random corruptions. 3. Detailed Explanations: The paper provides a clear explanation of the proposed method supported by mathematical derivations experiments and theoretical bounds. The Twomoon example aids in understanding the biasing effect of the extra modality. 4. Experimental Validation: Extensive experiments on datasets like AVMNIST RAVDESS and VGGsound demonstrate the efficacy of the proposed method and its compatibility with different robust techniques. 5. Comprehensiveness: The paper covers various aspects from the introduction of the problem to theoretical derivations numerical algorithms experimental results and future research directions.  Weaknesses: 1. Missing Deduction: A section titled A missing deduction is mentioned but the content is not provided. This should either be included in the paper or explained as removed for brevity.  Questions to the Authors: 1. Could you elaborate on why the method showed a drop in accuracy on clean data in the perturbation on the image modality experiments in Table 4 and Table 6 How do you plan to address this issue 2. Have you considered the robustness of the proposed method to domain shifts where test data is from a different distribution than the training data How could your approach be adapted for such scenarios  Feedback to the Authors: 1. Consider providing the missing content on the deduction in a separate section or integrate it into the main text for completeness. 2. Clarify the reasons behind the drop in accuracy on clean data in certain extreme cases and discuss potential strategies to mitigate this in future work.  Review Summary: The paper presents a significant contribution with a novel trainingfree robust latefusion method for multimodal learning. The proposed approach shows promising results in improving model robustness against adversarial attacks and random corruptions. The detailed explanations thorough experiments and theoretical justification strengthen the credibility of the presented method. However addressing the missing content and delving into the accuracy drop on clean data in particular scenarios would enhance the clarity and applicability of the research. Overall considering the strengths and originality of the proposed approach I recommend the paper for acceptance after addressing the mentioned points.", "g4nVdxU9RK": " Review of \"Rewardless OpenEnded Learning: Combining Unsupervised RL with OpenEnded Learning\"  1) Summary: The paper introduces an innovative algorithm Rewardless OpenEnded Learning (ROEL) which combines unsupervised reinforcement learning (RL) with openended learning to train agents to learn a diverse set of complex skills. ROEL is based on the Paired OpenEnded Trailblazer (POET) framework and aims to generate agents capable of solving increasingly complex and novel problems by constantly evolving a diverse set of skills. The approach is demonstrated on the bipedal walker environment showing improved skill diversity generalization and performance.  2) Strengths and Weaknesses: Strong Points:  Innovative Approach: The integration of unsupervised RL with openended learning in the ROEL algorithm is a significant contribution to the field.  Empirical Results: The experimental results demonstrate the effectiveness of the proposed method in learning diverse and general behaviors.  Clear Explanation: The paper provides detailed explanations of the methodology algorithm and results.  Comparison: The comparison with a baseline method (DADS) and qualitative analysis of skills learned provide a comprehensive evaluation. Weak Points:  Evaluation Metrics: The paper uses the return as a heuristic to measure skill diversity which may not fully capture the diversity of the learned skills. Consider using additional metrics to assess skill generalization and diversity.  Complexity: While the paper discusses the algorithm comprehensively some sections may be overly technical for readers not familiar with deep RL and openended learning.  Questions and Feedback: 1. Evaluation Metrics: How do you plan to further validate the diversity and generalization of the learned skills in ROEL Consider discussing additional evaluation metrics or experiments to provide a more comprehensive assessment. 2. Computational Efficiency: Can you provide insights into the computational efficiency of the ROEL algorithm compared to existing methods How scalable is the approach to more complex environments or tasks 3. RealWorld Applications: How do you envision the applicability of ROEL in realworld scenarios beyond the bipedal walker environment Are there specific domains or industries where this approach could have significant impact  3) Review Summary: The paper presents a novel algorithm ROEL which combines unsupervised RL with openended learning to generate diverse and general behaviors in agents. The empirical results demonstrate the effectiveness of the approach in training agents capable of solving complex tasks in diverse environments. The paper is wellstructured providing a clear explanation of the methodology and results. While the evaluation metrics could be further enhanced the comparative analysis with a baseline method and qualitative assessment of skills learned add depth to the study. Overall considering the significant contribution to the field and promising results I recommend acceptance of this paper with minor revisions to address evaluation metrics and readability for a wider audience.", "xxyTjJFzy3C": "Abstract: The paper introduces a novel approach CoLAV for learning 3D shape descriptors using selfsupervised contrastive learning with dynamically generated adversarial views. The CoLAV method is designed to address the challenges of requiring annotated data and static view inputs faced by existing 3D shape representation methods. By leveraging adversarial views and dynamic rendering CoLAV demonstrates superior performance in 3D shape classification without the need for labeled data. Experimental results on ModelNet40 dataset showcase the effectiveness of CoLAV in learning informative 3D shape representations. Strengths: 1. Innovative Approach: The introduction of adversarial views and dynamic generation of shapeinstancedependent views is a novel and promising approach in 3D shape representation learning. 2. SelfSupervised Learning: CoLAVs ability to learn robust 3D shape representations without labeled data is a significant advantage over existing techniques. 3. Performance: The experimental results demonstrate superior performance in 3D shape recognition compared to existing methods highlighting the efficacy of CoLAV. 4. Robustness to Rotation: CoLAV exhibits high resilience to rotation showcasing its adaptability in handling varying orientations of 3D objects. Weaknesses: 1. Complexity: The paper presents a complex framework involving adversarial views dynamic rendering and contrastive learning which may be challenging for readers not familiar with these concepts. 2. Assumptions: Some assumptions made in the method such as effectiveness of adversarial views could benefit from more detailed explanation or empirical evidence. 3. Comparison Metrics: The paper compares CoLAV with previous methods in terms of performance metrics primarily but more detailed comparative analysis with respect to computational efficiency scalability or other aspects could be insightful. 4. Reproducibility: The implementation details provided could be further elaborated to aid reproducibility and facilitate the adoption of CoLAV by the research community. Questions and Feedback to the Author: 1. Can you elaborate on the applicability of CoLAV to realworld 3D vision tasks beyond the experimental datasets used in the study 2. How does the dynamic generation of adversarial views impact training convergence and model generalizability compared to static approaches 3. Could you discuss potential limitations or failure cases of CoLAV and provide insights into how these challenges could be addressed in future research 4. Have you considered the scalability and computational efficiency of CoLAV for largescale 3D shape datasets or realtime applications Review Summary: Overall the paper presents an innovative approach in CoLAV for selfsupervised contrastive learning of 3D shape representations with dynamically generated adversarial views. The experimental results showcase superior performance in 3D shape recognition and retrieval tasks. The papers strength lies in its novel methodology selfsupervised learning capabilities and robustness to rotation. However the complexity of the framework assumptions made comparison metrics and reproducibility aspects could be further refined. Considering the strengths and the potential impact of CoLAV in advancing 3D shape descriptor learning I recommend further clarity on the aforementioned aspects to enhance the papers contribution and applicability.", "yzDTTtlIlMr": "Summary: The paper delves into analyzing the implicit bias of momentumbased optimizers specifically focusing on Stochastic Gradient Descent with Momentum (SGDM) and Adam optimizer in the context of linear classification with exponentialtailed loss. The key findings include proving the convergence of SGDM and Adam to the L2 maxmargin solution indicating that the momentum does not alter the implicit bias of these optimizers. The theoretical analysis is supported by the establishment of new Lyapunov functions and significant contributions in deepening the understanding of generalization properties. Strengths: 1. Significant Contribution: The paper makes a significant contribution by addressing the open problem of analyzing the implicit bias of momentumbased optimizers in linear classification. 2. Technical Innovation: The introduction of new Lyapunov functions for analyzing the convergence of SGDM and Adam demonstrates technical innovation. 3. Rigorous Analysis: The rigorous theoretical analysis provides insights into the generalization behaviors of the optimizers and their convergence properties. 4. Consistency with Empirical Results: The findings align with existing experimental observations adding credibility to the theoretical results. 5. Detailed Exposition: The paper provides detailed sections outlining related works assumptions methodologies and discussions enhancing the clarity and understanding of the analysis. Weaknesses: 1. Limited Application: The analysis is restricted to linear classification with exponentialtailed loss limiting the broader applicability of the findings to more complex scenarios. 2. Theoretical Focus: While the theoretical analysis is robust a lack of empirical validation or experimental results might limit the practical applicability of the findings. Questions and Feedback: 1. Have you considered extending the analysis to nonlinear models or applications beyond linear classification to assess the generalization properties in more diverse scenarios 2. How do you plan to validate the theoretical findings through empirical experiments or realworld applications to corroborate the results 3. Can you further elaborate on the practical implications of the derived theorems on optimizing the performance of deep learning models in realworld applications 4. How do you plan to disseminate and utilize these findings to inform the development of optimization algorithms and enhance their generalizability in practical machine learning tasks Review Summary: The paper presents a significant contribution by theoretically analyzing the implicit bias of momentumbased optimizers in linear classification with exponentialtailed loss showcasing technical innovation and rigorous analysis. While the paper is thorough and detailed in its theoretical exposition extending the analysis to more complex scenarios and validating the findings through empirical experiments could enhance the practical relevance of the research. Overall the paper demonstrates depth in understanding the generalization properties of optimization algorithms but could benefit from broader applicability and validation in realworld applications.", "u6TRGdzhfip": " Review:  1. Summary: The paper introduces a new approach called Introspective Adversarial Distillation (IAD) for improving adversarial robustness in deep neural networks. The traditional adversarial distillation methods rely on guidance from teacher models but this guidance becomes unreliable as the training progresses. IAD addresses this challenge by partially trusting teacher guidance based on their performance on both natural and adversarial data. The paper presents detailed theoretical analysis the rationale for IAD and validates its effectiveness through experiments on benchmark datasets like CIFAR10CIFAR100 and TinyImageNet.  2. Strengths:  Innovative Approach: The concept of partial trust and selfintrospection in adversarial distillation is novel and addresses a critical issue in leveraging teacher models for robustness.  Comprehensive Analysis: The paper provides a thorough analysis of the challenges in traditional adversarial distillation methods and justifies the need for the proposed IAD method.  Experimental Validation: Extensive experiments on multiple datasets demonstrate the superiority of IAD over existing methods in improving adversarial robustness while maintaining or even enhancing natural accuracy.  3. Weaknesses:  Clarity of Presentation: The paper while comprehensive is dense with technical details which may be challenging for readers not deeply familiar with the subject matter.  Experimental Setup Reproducibility: More details on hyperparameters used and implementation specifics could enhance the reproducibility of the experiments.  Comparison with StateoftheArt: While the paper compares IAD with existing methods a more detailed comparison with stateoftheart approaches in terms of robustness metrics could further support the effectiveness of IAD.  Questions to the Author: 1. How sensitive is the proposed IAD method to changes in hyperparameters like \u03b3 \u03c4 and \u03b1 Have you conducted sensitivity analysis on these parameters 2. Can you elaborate on the potential scalability of IAD to larger datasets and more complex neural network architectures  Feedback: The paper presents a novel and promising approach with strong theoretical underpinnings and empirical validation. To enhance the clarity and impact of the work consider:  Providing a clearer explanation of complex technical concepts for broader readership.  Including more detailed experimental setup information and comparison with stateoftheart methods to bolster the papers contributions.  Conducting sensitivity analyses on key hyperparameters to demonstrate the robustness and generalizability of the proposed IAD method.  Review Summary: The paper introduces a novel method Introspective Adversarial Distillation (IAD) to address the challenge of unreliable teacher guidance in adversarial distillation. The thorough theoretical analysis and experimental validation demonstrate the effectiveness of IAD in improving adversarial robustness. While the paper is innovative and wellsupported improvements in clarity reproducibility and comparative analysis with stateoftheart methods could enhance its impact in the research community. Therefore I recommend acceptance with minor revisions to address the feedback provided.", "nuWpS9FNSKn": " Summary of the Paper The paper explores the application of selfsupervised learning to data generated by topic models in the context of Natural Language Processing (NLP). It focuses on the benefits of selfsupervised learning specifically in relation to the robustness to model misspecification. The authors provide theoretical proofs and empirical evidence to support their claims demonstrating that selfsupervised learning can recover useful posterior information for general topic models regardless of the specific model used to generate the data. The paper presents theoretical guarantees for both reconstructionbased and contrastive selfsupervised learning objectives and explores their performance on synthetic data generated by various topic models. Additionally the authors perform experiments on real datasets to validate the effectiveness of their approach for document representation learning.  Strong Points of the Paper 1. Theoretical Rigor: The paper provides detailed theoretical analyses and proofs to support its claims regarding the effectiveness of selfsupervised learning in topic model settings. 2. Empirical Evaluation: The authors conduct thorough experiments on synthetic and real datasets to validate the theoretical findings and demonstrate the practical utility of their approach. 3. Novelty: The paper addresses an important and timely topic in NLP research exploring the application of selfsupervised learning in the context of topic models offering new insights and potential solutions to existing challenges. 4. Robustness: The study highlights the robustness of selfsupervised learning to model misspecification providing a valuable contribution to the field.  Weak Points of the Paper 1. Complexity of Presentation: The paper contains dense theoretical content that may be challenging for readers without a strong background in the subject matter to fully comprehend. 2. Reproducibility: The details regarding the exact implementation of the neural network models used for the experiments could be more explicitly provided to facilitate replication by other researchers. 3. Discussion of Limitations: While the paper showcases the benefits of selfsupervised learning more discussion on the limitations or potential challenges of the proposed approach could enhance the overall discussion.  Questions and Feedback to the Authors 1. Can you provide more insights into the choice and design of the neural network architectures used in your experiments How sensitive are the results to variations in the network structures 2. Have you considered analyzing the impact of hyperparameters such as learning rate or batch size on the performance of the selfsupervised learning objectives in your experiments 3. How do you envision extending this research beyond topic models to explore other domains or applications in NLP where selfsupervised learning could provide similar benefits 4. Have you considered the computational cost and scalability of your approach to larger datasets or more complex models in practical NLP applications  Review Summary The paper presents a comprehensive and wellexecuted study on the application of selfsupervised learning in the context of topic models in NLP. The combination of rigorous theoretical analysis experimental validation and practical insights enhances the credibility and significance of the research findings. While the paper showcases strong theoretical foundations and empirical results supporting the advantages of selfsupervised learning addressing the complexity of presentation and providing more details on implementation could further strengthen the impact and reproducibility of the study. Overall I recommend acceptance of this paper for publication considering its contributions to the field of NLP and the valuable insights it offers for advancing research in selfsupervised learning methodologies.", "nbC8iTTXIrk": " Summary of the Paper: The paper introduces a novel implicit model called Multibranch Optimization induced Equilibrium networks (MOptEqs) designed for multiresolution recognition tasks. The model is inspired by Optimization Induced Equilibrium Networks (OptEqs) and aims to leverage hierarchical patterns for recognition tasks while maintaining a connection to an optimization problem. The paper proposes a Hierarchical Heritage and Diversity Module for MOptEqs along with a Perturbation Enhanced strategy. Experimental results demonstrate the effectiveness of the proposed model in comparison to stateoftheart models like MDEQ showcasing better performance explainability and parameter efficiency.  Strong Points: 1. Innovative Model Design: Introducing a multibranch implicit model for multiresolution recognition tasks sets the paper apart from existing models and addresses the limitations of prior implicit models. 2. Clear Explanation of Model Components: The paper provides thorough explanations of the models components such as the Hierarchical Heritage and Diversity Module and the Perturbation Enhanced strategy enhancing the transparency of the proposed methods. 3. Experimental Validation: The paper conducts a comprehensive set of experiments on CIFAR10 CIFAR100 and ImageNette datasets to demonstrate the superiority of MOptEqs over existing models showcasing better performance and stability.  Weak Points: 1. Technical Jargon and Complexity: The paper contains complex mathematical formulations and technical details that may be challenging for readers not familiar with the specific domain. Simplifying the language without sacrificing technical accuracy could enhance readability. 2. Lack of Comparison Metrics: While the experimental results show the superiority of MOptEqs providing additional comparison metrics and analyses to further highlight the models advantages would strengthen the paper.  Question to the Author: Could you elaborate on the scalability of MOptEqs in terms of handling larger datasets or more complex recognition tasks How does the model perform in scenarios with increased data dimensions and classes  Feedback to the Author: 1. Consider providing more intuitive visual aids to complement the detailed explanations of model components and processes enhancing understanding for readers. 2. Address the potential scalability and adaptability of MOptEqs for broader applications in the discussion to showcase the models versatility across various tasks and datasets.  Review Summary: The paper introduces a novel multibranch implicit model MOptEqs designed for multiresolution recognition tasks. The innovative model architecture along with the proposed Hierarchical Heritage and Diversity Module and Perturbation Enhanced strategy demonstrates superior performance explainability and efficiency compared to stateoftheart models. While the papers technical depth is commendable simplifying complex concepts for broader accessibility and providing further insights into scalability and adaptability would enhance the papers impact. Overall the paper contributes significantly to the field of implicit modeling and warrants consideration for publication pending minor revisions and additional clarity in presentation.", "lpkGn3k2YdD": "Peer Review: Summary: The paper introduces the randomized return decomposition (RRD) algorithm to address the problem of episodic reinforcement learning with trajectory feedback. RRD uses a MonteCarlo estimator to establish a surrogate optimization problem for return decomposition aiming to improve scalability in longhorizon tasks. The paper provides theoretical analyses and experimental results demonstrating the effectiveness of RRD compared to existing baseline algorithms in the context of episodic reinforcement learning. Strengths: 1. Novel Contribution: The paper introduces a novel algorithm RRD which addresses the challenges of episodic reinforcement learning with trajectory feedback. The proposed approach of using a MonteCarlo estimator for surrogate optimization is innovative. 2. Theoretical Analysis: The paper provides detailed theoretical analyses of the algorithmic properties of RRD connecting it with existing methods in reward redistribution. The formal characterizations and proofs enhance the credibility of the proposed approach. 3. Comparative Experiments: The experimental evaluation on MuJoCo benchmark tasks provides a comprehensive comparison of RRD with several existing algorithms demonstrating superior performance in terms of sample efficiency and policy quality. The ablation study on the hyperparameter K adds depth to the analysis. 4. Clarity and Structure: The paper is wellstructured with a clear introduction of the problem setting detailed methodology theoretical analyses experimental results and discussions. The language is concise technical but understandable suitable for the target audience. Weaknesses: 1. Baseline Comparison: While the paper compares RRD with relevant baseline algorithms a more detailed comparison and discussion of the limitations of existing methods in handling episodic reinforcement learning with trajectory feedback would provide additional insights. 2. Generalization and Applicability: The paper could discuss more about the generalizability and applicability of RRD beyond the specific scenarios tested in the experiments. How the proposed algorithm could be adapted and extended to other problem domains should be elaborated upon. Questions: 1. How does the proposed RRD algorithm handle complex environments or tasks with highdimensional state spaces Are there any scalability issues when dealing with such environments 2. Considering the sample efficiency improvement shown in the experiments are there any limitations or areas where further improvement or finetuning of the algorithm is needed Feedback for Authors: 1. Provide a more detailed discussion on the limitations and challenges of existing methods in handling episodic reinforcement learning issues and how RRD addresses these challenges effectively. 2. Discuss potential future research directions applications and extensions of the RRD algorithm in tackling other related problems in reinforcement learning. Review Summary: Overall the paper presents a wellstructured and technically sound study on the RRD algorithm for episodic reinforcement learning with trajectory feedback. The theoretical analyses comparative experiments and discussions provide a strong foundation for the proposed approach. With a few enhancements in discussing the limitations of existing methods and further generalizing the applicability of RRD the paper has the potential to make a significant contribution to the reinforcement learning research field. Therefore based on the strengths clarity of presentation and experimental results I recommend the paper for acceptance after addressing the mentioned feedback and questions.", "gi4956J8g5": "Summary of the Paper: The paper introduces the SecondOrder unsupervised Feature selection via knowledge contrastive disTillation (SOFT) model aiming to incorporate secondorder relationships between features for more effective feature selection. In the first stage a sparse attention matrix is learned to explore secondorder feature relationships. The second stage involves graph segmentation on the attention matrix for feature selection. Experimental results on 12 public datasets demonstrate the superiority of SOFT over classical and recent stateoftheart methods showcasing its effectiveness in unsupervised feature selection. Strong Points: 1. Innovative Approach: The incorporation of secondorder relationships between features sets SOFT apart from traditional feature selection methods that focus solely on intrinsic properties of data. 2. Effective Feature Representation: By learning a sparse attention matrix SOFT distills structural knowledge ensuring that crucial feature relationships are captured while redundant information is excluded. 3. Superior Performance: Experimental results reveal that SOFT outperforms other methods on multiple datasets highlighting its efficacy in unsupervised feature selection. Weak Points: 1. Stability of Results: The paper acknowledges that the performance of SOFT may not be as stable as other methods in certain scenarios especially as the number of selected features increases. Further analysis on stabilizing the algorithms performance could enhance its reliability. 2. Complexity: The complexity of the SOFT model particularly in learning the attention matrix and conducting graph segmentation may pose challenges in practical implementations especially with large datasets. Question to the Author: 1. Did you consider any additional experiments to further explore the stability of SOFTs performance across different datasets or varying percentages of selected features Feedback to the Author: 1. Providing a more detailed comparison with other stateoftheart methods on specific aspects such as computational efficiency or scalability could enhance the papers completeness. Review Summary: The paper introduces a novel SOFT model for unsupervised feature selection showcasing its effectiveness in capturing secondorder feature relationships and achieving superior performance compared to existing methods. While the innovative approach and strong empirical results highlight the papers contributions addressing concerns regarding algorithm stability and complexity could further strengthen its impact in the field. Overall the paper presents a valuable contribution to the domain of unsupervised feature selection.", "oWZsQ8o5EA": " 1. Summarize the Paper: The paper provides new informationtheoretic upper bounds for generalization error of machine learning models specifically neural networks trained with SGD. It builds on the work of Neu et al. (2021) and extends the analysis of the generalization behavior of linear and twolayer ReLU networks using these bounds. The study also introduces a new regularization scheme called Gaussian Model Perturbation (GMP) and demonstrates its effectiveness in reducing generalization error. Theoretical derivations experiments and comparisons with existing regularization techniques are provided to support the findings.  2. Strong and Weak Points Questions and Feedback:  Strong Points:  Theoretical Depth: The paper presents a comprehensive theoretical analysis extending previous work and introducing novel concepts like gradient dispersion and auxiliary weight processes in the context of generalization bounds for neural networks.  Experimental Validation: The empirical study supports the theoretical claims demonstrating practical implications of the proposed bounds and the effectiveness of the GMP regularization scheme.  Methodological Innovation: The paper introduces GMP as a novel regularization scheme based on the theoretical insights derived from the informationtheoretic bounds adding to the methodological innovations in the field.  Clear Presentation: The structure of the paper is wellorganized leading the reader through the theoretical developments experimental results and practical implications in a logical flow.  Weak Points:  Complexity: The paper delves into advanced mathematical concepts and may be challenging for readers not wellversed in information theory and machine learning theory.  Questions: 1. Could the paper provide more insights into practical applications of the derived bounds especially in realworld scenarios or specific problem domains 2. How does the proposed GMP regularization scheme compare to other stateoftheart techniques in terms of computational efficiency and scalability  Feedback:  Clarification on how the proposed regularization scheme addresses specific challenges encountered in training neural networks could enhance the practical relevance of the study.  Providing more intuitive explanations or visual aids to help readers grasp the key concepts particularly in the theoretical derivations could improve the accessibility of the paper.  3. Review Summary: The paper presents a significant contribution to the understanding of generalization bounds for neural networks trained with SGD. The theoretical developments supported by experimental validation demonstrate the effectiveness of the derived bounds and the practical implications of the novel Gaussian Model Perturbation (GMP) regularization scheme. While the paper showcases strong theoretical depth and methodological innovation it could benefit from enhancing clarity for readers less familiar with the intricate mathematical concepts presented. Overall the paper is recommended for publication due to its original insights and the impact it can have on advancing the field of machine learning theory and regularization techniques.", "t3BFUDHwEJU": " Summary The paper proposes a novel family of delayed discounted objective functions to address the limitations of traditional geometric discounting in reinforcement learning (RL) algorithms. By generalizing the discount factors the authors aim to optimize both stationary and nonstationary policies in RL settings particularly in hard exploration tasks and continuous robotics benchmarks.  Strengths 1. Innovative Approach: The paper introduces a novel family of delayed discounted criteria which extends traditional geometric discounting providing a more flexible framework for modeling optimal behaviors. 2. Theoretical Analysis: The authors provide a detailed theoretical analysis of the proposed criteria including the derivation of optimal control solutions for both stationary and nonstationary policies. 3. Practical Validation: The paper demonstrates the effectiveness of the proposed algorithms in solving hard exploration problems in tabular environments and improving sample efficiency in classic simulated robotics benchmarks. 4. Experimental Design: The experiments are welldesigned comparing the proposed algorithms with existing methods like Soft Actor Critic (SAC) and showcasing performance improvements in continuous control benchmarks.  Weaknesses 1. Complexity: The paper is quite technical and may be challenging for readers not deeply familiar with RL concepts and algorithms. 2. Empirical Results: While the results demonstrate improvement in sample efficiency a more thorough analysis of the scalability and computational requirements of the proposed algorithms could strengthen the paper.  Questions for the Authors 1. Scalability: How do the proposed algorithms scale with increasing problem complexity or dimensionality especially in realworld applications 2. Computational Efficiency: Can you provide insights into the computational requirements of the novel delayed discounted criteria compared to traditional geometric discounts in practical implementations 3. Generalization: Have you considered the generalizability of your approach to different types of environments or tasks beyond the ones tested in the experiments 4. Robustness: How robust are the proposed algorithms to variations in hyperparameters or different initializations in more complex RL settings  Feedback  Clarity: Enhancing the clarity of key concepts and notations especially for readers less familiar with RL theory would improve the accessibility of the paper.  Discussion: Providing deeper insights into the limitations and potential extensions of the proposed methods could enrich the impact of the work.  Review Summary The paper introduces a novel family of delayed discounted criteria in RL algorithms addressing limitations of traditional geometric discounts in modeling optimal behaviors. The theoretical analysis and experimental validation demonstrate the effectiveness of the proposed algorithms in solving hard exploration problems and improving sample efficiency in continuous robotic benchmarks. Despite the complexity of the topic the innovative approach and promising results make the paper a valuable contribution to the field of reinforcement learning. Strengthening the discussion and addressing scalability and computational efficiency concerns would further enhance the impact of the research.", "iUuzzTMUw9K": " Review of \"StyleNeRF: A 3DAware Generative Model for HighResolution Image Synthesis\"  1) Summary: The paper introduces StyleNeRF a 3Daware generative model for photorealistic highresolution image synthesis with high multiview consistency. StyleNeRF integrates the neural radiance field (NeRF) into a stylebased generator to address challenges seen in existing approaches. The model can synthesize highresolution images with fine details preserve 3D consistency and enable control over style attributes and explicit 3D camera poses. Various designs are proposed to enhance rendering efficiency and 3D consistency making StyleNeRF suitable for interactive highresolution image generation. Extensive evaluations on challenging datasets demonstrate the effectiveness of StyleNeRF in producing highquality images at interactive rates.  2) Strengths and Weaknesses:  Strengths:  Innovative Approach: The integration of NeRF into a stylebased generator is a novel approach to address challenges in highresolution image synthesis.  High Quality Results: StyleNeRF demonstrates superior visual quality and multiview consistency compared to existing methods.  Efficiency: The model enables interactive rates for highresolution image synthesis overcoming computational inefficiencies seen in traditional NeRFbased models.  Control and Flexibility: StyleNeRF allows effective control over camera poses style attributes style mixing semantic editing and explicit camera control.  Weaknesses:  Theoretical Analysis: The paper could benefit from more theoretical analysis to provide deeper insights into the models behavior and limitations.  Geometry Preservation: While StyleNeRF efficiently generates highresolution images with explicit camera control it may sacrifice some details in capturing underlying geometry compared to pure NeRFbased models for applications requiring detailed geometry reconstruction.  Question and Feedback to the Author:  Could you elaborate more on the theoretical underpinnings of StyleNeRF and how it guarantees 3D consistency  Can you discuss potential applications beyond image synthesis where StyleNeRF could be beneficial  How does StyleNeRF handle occlusions or complex scenes with multiple objects  Have you considered conducting user studies for subjective evaluation of StyleNeRFgenerated images  3) Review Summary: Overall the paper presents a pioneering approach in highresolution image synthesis with impressive results in terms of visual quality multiview consistency and efficiency. The extensive evaluations and comparisons demonstrate the superiority of StyleNeRF over existing methods. The control and flexibility offered by StyleNeRF along with its efficiency in rendering highresolution images at interactive rates make it a valuable contribution to the field. However further exploration on theoretical analysis and considerations for capturing detailed geometry could enhance the robustness of the model. Given the potential impact on disinformation and forgery detection it is commendable that the paper addresses these ethical considerations. The provided code availability and reproducibility assurance further strengthen the credibility of the research. Based on the strengths weaknesses and potential improvements identified I recommend this paper for publication after addressing the theoretical analysis requirements and discussing the implications of detailed geometry preservation in future work.", "ugxdsne_TlO": " Summary: The paper introduces a new algorithm generalized causal forest (GCF) for estimating heterogeneous treatment effects with continuous treatments. By extending causal forest (CF) with nonparametric doseresponse functions (DRFs) GCF aims to overcome limitations of existing machine learning methods when dealing with continuous treatments. The proposed algorithm utilizes kernelbased DoubleDebiased ML estimators for locally estimating DRFs and introduces a distancebased splitting criterion in the functional space of Partial DRFs to capture heterogeneity. The effectiveness of GCF is demonstrated through synthetic data and largescale realworld datasets. The paper provides a detailed overview of the algorithm discusses related works introduces notations and assumptions and presents the experimental results comparing GCF to stateoftheart methods.  Strengths:  Innovative Algorithm: The introduction of GCF addresses the need for accurate heterogeneous treatment effect estimation with continuous treatments using nonparametric DRFs.  Integration with Apache Spark: Implementing the algorithm on Apache Spark for largescale data processing enhances its practical utility.  Comprehensive Evaluation: The paper presents thorough experimental results on both synthetic and realworld datasets showcasing the effectiveness of GCF compared to existing methods.  Theoretical Foundations: The paper extensively discusses the theoretical underpinnings of DRF estimation kernel regression and doubledebiased machine learning enhancing the credibility of the proposed algorithm.  Weaknesses:  Technical Complexity: The paper delves deep into technical details which might make it less accessible to readers not wellversed in machine learning and causal inference.  Lack of Interpretation: While the results show the superiority of GCF the paper could benefit from a more indepth interpretation of the findings and the practical implications of adopting this algorithm in realworld settings.  Questions and Feedback to the Authors: 1. Could you provide more insights into the practical implementation challenges of GCF especially when dealing with highdimensional data or heterogeneous treatment effects 2. Have you considered the robustness of GCF when faced with noisy or unbalanced datasets and have you encountered any limitations in terms of generalizability across various domains 3. In the future do you plan to extend the evaluation of GCF to more diverse datasets or realworld scenarios to further validate its robustness and applicability  Review Summary: The paper presents a highly innovative approach in the field of heterogeneous treatment effect estimation with continuous treatments. By integrating nonparametric DRFs with a novel splitting criterion the proposed GCF algorithm demonstrates superior performance compared to existing methods. The rigorous experimental evaluation theoretical foundations and practical implementation on Apache Spark contribute to the strength of the paper. However clearer explanations and potential realworld applications could further enhance its impact. Overall based on the novelty robust evaluation and theoretical soundness of the proposed algorithm I recommend the paper for publication pending minor revisions for enhanced readability and practical relevance.", "jbrgwbv8nD": " Summary of the Paper: The paper introduces a novel method called RegularConstrained Conditional Random Fields (RegCCRF) to relax the Markov assumption in linearchain CRFs by enforcing constraints specified as regular languages on the output sequences. By incorporating these constraints during training RegCCRFs overcome the limitations of standard CRFs in representing nonlocal dependencies and exhibit improved performance. The paper shows empirical evidence of the models effectiveness on semantic role labeling tasks achieving stateoftheart results on a standard dataset.  Strong Points: 1. Innovative Methodology: The paper proposes a novel approach RegCCRF to relax the Markov assumption in CRFs by enforcing regularlanguage constraints during training addressing the limitations of standard CRFs. 2. Empirical Validation: The empirical evaluation on synthetic data and realworld applications particularly in semantic role labeling demonstrates the effectiveness of RegCCRFs in capturing complex interdependencies within output structures. 3. Theoretical Soundness: The paper provides a detailed theoretical foundation for RegCCRFs discussing constraints model formalisms construction principles and connections to weighted finitestate transducers enhancing the rigor and credibility of the proposed method. 4. Comparison with Baselines: The comparison with baseline models including CRFs trained without constraints highlights the superiority of RegCCRFs in modeling structured output distributions with constraints.  Weak Points: 1. Complexity of Regular Language Construction: The paper acknowledges the challenge of constructing regular languages for constraints and discarding rare role labels for practicality. Future work could explore automated methods for regular language induction to alleviate manual specification efforts. 2. Performance Evaluation Limitations: While the paper shows empirical improvements further analyses on larger datasets and diverse applications would strengthen the generalizability of RegCCRFs and provide deeper insights into the models capabilities in various scenarios. 3. Algorithmic Efficiency: The paper notes potential issues related to the computational complexity of RegCCRFs with large tag sets calling for investigations into optimization techniques to enhance training and inference efficiency.  Questions and Feedback to the Author: 1. Scalability: How scalable is the RegCCRF approach when handling larger datasets with more complex constraints especially in terms of training time memory requirements and computational efficiency 2. Generalizability: How adaptable is the RegCCRF method to other structured prediction tasks beyond semantic role labeling such as relation extraction or entity recognition where distinct constraints may apply 3. Comparison Metrics: Could the paper provide additional insights into how RegCCRFs compare against other stateoftheart models in structured prediction tasks showcasing strengths in specific scenarios or domains  Review Summary: The paper presents an innovative approach RegCCRF to relax the Markov assumption in CRFs by enforcing regularlanguage constraints during training. The empirical results on synthetic data and semantic role labeling tasks demonstrate the models effectiveness in capturing complex dependencies. While the paper is thorough in its theoretical foundation and experimental validation there are opportunities to explore scalability generalizability and comparisons with alternative models. Overall with its strong methodology empirical validation and theoretical soundness the paper warrants publication with minor revisions to address scalability and broader applicability aspects.", "kF9DZQQrU0w": " Summary: The paper investigates the Information Bottleneck (IB) principle in deep neural networks by studying the mutual information (MI) between hidden layers input and output. The study confirms the existence of both fitting and compression phases during training focusing on different binning methods that can lead to conflicting results in IB analyses. By using quantized neural networks to compute the MI exactly the study verifies previous claims and critiques shedding light on the dynamics of information flow in deep learning systems.  Strengths: 1. Thorough Investigation: The paper provides a detailed examination of the IB principle in neural networks addressing controversies and critiques from previous studies. 2. Experiments and Results: The utilization of quantized networks to compute exact MI provides valuable insights into the information dynamics during training. The results offer a comprehensive understanding of the fitting and compression phases in different network architectures. 3. Contributions: By eliminating measurement artifacts and focusing on exact computations the paper makes a significant contribution to the understanding of IB in neural networks and sets a solid foundation for future research.  Weaknesses: 1. Limitations of IB: The paper acknowledges the limitations of the IB analysis particularly in complex networks where trivial information planes are observed reflecting challenges in practical deep learning scenarios. 2. Controversy Resolution: While the study confirms the impact of network architecture on the compression phase it falls short of fully resolving the controversy surrounding the phenomenon. Future studies may need to delve deeper into this aspect.  Questions and Feedback: 1. Question: How would the findings and conclusions of this study impact the design and training of neural networks in practical applications 2. Feedback: Providing insights into the limitations of the IB analysis and the need for more nuanced approaches in studying information flow could enhance the impact and relevance of the paper in the machine learning community.  Review Summary: The paper offers a significant contribution by investigating the information bottleneck principle in deep neural networks using quantized networks for exact MI computations. By addressing past critiques and confirming key observations the study advances the understanding of fitting and compression phases in network dynamics. While limitations in applying the IB analysis to complex networks are highlighted the study sets a solid foundation for future research by eliminating measurement artifacts and paving the way for more accurate analyses in deep learning systems. Hence based on the comprehensive experiments and valuable insights provided I recommend the acceptance of this paper for publication.", "yK_jcv_aLX": " Summary of the Paper: The paper introduces a novel approach to learning minimal state representations for decisionmaking in partially observable environments. The proposed method ActionSufficient state Representations (ASRs) focuses on extracting essential state representations that capture sufficient information for policy learning. The paper presents a generative environment model to characterize structural relationships in the system and develops a Structured Sequential Variational AutoEncoder to estimate the ASRs. Experimental results on CarRacing and VizDoom demonstrate the effectiveness of using ASRs for policy learning improving both efficiency and efficacy.  Strengths of the Paper: 1. Innovative Approach: The paper introduces a novel framework for learning minimal state representations for decisionmaking focusing on sufficiency and essential information for policy learning. 2. Theoretical Foundation: The paper provides a strong theoretical foundation introducing the concept of ASRs and their characterization based on generative environment modeling and structural constraints. 3. Empirical Validation: Empirical results on CarRacing and VizDoom demonstrate the advantages of using ASRs for policy learning showing improved efficiency and effectiveness over existing methods. 4. Detailed Methodology: The paper thoroughly explains the structured sequential VAE for estimating ASRs as well as the procedure for policy learning with ASRs in both modelfree and modelbased scenarios.  Weaknesses of the Paper: 1. Complexity: The paper may be challenging for readers not familiar with reinforcement learning VAEs and related concepts due to the technical depth and complexity of the proposed methodology. 2. Evaluation: While the experimental results demonstrate the effectiveness of the proposed approach a more extensive evaluation on a broader range of environments and comparison with additional methods could further strengthen the paper.  Questions for the Authors: 1. Identifiability: Can the authors elaborate on the identifiability conditions for the proposed model particularly in complex nonlinear cases 2. Generalization: How well does the proposed approach generalize to different environments and tasks beyond CarRacing and VizDoom Have you considered evaluating the method on diverse benchmarks 3. Scalability: What are the computational requirements of the proposed approach especially in terms of training time and resource efficiency on largerscale environments  Feedback for the Authors: 1. Clarification: Provide additional explanations or visualizations to aid in understanding the complex model structures for readers unfamiliar with the domain. 2. Comparison: Consider expanding the comparison with more stateoftheart methods and environments to provide a broader perspective on the performance of the proposed ASRs. 3. Reproducibility: Emphasize the importance of reproducibility by providing clear guidelines and access to the source code and datasets for researchers interested in replicating the experiments.  Review Summary: The paper presents a novel framework for learning minimal state representations for policy learning in partially observable environments through ASRs and a structured VAE approach. With strong theoretical foundations and promising empirical results the proposed method shows potential for improving efficiency and effectiveness in decisionmaking tasks. Further enhancements in clarity comparison with a wider range of methods and considerations for reproducibility can strengthen the papers impact and contribution to the field. Based on the innovative approach and empirical validation I recommend this paper for acceptance with suggestions for minor revisions and enhancements as outlined above.", "lY0-7bj0Vfz": " 1. Summary of the Paper The paper introduces a Memory Concept Attention (MoCA) module that utilizes prototype memory priors in improving fewshot image generation quality in computer vision tasks. Inspired by recent neurological findings regarding the existence of highly selective complex feature detectors in the visual cortex the authors propose MoCA to enhance image synthesis processes by leveraging memorybased attention mechanisms. The paper discusses the computational advantages and functional role of these \"grandmother cells\" and their potential as memory prototypes for modulating image synthesis processes.  2. Strengths and Weaknesses Questions and Feedback  Strong Points: 1. Innovative Approach: The introduction of MoCA as a novel module leveraging prototype memory priors is a commendable innovation in the field of image generation. 2. Theoretical Foundation: The paper provides a solid theoretical foundation by linking neurological findings to computational advantages and functional roles. 3. Comparative Analysis: Extensive experiments on various datasets demonstrate the effectiveness of MoCA in improving image synthesis quality interpretability and model robustness. 4. Robustness: The study on robustness against noise showcases the potential strength of MoCA in handling variations and perturbations.  Weak Points: 1. Clarity of Presentation: The complexity of the content particularly the technical details of the MoCA module might be challenging for general readers and require simplification. 2. Evaluation Metrics: While FID and KID are widely used additional metrics or comparisons could enhance the completeness of the evaluation. 3. Discussion on Neural Correlates: Further discussion on how the proposed model aligns or diverges from known neurological mechanisms could deepen the understanding of the biological and computational linkages.  Questions and Feedback: 1. Additional Experiments: Have you considered conducting experiments to compare MoCA with other stateoftheart image generation techniques beyond FastGAN and StyleGAN2 for a more comprehensive evaluation 2. Interpretability: Can you elaborate on how the interpretability of visual concept clusters learned by MoCA could be beneficial in realworld applications beyond image generation tasks 3. Biological Insights: How do you envision future research leveraging the insights from this study to contribute to our understanding of neural computational principles in the visual cortex  3. Review Summary The paper makes a significant contribution by introducing the MoCA module which leverages prototype memory priors to enhance fewshot image generation quality. The theoretical underpinnings extensive experiments and focus on model robustness provide compelling evidence of the effectiveness of the proposed approach. While the paper excels in innovation and experimental validation improving clarity for a wider audience and exploring the implications of the findings for bridging computational and neurological insights could further enhance its impact.  Recommendation: Accept with Minor Revisions The paper offers substantial contributions to the field of image generation through the MoCA module. With some improvements in clarity further experiments and deeper discussions on biological relevance the paper has the potential to be a valuable addition to the existing literature.", "r4PibJdCyn": " Peer Review  Summary: The paper proposes a bidirectional candidates generation framework TotalRecall (TR) to serve both Recommender Systems (RS) and Potential Users Mining Systems (PUMS) simultaneously in Ecommerce companies. The framework is designed to generate item candidates for users (u2i) and identify highpotential users for items (i2u) efficiently. The paper claims superiority over stateoftheart algorithms in terms of modeling objectives DNN architectures achieving SOTA results and faster convergence. The approach is empirically validated through experiments on largescale datasets.  Strengths: 1. Novel Framework: The bidirectional candidates generation framework presented in the paper is innovative and addresses the dual challenges faced by RS and AS in Ecommerce platforms effectively. 2. Theoretical Analysis: The theoretical equivalence established between modeling objectives with Bernoulli and Multinomial distributions adds depth to the study and enhances understanding. 3. Practical Applications: The practical applications of the TR framework in largescale PUMS and SME marketing scenarios are welldocumented providing realworld relevance to the research. 4. Comprehensive Experiments: The experiments conducted on diverse datasets demonstrate the effectiveness of the proposed framework in achieving competitive results and faster convergence compared to existing methods.  Weaknesses: 1. Clarity and Organization: While the paper covers a wide range of topics the organization of content and clarity of presentation can be improved to enhance readability and understanding. 2. Limited Explanation: Some technical aspects such as the exact implementation details of the proposed TR framework and losses could be elaborated further for better comprehension. 3. Evaluation Metrics: The paper could benefit from a more detailed discussion on the choice of evaluation metrics for comparing the proposed TR framework with existing algorithms in the experiments.  Questions: 1. What specific features or characteristics of the TotalRecall framework enable it to achieve faster convergence compared to existing algorithms and can this be further elaborated for better understanding 2. How does the TR framework handle scalability issues concerning large datasets in practical Ecommerce settings especially considering the bidirectional nature of candidates generation 3. Are there any specific limitations or challenges faced during the experimentation phase with regards to implementing and optimizing the TR framework that could influence its practical adoption  Feedback:  The paper should aim to enhance the clarity of technical details and implementation aspects of the TotalRecall framework to enable readers to understand the methodology more comprehensively.  Further explanation on the practical implications and scalability of the TR framework in realworld Ecommerce scenarios can enrich the papers relevance and impact.  Addressing potential limitations and challenges faced during the experimentation phase as well as discussing future directions for research based on the studys findings would strengthen the papers overall contribution to the field.  Review Summary: The paper introduces an innovative bidirectional candidates generation framework TotalRecall (TR) to handle Recommender Systems (RS) and Potential Users Mining Systems (PUMS) collectively in Ecommerce platforms. With a strong theoretical foundation and empirical validation through extensive experiments the TR framework showcases competitive results and faster convergence rates compared to existing algorithms. While there are strengths in the novelty of the approach theoretical analysis and practical applications the paper could benefit from enhancing clarity in presentation providing more detailed technical explanations and addressing scalability and practical deployment considerations. Overall the paper holds promise in advancing research in Ecommerce systems but requires refinement in certain areas for broader impact and understanding.", "rF5UoZFrsF4": " Summary: The paper introduces VUT a Versatile UI Transformer designed to handle multimodal input for various UI tasks including object detection language generation and command grounding simultaneously. The model is composed of two Transformer architectures ImageStructure and QuestionAnswer models addressing different aspects of UI modeling. Experimental results demonstrate the effectiveness of VUT in achieving multiple tasks with high accuracy while reducing the need for separate models for each task.  Strengths: 1. Innovative Architecture: The incorporation of both ImageStructure and QuestionAnswer models in VUT is a novel and effective approach for handling diverse UI tasks. 2. Comprehensive Experimentation: The paper provides thorough experimental validation on five distinct datasets showcasing the models capability to perform multiple tasks with competitive accuracy. 3. Efficiency  Simplification: By jointly training for multiple tasks VUT reduces the number of models and storage requirements offering a practical and resourcesaving solution. 4. Clear Problem Formulation: The paper clearly defines the five UI tasks considered and explains how each task is formulated in terms of input data and expected outputs.  Weaknesses: 1. Complexity and Interpretability: The detailed architectural design and formulations may be challenging for readers to grasp easily without a deeper understanding of Transformer models and UI modeling. 2. Evaluation Metrics: While the paper reports consistent performance across tasks a discussion on additional metrics or comparisons with stateoftheart models could enhance the evaluation section.  Questions  Feedback: 1. Scalability: How does the performance of VUT scale with the complexity and size of UI interfaces and datasets Have you tested the model on larger and more diverse UI datasets 2. Generalizability: How well does VUT generalize to unseen UI designs or novel UI tasks Have you evaluated its performance on UI tasks beyond the ones considered in this study 3. Model Interpretability: Is there any analysis on the interpretability of VUTs decisions for different tasks How transparent is the model in providing explanations for object detection command grounding and other tasks  Review Summary: The paper introduces a Versatile UI Transformer VUT that efficiently handles multimodal data for diverse UI tasks. The innovative architecture and extensive experiments demonstrate the models capability in achieving multiple tasks with competitive accuracy. While the paper could benefit from improved interpretability analysis and scalability validations the overall contribution and effectiveness of VUT in reducing model complexity and achieving multitask UI modeling are significant. Therefore I recommend accepting this paper with minor revisions to address the weaknesses identified.", "rN9tjzY9UD": " Summary: The paper introduces a novel algorithm GreedyTN to jointly learn the structure and parameters of a Tensor Network (TN) from data. The method employs a greedy approach starting from a rank one tensor and incrementally identifying the most promising tensor network edges. The proposed algorithm adapts to differentiable objective functions and demonstrates superior performance in various tasks such as tensor decomposition completion and model compression compared to existing methods. Experiments showcase the effectiveness of GreedyTN in optimizing TN structures with fewer parameters.  Strong Points: 1. Novel Algorithm: The paper presents a novel adaptive algorithm GreedyTN to learn TN structures efficiently. 2. Comprehensive Experiments: The experiments cover various tasks like tensor decomposition completion and model compression showcasing the effectiveness of the proposed algorithm. 3. Superior Performance: GreedyTN outperforms existing methods in terms of efficiency and accuracy across different tasks. 4. Adaptive Learning: The algorithm can adaptively identify optimal TN structures for differentiable objective functions demonstrating its flexibility.  Weak Points: 1. Limitation in Computational Complexity: The paper acknowledges the computational complexity of the algorithm which might be limiting in some scenarios. 2. Possible Local Optima: There is a mention that the algorithm may converge to locally optimal TN structures suggesting a need for exploration of more efficient optimization techniques.  Questions and Feedback: 1. How does GreedyTN handle scalability issues when dealing with larger datasets or more complex TN structures 2. Can you provide insights into how the proposed weight transfer mechanism impacts the efficiency and performance of the algorithm 3. What computational resources were utilized for the experiments and can the algorithm be efficiently parallelized for faster convergence 4. Could you elaborate on the potential applications of GreedyTN in realworld scenarios beyond the experimental tasks presented in the paper  Review Summary: Overall the paper presents an innovative algorithm GreedyTN aimed at adaptive learning of TN structures efficiently. The comprehensive experiments demonstrate the algorithms superiority over existing methods in various tasks. While the algorithm shows promise in optimizing TN structures addressing scalability and local optima issues will be crucial for its wider applicability. Therefore considering the algorithms computational complexity and potential improvements in optimization strategies are essential for its further advancement. Given the substantial performance gains exhibited in the experiments the proposed GreedyTN algorithm holds significant potential for advancing research in the field of Tensor Networks and machine learning applications.", "hR_SMu8cxCV": " Summary of the Paper: The paper presents an empirical study on the scaling properties of encoderdecoder Transformer models in neural machine translation (NMT). Key findings include the proposal of a scaling formula for crossentropy loss as a function of encoder and decoder sizes the influence of data composition bias on scaling behavior and the relationship between scaling improvements in loss and generation quality. The study covers multiple language pairs and scaling strategies to understand the impact of model size on NMT performance.  Strong Points of the Paper: 1. Original Contribution: The paper fills a gap in the literature by systematically studying the scaling laws for encoderdecoder NMT models providing detailed insights into the impact of model size on performance across different language pairs and scaling approaches. 2. Methodology Clarity: The study is wellstructured outlining the experimental settings model architectures and training details comprehensively. This clarity enhances the reproducibility of the study. 3. Practical Implications: The findings have practical implications for optimizing the allocation of parameters between encoder and decoder as well as understanding the effect of dataset composition bias on model scaling and generation quality.  Weak Points of the Paper: 1. Scope of Analysis: While the study focuses on significant aspects like scaling laws and data composition bias it may benefit from exploring the impact of other factors such as optimization algorithms or attention mechanisms on NMT model scaling and performance. 2. Generalization: The study primarily focuses on largescale models limiting the analysis to small models and their potential scaling behaviors could enrich the study and provide a more holistic perspective. 3. Model Evaluation: The paper could delve deeper into evaluating the performance of NMT models using metrics beyond BLEU scores such as fluency and adequacy measures to provide a more comprehensive analysis of generation quality.  Questions and Feedback to the Authors: 1. Could you elaborate on the potential implications of data composition bias on realworld applications of NMT 2. How do you envision the findings of this study impacting the design and training practices for NMT models in the future 3. Considering the observed divergence in behavior on sourceoriginal test sets what further investigations do you suggest to better understand its implications on NMT model performance  Review Summary: The paper presents a rigorous study on the scaling properties of encoderdecoder Transformer models in NMT shedding light on the intricate relationship between model size data composition bias and generation quality. The detailed analysis along with the proposed scaling laws provides valuable insights into optimizing NMT models for improved performance. However further exploration of the generalization to smaller models consideration of additional evaluation metrics and realworld implications of the findings would enhance the robustness and applicability of the study. Overall the papers contribution to understanding NMT model scaling is commendable and the findings have significant implications for advancing NMT research and practice.", "pFyXqxChZc": " Summary: The paper proposes IntSGD a family of adaptive integer compression operators for distributed Stochastic Gradient Descent that achieve efficient communication without sending floatingpoint numbers. It introduces adaptive scaling factors for integer rounding operations ensuring convergence and computational efficiency. The algorithm is compared with Heuristic IntSGD and other baseline methods in various experiments on image classification and language modeling tasks.  Strengths: 1. Novel Contribution: The paper introduces a novel adaptive integer compression approach for efficient distributed SGD without compromising convergence guarantees. 2. Theoretical Rigor: The paper provides detailed theoretical analyses for the proposed algorithm establishing convergence rates for various problem types (convex nonconvex smooth and nonsmooth). 3. Empirical Evaluation: The extensive experimental evaluation on realworld tasks provides strong evidence of the effectiveness of the proposed IntSGD algorithm. 4. Comparative Analysis: Comparisons with existing methods highlight the superiority of IntSGD in terms of convergence communication efficiency and empirical performance.  Weaknesses: 1. Complexity: The theoretical analysis may be demanding for readers without a strong background in optimization theory or distributed algorithms. 2. Limited Compression Ratio: The compression ratio of 4 may restrict the algorithms applicability in scenarios that necessitate higher compression efficiency. 3. Algorithm Complexity: The adaptiveness of the scaling factors and randomized rounding may introduce additional computational overhead.  Questions to the Authors: 1. How does the proposed IntSGD algorithm handle scenarios with dynamically changing data distributions across devices 2. Have you considered scenarios where communication bottlenecks shift to other aspects of distributed SGD such as allreduce implementations 3. Can you provide more insights into the practical implications of the limited compression ratio of 4 in realworld applications with large model sizes and communication constraints  Feedback to the Authors: 1. Consider expanding on the practical implications and potential limitations of the limited compression ratio in realworld distributed training scenarios. 2. Provide additional insights into how the adaptive scaling factors may impact the convergence behavior in scenarios with nonstationary data distributions. 3. Consider addressing the potential algorithmic overhead introduced by the adaptiveness of the scaling factors and randomized rounding in practical settings and providing guidance on mitigating such overhead.  Review Summary: The paper proposes a novel adaptive integer compression algorithm IntSGD for efficient distributed SGD with theoretical convergence guarantees and promising empirical results. Despite the complexity of the theoretical analysis and the limitation of a compression ratio of 4 the paper contributes significantly to the field of distributed optimization. The detailed theoretical foundation and strong empirical evaluations support the recommendation for acceptance pending minor revisions addressing the outlined questions and feedback.", "tyTH9kOxcvh": "Summary: The paper introduces the Multilabel Box Model (MBM) for multilabel classification which leverages the concept of box embeddings to model relationships between labels in a taxonomy without requiring explicit specification of the taxonomy. The model combines neural networks with probabilistic box embeddings to enhance interpretability and coherence in predictions. The paper presents theoretical foundations empirical evaluations on twelve datasets and comparisons with existing methods. Strengths: 1. Innovative Approach: The integration of probabilistic box embeddings with neural networks to capture label relationships without explicit taxonomic information is a novel and innovative approach. 2. Interpretability: The models ability to provide high interpretability by representing labels as boxes and learning relationships from data enhances its utility in realworld applications. 3. Experimental Evaluation: The extensive empirical evaluations on twelve datasets demonstrate the models ability to improve coherence while maintaining stateoftheart predictive performance. 4. Comparison with Baselines: The comparison with existing models including MVM and MHM provides valuable insights into the strengths of the proposed MBM model. 5. Reproducibility: The availability of code and implementation details on GitHub enhances the reproducibility of the study. Weaknesses: 1. Complexity: The paper introduces several technical concepts related to box embeddings which may be challenging for readers unfamiliar with this domain. 2. Evaluation Metrics: While the paper introduces new metrics like Constraint Violation (CV) and Mean Average Precision post Coherence correction (CMAP) for coherence analysis there may be a need for further clarity on how these metrics correlate with existing metrics like MAP. Questions: 1. Could you provide more insights into the computational complexity of the MBM model compared to other existing models especially in terms of scalability to large label spaces 2. How does the model handle noisy or incomplete taxonomic information and are there any strategies proposed to mitigate the impact of such data imperfections on model performance Feedback to the Author: 1. Consider providing more intuitive explanations for technical concepts to make the paper more accessible to a wider audience. 2. Clarify the significance of the new evaluation metrics introduced in the paper and how they complement existing metrics. Review Summary: Overall the paper presents an innovative approach with the Multilabel Box Model (MBM) for multilabel classification showcasing improvements in coherence interpretability and predictive performance. The experimental evaluations and comparisons with baselines highlight the effectiveness of the proposed model. Addressing the complexity of technical concepts and providing further clarity on evaluation metrics would enhance the readability and impact of the paper. The availability of code for reproducibility is commendable. I recommend accepting the paper for its novel approach and promising results with a suggestion for minor revisions to improve accessibility and clarity.", "vBn2OXZuQCF": " Summary: The paper investigates the effectiveness of contrastive pretraining in improving domain adaptation without the need for largescale unlabeled data. Through controlled experiments the authors compare contrastive pretraining with other unsupervised domain adaptation methods on benchmark vision datasets. They introduce a conceptual model based on connectivity via data augmentations to explain the success of contrastive pretraining for domain adaptation. The study demonstrates that contrastive pretraining achieves competitive or better performance compared to stateoftheart methods in all cases considered emphasizing the importance of domain connectivity for successful adaptation.  Strengths: 1. Original Conceptual Model: The paper introduces a novel conceptual model based on connectivity via data augmentations to explain the underlying mechanism of contrastive pretraining for domain adaptation providing valuable insights. 2. Comprehensive Experimental Evaluation: The study conducts controlled experiments on multiple benchmark vision datasets to analyze the performance of contrastive pretraining in unsupervised domain adaptation highlighting its effectiveness. 3. Clear Methodology: The paper provides a detailed description of the methodology used including datasets models metrics and evaluation methods enhancing the clarity and reproducibility of the study. 4. Strong Empirical Results: The results indicate that contrastive pretraining outperforms or matches stateoftheart domain adaptation methods on various benchmark datasets demonstrating the efficacy of the proposed approach. 5. Ethical Considerations: The paper acknowledges potential ethical considerations regarding data privacy in largescale unsupervised learning displaying ethical awareness in research.  Weaknesses: 1. Evaluation Diversity: While the experiments are comprehensive the study could benefit from evaluating contrastive pretraining on a wider array of datasets and tasks to enhance the generalizability of the findings. 2. Theoretical Validation: The paper could include more theoretical validation or formal proofs to support the proposed conceptual model strengthening the theoretical foundation of the study.  Questions: 1. Generalization Beyond Vision Tasks: How transferable do you think the findings and insights gained from the study on vision datasets are to other domains or modalities  Feedback: The paper presents a wellstructured and insightful analysis of contrastive pretraining for domain adaptation showcasing a novel conceptual model to explain its efficacy. Strengthening theoretical validation including a wider range of evaluation tasks and discussing the potential applicability of the findings beyond vision tasks would further enrich the impact and relevance of the research.  Recommendation: Based on the comprehensive experimental evaluation original conceptual framework and clear presentation of methodology I recommend accepting the paper for publication after addressing the suggested improvements and inquiries provided above.", "huXTh4GF2YD": " Summary of the Paper: The paper proposes a novel background class regularization (BCR) method suitable for openset recognition (OSR) by utilizing distancebased classifiers trained with a classinclusion loss function. The proposed method aims to improve the robustness of OSR classifiers by restricting the feature space of knownclass data while effectively distinguishing unknownclass samples. The study extensively discusses the problem of openset recognition the motivation behind designing the new BCR method the formulation of the loss function and the experimental evaluation of the proposed approach using various datasets and settings.  Strong Points of the Paper: 1. Innovative Approach: The paper introduces a novel BCR method for OSR that utilizes distancebased classifiers with a classwise regularization loss function offering a new perspective in improving openset recognition performance. 2. Comprehensive Background: The paper provides a thorough background on the OSR problem discusses existing approaches and justifies the need for a new BCR method demonstrating a deep understanding of the research area. 3. Detailed Methodology: The methodology and formulation of the proposed classinclusion loss function are welldefined highlighting the operational aspects of the new approach. 4. Extensive Experiments: The experimental evaluation is robust covering multiple datasets and settings providing quantitative results and analysis to demonstrate the effectiveness of the proposed BCR method.  Weak Points of the Paper: 1. Complexity of Explanations: The technical details and mathematical formulations could be daunting for readers with limited mathematical background or expertise making it challenging to grasp the intricacies of the proposed BCR method. 2. Limited Comparison: The comparison with existing stateoftheart methods for OSR may be lacking depth as the paper mainly focuses on showcasing the superiority of the proposed BCR method without extensive comparisons with a wider range of existing techniques.  Questions and Feedback to the Author: 1. Clarity in Explanations: Could the paper benefit from simplifying some technical explanations or providing more intuitive examples to enhance the readability of the content for a broader audience 2. Comparison with StateoftheArt: How does the proposed method compare to the latest advancements in the field of openset recognition Providing a more detailed comparison with existing stateoftheart methods could strengthen the contribution of the paper. 3. Scalability and Practical Implications: How scalable is the proposed method in realworld applications and are there any practical considerations or limitations that should be acknowledged in implementing the proposed BCR framework  Review Summary: The paper presents a novel background class regularization method for openset recognition using distancebased classifiers with a classinclusion loss function showcasing promising results in improving the performance of classifiers in distinguishing unknownclass samples. The comprehensive experimental evaluation and detailed methodology demonstrate the effectiveness of the proposed approach. However improvements in clarity of explanations and further comparative analysis with existing stateoftheart methods could enhance the impact and relevance of the research. Overall the paper contributes significantly to the field of openset recognition and offers a novel perspective on addressing the challenges of unknownclass sample detection.", "zz9hXVhf40": " Summary of the Paper: The paper focuses on offline reinforcement learning specifically in the context of offline modelbased reinforcement learning (MBRL). It addresses the challenge of mitigating policy exploitation in inaccurate parts of the model within MBRL by employing a pessimistic MDP framework with uncertainty penalties. The paper compares various uncertainty heuristics explores interactions with key hyperparameters and presents insights on improving performance in offline RL benchmarks using Bayesian optimization. It also introduces novel protocols to evaluate uncertainty penalties and their performance under different scenarios.  Strengths of the Paper: 1. Comprehensive Analysis: The paper thoroughly examines the relationship between uncertainty penalties key hyperparameters and model performance in MBRL providing a detailed understanding of their interactions. 2. Novel Evaluation Protocols: The introduction of new evaluation protocols to assess uncertainty penalties under different scenarios such as outofdistribution errors and imaginary rollouts adds depth to the comparative analysis. 3. Performance Improvement: By using Bayesian optimization to tune key hyperparameters the paper demonstrates significant performance gains in offline RL benchmarks showing practical relevance and applicability of the proposed methods. 4. Experimental Rigor: The experimental setup is wellstructured covering various environments tasks and comparison with stateoftheart methods enhancing the credibility of the findings.  Weaknesses of the Paper: 1. Theoretical vs. Practical Gap: The paper points out a disconnect between theoretical foundations and practical implementations in existing methods but does not delve deeply into addressing this gap effectively. 2. Limited Discussion on RealWorld Applications: While the study provides insights into offline RL techniques more discussion on the applicability of the findings to realworld scenarios could enhance the practical relevance of the research.  Questions and Feedback to the Author: 1. Question: Could you provide more insights into the implications of your findings on addressing the challenges of applying offline RL techniques in realworld settings considering the modelaction discrepancies and uncertainty penalties identified in your study 2. Feedback: Enhancing the discussion on bridging the gap between theoretical underpinnings and practical implementations in offline RL would be beneficial for readers seeking to apply these techniques in realworld tasks.  Review Summary: The paper offers a comprehensive analysis of uncertainty penalties and key hyperparameters in offline MBRL showcasing substantial performance improvements through Bayesian optimization. While the experimental setup and insights presented are commendable further exploration into the practical implications of the findings and bridging the theoreticalpractical gap would enhance the impact and applicability of the research. Overall the rigorous evaluation and novel protocols introduced in the paper contribute significantly to advancing knowledge in the field of offline reinforcement learning. I recommend acceptance for publication with minor revisions focusing on realworld implications and bridging the theoreticalpractical gap.", "gPvB4pdu_Z": " Summary of the Paper The paper introduces a novel endtoend training framework called compositional training for deep AUC maximization (DAM). Previous studies use a twostage approach for DAM first training the network with a traditional loss function and then finetuning it with an AUC loss. The proposed compositional training combines minimizing a traditional loss function (e.g. crossentropy) with minimizing an AUC loss in a systematic way. The paper also introduces an efficient stochastic optimization algorithm for optimizing the compositional objective. Extensive empirical studies on benchmark and medical image datasets confirm the effectiveness of the proposed method in improving feature representations and AUC scores compared to traditional deep learning approaches.  Strengths of the Paper 1. Innovative Approach: The paper introduces a novel compositional training framework for deep AUC maximization addressing the challenge of endtoend training. 2. Theoretical Grounding: The paper provides a theoretical basis for the proposed method introducing an efficient stochastic algorithm with provable convergence for optimizing the compositional objective. 3. Empirical Validation: Extensive empirical studies on benchmark and medical image datasets demonstrate the effectiveness of the proposed method outperforming traditional approaches such as optimizing CE or AUC alone. 4. Comparative Analysis: The paper compares the proposed compositional training method with baseline methods showcasing the superiority of the proposed approach in improving feature representations and AUC scores.  Weaknesses of the Paper 1. Complexity: The algorithmic complexity of the proposed method may pose challenges in practical implementation and scalability. 2. Limited Dataset Exploration: While the empirical studies show effectiveness the paper could benefit from exploring a wider range of datasets to generalize the findings.  Questions for the Authors 1. Can you elaborate on the computational resources required for implementing the proposed compositional training method compared to existing twostage approaches 2. Have you considered scenarios where the dataset sizes are extremely large and how well does the proposed method scale in such cases  Feedback for the Authors 1. Providing a more detailed discussion on the limitations and potential scalability of the proposed method would enhance the papers completeness. 2. Exploring the impact of hyperparameters such as learning rate and batch size on the performance could further strengthen the empirical validation section.  Review Summary The paper presents a novel compositional training framework for deep AUC maximization addressing existing challenges in endtoend training. With theoretical grounding efficient stochastic optimization algorithms and comprehensive empirical studies the proposed method demonstrates superior performance compared to traditional approaches. However the paper could benefit from further discussions on practical implementation aspects and generalizability. Overall the innovative approach along with the empirical validation supports a positive recommendation for publication suggesting the potential for advancing the field of deep learning optimization.", "iw-ms2znSS2": " Paper Summary: The paper investigates the interplay of policy and value networks in deep neural networkbased bestfirst search algorithms on the Sokoban domain. It explores the implications of using deep learning to address PSPACEhard planning problems and introduces an abstract tree model to explain the occurrence of left heavy tails. The paper provides experimental evidence on the effectiveness of policy networks value networks uncertaintyaware networks and random restart strategies in enhancing search efficiency and solving hard planning instances.  Strong Points: 1. Innovative Approach: The paper presents a novel approach by combining deep neural networks with bestfirst search algorithms to tackle PSPACEhard planning problems like Sokoban. 2. Experimental Rigor: The extensive experiments conducted on a large dataset of Sokoban instances demonstrate the effectiveness of the proposed method in solving challenging planning problems. 3. Theoretical Contribution: The introduction of an abstract tree model to explain left heavy tails and the discussion on the implications of neural networks generalization on scaling performance contribute significantly to the theoretical understanding of search algorithms.  Weak Points: 1. Clarity in Presentation: The complex theoretical discussions and formal frameworks might be challenging for readers unfamiliar with the subject matter to comprehend fully. Simplifying the language and providing more intuitive explanations could enhance readability. 2. Comparative Analysis: While the paper extensively discusses the proposed methods effectiveness a more indepth comparison with existing stateoftheart approaches and benchmarks could provide additional insights into the methods superiority. 3. Practical Implications: While the theoretical findings are valuable more discussion on the practical implications of the results for realworld applications could enhance the papers impact and relevance.  Questions for the Author: 1. Can you elaborate on the practical significance of the left heavy tails observed in the runtime distributions of search algorithms especially in terms of realworld planning applications 2. How does the proposed abstract tree model align with or contribute to existing theories on search algorithms and how can it be generalized to other problem domains beyond Sokoban 3. What are the potential avenues for future research building upon the findings of this study and how do you plan to validate the practical efficacy of the proposed approach in realworld planning scenarios  Feedback: The paper presents a comprehensive exploration into the application of deep neural networks in conjunction with bestfirst search algorithms for addressing PSPACEhard planning problems. Strengthening the clarity of presentation further elaborating on practical implications and providing a more detailed comparative analysis would enhance the papers overall impact and value to the research community. The theoretical contributions and experimental findings lay a solid foundation for further research directions in the intersection of deep learning and combinatorial search algorithms.", "rFbR4Fv-D6-": "Abstract: This paper proposes a framework called AUTOSSL for automated selfsupervised task search in graph representation learning. The paper addresses the challenge of effectively leveraging multiple selfsupervised pretext tasks without direct access to ground truth labels. By making use of the concept of homophily in realworld graphs the framework aims to maximize the mutual information between pseudolabels and downstream labels. The proposed framework includes two strategies  AUTOSSLES utilizing the covariance matrix adaptation evolution strategy (CMAES) and AUTOSSLDS employing differentiable search via metagradient descent. Experimental results on 8 realworld datasets demonstrate the effectiveness of AUTOSSL in boosting performance on downstream tasks such as node clustering and node classification compared to training under individual tasks. Strengths: 1. Novel Approach: The paper introduces a novel framework AUTOSSL to automatically search over combinations of selfsupervised tasks for graph representation learning. 2. Theoretical Foundation: The paper provides theoretical understanding of the concept of homophily and pseudohomophily in selfsupervised learning. 3. Empirical Evaluation: Extensive experiments on 8 realworld datasets demonstrate significant improvements in performance on downstream tasks achieved by AUTOSSL compared to individual selfsupervised tasks. 4. Detailed Explanations: The paper provides detailed explanations of the AUTOSSL framework the search algorithms employed and the experimental settings and results. Weaknesses: 1. Complexity: The paper involves complex concepts and formulations which may be challenging for readers without a strong background in graph neural networks and selfsupervised learning. 2. Lack of Comparative Analysis: While the paper extensively evaluates the proposed AUTOSSL framework a direct comparison with existing stateoftheart methods would provide further insight into its effectiveness. Questions and Feedback to the Author: 1. Question: How does the proposed AUTOSSL framework compare with other automated selfsupervised learning frameworks in terms of computational efficiency and scalability 2. Clarification: Can you provide more insights into the tradeoffs between AUTOSSLES and AUTOSSLDS in terms of task weight optimization and convergence 3. Feedback: Including a comparison with existing stateoftheart methods would strengthen the contribution and impact of the proposed framework. Review Summary: The paper presents a novel and theoretically grounded framework AUTOSSL for automated selfsupervised task search in graph representation learning. The extensive experimental evaluation on realworld datasets demonstrates the effectiveness of the proposed framework in improving performance on downstream tasks. The detailed explanations and comprehensive methodology make a strong case for the significance of AUTOSSL in advancing graph selfsupervised learning. However the complexity of the content and the lack of direct comparisons with existing methods may require further clarification to enhance the papers impact and relevance in the field. Overall based on the strengths in theoretical foundation empirical evaluation and novel approach I recommend acceptance of this paper for publication with minor revisions for clarity and additional comparisons with existing methods.", "oOuPVoT1kA5": " PeerReview:  1) Summary: The paper presents a novel protocol FEVERLESS for secure Vertical Federated Learning (VFL) over distributed labels using XGBoost. The protocol leverages secure aggregation and Global Differential Privacy to prevent private information leakage during training. The paper provides a comprehensive security and efficiency analysis and empirical results demonstrating FEVERLESS is both fast and secure.  2) Strengths and Weaknesses:  Strong Points:  Original Contribution: The paper addresses a fundamental gap in research by proposing a practical VFL protocol for training XGBoost securely over distributed labels.  Security Measures: FEVERLESS utilizes secure aggregation and Global Differential Privacy to ensure label and data privacy even in the case of collusion among clients.  Efficiency: Compared to existing solutions based on AHE and MPC FEVERLESS demonstrates better efficiency in training XGBoost.  Empirical Results: The experimental results showcase the effectiveness of FEVERLESS in maintaining accuracy while ensuring security.  Weak Points:  Accuracy Degradation: The paper notes a small decrease in accuracy when using FEVERLESS especially in scenarios with complex tree structures which might impact the practicality of the solution.  Limited datasets and scenarios: The paper mainly focuses on experimental results with limited datasets and practical application scenarios hence may lack generalizability to other settings.  Lack of Comparative Analysis: While comparing with baseline XGBoost and approaches like LDP and AHE a more detailed comparative analysis in terms of accuracy runtime and security guarantees would enhance the paper.  Question and Feedback to the Author: 1. Scalability: How does the proposed protocol scale with an increasing number of clients or feature dimensions Have you considered scenarios with more diverse or realworld datasets to evaluate scalability 2. Tradeoff: Could you elaborate more on the tradeoff between privacy guarantees and accuracy degradation especially in scenarios with complex tree structures 3. Robustness: How does FEVERLESS handle potential attacks beyond semihonest adversaries such as malicious clients or data poisoning attacks  3) Review Summary: The paper presents an innovative protocol FEVERLESS for secure VFL over distributed labels using XGBoost. The proposed system demonstrates strong security guarantees through secure aggregation and Global Differential Privacy while maintaining efficiency in training. The experimental results highlight the effectiveness of FEVERLESS in balancing security with accuracy and runtime performance. However the paper could benefit from further exploration of scalability tradeoffs between privacy and accuracy and robustness against more complex attack scenarios. Overall the paper makes a significant contribution to the field of privacypreserving VFL but could be strengthened with a more detailed comparative analysis and robustness evaluation.  Recommendation: Based on the originality of the research robust security guarantees and promising experimental results I recommend accepting the paper with minor revisions addressing the scalability tradeoffs and robustness aspects. Resolving these concerns will further enhance the contribution and applicability of the proposed FEVERLESS protocol in practical VFL scenarios.", "inA3szzFE5": " Summary: The paper explores the spatial frequency sensitivity of deep neural networks for computer vision tasks and proposes a novel measure utilizing inputJacobian in the Fourierbasis. The authors investigate the impact of spatial frequency sensitivity on model performance and introduce spatial frequency regularizers to improve generalization on outofdistribution datasets with unfavorable Fourierstatistics shifts. Experimental evaluations on various datasets demonstrate the effectiveness of the proposed method in enhancing model robustness.  Strengths: 1. Innovative Approach: The paper introduces a novel measure of spatial frequency sensitivity and proposes spatial frequency regularizers contributing to the understanding and improvement of model robustness. 2. Rigorous Methodology: The use of inputJacobian in the Fourierbasis for measuring spatial frequency sensitivity and the proposal of differentiable spatial frequency regularizers reflect a sound and principled approach. 3. Comprehensive Experiments: The extensive experimental evaluations on CIFAR10 CIFAR100 SVHN and ImageNet datasets demonstrate the effectiveness of the proposed method in enhancing model robustness under various scenarios such as Fourier filtering Fouriernoise corruptions and corruptions in CIFAR10C. 4. Insightful Analysis: The paper provides detailed insights into the spatial frequency sensitivity of models across datasets architectures and training techniques offering valuable observations on the impact of Fourierstatistics shifts on model performance.  Weaknesses: 1. Complexity of Method: While the proposed method is innovative its complexity might limit its practical adoption and reproducibility as it involves intricate Fourierbased transformations and regularizers. 2. Limited Comparison: The comparison with existing methods like data augmentations and adversarial training could have been more detailed to provide a clearer understanding of the proposed methods superiority. 3. Clarity in Experiments: Some experimental setups and results could be better explained to enhance clarity and facilitate understanding.  Questions to Authors: 1. Could you provide more insights into the computational complexity of the proposed method compared to existing techniques for spatial frequency sensitivity modification 2. How does the proposed spatial frequency regularization method perform in scenarios where the Fourierstatistics shifts are not solely related to spatial frequencies 3. Have you considered the scalability and applicability of the proposed approach to larger datasets and more complex architectures beyond the ones evaluated in this study  Feedback: The paper presents a novel and insightful study on spatial frequency sensitivity in deep neural networks for computer vision. The comprehensive experiments and detailed analysis showcase the potential of the proposed spatial frequency regularization method in enhancing model robustness. To strengthen the paper further clarification on the methods complexity comparisons with existing techniques and potential applicability to broader scenarios would be beneficial. Overall the innovative approach and promising results make this work a valuable contribution to the field of computer vision.  Recommendation: Based on the innovative approach rigorous methodology and comprehensive experimental results presented in the paper I recommend the acceptance of this work for publication with minor revisions to address clarity and provide additional insights into the methods complexity and applicability.", "qhC8mr2LEKq": " Summary The paper introduces a new neural network approach CROSSBEAM for program synthesis focusing on learning to search for programs efficiently. CROSSBEAM is designed to guide a bottomup enumerative search algorithm by selecting how to combine previouslyexplored programs. This model considers the full search context and is trained onpolicy using data extracted from its own search process. The evaluation of CROSSBEAM in string manipulation and logic programming domains demonstrates significant improvements over stateoftheart methods in terms of efficient search and solution success rates.  Strengths  Innovative Approach: CROSSBEAM introduces a novel neural network approach for program synthesis combining bottomup search with neural guidance.  Efficiency: The paper demonstrates that CROSSBEAM explores smaller portions of the program space compared to existing methods showing significant improvements in efficiency.  Completion and Depth: The paper is comprehensive covering the algorithm model architecture training details experiments results and comparisons thoroughly.  Weaknesses  Complexity: The technical depth of the paper may be a barrier for readers unfamiliar with program synthesis or neural network models.  Generalization: While the experiments show impressive results more analysis on the generalization capacity of CROSSBEAM to different domains or more complex programs would strengthen the paper.  Comparative Analysis: The comparison with stateoftheart methods could be expanded to include more diverse benchmarks to provide a more comprehensive evaluation.  Questions for the Author 1. Can you elaborate on how CROSSBEAMs training onpolicy affects the models ability to efficiently navigate the search space compared to offpolicy training methods 2. How does CROSSBEAM handle situations where multiple feasible search directions are possible and how does it prioritize among these alternatives during the search process  Feedback The paper presents a significant advancement in efficient program synthesis using neural guidance for bottomup search. To enhance the impact and completeness of the work providing additional insights or case studies on the broader applicability of CROSSBEAM to diverse program synthesis tasks would be valuable. Additionally expanding the discussion on the scalability and robustness of the model to larger more complex programs could further strengthen the paper. Overall the papers innovative approach and promising results make it a valuable contribution to the field of program synthesis with neural models.", "neqU3HWDgE": "PeerReview Draft: Summary: The paper proposes a novel approach to learning representations using a torus structure for latent representations instead of traditional vectors. The authors argue that this leads to naturally disentangled representations and effectively captures the generative factors. The proposed method TDVAE is compared with existing VAE architectures on various datasets through experiments measuring disentanglement completeness and informativeness. The results demonstrate the superiority of the torus representation in terms of performance metrics. Strengths: 1. Innovative Approach: The use of a torus structure for latent representations is novel and offers a unique perspective on disentanglement learning compared to traditional vectorbased methods. 2. Theoretical Foundation: The paper provides a solid theoretical grounding for the proposed method by drawing parallels to entanglement entropy in quantum physics. 3. Experimental Rigor: The extensive experiments conducted on multiple datasets provide comprehensive evaluation and comparison of the proposed method against existing VAE architectures. 4. Quantitative Metrics: The introduction of metrics like DCscore for combined assessment of disentanglement and completeness offers a holistic evaluation of the proposed approach. 5. Reproducibility: Availability of code for reproducing experiments enhances the transparency and reproducibility of the results. Weaknesses: 1. Scalability Concerns: The limitation of the torus latent representation in scaling to datasets with a large number of generative factors may pose challenges when dealing with complex highdimensional distributions. 2. Generalization: The paper lacks exploration of the generalization capabilities of the proposed method across a wider range of datasets and scenarios beyond those presented. 3. Model Complexity: The complexity of the model due to the exponential increase in dimensionality with the torus topology may raise concerns about computational efficiency and practicality. 4. Comparative Analysis: It would be beneficial to have a more detailed comparative analysis of the proposed method with existing stateoftheart models in the field of unsupervised disentanglement learning. Questions to the Authors: 1. Have you considered potential strategies to address the scalability limitations of the torus latent representation when dealing with datasets containing a large number of generative factors 2. How does the proposed method perform in scenarios where the underlying data distribution is noncompact and may not naturally align with a torus structure 3. Could you elaborate on the practical implications of the model complexity and how it might impact realworld applications and deployment Feedback to the Authors: 1. Expand on the discussion regarding the scalability concerns related to the torus topology and provide insights into potential avenues for overcoming these limitations. 2. Consider conducting experiments on a more diverse set of datasets to evaluate the generalization capabilities of the proposed method and its performance across various data distributions. 3. Address the concerns related to model complexity and explore approaches to streamline the computational demands while maintaining the efficacy of the torus latent representation. Review Summary: The paper presents an innovative approach to unsupervised disentanglement learning using a torus structure for latent representations. The proposed method demonstrates promising results in terms of disentanglement completeness and informativeness compared to existing VAE architectures. However scalability issues model complexity and generalization capabilities need further exploration and clarification. Despite these limitations the paper contributes valuable insights to the field of representation learning. With some refinements and additional experimentation the proposed method could potentially offer significant advancements in unsupervised learning paradigms.", "kG0AtPi6JI1": " Summary: The paper introduces the concept of latent domain learning where models are trained over multiple domains without explicit domain annotations. Traditional multidomain learning techniques rely on domain labels for training but this paper explores learning without such annotations. The authors propose a sparse latent adaptation strategy to dynamically adapt feature representations to different latent domains within the data. Experiments demonstrate that standard models and existing multidomain approaches struggle with latent domain learning emphasizing the need for customized solutions. The proposed method improves performance and can be seamlessly integrated with existing models benefiting related tasks such as empirical fairness problems and longtailed recognition.   Strengths: 1. Novel Concept: The introduction of latent domain learning as a new research problem is innovative and addresses a practical challenge in transfer learning and multidomain tasks. 2. Methodological Innovation: The formulation of the sparse latent adaptation strategy is a novel approach to addressing the lack of domain annotations demonstrating the adaptability and effectiveness of the proposed method. 3. Comprehensive Analysis: The paper provides a rigorous quantitative analysis comparing the proposed method with existing multidomain techniques on various benchmarks demonstrating the effectiveness of the sparse latent adaptation method. 4. Experimental Validation: The experiments conducted on benchmark datasets demonstrate the superiority of the proposed method over traditional multidomain learning techniques providing evidence of the methods efficacy.   Weaknesses: 1. Complexity of Model: While the sparse latent adaptation strategy shows promise the complexity introduced by the method may raise concerns about interpretability and computational efficiency. 2. Limited RealWorld Application: The experiments focus on specific benchmarks and the generalizability of the proposed method to realworld scenarios with diverse data sources needs further exploration. 3. Lack of Ablation Studies: A more indepth analysis through ablation studies to understand the specific contributions of each component of the proposed method could further enhance the understanding of the approach.   Questions: 1. Scalability: How scalable is the proposed method in terms of computational resources and its ability to handle larger datasets with more complex domain structures 2. Interpretability: Can the models decisions be interpreted to understand how latent domains are being learned and utilized in the feature representations   Feedback: The paper presents a significant contribution to the field by introducing latent domain learning and proposing a sparse latent adaptation method to address the challenges of training models over multiple domains without explicit annotations. The experimental results are promising and showcase the superiority of the proposed method over existing techniques. However further discussions on the interpretability scalability and robustness of the method in realworld applications would enhance the papers impact. Overall the paper is wellstructured thoroughly researched and provides valuable insights into advancements in multidomain learning. Considering the strengths of the proposed method and its potential impact in addressing the limitations of current multidomain approaches I recommend accepting this paper with minor revisions to address the noted weaknesses and questions for further clarification.", "h-z_zqT2yJU": " Summary of the Paper: The paper addresses the issue of performance degradation in knowledge distillation where larger teachers result in unexpectedly worse student performance. The authors propose Adaptive Temperature Knowledge Distillation (ATKD) to tackle this problem by quantifying the sharpness gap between the teacher and student outputs. By adjusting temperatures based on sharpness metrics ATKD significantly improves performance on CIFAR100 and ImageNet datasets.  Strong Points: 1. Original Contribution: The paper introduces a novel metric to quantify the sharpness of model outputs providing a new perspective on the knowledge distillation problem. 2. Innovative Solution: ATKD offers a unique approach to dynamically adjust temperatures based on the sharpness gap reducing the burden of manually optimizing temperatures. 3. Comprehensive Experiments: Extensive experiments on CIFAR100 and ImageNet datasets demonstrate the effectiveness of ATKD in alleviating performance degradation and obtaining stateoftheart results.  Weak Points: 1. Complexity: The papers technical depth might limit accessibility to less experienced readers. Authors could simplify certain sections for broader understanding. 2. Validation Approach: While the adaptive temperature concept is intriguing a more detailed analysis of the validation process could enhance the robustness and generalizability of the proposed method.  Questions to the Author: 1. How does ATKD compare to other stateoftheart knowledge distillation methods in terms of computational efficiency 2. Have you considered conducting experiments on datasets beyond CIFAR100 and ImageNet to further validate the effectiveness of ATKD across diverse domains 3. Can you elaborate on any potential realworld applications or scenarios where ATKD could demonstrate significant benefits  Feedback to the Author: 1. Clarifying the practical implementation steps of ATKD especially in realworld applications would enhance the papers impact and applicability. 2. Addressing potential scalability issues and runtime considerations when implementing ATKD in largescale systems could broaden its practical utility.  Review Summary: Overall the paper presents a wellstructured and innovative solution ATKD to address the performance degradation issue in knowledge distillation. The proposed methodology is supported by strong experimental results and offers a promising direction in the field of model compression and optimization. By addressing the sharpness gap and providing a systematic framework for adaptive temperature adjustments ATKD demonstrates substantial improvements in model performance. However additional validation on diverse datasets and further investigation into practical implications could strengthen the paper\u2019s contributions and broaden its applicability in realworld scenarios. Thus based on the significant advancements and positive outcomes demonstrated by ATKD I recommend the paper for publication pending minor revisions to address the mentioned weak points and the feedback provided.", "tFQyjbOz34": " Summary of the Paper: The paper investigates the modularity exhibited by neural networks by proposing two proxies  importance and coherence  to assess the modularity of a partitioning of a networks neurons. The proxies are measured using statistical methods based on singleneuron interpretability techniques. The study applies these proxies to various networks including MLPs CNNs and ImageNet models to analyze the modularity captured by partitions generated through spectral clustering of the networks neurons. The results suggest that graphbased partitioning methods can reveal modularity in deep neural networks.  Strong Points:  The paper addresses an important topic by exploring modularity in neural networks through statistical methods and spectral clustering.  The proposed proxies  importance and coherence  offer a quantitative approach to assess modularity in network partitioning.  Rigorous evaluation of the proxies using statistical methods enhances the reliability of the results presented.  The experiments cover a range of networks from small MLPs to ImageNet models demonstrating the applicability of the methods across different scales.  Weak Points:  The paper could improve clarity in explaining the statistical methods used for measuring modularity proxies to aid readers in understanding the technical details.  While the study provides evidence of modularity in the network partitionings the practical implications for building more modular networks or enhancing interpretability could be further elaborated.  QuestionsFeedback to the Author: 1. Could you provide more insights into how the proposed importance and coherence proxies can be practically utilized for improving the interpretability generalizability or design of neural networks 2. It would be beneficial to elaborate on the limitations of the study in more detail to help readers understand the scope and implications of the proposed methods. 3. Have you considered potential applications of your findings in realworld scenarios such as optimizing network architectures or enhancing model explainability  Review Summary: The paper presents a systematic analysis of modularity in neural networks through the development of proxies importance and coherence and the application of statistical methods to assess these proxies. The experiments conducted on various network architectures demonstrate the ability of graphbased partitioning to capture modularity in deep neural networks. While the study shows promising results in identifying modular subsets of neurons further exploration of the practical implications and limitations of the proposed methods is necessary. Overall the rigorous evaluation and comprehensive experimentation make this work valuable for researchers interested in understanding and improving the modularity of neural networks.", "upnDJ7itech": "Peer Review: 1) Summary of the Paper: The paper introduces Knowledge Infused Decoding (KID) a novel decoding algorithm for generative language models (LMs) that dynamically integrates external knowledge into each step of the LM decoding process. KID aims to address the limitations of current LMs in recalling accurate knowledge given a context resulting in counterfactual or hallucinatory generation in knowledgeintensive natural language generation tasks. By dynamically infusing external knowledge during decoding KID significantly outperforms conventional beam search and sampling decoding methods on diverse knowledgeintensive NLG tasks showcasing improved generation quality reduced exposure bias and stable generation quality for longer sequences. The experimental results demonstrate the effectiveness of KID on tasks such as abstractive question answering logiccentric writing and dialogue generation outperforming stateoftheart models in certain scenarios. 2) Strong and Weak Points Questions and Feedback: Strong Points:  Novelty: KID introduces a unique decoding algorithm that dynamically infuses external knowledge into each step of the LM decoding process improving generation quality.  Performance Improvement: The results indicate that LMs equipped with KID outperform conventional decoding methods on diverse knowledgeintensive NLG tasks demonstrating its effectiveness.  Abstraction and Generality: KID is model and task agnostic showcasing its versatility across different generative LMs and tasks.  Fewshot Performance: KID shows significant improvement in fewshot scenarios highlighting its generalization ability and performance in lowresource settings. Weak Points:  Evaluation Method: Although the human evaluation results are positive for KID more details on the specific criteria used and potential biases in the evaluation process would enhance the papers credibility.  Knowledge Retrieval: Further exploration of the impact of different knowledge retrieval methods and their effects on KIDs performance would provide additional insights.  Scalability: The paper could discuss the scalability of KID when applied to larger and more complex tasks especially with respect to computational efficiency and resource requirements. Questions:  How does KID handle cases where the retrieved knowledge may be conflicting or ambiguous especially in scenarios with multiple valid interpretations  Could the authors elaborate on the potential challenges or limitations related to implementing KID in realworld applications especially considering practical constraints and computational expenses  How does KID adapt to new domains or unseen entities during generation and what mechanisms are in place to ensure the relevance and accuracy of the infused knowledge Feedback to the Author:  Provide more details on the human evaluation methodology including specific instructions given to participants the criteria used for evaluation and potential sources of bias.  Consider discussing the potential applications of KID beyond the tasks evaluated in the paper and exploring its scalability to larger and more complex knowledgeintensive NLG tasks.  Address the limitations of KID such as handling conflicting knowledge sources and ensuring adaptability to new domains to provide a more comprehensive perspective on its feasibility and robustness. 3) Review Summary: Overall the paper presents a novel decoding algorithm KID which effectively integrates external knowledge into generative language models to enhance generation quality in knowledgeintensive NLG tasks. The results demonstrate significant performance improvements over conventional decoding methods especially in fewshot scenarios. While the paper showcases the strengths of KID in improving generation quality and reducing exposure bias further exploration of scalability knowledge retrieval strategies and realworld applications would add depth to the analysis. Considering the novelty performance gains and potential for generalization I recommend acceptance after addressing the suggested points for improvement.", "t98k9ePQQpn": " PeerReview of the Paper on LongTailed Recognition and PostHoc Correction  Summary: The paper introduces an alternative approach to posthoc correction in the context of longtailed recognition problems. By utilizing optimal transport (OT) theory the authors propose a method that automates the learning of the cost matrix through linear mapping thereby surpassing previous statistical methods. The paper provides a comprehensive overview of the problem outlines the methodology and presents experimental results showing improved performance on three benchmark datasets.  Strong Points: 1. Innovative Approach: The paper presents a novel method for posthoc correction from an optimal transport perspective demonstrating originality in problemsolving. 2. Detailed Explanation: The paper thoroughly explains the problem formulation methodology and the optimization algorithms used in the proposed approach. 3. Experimental Validation: The authors provide empirical evidence demonstrating the effectiveness of their approach outperforming existing methods on benchmark datasets. 4. Comparison and Analysis: The comparison with stateoftheart methods and detailed analysis of computational costs convergence behavior and distribution matching showcase a thorough evaluation of the proposed technique.  Weak Points: 1. Complexity: While the paper delves into intricate mathematical formulations and algorithms the complexity might be a barrier for readers not wellversed in optimal transport theory. 2. Assumptions and Limitations: The assumption of a known marginal distribution could limit the practical applicability of the proposed method as realworld datasets may not always have uniform distributions. 3. Practical Implementation: The paper lacks practical implementation details of the proposed method such as code snippets or guidelines for replicating the results.  Questions and Feedback to the Author: 1. Scalability: How does the proposed method scale with larger datasets or more complex models Have you tested the performance with varying dataset sizes 2. RealWorld Applicability: How feasible is it to apply the proposed approach in realworld scenarios with unknown marginal distributions or dynamic data changes 3. Comparison with Traditional Methods: Did you compare the performance of the proposed OTbased approach with traditional posthoc correction methods in terms of computational efficiency and ease of implementation  Review Summary: The paper presents a novel posthoc correction approach for longtailed recognition problems based on optimal transport theory. Despite its complexity the method shows promising results in terms of performance improvement over existing methods. The detailed experimental evaluation and thorough analysis strengthen the credibility of the proposed technique. However addressing scalability realworld applicability and comparisons with traditional methods would enhance the practical value of the proposed approach. Overall based on the innovative methodology and positive experimental results I recommend the paper for further consideration with minor revisions to address the weak points highlighted.", "roxWnqcguNq": " Review of \"Incorporating Constituency Tree Representation for Argument Discourse Unit Prediction: A Graph Neural Network Approach\"  1) Summary: The paper investigates the benefits of incorporating constituency tree representation of sentences for argument discourse unit (ADU) prediction. The study compares different maximum depths for constituency trees and combines this structure with graph neural networks (GNNs) and Conditional Random Field (CRF) models. The research improves existing models for argument unit recognition at the token level and uses interpretability methods to evaluate the model architecture. Experimental results indicate the effectiveness of the proposed approach in enhancing ADU recognition.  2) Strengths and Weaknesses:  Strong Points: 1. Novel Approach: Introducing constituency tree representation for ADU prediction is innovative and adds value to the field. 2. Thorough Evaluation: The paper conducts a detailed comparison of different model configurations and provides interpretable results. 3. Model Interpretability: Utilizing interpretability methods enhances understanding of model predictions. 4. Performance Improvement: Demonstrating improved results in tokenlevel ADU recognition showcases the efficacy of the proposed approach.  Weak Points: 1. Model Complexity: The GNNCRF model configuration consists of many hyperparameters which might make it less accessible for adoption by practitioners. 2. Interpretability Clarity: While the interpretability analysis is insightful providing more details on how insights from interpretability were utilized for model enhancement could add clarity.  Question and Feedback for the Author: 1. Model Complexity: Could you discuss strategies to simplify the model architecture without compromising performance considering the high number of hyperparameters in the GNNCRF model 2. Further Analysis: Have you considered conducting additional experiments to understand the impact of different hyperparameters on model performance comprehensively 3. Realworld Applicability: How do you envision the practical implementation of the proposed approach in realworld scenarios and what challenges do you foresee  3) Review Summary: The paper presents a wellstructured and valuable contribution to the field of ADU prediction through the incorporation of constituency tree representation and GNNCRF models. The thorough evaluation and use of interpretability methods enhance the credibility of the findings. The demonstrated performance improvement in tokenlevel ADU recognition supports the efficacy of the proposed approach. However addressing model complexity and providing more insights into practical applications could further strengthen the paper.  Recommendation: Based on the innovative approach thorough evaluation and performance improvements demonstrated in the study I recommend accepting this paper for publication pending minor revisions to address the model complexity and provide more insights into practical applications.", "pbduKpYzn9j": " Summary: The paper presents a novel approach for distilling unconditional generative adversarial networks (GANs) specifically focusing on the popular StyleGAN2 architecture. The main issue addressed is the output discrepancy between the teacher and student models which poses a challenge in distilling unconditional GANs. The paper identifies that the style module plays a crucial role in ensuring output consistency and proposes an effective initialization strategy for the student model. A latentdirectionbased distillation loss is introduced to enhance semantic consistency between the teacher and student models. Extensive experiments demonstrate the effectiveness of the proposed approach outperforming existing methods significantly.  Strengths: 1. Innovative Approach: The paper tackles an important problem in GAN distillation uncovering the output discrepancy issue and proposing novel solutions to address it. 2. Thorough Analysis: The paper provides indepth analysis and insights into the challenges of distilling unconditional GANs particularly focusing on the role of the style module. 3. Experimental Validation: Extensive experiments are conducted to validate the proposed approach demonstrating superior performance compared to existing methods. 4. Clear Explanations: The paper offers clear explanations of the methodology enabling readers to follow the proposed techniques and their implications effectively.  Weaknesses: 1. Consolidate Terminology: While the paper provides a detailed explanation there are instances where the terminology could be consolidated for better readability and clarity. 2. Comparison with Baselines: The paper could benefit from a more detailed comparison with existing baselines to establish a clearer benchmark for the proposed methods performance.  Questions and Feedback: 1. How does the proposed latentdirectionbased distillation loss compare to other distillation techniques in terms of computational complexity and ease of implementation 2. Could you further clarify how the proposed method addresses scalability issues when training largescale GAN models like StyleGAN2  Review Summary: The paper presents a wellstructured and innovative approach to distilling unconditional GANs focusing on the challenging output discrepancy issue. The thorough analysis experimental validation and clear explanations of the proposed methodology contribute to the significance of the research. While the paper could benefit from refining the terminology and expanding the comparison with existing baselines the overall quality and contribution of the work suggest that it is wellsuited for publication. Therefore I recommend acceptance of the paper with minor revisions to address the identified points.", "n54Drs00M1": " Summary: The paper introduces a novel framework BERTNN which leverages Bidirectional Encoder Representations from Transformers (BERT) to generate extended affective lexicons. The model is trained on synthetic data derived from an affective dictionary and finetuned using a neural network and regression model. The results indicate that BERTNN outperforms existing methods in estimating affective meanings across various metrics.  Strong Points: 1. Innovative Approach: The paper presents a unique approach of utilizing BERT models to enhance affective lexicons addressing the limitations of traditional surveybased methods. 2. StateoftheArt Performance: The BERTNN framework achieves stateoftheart accuracy in estimating affective lexicons as demonstrated by comparison with existing methods. 3. Comprehensive Methodology: The paper details the entire process from data preprocessing to model training and evaluation providing a clear understanding of the methodology. 4. Thorough Evaluation: The comparison with other methods correlation analysis and discussion on differentiation between different meanings of words showcase the thoroughness of the evaluation.  Weak Points: 1. Limited Discussion on Dataset: While the paper mentions the use of synthetic data generated from an affective dictionary more details on the dataset its composition and potential biases could strengthen the study. 2. Lack of RealWorld Application: The paper focuses on methodology and performance metrics but it could benefit from discussing practical applications and implications of the developed framework. 3. Complexity of Model: The complexity of the BERTNN framework and neural network architecture might be a barrier for practitioners looking to implement the approach in realworld scenarios.  Questions for the Authors: 1. Dataset Origin: Can you elaborate on the characteristics of the affective dictionary used to generate synthetic data How representative is this dataset of realworld affective meanings 2. Practical Implementation: How feasible is the implementation of the BERTNN framework in realworld contexts considering the computational complexity and specialized knowledge required 3. Future Directions: What are the potential avenues for future research or extensions of the BERTNN framework Are there plans to test the model on diverse datasets or in different applications  Feedback: The paper demonstrates an innovative approach to enhancing affective lexicons using BERT models showcasing stateoftheart performance. Strengthening the discussion on the dataset practical implementation considerations and potential future directions would enhance the papers impact. Additionally clarifying the methodology and addressing the weak points highlighted could further strengthen the studys credibility and potential for realworld applications. Overall the paper presents a valuable contribution to sentiment analysis research.", "uVTp9Z-IUOC": " Summary: The paper proposes a novel method for fully testtime adaptation of deep neural networks focusing on improving performance on shifted data distributions specifically data affected by corruptions. The proposed method includes a novel loss function diversity regularizer and a parametrized input transformation module. The key contributions include nonsaturating losses that address premature convergence and the instability of entropy minimization a diversity regularizer to prevent convergence to trivial solutions and an input transformation module to partially undo testtime distribution shifts. The method is demonstrated to outperform previous approaches in improving model robustness to common corruptions on challenging benchmarks such as ImageNetC and ImageNetR.  Strong Points: 1. Innovative Approach: The paper introduces a novel method for fully testtime adaptation that addresses challenges such as premature convergence and instability of entropy minimization. 2. Comprehensive Evaluation: The paper thoroughly evaluates the proposed method on various benchmarks demonstrating significant improvements in model robustness to corruptions and domain adaptation. 3. Clear Explanations: The paper provides detailed explanations of the proposed losses regularizers and input transformation module enhancing the understanding of the methodology.  Weak Points: 1. Complexity: The papers content is highly technical and quite dense which may make it challenging for nonexperts in the field to grasp all the intricacies of the proposed method. 2. Comparative Analysis: While the paper compares the proposed method with previous works a more indepth comparative analysis with stateoftheart approaches could further strengthen the evaluation. 3. Visualization: More visual aids or figures could aid in understanding the concepts and methodologies better.  Questions: 1. How does the proposed method compare to stateoftheart approaches in terms of computational efficiency and scalability 2. Could the impact of hyperparameter tuning be elaborated further especially in terms of the robustness and generalization of the model 3. Are there any limitations or failure cases observed during the evaluation of the proposed method that could be discussed  Feedback to the Author: 1. Consider simplifying the content for better accessibility especially for readers less familiar with the technical details of deep learning. 2. Enhance the comparative analysis by including a more comprehensive comparison with relevant stateoftheart methods. 3. Adding more visualizations or diagrams could improve the clarity and understanding of the proposed methodology. In conclusion the paper presents a novel and promising method for fully testtime adaptation of deep neural networks demonstrating significant improvements in model robustness to data shifts. With some enhancements in presentation and comparative analysis this work has the potential to make a valuable contribution to the field of deep learning research.", "o8iGesI9HN-": " Peer Review  1) Summary of the Paper: The paper proposes a new convolutional operator called optimized separable convolution to address the increasing complexity and demand for efficient deep learning models. The research focuses on reducing model size by improving the efficiency of convolution operations through principled design. The proposed optimized separable convolution achieves a complexity of O(C32K) and can be further optimized to achieve a lower complexity of O(Clog(CK2)). Experimental results demonstrate the effectiveness of this approach in achieving improved accuracyParams tradeoffs over conventional depthwise and depthspatial separable convolutions across various deep learning architectures like ResNet DARTS and MobileNet.  2) Strengths and Weaknesses: Strong Points:  The paper addresses an important issue in deep learning research and proposes a novel and principled convolution operator to reduce model size.  The research is wellstructured with clear explanations of the proposed optimized separable convolution and its benefits over existing convolution types.  Experimental results demonstrate the efficacy of the proposed approach in improving accuracy while reducing the number of parameters.  The comparison with existing convolution types and architectures like ResNet DARTS and MobileNet provides a comprehensive evaluation of the proposed approach. Weak Points:  The paper could benefit from more detailed explanations in certain sections such as the rationale behind specific choices in the optimization process.  While the experimental results are promising more analysis on scalability and generalizability to other architectures and datasets might strengthen the papers impact.  Providing a more comprehensive discussion on potential limitations or challenges of implementing the proposed optimized separable convolution could add depth to the research.  Questions and Feedback to the Author: 1. Could you elaborate more on the rationale behind setting certain hyperparameters such as the internal channel size and kernel sizes in the optimization process for the separable convolution design 2. How does the proposed optimized separable convolution perform in scenarios with larger datasets or more complex architectures beyond the ones tested in this research 3. Have you considered potential limitations or challenges in implementing the proposed convolution operator especially in realworld applications or with different optimization objectives  3) Review Summary: The paper presents a novel approach the optimized separable convolution to address the escalating complexity of deep learning models and reduce the number of parameters required for convolution operations. The research is wellstructured provides clear explanations of the proposed operator and demonstrates promising experimental results across multiple architectures. The comparisons with existing convolution types and architectures highlight the efficiency of the proposed approach. While the paper showcases significant potential further exploration on scalability generalizability and potential limitations could enhance the depth and impact of the research. Overall I recommend accepting this paper with minor revisions to address the weak points highlighted above and provide more comprehensive insights into the implications and limitations of the proposed optimized separable convolution.", "mQxt8l7JL04": "1) Summary: The paper introduces a regularized autoencoder approach for representation learning that focuses on preserving the geometry of the data manifold by learning a set of geometrypreserving latent space coordinates. The authors propose a hierarchy of geometrypreserving mappings (isometry conformal mapping of degree k areapreserving mappings) and define a family of coordinateinvariant relaxed distortion measures to evaluate the proximity of the mapping to a scaled isometry. They formulate isometric regularization for the autoencoder and introduce a postprocessing step for further flattening the latent space towards a more isometric representation. Experimental results on diverse datasets and an unsupervised human face retrieval task show that the proposed method outperforms existing approaches. 2) Strengths: Strong Points:  The paper addresses an important problem in representation learning focusing on preserving the geometry of data manifolds.  The introduction of a hierarchy of geometrypreserving mappings and a family of coordinateinvariant distortion measures provides a solid theoretical foundation.  The proposed regularized autoencoder method shows significant improvements in data representation quality without sacrificing reconstruction accuracy.  The experimental results especially in the human face retrieval task demonstrate the effectiveness of the proposed method compared to existing approaches. Weak Points: Weak Points:  One weakness is that the paper can be overly technical and the complexity of the method might be challenging for some readers to grasp without a strong background in mathematical concepts.  Clarity in explaining the practical implementation of the algorithms and how to apply them to realworld datasets could be improved for easier understanding by a broader audience.  While the experimental results are promising additional insights into the computational efficiency scalability and robustness of the proposed method would further enhance the papers contributions. Questions and Feedback to the Authors: 1. Could you provide more insights into the computational complexity of the proposed method and discuss its scalability in handling largescale datasets 2. It might be beneficial to include discussions on potential applications beyond image data and how the proposed approach can be generalized to other types of highdimensional datasets. 3. Providing a more detailed analysis of the limitations or potential challenges of the method in realworld applications would strengthen the practical implications of the research. 3) Review Summary: The paper presents a novel regularized autoencoder framework for learning geometrypreserving data representations showcasing improvements in data manifold learning. The theoretical foundation including the hierarchy of mappings and the family of distortion measures reinforces the methodologys validity. The experimental results especially in the context of unsupervised human face retrieval support the papers claims of superior performance compared to existing methods. However the technical complexity and the need for further clarity in practical implementation and realworld applicability should be addressed. Overall the paper makes significant contributions to representation learning but would benefit from additional discussions on scalability generalizability and practical implications. Recommendation: Based on the strong theoretical foundation and promising experimental results I recommend this paper for publication contingent upon addressing the clarification of the implementation details and discussing the methods practical implications in more depth.", "tm9-r3-O2lt": " Peer Review of the Paper  1. Summary of the Paper: The paper proposes a novel method based on Generative Adversarial Networks (GANs) to modify and control the memorability of face images. The authors use StyleGANs and an extended latent space to separate face images based on their memorability scores and they modify the memorability of images by moving in the direction of the separating hyperplane normal vector. The proposed method is evaluated on both real and synthetic face images to demonstrate its effectiveness in controlling face memorability while maintaining image quality and realism.  2. Strong Points:  Innovative Approach: The use of GANs and latent space manipulation to modify face memorability is novel and can have significant practical applications.  Clear Methodology: The paper provides a detailed explanation of the proposed method including dataset creation preprocessing steps memorability score prediction and memorability modification.  Experimental Evaluation: The paper includes thorough experimentation on both synthetic and real face images to validate the effectiveness of the proposed method.  Realness Evaluation: Assessing the realness of the modified faces using FID and KID scores demonstrates the methods ability to maintain image quality.  Weak Points:  Theoretical Basis: The paper lacks a detailed discussion on the theoretical underpinnings of memorability in images which could strengthen the conceptual framework.  Comparison with Existing Methods: More detailed comparisons with existing approaches for image memorability modification could highlight the advantages of the proposed method.  Limitations and Future Directions: Further discussion on the limitations of the proposed method and potential future research directions would enhance the papers impact and completeness.  Questions and Feedback to the Authors: 1. Could you elaborate on the specific face attributes that were found to contribute significantly to image memorability in your analysis 2. How does the proposed method compare in terms of computational efficiency and scalability with existing approaches for altering image memorability 3. Have you considered the ethical implications of modifying face memorability in terms of potential misuse or unintended consequences  3. Review Summary: Overall the paper presents an innovative approach to modifying face memorability using GANs and latent space manipulation. The methods effectiveness in controlling image memorability while preserving realism is demonstrated through comprehensive experimentation on both synthetic and real face images. Despite some areas for improvement in discussing theoretical foundations and comparing with existing methods the papers novelty and practical applications make it a valuable contribution to the field. Therefore I recommend accepting this paper with minor revisions to enhance clarity and completeness.  This review provides a detailed evaluation of the papers strengths weaknesses and areas for improvement aimed at providing constructive feedback to the authors and guiding the decisionmaking process for acceptance or revision.", "y7tKDxxTo8T": " Peer Review  1. Summary: The paper introduces ZEroShot Recommenders (ZESREC) to tackle the issue of data scarcity in earlystage products by enabling generalization from an old dataset to a new dataset in recommender systems. ZESREC uses item specific features such as naturallanguage descriptions images and videos as continuous indices to generalize to unseen items and user interactions with items to represent users for generalization to unseen users. The model is evaluated on realworld datasets and demonstrates successful recommendations in a zeroshot setting.  2. Strengths and Weaknesses:  Strengths:  Innovative Approach: Introduces a novel solution to the chickenandegg problem in recommender systems.  Robust Methodology: Comprehensive explanation of the zeroshot learning method proposed.  Experimental Validation: Conducted experiments on realworld datasets to showcase the effectiveness of ZESREC.  Comparative Analysis: Provides a comparison with indomain and zeroshot baselines.  Case Studies: Case studies provide insightful examples of how ZESREC learns and performs in different scenarios.  Weaknesses:  Overwhelming Technical Detail: Some parts of the paper might be too technical for a general audience and could benefit from simplification.  Limited Evaluation on Datasets: While the experimental setup is thorough additional datasets and scenarios could further validate the proposed methodology.  Validation on More Metrics: Incorporating additional evaluation metrics could provide more comprehensive insights into the models performance.  Question and Feedback to the Author: 1. Scalability: How does the proposed ZESREC scale with larger datasets and more diverse domains 2. Generalizability: Have you considered how ZESREC would perform with datasets having a different distribution of useritem interactions 3. Interpretability: Can you provide insights into the interpretability of the latent item and user embeddings generated by ZESREC for domainspecific insights  3. Review Summary: The paper presents a novel approach to tackle the data scarcity challenge in recommender systems using zeroshot learning with ZESREC. The method is wellexplained with a robust methodology and experimental validation on realworld datasets. Comparison with baselines case studies and a detailed explanation of the models components are strong points. However there are opportunities to simplify certain technical aspects for broader understanding and include more evaluation metrics for a comprehensive assessment. Overall based on the innovative approach thorough methodology and promising experimental results I recommend acceptance of the paper with minor revisions addressing the technical complexity and additional experiment validations.", "hUr6K4D9f7P": " Summary: The paper introduces Weighted Truncated Adversarial Weight Perturbation (WTAWP) a new method to improve the robustness and natural generalization of graph neural networks. It addresses the vanishinggradient issue inherent in existing weight perturbation techniques when applied to Graph Neural Networks (GNNs). By restricting the perturbation on selected layers and weighting the perturbation loss WTAWP is shown to consistently enhance the generalization performance of GNNs across various graph learning tasks.  Strengths: 1. Innovative Methodology: The introduction of WTAWP to address the vanishinggradient issue in GNNs is a novel and important contribution to the field. 2. Comprehensive Experimental Evaluation: The paper conducts thorough experiments on various datasets and tasks to demonstrate the effectiveness of WTAWP in improving the performance of GNNs. 3. Strong Empirical Results: The results show consistent and statistically significant improvements in both natural and robust generalization when using WTAWP compared to baseline models. 4. Theoretical Analysis: The paper provides a theoretical analysis of AWP on noni.i.d. graph data offering insights into the connections between flatness and generalization in GNNs.  Weaknesses: 1. Clarity and Structure: The papers structure is dense and may be challenging for readers to follow. Improved clarity and organization of concepts could enhance reader understanding. 2. Detailed Methodology Explanation: Some details in the methodology such as the rationale behind selecting specific hyperparameters could be further elaborated for better comprehension. 3. Further Comparison: While the paper extensively evaluates WTAWP against baseline methods additional comparison with stateoftheart techniques could provide further context and depth to the evaluation.  Questions to Authors: 1. How did you determine the optimal hyperparameters (\u03bb \\xcf\\x81) for WTAWP on different datasets and GNN architectures 2. Can you provide insights on the scalability of WTAWP to larger graph datasets and more complex GNN structures 3. Have you considered investigating the computational overhead of implementing WTAWP in comparison to traditional weight perturbation methods  Feedback: The paper presents a valuable contribution to the field of GNNs with the introduction of WTAWP. Strengthening the papers clarity expanding the discussion on hyperparameter selection and considering comparisons with stateoftheart methods can further enhance its impact in the research community. Overall the experimental results and theoretical analysis support the validity and significance of WTAWP in improving the generalization properties of GNNs.  Recommendation: Based on the innovative methodology comprehensive experimental evaluation and strong empirical results presented in the paper I recommend the acceptance of this manuscript for publication pending minor revisions to enhance clarity and address the points raised above.", "ghTlLwlBS-": " Summary of the Paper The paper introduces a Feudal Reinforcement Learning (FRL) model to address the challenging task of reading to act where agents must comprehend highlevel natural language instructions and translate them into lowlevel actions. The FRL model comprises a manager agent which generates multihop plans based on textual instructions and a worker agent which executes the subgoals proposed by the manager in a visual environment. The model aims to bridge the semantic mismatch between linguistic instructions and lowlevel actions achieving competitive performance on tasks such as Read to Fight Monsters (RTFM) and Messenger without requiring humandesigned curriculum learning.  Strengths of the Paper 1. Novel Approach: The FRL models hierarchical framework of managerworker agents presents a novel solution to the semantic mismatch challenge between textual instructions and lowlevel actions. 2. Generalization: The paper demonstrates the models effectiveness across various forms of environments instructions and manuals showcasing its potential for realworld applications. 3. Performance: The experimental results highlight the models competitive performance on challenging tasks without the need for humandesigned curriculum learning. 4. Comprehensive Methodology: The detailed description of the FRL formulation task settings worker and manager architecture and training strategy provides a thorough understanding of the proposed approach.  Weaknesses of the Paper 1. Complexity: The papers technical depth and detailed methodology may pose challenges for readers unfamiliar with reinforcement learning and natural language processing concepts. 2. Validation: While the model demonstrates impressive performance on benchmarks further validation on realworld scenarios or more diverse environments could strengthen the papers conclusions. 3. Evaluation Limitation: The papers focus on specific benchmarks may limit the generalizability of the findings to diverse applications or environments.  Questions for the Authors 1. How does the models performance vary when applied to different types of environments or tasks beyond RTFM and Messenger 2. Are there specific realworld applications or scenarios for which the proposed FRL model is particularly wellsuited 3. How does the model handle noisy or ambiguous textual instructions and what potential challenges or limitations might arise in such cases 4. Have you considered incorporating realtime feedback mechanisms to address potential error propagation issues mentioned in the papers discussion section  Feedback for the Authors 1. Consider discussing potential future directions for research such as exploring adaptive learning mechanisms or integrating external knowledge sources to enhance the models capabilities. 2. Provide additional insights on how the proposed FRL framework could be extended to address more complex tasks or scenarios beyond the current benchmarks. 3. Enhance the discussion on the models scalability and applicability to broader domains to strengthen the papers potential impact and practical relevance. Overall the paper presents a novel and effective solution to the readingtoact challenge showcasing strong performance on benchmark tasks. Further validation exploration of realworld applications and addressing potential limitations would enhance the papers impact in the field of reinforcement learning and natural language processing.  Recommendation Given the innovative approach competitive performance and comprehensive methodology presented in the paper I recommend acceptance after addressing the questions feedback and suggestions provided above.", "wk5-XVtitD": "1) Summary of the Paper: The paper investigates the effectiveness of language model (LM) pretraining in scaffolding learning and generalization in autonomous decisionmaking tasks. Using a pretrained GPT2 LM the authors initialize policy learning in a simulated household environment featuring partial observability large action spaces and long time horizons. The study aims to explore if LM pretraining capabilities initially designed for language processing tasks can be extended to general machine learning problems. They present experiments in VirtualHome environment to demonstrate the impact of LM pretraining on policy learning generalization. 2) Strong and Weak Points Questions and Feedback: Strong Points:  The paper addresses an intriguing question about leveraging LM pretraining for general machine learning problems beyond language tasks.  Rigorous experiments are conducted with clear methodology and detailed results showcasing the impact of LM pretraining on policy learning generalization.  The study demonstrates that language modeling induces representations useful for modeling natural goals and plans aiding learning and generalization even outside of language processing tasks.  The experiments are wellstructured exploring different approaches to input encodings and providing valuable insights into the role of languagebased encodings in LM pretraining benefits. Weak Points:  The paper could benefit from a more indepth discussion on the implications of the findings for realworld applications beyond the simulated environment.  While the experiments are detailed additional analysis on the computational efficiency or scalability of using LM pretraining could strengthen the practical relevance of the study. Questions and Feedback to the Author:  Could you discuss potential practical applications or industries where the findings of leveraging LM pretraining for policy learning could be most impactful  It would be insightful to explore the computational efficiency and scalability aspects of LM pretraining for realworld autonomous systems deployment.  Further elaboration on the scalability and transferability of the proposed approach to diverse interactive tasks and environments would enhance the implications of this research. 3) Review Summary: The paper presents a comprehensive study on the effectiveness of LM pretraining for policy learning in autonomous decisionmaking tasks. Through detailed experiments in a simulated household environment the authors demonstrate that LM pretraining significantly improves generalization in interactive tasks. The discussion on languagebased encodings and their impact on LM pretraining benefits adds depth to the study. While the paper offers valuable insights into the potential of LM pretraining beyond language tasks further exploration of practical applications computational efficiency and scalability aspects could enhance the significance and applicability of the research findings. Overall Recommendation: The paper is wellstructured and provides valuable insights into the role of LM pretraining in autonomous decisionmaking tasks. With some additional discussions on practical implications and scalability considerations it can be recommended for publication.", "oMI9PjOb9Jl": "1) Summary of the Paper: The paper introduces a novel query formulation using dynamic anchor boxes for DETR (DEtection TRansformer) to enhance the performance of object detection models. It directly utilizes box coordinates as queries in Transformer decoders dynamically updating them layer by layer. By incorporating explicit positional priors based on anchor box coordinates the proposed method aims to improve querytofeature similarity address slow training convergence issues and modulate positional attention maps using box width and height information. Extensive experiments demonstrate that the approach achieves superior performance on the MSCOCO benchmark compared to other DETRlike models. 2) Strengths and Weaknesses: Strong Points:  Novel Methodology: The paper introduces a unique approach of using anchor boxes as queries offering a deeper understanding of the role of queries in DETR.  Improved Performance: The proposed method achieves superior performance on the MSCOCO benchmark demonstrating the effectiveness of the dynamic anchor box formulation.  Comprehensive Experiments: Extensive experiments are conducted to validate the proposed approach and confirm the effectiveness of the methodology.  Clear Presentation: The paper provides a clear explanation of the methodology experimental setup and results making it easier for readers to understand the contributions. Weak Points:  Validation: While the results show improved performance more indepth analysis or comparisons with stateoftheart methods could strengthen the papers impact.  Comparison: The paper could provide a more detailed comparison with existing methods to highlight the unique advantages of the proposed dynamic anchor box formulation.  Clarity in Explanations: Some technical details especially related to the anchor box formulation and its impact could be explained in simpler terms for readers from diverse backgrounds. Questions and Feedback to the Author:  Question: How does the proposed dynamic anchor box formulation compare with other methods that aim to improve DETR convergence and performance  Feedback: Providing a more detailed comparison with related works and explaining the impact of the introduced dynamic anchor boxes in a concise and clear manner would further enhance the papers contribution and reader understanding. 3) Review Summary: Recommendation: Overall the paper presents a novel and promising method of using dynamic anchor boxes in DETR for object detection. The approach shows significant improvements in performance addressing slow convergence issues and enhancing querytofeature similarity. The comprehensive experiments support the efficacy of the proposed methodology. However further analysis and comparisons with stateoftheart methods would strengthen the papers impact. Overall the papers clear presentation of the methodology and results makes it a valuable contribution to the field of object detection. Therefore I recommend this paper for acceptance with minor revisions to address the areas of improvement mentioned above.", "gJLEXy3ySpu": " 1. Summary of the Paper: The paper introduces a novel approach for certifying the robustness of topk predictions against \\xe2\\x84\\x930norm adversarial perturbations using randomized smoothing. The work aims to bridge the gap in existing studies that typically focus on certifying \\xe2\\x84\\x930norm robustness of top1 predictions or \\xe2\\x84\\x932norm robustness of topk predictions. The proposed method is evaluated on CIFAR10 and ImageNet datasets demonstrating significant improvements in certified topk accuracies compared to existing methods.  2. Strengths and Weaknesses: Strong Points:  Theoretical Contribution: The paper presents a significant theoretical contribution by deriving an almost tight \\xe2\\x84\\x930norm certified robustness guarantee for topk predictions using randomized smoothing.  Empirical Evaluation: The method is empirically evaluated on CIFAR10 and ImageNet demonstrating substantial improvements in certified topk accuracies compared to existing approaches.  Technical Differences: The paper highlights technical differences with previous works showcasing its unique approach and contributions to certifying robustness against adversarial perturbations.  Impact of Parameters: The study evaluates the impact of various parameters such as k e n and \u03b1 providing insights into the behavior and performance of the proposed method. Weak Points:  Limited Comparison: While the paper provides comparisons with existing methods the evaluation could be further enhanced by including more diverse baselines and stateoftheart approaches.  Practical Implementation: The complexity of computing the certified radius and estimating probability bounds may pose practical challenges for realworld implementation. Providing more insights into implementation feasibility would be beneficial.  Generalization: The evaluation is limited to CIFAR10 and ImageNet datasets limiting the generalizability of the findings to other datasets and scenarios. Question and Feedback to the Author: 1. Extension to Other Datasets: Have you considered extending the evaluation to other datasets to validate the generalizability of the proposed method across different domains 2. Scalability: How scalable is the proposed approach in practical scenarios with largescale neural networks and diverse input data types 3. Comparison Metrics: Did you consider using additional evaluation metrics apart from certified topk accuracies to provide a more comprehensive assessment of the methods performance  3. Review Summary: The paper presents a novel approach for certifying \\xe2\\x84\\x930norm robustness of topk predictions against adversarial perturbations using randomized smoothing. The theoretical contributions are significant with clear technical differentiators from existing works. The empirical evaluations on CIFAR10 and ImageNet demonstrate substantial improvements in certified accuracies. However further validations scalability considerations and diverse comparisons could enhance the papers impact. Overall the paper showcases a promising direction in enhancing the robustness of classifiers against adversarial attacks.", "m716e-0clj": " Peer Review:  1) Summary: The paper proposes a novel decentralized adaptive method DAGAdam to overcome the limitations of existing adaptive gradient methods in deep learning tasks. The key innovation is the introduction of a communicatethenadapt structure where the adaptive step is taken after neighborhood communications. The method is theoretically guaranteed to converge to the stationary solution in nonconvex scenarios. Experimental results on computer vision and natural language processing tasks demonstrate the superiority of DAGAdam over existing decentralized adaptive methods.  2) Strengths and Weaknesses: Strengths:  The paper addresses an important issue in decentralized adaptive methods and proposes a novel algorithm with theoretical guarantees.  The proposed communicatethenadapt structure in DAGAdam is shown to outperform existing methods in practical applications.  The paper provides detailed mathematical derivations convergence analyses and experimental results to validate the effectiveness of DAGAdam. Weaknesses:  The paper could benefit from a more thorough comparison with additional stateoftheart methods to establish the superiority of DAGAdam more comprehensively.  The significance of the proposed approach in practical scenarios with largescale distributed systems could be further emphasized.  It would be valuable to discuss the computational complexity and scalability aspects of DAGAdam compared to existing methods.  Questions and Feedback to the Author: 1. Can you provide more insights into how the choice of hyperparameter \u03bd impacts the performance of DAGAdam across different datasets and network configurations 2. How does DAGAdam compare in terms of computational efficiency and scalability when deployed in realworld distributed systems with a large number of computing nodes 3. Considering the practical applications of decentralized adaptive methods could you elaborate on potential extensions or improvements to DAGAdam that could further enhance its performance or applicability in diverse scenarios  3) Review Summary: The paper presents a novel decentralized adaptive method DAGAdam addressing the limitations of existing adaptive gradient methods. The proposed communicatethenadapt structure is theoretically sound and outperforms stateoftheart methods in experimental evaluations. The paper is wellstructured providing detailed theoretical derivations convergence analyses and empirical results. However further comparisons with a broader range of methods and discussions on scalability and practical implications would strengthen the paper. Overall the proposed DAGAdam algorithm presents a significant contribution to the field of decentralized optimization in deep learning.  Recommendation: Based on the comprehensive analysis and validation presented I recommend this paper for acceptance with minor revisions regarding additional comparisons and practical implications.", "nnU3IUMJmN": " Summary The paper explores the incorporation of structural locality information into nonparametric language models focusing on two domains  Java source code and Wikipedia text. It proposes a method to enhance model efficacy by adding learned parameters to improve the likelihood of retrieving examples from local neighborhoods. Experimental results demonstrate improved model performance in both domains with the addition of locality features highlighting the importance of considering both contextual distance and structural characteristics of data for effective language modeling.  Strengths 1. Innovative Approach: The paper introduces a novel method of incorporating structural locality into nonparametric language models addressing an important aspect that is often overlooked in traditional models. 2. Comprehensive Analysis: The paper provides detailed case studies experiments and analysis showing how locality features contribute to improved LM performance in two distinct domains. 3. Experimental Validation: The experiments on Java source code and Wikipedia text datasets provide empirical evidence supporting the effectiveness of the proposed approach demonstrating significant performance gains. 4. Interpretability: The paper offers interpretable insights especially through the analysis of how locality features impact nearest neighbor retrieval and model predictions enhancing the readers understanding of the methodology.  Weaknesses 1. Domain Specificity: The proposed method may be limited to domains where locality effects are prominent potentially restricting its applicability to broader datasets or domains where such structural characteristics are not prevalent. 2. Optimal Feature Selection: The paper acknowledges the importance of selecting the right locality features but it would benefit from further discussion or experimentation on the optimal choice of features for various domains. 3. Scalability: It would be valuable to discuss the scalability of the proposed method to larger datasets or more complex hierarchical structures as this could impact the practical applicability of the approach.  Questions and Feedback to the Author 1. Scalability and Generalizability: Have you explored the scalability of your method to larger datasets or more diverse domains How generalizable do you believe your approach is beyond the specific domains of Java code and Wikipedia text 2. Feature Selection: How did you determine the specific locality features used in your study for Java source code and Wikipedia text Have you considered other potential locality features that could be beneficial in different domains 3. RealWorld Applications: Can you provide insights into how this method could be practically applied in realworld scenarios beyond language modeling such as in information retrieval or recommendation systems  Review Summary The paper presents a novel and effective method of integrating structural locality into nonparametric language models demonstrating significant performance improvements in Java source code and Wikipedia text domains. The comprehensive analysis detailed experiments and interpretability of results strengthen the papers contributions. However considerations regarding scalability generalizability and optimal feature selection could enhance the practical applicability of the proposed approach. Overall based on the strengths and the potential impact of the research I recommend this paper for publication pending minor revisions to address the weaknesses and questions raised.", "zXne1klXIQ": " Summary The paper introduces LISA (Learning Invariant Representations via Selective Augmentation) a technique that addresses distribution shifts by selectively interpolating samples to eliminate domainrelated spurious correlations. LISA is compared against existing methods on nine benchmark datasets showcasing superior performance in handling domain shifts and subpopulation shifts. The empirical findings are supported by theoretical analyses and ablation studies.  Strong Points 1. Innovative Approach: LISA introduces a novel method that eliminates spurious correlations through selective augmentation setting it apart from traditional domain alignment techniques. 2. Empirical Results: Extensive experiments on diverse datasets validate the effectiveness of LISA in outperforming existing stateoftheart methods consistently. 3. Theoretical Analysis: The paper includes rigorous theoretical analyses supporting the empirical results providing deeper insights into the proposed approachs effectiveness. 4. Comprehensive Evaluation: The paper covers a wide range of experiments ablation studies and theoretical analyses showcasing the thoroughness of the research.  Weak Points 1. Clarity and Readability: The paper contains technical jargon and dense mathematical formulations which might be challenging for readers not wellversed in the field. 2. Evaluation Metrics: While the paper extensively evaluates the proposed method some additional comparison metrics or baseline methods could further enhance the papers credibility. 3. Visualization: Visual aids such as figures or diagrams could help illustrate complex concepts making the paper more accessible to a wider audience.  Questions and Feedback to Authors 1. Can you elaborate on any potential limitations or constraints of LISA in scenarios with highly complex or noisy data distributions 2. How does the choice of the interpolation ratio impact the performance of LISA and have you explored different interpolation strategies beyond the Beta distribution 3. It would be beneficial to provide more insights into the scalability and computational efficiency of LISA especially when applied to larger datasets or realworld scenarios.  Review Summary The paper presents a novel approach LISA for addressing distribution shifts through selective data interpolation to eliminate domainrelated spurious correlations. The extensive empirical evaluations ablation studies and theoretical analyses collectively highlight the effectiveness and superiority of LISA over existing methods. Despite some technical complexity and potential areas for improvement in terms of clarity and additional evaluation metrics the innovative approach comprehensive evaluations and robust theoretical foundations support a strong recommendation for the acceptance of this paper.", "xp2D-1PtLc5": " Summary of the Paper: The paper introduces a novel voice conversion framework termed ClsVC which aims to effectively separate content and timbre information in voice conversion tasks using only one encoder. The proposed framework utilizes classification tasks to differentiate content and speaker information in the latent space. Experimental results on the VCTK dataset demonstrate that ClsVC outperforms existing methods in terms of naturalness and speaker similarity in voice conversion tasks.  Strong Points: 1. Innovative Approach: The proposed ClsVC framework addresses the challenge of separating content and timbre information in voice conversion tasks using a novel classificationbased approach. 2. Experimental Validation: The paper provides extensive experimental results on the VCTK dataset showcasing the effectiveness of the proposed method in achieving highquality voice conversion. 3. Comparative Analysis: The paper includes thorough comparisons with baseline models such as AutoVC VQVC and VAE demonstrating the superiority of ClsVC in both traditional and oneshot voice conversion tasks. 4. Technical Detail: The paper provides detailed explanations of the theoretical foundations behind the proposed method including the use of gradient reversal layers mutual information estimators and loss functions.  Weak Points: 1. Complexity: The paper contains a high level of technical detail which may make it challenging for readers unfamiliar with voice conversion techniques to understand the proposed framework fully. 2. Evaluation Metrics: While the paper presents a diverse set of objective and subjective evaluation metrics there could be further discussion on the limitations or potential biases associated with these metrics.  Questions and Feedback to the Author: 1. Could you elaborate on how the proposed ClsVC framework compares in terms of computational efficiency and training time compared to existing methods 2. Have you considered potential realworld applications or scenarios for the ClsVC framework beyond the VCTK dataset How do you envision the scalability and generalizability of this method to different datasets or languages 3. It would be beneficial to include discussions on potential drawbacks or limitations of the ClsVC framework in realworld applications such as handling noisy speech inputs or variations in speaking styles.  Review Summary: The paper presents a sophisticated and innovative approach to voice conversion through the ClsVC framework. The comprehensive experimental results and comparisons validate the effectiveness of the proposed method in achieving superior naturalness and speaker similarity in voice conversion tasks. Despite the technical complexity of the paper the thorough analysis and detailed explanations make a compelling case for the novelty and performance of ClsVC. Therefore based on the strong empirical evidence provided I recommend the acceptance of this paper for publication.", "xbu1tzbjvd": "Summary: The paper introduces DYNAMO an algorithm designed to construct lowdimensional manifolds that represent neural network models in a way that captures similarities in highlevel computational processes. DYNAMO takes a collection of pretrained neural networks as input and outputs a metamodel that mimics the hidden states and outputs of any model in the collection. By using model embedding vectors DYNAMO enables tasks such as clustering model averaging and semisupervised learning. The algorithm is applied to RNNs and CNNs and results in meaningful model embedding spaces that facilitate novel applications and insights into neural network dynamics. Strong Points: 1. Innovative Approach: The paper presents a novel algorithm DYNAMO that addresses the challenge of interpreting deep neural networks by constructing model embedding spaces. This approach can offer valuable insights into the highlevel computational processes carried out by neural networks. 2. Wide Applicability: DYNAMOs versatility in working with both RNNs and CNNs demonstrates its potential to be applied to various types of neural network architectures increasing its practical utility. 3. Useful Applications: The paper showcases various applications of DYNAMO such as clustering model averaging semisupervised learning and insights into the dynamical behavior of neural networks. These applications contribute to a deeper understanding of neural network behavior. Weak Points: 1. Complexity of Algorithm: The DYNAMO algorithm appears intricate and may have a steep learning curve for users not familiar with advanced neural network techniques. The paper could provide more intuitive explanations or examples to aid comprehension. 2. Lack of Comparative Results: While DYNAMOs applications and results are detailed there is a lack of direct comparison with existing methods or benchmarks. Including comparative experiments could further demonstrate the effectiveness of DYNAMO. Questions and Feedback: 1. Can you provide more insight into the computational complexity of the DYNAMO algorithm especially in terms of scalability with increasing numbers of base models 2. Have you considered benchmarking DYNAMO against existing techniques for model interpretation and analysis to showcase its superiority or distinct advantages Review Summary: The paper introduces an innovative algorithm DYNAMO which addresses the challenge of interpreting deep neural networks by constructing model embedding spaces. The paper effectively presents various applications and insights derived from using DYNAMO showcasing its potential for clustering model averaging and semisupervised learning in neural networks. Although the algorithms complexity and the lack of comparative results are minor concerns the overall contributions and practical applications of DYNAMO make it a valuable addition to the field of neural network analysis. Hence based on the strengths of the paper in presenting a novel algorithm and its applications I recommend the acceptance of this paper with minor revisions to address the weak points mentioned above.", "qTTccuW4dja": " PeerReview of \"AriEL: Mapping of Language into Volumes for Efficient Retrieval with Random Sampling\"  1) Summary: The paper introduces AriEL a method for mapping language sequences into volumes in a continuous space for efficient retrieval using random sampling. The study analyzes the performance of standard deep learning techniques in generating sentences through representation sampling and compares them with AriELs method of using the latent space. The evaluation is done on a toy grammar dataset and a dataset of human dialogues using GPT2 inside AriEL to assess the language learned and generated.  2) Strengths and Weaknesses: Strong Points:  The introduction of AriEL as a unique method for mapping language sequences into volumes is innovative and can potentially improve retrieval efficiency.  The detailed description of methodology including the representation coding algorithm makes the approach clear and reproducible.  Comprehensive evaluation on both toy and human datasets provides robust insights into the performance of AriEL compared to standard techniques. Weak Points:  The paper could benefit from elaborating more on the technical implementation details of AriEL especially when integrating it with GPT2 for language modeling.  The limitations or potential drawbacks of AriEL such as computational complexity or scalability issues could be discussed for a more balanced view.  The discussions on the implications and realworld applications of using AriEL could be expanded to further highlight its practical significance. Questions and Feedback to the Author: 1. Can you provide more insights into the computational complexity of AriEL particularly in comparison to standard deep learning techniques like AE VAE and Transformer 2. How does the performance of AriEL vary with the increase in the latent space dimensionality (d) and how does it impact the retrieval and generation capabilities 3. Could you elaborate on the potential challenges or limitations in implementing AriEL in largescale language processing tasks considering realworld applications 4. In what ways do you foresee the integration of AriEL with pretrained language models like GPT2 could enhance language generation tasks beyond the datasets evaluated in this study  3) Review Summary: The paper provides a novel approach AriEL for representing language sequences in volumes to improve information retrieval with random sampling. While the methodology and evaluation on toy and human datasets showcase the effectiveness of AriEL in generating diverse and valid language sequences some aspects warrant further clarification. The comparative analysis with standard techniques and the integration with GPT2 demonstrate the potential of AriEL in advancing language modeling. However addressing the computational complexity scalability and practical implications of AriEL could strengthen the papers impact and applicability. Based on the innovative approach and promising results presented in the study I recommend acceptance of the paper with the suggested improvements and clarifications in the areas mentioned above. The findings offer valuable insights into enhancing language representation and generation using volume coding techniques like AriEL. Overall the paper contributes significantly to the field of natural language processing and deep learning by introducing a novel method for efficient information retrieval and representation in language models.", "tvwNdOKhuF5": " Peer Review: 1) Summary of the Paper: The paper presents a novel multistage learning framework for training intelligent agents in firstperson shooter (FPS) games using general reinforcement learning (RL) techniques. The proposed framework addresses the challenges posed by FPS games such as imperfect information dynamic state transitions and multiple opponents with diverse strategies. The agent named DSCAgent surpasses existing top agents in the open ViZDoom AI Competitions by a significant margin. The framework consists of techniques like hindsight experience replay proximal policy optimization ruleguided policy search fictitious selfplay and diversified strategic control. The paper demonstrates the effectiveness of each component and provides comprehensive experimental results to support the claims. 2) Strengths and Weaknesses: Strengths:  Comprehensive framework: The paper proposes a detailed and comprehensive multistage learning framework for training intelligent agents in FPS games.  Novel contributions: The incorporation of diverse techniques like HER HPPO RGPS PFSP and DSC in a unified system is a significant contribution to the field.  Experimental validation: The extensive experiments and comparisons with existing top agents demonstrate the superiority of the proposed DSCAgent.  Clear methodology: The paper provides a clear explanation of each stage of the learning framework making it easy to understand the approach. Weaknesses:  Lack of comparative analysis: While the paper extensively evaluates the proposed framework a more detailed comparison with similar approaches or baselines could provide deeper insights into its performance.  Theoretical explanation: The paper focuses more on empirical results and a deeper theoretical analysis or justification of the proposed techniques could enhance the scientific rigor of the work.  Reproducibility: Providing more details on hyperparameters network architectures and code availability would enhance the reproducibility of the experiments and facilitate further research in the field. Questions and Feedback to the Author: 1. Clarifications required: Could you provide more insights into the choice of specific hyperparameters in the training stages and how they impact the performance of DSCAgent 2. Comparative analysis: How does the proposed framework compare with stateoftheart RL methods in FPS games outside of the ViZDoom environment A comparative analysis could highlight the generalizability of the approach. 3. Theoretical justification: Can you delve deeper into the theoretical rationale behind the chosen techniques such as HER PFSP and RGPS and how they contribute to the agents performance in FPS games  Review Summary: The paper proposes an innovative multistage learning framework for training intelligent agents in FPS games showcasing superior performance with the DSCAgent in the ViZDoom environment. While the paper offers a comprehensive approach with strong experimental validation addressing comparative analyses enhancing theoretical justifications and providing more reproducibility details could further strengthen the scientific contributions of the work. Overall the paper makes significant strides in advancing the application of RL techniques in challenging gaming environments and sets a solid foundation for future research in the field.", "rS9t6WH34p": " Summary: The paper introduces ObSuRF a method that converts a single image of a scene into a 3D model expressed as a set of Neural Radiance Fields (NeRFs) where each NeRF represents a different object in the scene. The approach involves using an encoder network to output latent vectors describing the objects which are utilized to condition NeRF decoders for defining geometry and appearance. By reframing NeRFs volumetric rendering process as a Poisson process and using RGBD inputs for training the model can efficiently learn to reconstruct 3D geometry and segment objects in unsupervised settings across various datasets. The paper provides detailed insights into the methodology related work and experimental results on both 2D segmentation benchmarks and novel 3D datasets demonstrating the models effectiveness.  Strengths:  Novel Approach: Introducing a method that leverages NeRFs for unsupervised 3D object segmentation from single images is a significant contribution.  Efficiency Enhancement: Deriving a loss function that enables training NeRFs with RGBD inputs without ray marching makes learning more computationally efficient.  Comprehensive Experimental Evaluation: The paper conducts extensive experiments on 2D and 3D benchmarks showcasing the models performance and superiority over existing baselines.  Clear Exposition: The paper provides a detailed explanation of the methodology related work and experimental setup aiding in understanding the proposed method.  Weaknesses:  Empirical Evaluation: While the experimental results are comprehensive more insights into the limitations of the proposed method or failure modes could have been useful.  Deployment Considerations: It would be beneficial to discuss practical considerations scalability and potential deployment challenges for adopting ObSuRF in realworld applications.  Questions: 1. How does ObSuRF handle complex scenes with multiple occlusions and intricate object interactions 2. Can you elaborate on the robustness of the model to noisy depth supervision in realworld scenarios 3. Have you considered the interpretability of the latent codes generated by the encoder and their relation to specific object representations  Feedback:  Consider providing a more detailed discussion on the interpretability and generalization capabilities of ObSuRF on varying scene complexities.  Highlight any potential scenarios where the model struggles or areas for further improvement.  Discuss implications and future directions for leveraging the learned object representations in downstream tasks or realworld applications.  Review Summary: The paper introduces ObSuRF a novel method for unsupervised 3D object segmentation utilizing NeRFs showcasing significant advancements in efficient 3D reconstruction and segmentation from single images. The work is wellstructured offers novel insights and provides compelling experimental results validating the models performance. Addressing minor weaknesses related to interpretability robustness and practical implementations would further enhance the papers impact. Consequently based on the methods innovation performance and potential implications for various applications I recommend accepting this paper for publication.", "vA7doMdgi75": " Summary The paper introduces a framework for robust subspace recovery (RSR) called Dual Principal Component Pursuit (DPCP). The framework aims at recovering a subspace from data points that are potentially corrupted by outliers without prior knowledge of the true subspace dimension. The paper proposes a projected subgradient descent method (PSGM) to solve the DPCP formulation for subspaces of unknown codimension. The authors demonstrate the effectiveness of the approach through theoretical analysis and empirical results on both simulated and real datasets.  Strengths 1. Innovative Approach: The paper introduces a novel framework that addresses the challenging problem of subspace recovery without requiring prior knowledge of the subspace dimension showcasing the utility of the proposed PSGM algorithm. 2. Theoretical Rigor: The paper provides indepth theoretical analysis with Lemmas and Theorems demonstrating convergence guarantees of the PSGM algorithm under various scenarios substantiating the proposed frameworks robustness. 3. Empirical Validation: Empirical results on simulated and real datasets support the theoretical claims showcasing the practical effectiveness of the proposed approach in recovering subspaces of unknown codimension. 4. Comparative Analysis: The comparison with existing methods like DPCPIRLS and RSGM provides a clear understanding of the advantages of the proposed DPCPPSGM approach.  Weaknesses 1. Lack of Complexity Analysis: The paper could benefit from a detailed complexity analysis to understand the computational efficiency of the proposed PSGM algorithm especially in comparison with existing methods. 2. Clarity in Notations: The paper uses a multitude of notations and symbols which can be overwhelming. Simplifying and standardizing notations could enhance the readability of the paper for a wider audience. 3. Discussion on Practical Applications: While the theoretical and empirical aspects are wellcovered a discussion on potential realworld applications and scenarios where the proposed approach can be applied would add value to the paper.  Questions to the Authors 1. How does the proposed PSGM algorithm scale with increasing data dimensionality and the presence of a larger number of outliers 2. Have you considered incorporating additional constraints or regularization terms in the PSGM algorithm to further improve the recovery performance in more complex scenarios 3. Could you provide insights into the limitations or potential challenges faced during the empirical validation on real datasets  Review Summary The paper presents a novel framework for robust subspace recovery using the DPCP formulation and the PSGM algorithm allowing recovery without prior knowledge of the subspace codimension. The theoretical analysis provides robust convergence guarantees with empirical results supporting the theoretical claims. While the paper showcases innovative contributions and theoretical robustness some improvements in terms of clarity in notations complexity analysis and practical applications discussion could enhance the overall impact of the work. In conclusion considering the innovative approach comprehensive theoretical foundations and empirical validations the paper is recommended for publication with the suggested improvements in clarity and additional practical insights.", "mFpP0THYeaX": " Summary of the Paper The paper focuses on the problem of domain adaptation with a specific interest in shifting the model towards the target distribution rather than learning domaininvariant representations. The authors propose a method called GIFT (Gradual Interpolation of Features toward Target) that aims to adapt the model to the target distribution by creating virtual samples from intermediate distributions through linearly interpolating representations of examples from the source and target domains. The effectiveness of GIFT is studied through experiments on synthetic and natural distribution shift datasets comparing it with iterative selftraining approaches.  Strengths of the Paper 1. Innovative Approach: The introduction of GIFT as a method for gradual domain adaptation through feature interpolation is novel and addresses the limitations of iterative selftraining in scenarios where the source and target distributions do not overlap sufficiently. 2. Thorough Experiments: The paper conducts comprehensive experiments on synthetic and natural distribution shift datasets showcasing the efficacy of GIFT compared to iterative selftraining approaches. 3. Theoretical Justification: The paper provides a solid theoretical background discussing the importance of a curriculum for successful selftraining algorithms in domain adaptation scenarios.  Weaknesses of the Paper 1. Lack of RealWorld Benchmarking: The paper acknowledges the limitation of not providing stateoftheart results on specific benchmarks designed for gradual domain adaptation. A comparison with existing benchmark datasets could enhance the validation of the proposed method. 2. Limited Scope: The experiments are focused on image classification tasks limiting the generalizability of the proposed GIFT method to other data modalities.  Questions for the Authors 1. Have you considered applying the GIFT method to other domains beyond image classification to evaluate its effectiveness 2. How would GIFT perform in scenarios with highly complex distribution shifts or extreme domain gaps  Feedback to the Authors 1. Consider expanding the scope of experiments to cover diverse data modalities to showcase the robustness and versatility of the GIFT method. 2. Provide more insights on the computational complexity and scalability of GIFT as this could be crucial for realworld applicability.  Review Summary The paper introduces a novel method GIFT for gradual domain adaptation through feature interpolation and demonstrates its superior performance over iterative selftraining approaches in scenarios with nonoverlapping distributions. While the paper presents strong theoretical justifications and conducts thorough experiments further benchmarking on realworld datasets and discussions on scalability would enhance the impact and applicability of the proposed method. Overall the innovative approach of GIFT holds promise for addressing challenging domain adaptation scenarios.", "sTkY-RVYBz": "Summary: The paper explores the impact of batch normalization (BN) on deep neural network models outofdomain generalization capabilities. It highlights that BN incentivizes models to rely on frequent lowvariance features specific to the training data leading to poor generalization to outofdomain examples. The Counterbalancing Teacher (CT) method is proposed to address this issue. CT leverages an unnormalized model as a teacher to regularize the student networks learning for robust representations. The paper includes theoretical justifications empirical evaluations and comparisons with existing methods on various datasets and tasks. Strengths: 1. Clear problem statement: The paper clearly articulates the problem of BN impacting outofdomain generalization and offers a practical solution. 2. Theoretical justifications: Providing theoretical explanations for the observed phenomenon enhances the papers scientific rigor. 3. Novel approach: The CT method is innovative and fills a gap in improving model robustness without domainspecific adaptations. 4. Comprehensive experiments: The paper thoroughly tests the proposed method on various datasets architectures and tasks showcasing its effectiveness. 5. Comparative analysis: Comparisons with existing methods both augmentationbased and covariate shift adaptation provide valuable insights into the effectiveness of CT. Weaknesses: 1. Complexity of explanations: The theoretical analysis may be challenging to follow for readers not wellversed in deep learning concepts potentially limiting accessibility. 2. Visualization: While the paper discusses qualitative analysis using GradCAM including visual examples could enhance understanding for readers. 3. Impact discussion: Further discussion on the practical implications of the proposed method in realworld applications could strengthen the paper. Questions: 1. Can you elaborate on how the method performs when dealing with rapidly changing data distributions such as in dynamic environments 2. How does the computational cost of implementing the Counterbalancing Teacher method compare to traditional data augmentation approaches 3. Have you considered testing the method on more diverse datasets to evaluate its adaptability across different domains Feedback to the authors: 1. It would be beneficial to make the theoretical explanations more accessible to a wider audience by simplifying complex concepts where possible. 2. Including visual examples alongside the qualitative analysis could enhance the papers readability and impact. 3. Further discussion on the practical implications and potential applications of the Counterbalancing Teacher method in realworld scenarios would enrich the paper. Review Summary: The paper presents a compelling investigation into the impact of batch normalization on model generalization and introduces a novel Counterbalancing Teacher method to improve robustness. The theoretical justifications and comprehensive experiments support the efficacy of CT in enhancing model performance on various tasks. While the complexity of theoretical explanations may challenge some readers the innovative approach and thorough analysis make the paper a valuable contribution to the field. Therefore I recommend acceptance after addressing the questions and feedback provided.", "xa6otUDdP2W": " Summary of the Paper: The paper introduces a novel scheduled growandprune (GaP) methodology for model sparsification in deep neural networks. The GaP method aims to reduce computation and memory costs while maintaining model quality. It does not require pretraining a dense model and instead repeatedly grows a subset of layers to dense and then prunes them back to sparse after some training epochs. The experimental results demonstrate that models pruned using the GaP method achieve superior performance compared to other stateoftheart sparsification algorithms matching or surpassing the quality of highly optimized dense models at high sparsity levels across various tasks such as image classification object detection 3D object part segmentation and translation.  Strong Points: 1. Innovative Methodology: The GaP methodology introduces a unique approach to model sparsification without the need for pretraining dense models addressing limitations of previous algorithms. 2. HighQuality Results: The experimental results demonstrate that models pruned using GaP outperform stateoftheart methods on a variety of deep learning tasks achieving comparable or superior performance to dense models at high sparsity levels. 3. Theoretical Justification: The paper provides theoretical guarantees for the GaP method and clarifies its convergence properties enhancing the credibility and reliability of the proposed approach.  Weak Points: 1. Resource Intensive Training: The paper acknowledges that the GaP method requires more training time compared to conventional pruning methods potentially limiting its applicability in resourceconstrained settings. 2. Limited Hyperparameter Analysis: Some key hyperparameters such as partition numbers and strategies are determined heuristically which may raise questions about the robustness and generalizability of the methodology. 3. Biases in Sparse Models: The limitations section addresses the inherent biases in predictions from sparse models compared to their dense counterparts indicating a potential challenge that the GaP methodology may not fully mitigate.  Questions to the Author: 1. Can you provide insights into any additional use cases or scenarios where the GaP methodology could be particularly beneficial or where it may face challenges 2. How sensitive is the GaP method to hyperparameter choices especially in scenarios with significantly different model architectures or data characteristics 3. Considering the resourceintensive training requirement do you have plans to explore methods to further optimize the training process of the GaP methodology in future research  Feedback to the Author: 1. Consider providing more detailed explanations or empirical support for the heuristic decisions related to hyperparameter settings to enhance the transparency and reproducibility of your methodology. 2. Explore potential strategies or techniques to address the biases inherent in predictions from sparse models as a future direction to further improve the applicability and reliability of the GaP methodology. 3. Clarify the practical implications and realworld benefits of the GaP methodology in terms of deployment scenarios and potential impact on the broader AI community. Overall the paper presents a novel and promising methodology for model sparsification with compelling experimental results and theoretical justifications. Addressing the raised concerns could further strengthen the validity and practical relevance of the proposed GaP approach.", "pz1euXohm4H": " Review of \"TargetSide Data Augmentation for Autoregressive Sequence Generation\"  1) Summary: The paper introduces a novel targetside data augmentation method for autoregressive sequence generation tasks. The proposed method leverages the decoders output probability distributions to enhance target token embeddings creating soft pseudo tokens. These pseudo tokens are mixed with original tokens to create synthetic targetside sequences improving model training without introducing extra complexity. The authors conduct experiments on dialog generation machine translation and abstractive summarization tasks demonstrating superior performance over baseline methods.  2) Strengths:  Innovative Approach: The paper addresses a significant gap in current data augmentation practices by focusing on enhancing targetside input for autoregressive sequence generation.  Comprehensive Experiments: The authors conduct experiments on multiple tasks and datasets showcasing the effectiveness of the proposed method across various scenarios.  Superior Performance: Results demonstrate that the proposed method outperforms strong baselines and existing data augmentation techniques indicating the methods efficacy.  Clear and Detailed Method Description: The paper provides a detailed explanation of the proposed method making it easy to understand and potentially implement.  Ablation Studies: The study includes thorough ablation studies on key hyperparameters and components of the method providing insights into the impact of different choices.  Weaknesses:  Theoretical Explanation: While the practical implementation and results are welldocumented additional theoretical analysis backing the proposed method could strengthen the paper.  Comparison Metrics: While BLEU ROUGE and METEOR scores are used for evaluation providing additional insights into qualitative aspects of generated sequences could enhance the evaluation.  Questions and Feedback to the Authors: 1. Have you considered exploring different divergence metrics apart from KL and JS divergence for the consistency loss How might other metrics impact the models performance 2. Given the iterative nature of the proposed data augmentation method what computational costs do additional rounds of augmentation incur during training 3. Could you elaborate on how the proposed method could be extended to handle scenarios where the vocabulary size or sequence length is notably large  3) Review Summary: The paper presents a novel targetside data augmentation method for autoregressive sequence generation tasks demonstrating significant performance improvements over existing techniques. The methods innovation coupled with comprehensive experiments across diverse tasks strengthens the papers contribution. Additionally the detailed method description and insightful ablation studies reflect a thorough experimental design. Addressing theoretical aspects and potentially expanding the comparison metrics could further enhance the papers impact. Overall based on the strong experimental results and methodological contributions I recommend acceptance after addressing the minor weaknesses and questions raised.", "mk0HzdqY7i1": "Abstract: The paper introduces an opensource benchmark suite for the NPhard Maximum Independent Set (MIS) problem both in its weighted and unweighted variants. It analyzes the performance of guided tree search algorithms presents comparisons with traditional solvers like GUROBI and KAMIS and evaluates a reinforcement learningbased solver. The key finding is that the success of tree search algorithms mainly relies on algorithmic techniques rather than the learned graph neural network (GNN) structure guidance. Review: Strengths: 1. Comprehensive Analysis: The paper conducts an extensive analysis with detailed comparisons between various solvers highlighting their performance on different types of graphs. 2. Benchmark Suite: The creation of an opensource benchmark suite for the NPhard MIS problem is a significant contribution to the research community. 3. Reproducibility: The paper focuses on the reproducibility of results and provides explicit details about the experimental setup and datasets used. 4. Clear Presentation: The paper is wellstructured with clear subsections that aid in understanding the research process and findings. 5. Ethics and Reproducibility Statements: The inclusion of ethics and reproducibility statements further strengthens the credibility and transparency of the study. Weaknesses: 1. Lack of Engagement with Previous Work: The paper seems to mostly discuss findings in isolation without engaging in a broader discussion with related work. 2. Limited Discussion on Future Work: The section on future work could be more detailed providing specific directions for potential research avenues. 3. Absence of Limitations: The paper could benefit from discussing the limitations of the study and potential sources of bias or error. Questions: 1. How do the findings of this study impact the broader field of combinatorial optimization research particularly in the context of machine learningbased solvers 2. How do the conclusions drawn regarding the reliance on algorithmic techniques versus learningbased approaches translate to other NPhard optimization problems beyond the MIS 3. How crucial is the role of hyperparameters in determining the performance of the solvers and were sensitivity analyses conducted in this study Feedback: 1. Consider expanding the discussion on the implications of the study findings for future research and practical applications. 2. Address the limitations and potential sources of bias in the study to provide a more balanced perspective. 3. Engage in a more comprehensive dialogue with related work to situate the studys findings within the broader research landscape. Review Summary: The paper provides a valuable contribution to the field of combinatorial optimization by offering an opensource benchmark suite for the NPhard MIS problem. It rigorously evaluates various solvers highlighting the importance of algorithmic techniques over learned structures in achieving good results. Despite its strengths in analysis and reproducibility further engagement with related work expansion on future research directions and discussion on limitations would enhance the overall impact and relevance of the study in advancing the understanding of solving NPhard optimization problems. Hence with these considerations I recommend the paper for further publication after addressing the suggested feedback and questions.", "sOK-zS6WHB": "Summary: The paper introduces a novel fingerprinting mechanism for generative models to enable responsible disclosure and regulation of stateoftheart GAN models. The mechanism involves embedding fingerprints into the generator parameters to allow for the adhoc generation of distinct fingerprinted models. The paper demonstrates the effectiveness scalability secrecy robustness and immunizability of the proposed mechanism. Experimental results show the mechanisms high efficacy in deep fake detection and attribution tasks surpassing stateoftheart methods. Strengths: 1. Innovative Approach: The paper introduces a novel fingerprinting mechanism that addresses the critical issue of responsible disclosure in generative models showcasing a unique approach to enhance model transparency and accountability. 2. Comprehensive Evaluation: The paper provides thorough experimental validation demonstrating the mechanisms effectiveness scalability secrecy robustness and immunizability through various experiments on different datasets and scenarios. 3. Practical Applications: The proposed mechanism showcases practical applications in deep fake detection and attribution offering a proactive approach to identifying and tracing sources of misinformation generated by GAN models. 4. Clear Writing: The paper is wellwritten and structured effectively conveying complex concepts in a clear and concise manner. The detailed sections and explanations contribute to the clarity of the research content. Weaknesses: 1. Complexity: The paper delves into intricate technical details and formulations which may be challenging for readers with limited background in generative models or deep learning. Simplifying certain sections could enhance accessibility for a broader audience. 2. Comparison to Existing Methods: While the paper compares the proposed mechanism to some existing baselines a more extensive comparison with a wider range of stateoftheart techniques would further strengthen the evaluation and highlight the mechanisms distinct advantages. Question to the Author: 1. Can you elaborate on the implications of the proposed fingerprinting mechanism for realworld applications particularly in combating deep fakes and misinformation Feedback to the Author: 1. Consider providing a more detailed discussion on the practical implementation and deployment of the fingerprinting mechanism in realworld scenarios to enhance the papers comprehension and relevance to potential stakeholders. Review Summary: The paper presents a novel fingerprinting mechanism for generative models to enable responsible disclosure and regulation showcasing impressive results in deep fake detection and attribution tasks. The innovative approach comprehensive evaluation and practical implications make the research significant in advancing transparency and accountability in the use of generative models. Addressing a critical issue in AI ethics the proposed mechanism holds promise for mitigating the risks associated with the misuse of deep generative models. Overall the paper is wellstructured wellsupported and provides valuable contributions to the field of AI security and ethics. Consider revisions for broader accessibility and a more extensive discussion on realworld implications for a more impactful contribution.", "htWIlvDcY8": " Peer Review:  Summary of the Paper: The paper introduces a metalearning framework called FALCON (FAst Learning of novel visual CONcepts) for learning new visual concepts quickly from multiple data streams: images descriptive sentences and supplemental sentences. The model represents visual concepts as axisaligned boxes in a highdimensional space and uses a neurosymbolic approach to learn and reason about new concepts. The paper demonstrates the effectiveness of the model on synthetic and realworld datasets focusing on visual reasoning tasks and questionanswering applications.  Strengths of the Paper: 1. Innovative Approach: The paper proposes a novel metalearning framework for quick concept learning from diverse data streams showcasing a unique combination of visual and linguistic information to learn new concepts effectively. 2. Comprehensive Framework: FALCON involves a thorough process from resolving reference expressions in sentences to inferring optimal concept embeddings using relationships with known concepts demonstrating a holistic approach to concept learning and reasoning. 3. Evaluation: The paper evaluates the model on synthetic and realworld datasets showcasing the effectiveness of the proposed approach in learning accurate representations for novel concepts compared to existing baselines. 4. Comparative Analysis: The paper provides indepth comparison with relevant baselines highlighting the advantages of FALCON in fast concept learning and its ability to outperform existing methods in concept learning tasks. 5. Robust Results: The model demonstrates robust performance across different datasets illustrating its versatility in handling varied scenarios and data complexities.  Weaknesses of the Paper: 1. Lack of Realworld Application: While the effectiveness of the model is demonstrated on synthetic and realworld datasets more emphasis on practical applications in realworld scenarios and datasets could enhance the relevance of the proposed framework. 2. Metric Clarity: The paper could benefit from providing more details on the specific metrics used for evaluation explaining how they correlate with the effectiveness of the model in learning novel visual concepts.  Questions and Feedback for the Authors: 1. Generalization: How does FALCONs performance translate to unseen scenarios and datasets beyond the ones evaluated in the paper 2. Scalability: How does the model perform when scaling up to a larger concept space with more complex relationships between concepts 3. Interpretability: Can the authors provide insights into how the models neuralsymbolic reasoning framework enhances interpretability and explainability of learned concept embeddings 4. Realworld Deployment: Do you envision realworld applications where FALCON can be beneficial especially in tasks with limited available data for new concept learning  Review Summary: The paper presents an innovative metalearning framework FALCON for fast visual concept learning leveraging a neurosymbolic approach to integrate visual and linguistic information. The comprehensive evaluation demonstrates the models efficacy in learning accurate representations for new concepts across different datasets. The comparative analysis with baselines highlights the superior performance of FALCON. With clear strengths in innovation comprehensive framework and robust results the paper provides valuable insights into advancing concept learning in visual reasoning tasks. Strengthening realworld applications providing additional clarity on evaluation metrics and addressing scalability concerns could enhance the impact of the proposed framework. Overall based on the strengths exhibited in the paper I recommend acceptance for publication after addressing the highlighted weaknesses and questions.", "u7PVCewFya": " Summary of the Paper: The paper focuses on addressing the performance issues of Differentially Private Stochastic Gradient Descent (DPSGD) in training deep neural networks with guarantees of Differential Privacy (DP). The authors introduce a novel loss function combining three terms to improve the tradeoffs between privacy and accuracy. The terms include the summed squared error focal loss and a regularization penalty. Using this new loss function the authors achieve improved accuracy for DPSGD on MNIST FashionMNIST and CIFAR10 notably enhancing the accuracy on CIFAR10 by 4 for a DP guarantee of \u03b5  3.  Strengths of the Paper: 1. Novel Contribution: The paper is the first to investigate the impact of the loss function on sensitivity in Deep Learning with DP. It introduces a new loss function tailored to DPSGD showing significant improvements in tradeoffs between privacy and accuracy. 2. Extensive Analysis: The paper provides comprehensive analysis and experimental results demonstrating the effectiveness of the proposed loss function. The experimental results on MNIST FashionMNIST and CIFAR10 showcase the superiority of the new loss function. 3. Effective Integration: The paper demonstrates that the proposed loss function can be easily integrated into existing implementations of deep learning with DP by a single line change making it practical for implementation.  Weaknesses of the Paper: 1. Limited Comparison: The paper does not provide a detailed comparison with other existing loss functions or methods beyond crossentropy limiting the scope of understanding the proposed approach against alternate strategies. 2. Missing Evaluation: The paper lacks a detailed evaluation on scalability and computational efficiency of the proposed loss function compared to existing methods. 3. Complexity and Interpretability: The combination of three terms in the new loss function may lead to increased complexity and reduced interpretability which could affect the practical implementation in some scenarios.  Question to the Authors:  How does the proposed loss function adapt to different network architectures or tasks beyond the datasets used in the experiments  Feedback to the Authors: The paper presents a significant contribution in improving the performance of DPSGD through a novel loss function tailored for privacypreserving deep learning. The extensive analysis and experimental results provide strong evidence supporting the effectiveness of the proposed approach. Addressing the weaknesses mentioned above and possibly expanding the evaluation on scalability and comparison with more methods could further enhance the impact of the paper. Overall I recommend the paper for publication considering its originality and importance in advancing privacypreserving deep learning techniques.", "u6sUACr7feW": " Summary of the Paper: The paper introduces DPPTTS a novel texttospeech model based on Determinantal Point Processes (DPPs) to enhance the diversity of prosody in speech synthesis. The DPPTTS model incorporates a prosody diversifying module (PDM) to generate more expressive speech samples while maintaining naturalness. The paper explores the application of DPPs in speech synthesis and proposes a method to diversify prosodic features by considering context words as conditions. Experimental results demonstrate that DPPTTS outperforms baseline models in terms of prosody diversity and speech quality.  Strong Points: 1. Innovative Approach: The paper introduces a novel method using DPPs for diversifying prosody in speech synthesis which is a unique and innovative approach. 2. Experimental Evaluation: The paper provides detailed experimental results comparing DPPTTS with stateoftheart models in terms of prosody diversity speech quality and inference speed effectively demonstrating the effectiveness of the proposed model. 3. Theoretical Foundation: The paper provides a solid theoretical foundation by explaining DPPs and conditional DPPs along with the integration of Soft Dynamic Time Warping for measuring similarity between prosodic features. 4. Clear Methodology: The methodology for training the prosody diversifying module (PDM) and constructing the DPP kernel is welldetailed and explained in a structured manner. 5. Impactful Results: The results indicate that DPPTTS generates more diverse speech samples while maintaining naturalness indicating the potential of the proposed model in improving the expressiveness of speech synthesis.  Weak Points: 1. Naturalness Comparison: The paper mentions a slight decrease in Mean Opinion Score (MOS) compared to the baseline model in terms of naturalness which could be further discussed or explained. 2. Evaluation Metrics: While the paper provides a range of evaluation metrics more detailed discussion on the implications of these metrics and their importance in assessing the models performance could enhance the clarity of the results. 3. RealWorld Applicability: While the inference speed results are mentioned a more indepth analysis of the models scalability and usability in realtime applications would provide additional insights.  Questions and Feedback to the Authors: 1. Explanation of MOS Results: Could you elaborate on the implications of the slight decrease in MOS in terms of naturalness compared to the baseline model 2. Future Work: Are there any plans for further research or improvements in the DPPTTS model especially in optimizing the model for better naturalness without compromising prosody diversity 3. Scalability: How scalable is the proposed DPPTTS model for larger datasets or different languages and are there any considerations for improving scalability in future iterations  Review Summary: The paper presents an innovative approach using DPPs to enhance prosody diversity in speech synthesis resulting in more expressive speech samples while maintaining naturalness. The detailed methodology theoretical foundation and solid experimental evaluation support the effectiveness of the proposed DPPTTS model. Despite a minor decrease in naturalness compared to the baseline the papers contributions and impactful results warrant a recommendation for publication after addressing minor points and expansions on certain aspects of the study.", "iMH1e5k7n3L": " Summary: The paper introduces the concept of rank coding (RC) inspired by spiking neural networks (SNNs) and applies it to conventional artificial neural networks (ANNs) like LSTMs. The goal is to achieve computational savings and speedups in training and inference while maintaining accuracy. RC involves early stopping of computation after the first \"spike\" or output event which is indicative of the networks classification decision.  Strengths: 1. Innovative Concept: The paper introduces a novel approach of applying temporal coding inspired by SNNs to conventional ANNs showing promising results in terms of speed and efficiency. 2. Comprehensive Experiments: The paper presents a thorough evaluation of the proposed RC training method across different tasks demonstrating clear comparisons and showcasing effectiveness. 3. Theoretical Foundation: The mathematical derivation and analysis of RCbased time optimization provide a deeper understanding and validation of the proposed approach. 4. Simplicity and Flexibility: RC training presents a simple yet effective method for achieving a speedaccuracy tradeoff by tweaking a few parameters making it versatile in realworld applications.  Weaknesses: 1. Limited Explanation: The description of the RC training algorithm is quite technical and may be challenging for readers not familiar with neural networks and spiking neurons. 2. Complexity in Results: The detailed analysis and diverse experiments may overwhelm readers seeking a more straightforward interpretation of the findings. 3. Lack of Visual Aids: The paper could benefit from more visual aids to help readers grasp the concepts and results more easily. 4. Impact on RealWorld Applications: While the experimental results are promising in toy problems and benchmark datasets more focus on the practical implications of RC training in realworld scenarios could enhance the papers relevance.  Questions: 1. How does the proposed RC method compare to existing strategies in terms of generalizability to diverse datasets and scalability to larger and more complex neural networks 2. Can you clarify the practical implications of RC training particularly in scenarios where speed and efficiency are critical such as realtime applications or resourceconstrained devices 3. Have you considered exploring the robustness of the RC approach to noisy or uncertain input data and how does it handle uncertainty in classification decisions  Feedback: 1. The paper would benefit from a clearer elucidation of the implications of RC training for realworld applications and how it addresses existing challenges in neural network training. 2. Consider providing more intuitive explanations and visuals to aid readers in understanding the complex concepts and experimental results presented in the paper. 3. Include a discussion on the potential extensions or refinements of the RC training method for tackling more challenging tasks or incorporating additional neural network architectures.  Review Summary: The paper proposes a novel rank coding (RC) method inspired by SNNs for improving speed and efficiency in conventional ANNs like LSTMs. With comprehensive experiments and theoretical analysis supporting the effectiveness of RC training the paper showcases promising results across varied tasks. While the technical complexity and extensive experimental details may pose challenges for some readers the innovative approach and potential applications make this paper a valuable contribution to the field. Strong justification thorough evaluation and potential impact recommend this paper for consideration for acceptance after addressing the minor weaknesses highlighted.", "r88Isj2alz": " Summary of the Paper: The paper introduces NODEAttack an energybased adversarial attack against Neural Ordinary Differential Equation (ODE) models. The objective of NODEAttack is to generate adversarial inputs that increase the energy consumption of Neural ODEs during inference time. The paper evaluates NODEAttack on two datasets and two popular ODE solvers Dopri5 and Adaptive Heun showing a significant increase in energy consumption compared to benign test data. The transferability of the attacks across solvers and architectures is also explored. Additionally a case study demonstrates the impact of NODEAttack on a mobile application using Neural ODE models.  Strong Points: 1. Innovative Contribution: The paper provides a novel contribution by introducing NODEAttack the first energybased adversarial attack against Neural ODE models filling a gap in existing research. 2. Comprehensive Evaluation: The evaluation of NODEAttack is thorough covering two datasets two popular ODE solvers and assessing effectiveness and transferability of the attacks. This adds credibility to the findings. 3. Robust Methodology: The approach of creating adversarial inputs through stepsize manipulation of ODE solvers is well explained and supported by detailed algorithms and experimental setups. 4. Practical Relevance: The case study demonstrating the impact of NODEAttack on a mobile application adds practical implications to the research highlighting the potential realworld consequences of energybased adversarial attacks.  Weak Points: 1. Limited Baseline Comparison: While the paper compares NODEAttack with natural corruption and perturbation techniques a more extensive comparison with a wider range of existing attacks could enhance the evaluation. 2. Assumptions and Simplifications: The study assumes particular scenarios and conditions for the experiments which may not fully represent the complexity and variability of realworld deployment.  Questions and Feedback to the Author: 1. Addressing Robustness: How does the proposed NODEAttack account for potential defenses or countermeasures that Neural ODE models could adopt to enhance robustness against energybased attacks 2. Generalization of Results: How applicable are the findings and conclusions of the study to other types of models or domains beyond the specific Neural ODE context considered 3. Ethical Considerations: Considering the implications of energy consumption for environmental sustainability have you considered the ethical dimensions of potential attacks that increase energy usage unnecessarily  Review Summary: The paper presents a novel contribution in introducing NODEAttack an energybased adversarial attack against Neural ODE models. The research is wellstructured provides a thorough evaluation and includes a practical case study to demonstrate the impact of the attacks. While the study could benefit from further baseline comparisons and addressing some limitations the innovative approach and comprehensive evaluation support a recommendation for acceptance with minor revisions. The implications of the findings on model robustness and ethical considerations should also be considered for future research directions.", "uHq5rHHektz": "Summary: The paper presents a novel approach to improving adversarial robustness in deep neural networks by fusing background and foreground features extracted from PlacesCNN and ImagenetCNN models. The fusion model is tested on CIFAR10 and MS COCO datasets using Gaussian blur and gradientbased adversarial attacks. The results show that the fusion model increases classification performance without compromising accuracy on clean data. Strengths: 1. Originality: The paper introduces a creative fusion approach inspired by biological principles to enhance adversarial robustness in neural networks. 2. Comprehensive Analysis: The study extensively explores the impact of adversarial attacks including Gaussian blur and FGSM on different feature domains and demonstrates the effectiveness of the fusion model. 3. Results and Discussion: The results are wellanalyzed and supported with detailed visualizations and comparisons between different classifiers in response to adversarial attacks. 4. Relevance: The paper addresses a critical issue in deep learning i.e. adversarial attacks and proposes a biologicallyinspired solution that has the potential to improve the robustness of neural networks. Weaknesses: 1. Experimental Setup: The paper lacks details on hyperparameter settings training procedures and model architectures which could provide more insights into the reproducibility and generalizability of the results. 2. Limitation of Scope: The study only considers two specific types of adversarial attacks and may benefit from exploring a wider range of attacks to validate the efficacy of the fusion model under diverse scenarios. 3. Comparative Analysis: While the paper discusses the performance of the fusion model compared to individual classifiers a more thorough comparison with existing stateoftheart adversarial defense mechanisms would enhance the impact of the study. Question to the Author: 1. Can you provide more insights into the process of selecting the hyperparameters for the fusion model and how these choices were validated 2. How do you envision scaling this fusion approach to incorporate additional modalities beyond visual data and what implications do you believe this would have on adversarial robustness Feedback to the Author: 1. Consider providing more details on model architectures training procedures and hyperparameter settings to enhance the reproducibility and transparency of your study. 2. Expand the discussion to compare the proposed fusion model with existing stateoftheart adversarial defense techniques to highlight its strengths and potential limitations in a broader context. Review Summary: Overall the paper introduces an innovative fusion model to enhance adversarial robustness in deep neural networks by integrating background and foreground features. The study presents compelling results showing improvements in classification performance under adversarial attacks without sacrificing accuracy on clean data. While the paper has significant strengths in originality and detailed analysis addressing the weaknesses through more comprehensive experimental details and broader comparisons with existing methods would strengthen the impact and contribution of this research. Therefore with some revisions and additions in experimental details and comparative analysis I recommend this paper for publication as it brings a valuable contribution to the field of adversarial robustness in neural networks.", "kHNKTO2sYH": " Summary of the Paper: The paper introduces a novel semisupervised model called Clean Subspace Variational Autoencoder (CLSVAE) for detection and automated repair of systematic errors in datasets. It addresses the challenge of outlier detection and data repair caused by systematic errors proposing a method that partitions the latent space to model clean and dirty patterns separately. The model aims to improve repair quality by minimizing mutual information between the clean and dirty pattern subspaces. Experimental evaluations are conducted on image datasets with synthetic systematic errors comparing CLSVAE with relevant baselines.  Strong Points: 1. Innovative Approach: The paper introduces an innovative approach of partitioning the latent space to model inliers and outliers separately for better detection and repair of systematic errors. 2. Effectiveness: CLSVAE demonstrates superior repair quality without human intervention even with a small trusted set size outperforming other models in repair tasks. 3. Empirical Evaluation: The paper provides detailed experiments and comparisons with baselines using various datasets and noise levels showcasing the effectiveness of the proposed model. 4. Theoretical Justification: The paper provides a strong theoretical justification for the proposed model highlighting the characteristics of systematic errors and the need for a model like CLSVAE to handle them effectively. 5. Penalty Approach: Enforcing a penalty term based on distance correlation to minimize mutual information between subspaces adds a valuable contribution to the stability and performance of the model.  Weak Points: 1. Complexity: The complexity of the model architecture might be a weakness potentially making it challenging to implement and understand for practitioners new to the field. 2. Dataset Limitation: The experiments are based on synthetic systematic errors in image datasets limiting the generalizability of the findings to other types of datasets. 3. Comparison Baselines: The choice and number of baselines used for comparison could be further elaborated for a more comprehensive evaluation. 4. Evaluation Metrics: While the paper uses relevant metrics for evaluation more detailed analysis on the limitations or assumptions of these metrics could be beneficial.  Questions and Feedback to the Author: 1. Can you elaborate on the computational efficiency of the proposed CLSVAE model especially in comparison to the baselines used in the experiments 2. How would the proposed model perform on datasets with different types of systematic errors such as text data or sensor data 3. Have you considered the scalability of the model to larger datasets and the potential impact on training time and resource requirements 4. How do you plan to address the limitations of using synthetic errors in image datasets and ensure the generalizability of the model to realworld datasets  Review Summary: The paper presents a wellthoughtout and innovative approach to outlier detection and automated repair of systematic errors in datasets using the CLSVAE model. The theoretical foundation experimental evaluations and comparative analyses provide strong support for the effectiveness of the proposed model. Despite some complexities and limitations the unique capabilities of CLSVAE in handling systematic errors make it a valuable contribution to the field. Therefore based on the strengths of the paper I would recommend acceptance after addressing the minor weaknesses and providing further clarification on certain aspects.", "kUtux8k0G6y": " Summary The paper introduces a novel training method aimed at addressing the issue of robust inaccuracy in stateoftheart robust models. The proposed method aims to jointly minimize robust inaccuracy while maximizing robust accuracy. By extending existing robust training approaches the authors demonstrate the effectiveness of their method on both empirical and certified robustness across multiple datasets and models. The approach involves leveraging robustness as an abstain mechanism and using compositional architectures to improve overall robustness without sacrificing accuracy.  Strengths  Innovative Approach: The paper introduces a novel method that addresses the critical issue of robust inaccuracy in current robust models.  Comprehensive Evaluation: The authors demonstrate the effectiveness of their approach on multiple datasets and stateoftheart models showcasing improvements in both empirical and certified robustness.  Clear Presentation: The paper is wellstructured and provides detailed explanations equations and figures to support the proposed methods and results.  Weaknesses  Tradeoff Between Robust Accuracy and Inaccuracy: The paper mentions a tradeoff between robust accuracy and inaccuracy which may limit the practical application of the proposed method in certain scenarios.  Computational Complexity: The introduction of a new training method and selector mechanism may incur significant computational overhead which could be a concern for deployment in realworld applications.  Limited Exploration of Low Data Regimes: The challenges of training in scenarios with minimal robust inaccuracy are acknowledged but further exploration on efficient training in low data regimes would be beneficial.  Question to the Authors  How do you plan to address the tradeoff between robust accuracy and inaccuracy to achieve a better balance in future iterations of the method  Feedback to the Authors The paper presents a significant contribution to addressing the issue of robust inaccuracy in current robust models. To enhance the impact of the work considering the implications of model deployment and computational feasibility in practical scenarios should be a key focus for future developments. Additionally further exploration into refining the method to achieve better balance between robust accuracy and inaccuracy will be crucial for wider adoption of the proposed approach. Overall the paper is wellstructured and provides valuable insights into improving the robustness of deep learning models.", "ljxWpdBl4V": " 1) Summary of the Paper: The paper proposes a novel approach for training generative models directly for data generation purposes in zeroshot learning (ZSL). The proposed method termed sample probing integrates closedform zeroshot learning models with generative models to give feedback on the quality of synthetic samples. By fitting a taskspecific ZSL model from generated samples and measuring its loss on novel samples the generator receives direct feedback to enhance training sample synthesis. Experimental results demonstrate improved ZSL performance when integrating sample probing into stateoftheart generative models.  2) Strengths: Strong Points: a) Innovative Approach: The use of sample probing to evaluate the quality of synthesized training examples is a novel and innovative approach in ZSL research. b) EndtoEnd Training: The method enables endtoend training of generative models for data synthesis in ZSL leading to potentially more effective training samples. c) Experimental Validation: Comprehensive experiments on mainstream datasets demonstrate the effectiveness of the proposed approach in improving ZSL results. d) Variety of Probe Models: Evaluation with different closedform probe models showcases the flexibility and potential adaptability of the approach. Weak Points: a) Reproducibility Concerns: The paper acknowledges the challenges in model selection and comparison in GZSL and these factors could affect reproducibility and generalizability of results. b) Hyperparameter Sensitivity: The performance heavily depends on hyperparameter tuning introducing variability in results which might limit generalizability. c) Limitation in Probe Models: Although the use of different closedform probe models is explored further investigation into additional probe models could strengthen the approach. Questions for the Authors: 1. How do you address the reproducibility challenges in GZSL given the difficulty in making fair comparisons across different models 2. Can you elaborate on the potential impact of hyperparameter tuning strategies on the generalizability of your approach across different datasets and scenarios 3. Do you plan to explore other types of closedform probe models to further enhance the versatility and effectiveness of the sample probing approach  3) Review Summary: The paper presents a novel sample probing approach in generative zeroshot learning demonstrating improved ZSL results and enhanced sample quality. The innovative methodology of evaluating generative models through closedform ZSL models is a significant contribution to the field. While the approach shows promise its reproducibility and generalizability might be affected by the challenges in model selection and hyperparameter sensitivity. To strengthen the paper further exploration into additional probe models and strategies to address reproducibility concerns would be valuable. Overall the paper offers a promising direction for enhancing ZSL performance through sample probing with potential for significant impact in the field.", "tlkMbWBEAFb": " Summary of the Paper: The paper presents a novel approach for constructing a steerable feedforward learningbased model using spherical decision surfaces for point cloud classification in 3D geometry. The model is based on 3D steerability constraints for hypersphere neurons obtained through conformal embedding. The key contributions include proving the activation variation of spherical neurons upon rotation and proposing a method to create fully steerable models for invariant class predictions. Experimental validation is conducted using synthetic point sets and real 3D skeleton data.  Strengths of the Paper:  Originality: The paper introduces a novel approach to constructing steerable models using 3D geometry and conformal modeling which adds to the existing body of research on steerable filters and equivariant networks.  Rigorous Mathematical Derivation: The paper provides a thorough mathematical derivation of the 3D steerability constraint and basis construction for spherical filter banks demonstrating a strong theoretical foundation.  Experimental Validation: The experiments conducted on synthetic and realworld datasets demonstrate the effectiveness and robustness of the proposed approach in achieving rotationinvariant predictions for point cloud classification.  Weaknesses of the Paper:  Complexity: The paper delves into intricate mathematical concepts such as conformal embedding and spherical harmonics which may pose challenges for readers less familiar with advanced geometric principles.  Lack of Comparative Analysis: While the experiments showcase the performance of the proposed approach comparing it with existing stateoftheart methods for point cloud classification could provide a more comprehensive evaluation.  Clarity in Presentation: Some sections of the paper particularly the background and methodological details may require more clarity to ensure easy understanding for a wider audience.  Questions for the Authors: 1. How does the proposed approach compare in terms of computational efficiency and generalization performance with existing methods for point cloud classification 2. Have you considered extending the application of steerable spherical neurons to other types of neural network architectures beyond the feedforward model explored in this work 3. How do you envision potential realworld applications of the proposed steerable models in areas such as autonomous driving humanrobot interaction or mixedreality installations  Feedback for the Authors:  Clarification: Enhance the clarity of the paper by providing more intuitive explanations of complex mathematical concepts to facilitate understanding for a broader audience.  Comparison: Consider including a comparative analysis with stateoftheart methods to highlight the advantages of the proposed approach.  Future Directions: Discuss potential avenues for further research or applications leveraging the capabilities of steerable spherical neurons beyond the scope covered in this paper.  Review Summary: The paper presents a novel and mathematically sound approach for constructing steerable feedforward models using spherical decision surfaces for 3D point cloud classification. While the theoretical foundation and experimental validation exhibit the effectiveness of the proposed method improvements in clarity and comparative analysis could enhance the impact and applicability of the work. Overall the paper showcases a promising direction in leveraging geometric principles for achieving rotationequivariant and invariant predictions in point cloud classification tasks.", "hW2kwAcXq5w": " PeerReview of Offline Imitation Learning Algorithm  1. Summary of the Paper: The paper introduces a novel offline imitation learning algorithm termed DiscriminatorWeighted Behavioral Cloning (DWBC) which aims to optimize expert behavior policy learning without online environment interactions. The key innovation of the proposed algorithm lies in introducing a discriminator to differentiate expert and nonexpert data and a cooperation strategy to enhance the performance of both tasks. By leveraging a generalized BC objective and weighting functions based on discriminator outputs DWBC aims to selectively imitate highquality transitions and discard suboptimal data from the offline dataset.  2. Evaluation of Strong and Weak Points Questions and Feedback:  Strong Points:  Innovative Approach: The introduction of a discriminator and cooperation strategy adds a fresh perspective to the offline imitation learning problem.  Enhanced Performance: Experimental results demonstrate that DWBC outperforms baseline algorithms in various scenarios achieving higher returns and faster training speed.  Clear Explanation: The paper provides detailed explanations of the methodology its relation to existing techniques and the experimental setup.  Weak Points:  Complexity of Methodology: The proposed algorithm involves several interrelated tasks and concepts that may require careful understanding for implementation.  Theoretical Assumptions: The paper heavily relies on theoretical derivations which could potentially be challenging for readers not familiar with the underlying concepts.  Evaluation Scope: The experiments focus on certain tasks and datasets limiting the generalizability of the results.  Questions and Feedback to Author:  Generalizability: How would the algorithm perform on more diverse and complex realworld tasks or environments  Scalability: Have you tested the scalability of your algorithm to larger datasets or more complex scenarios  Comparison with StateoftheArt: How does DWBC compare with other cuttingedge offline imitation learning approaches in terms of performance and scalability  3. Review Summary: The paper presents a wellstructured and detailed study on offline Imitation Learning proposing an innovative algorithm DWBC to address the challenges of learning from mixedquality demonstrations. The proposed method demonstrates promising results in outperforming baseline algorithms in various scenarios showcasing higher returns and faster training speed. While the complexity of the methodology and theoretical assumptions may present challenges the clear explanations provided in the paper lay a solid foundation for understanding the algorithm. The experimental evaluations and comparisons provide valuable insights into the effectiveness of DWBC. In conclusion the paper makes a significant contribution to the field of offline imitation learning by introducing a novel approach that shows potential for improving policy learning in suboptimal demonstration scenarios.  Recommendation: Based on the thorough evaluation of the paper I recommend accepting it for publication as it presents a novel and effective offline imitation learning algorithm with promising experimental results. The innovative approach clear explanations and comparative evaluations make the paper a valuable contribution to the field. Addressing the weak points by providing additional clarification on the scalability and generalizability of the algorithm would enhance the overall impact and relevance of the work.", "vruwp11pWnO": " Summary The paper focuses on outofdistribution detection in largescale multiclass and multilabel settings using highresolution images and thousands of classes. It introduces new benchmarks for anomaly detection in three different settings: ImageNet multiclass detection with a new dataset of anomalous species PASCAL VOC and COCO multilabel detection using ImageNet21K and anomaly segmentation with a new benchmark for road anomalies. The paper proposes a MaxLogit detector that outperforms existing baselines in all three largescale settings and establishes it as a new baseline for future research.  Strengths 1. Novelty and Contribution: The paper addresses an important gap in the outofdistribution detection research by focusing on largescale settings with realistic scenarios where existing baselines may not perform well. 2. Comprehensive Experiments: The experiments conducted are extensive covering multiple scenarios and datasets to thoroughly evaluate the proposed MaxLogit detector against existing baselines. 3. New Datasets and Benchmarks: The introduction of new benchmarks like the Species dataset and the CAOS benchmark for anomaly segmentation adds value to the research community by providing standardized evaluation setups. 4. Clear Presentation: The paper is wellstructured providing a clear motivation detailed methodology and indepth results discussion. The writing is cohesive and easy to follow.  Weaknesses 1. Evaluation Metrics: While the paper uses standard metrics like AUROC FPR95 and AUPR it could benefit from a more indepth discussion on the limitations and implications of these metrics in the context of outofdistribution detection. 2. Baseline Comparisons: The paper could provide more detailed comparisons with other stateoftheart anomaly detection baselines to better highlight the superiority of the proposed MaxLogit detector. 3. Generalizability: The paper could discuss the generalizability of the proposed solution to different types of anomalies and datasets beyond the ones explored in the study.  Questions 1. How does the proposed MaxLogit detector perform when handling anomalies that are not related to images such as textual data or timeseries data 2. Have you considered exploring the interpretability aspects of the MaxLogit detector to provide insights into why it outperforms other baselines  Feedback to Author 1. Provide more insights into the significance of the research results by discussing implications for practical applications and future research directions. 2. Strengthen the discussion on the limitations of the proposed approach and potential avenues for resolving them in future work to enhance the papers completeness.  Review Summary The paper provides valuable insights into outofdistribution detection in largescale settings introducing innovative approaches and benchmarks. While the proposed MaxLogit detector demonstrates superior performance the paper could enhance its impact by addressing potential limitations providing more robust baseline comparisons and discussing broader implications for the field. Overall the research is wellexecuted and contributes significantly to advancing the stateoftheart in outofdistribution detection. Therefore with some minor revisions and additions I recommend this paper for publication.", "wgR0BQfG5vi": " Summary The paper addresses the issue of overconfidence in neural networks and proposes a novel regularization scheme called adaptive label smoothing with selfknowledge as a prior label distribution. This approach dynamically adjusts the smoothing parameter based on the entropic level of the model probability distribution allowing for varying amounts of smoothing per instance during training. The paper provides theoretical analysis supporting the proposed method and experimental results demonstrate superior model performance and calibration compared to existing baselines particularly in machine translation tasks.  Strengths 1. Innovative Approach: The paper introduces a novel regularization scheme that dynamically adjusts the label smoothing parameter based on the models entropic level offering a unique solution to combat overconfidence. 2. Theoretical Support: The paper provides detailed theoretical analysis supporting the proposed regularization method particularly focusing on gradient rescaling and regularization effects. 3. Experimental Validation: Extensive experiments on multiple datasets demonstrate the effectiveness of the proposed method showcasing improvements in model performance and calibration metrics. 4. Comprehensive Discussion: The paper thoroughly discusses the limitations of existing label smoothing methods the connection to knowledge distillation and how the proposed method addresses these limitations.  Weaknesses 1. Limited Contextualization: While the paper extensively discusses the proposed method and its experimental results more comparative analysis with existing stateoftheart regularization techniques could provide additional insights. 2. Indepth Discussion on Hyperparameters: Further elaboration on the process of determining specific hyperparameters within the proposed scheme such as the normalization factor for the smoothing parameter could enhance the clarity of the methodology. 3. Generalization to Other Domains: While the paper hints at the potential applicability of the proposed method beyond Natural Language Processing more explicit exploration of its generalizability to other domains could strengthen the researchs broader impact.  Questions and Feedback to the Author 1. Could you provide more insights into how the proposed method compares with other advanced regularization techniques such as dropout batch normalization or mixup strategies 2. How does the proposed method handle cases of class imbalance within the datasets used for validation and testing 3. Have you considered exploring the application of the proposed regularization method on more diverse tasks beyond machine translation to validate its effectiveness in various domains  Review Summary The paper presents a novel regularization scheme adaptive label smoothing with selfknowledge as a prior to address overconfidence in neural networks. With strong theoretical support and compelling experimental results the proposed method showcases significant improvements in model performance and calibration. Overall the innovative approach comprehensive discussion and promising results position the paper as a valuable contribution to the field of neural network regularization warranting publication after addressing minor weaknesses and providing additional contextualization.", "hjlXybdILM3": "Summary: The paper introduces SimpleBits a method to synthesize simplified inputs by reducing information content and evaluates the impact of input simplification on neural network behavior in various scenarios. The method leverages the encoding bit size from a generative image model to quantify visual complexity aiming to simplify inputs while retaining taskrelevant information. The study covers perinstance simplification during training dataset condensation and posthoc simplification to understand neural network behavior. Strong Points: 1. Novel Methodology: SimpleBits introduces a novel approach to simplifying inputs without relying on domainspecific knowledge which can offer insights into neural network behavior. 2. Comprehensive Evaluation: The study investigates the impact of input simplification in different scenarios including perinstance simplification dataset condensation and posthoc analysis providing a thorough analysis. 3. RealWorld Applications: The paper demonstrates the practical implications of input simplification in tasks with injected distractors and realworld datasets like chest radiographs. Weak Points: 1. Evaluation Metrics: While the study evaluates the tradeoff between input simplicity and task performance there could be further discussion on the specific metrics used to measure information retention and task relevance. 2. Complexity: The methodology involves several components like training simplifier networks and optimizing loss functions which might be challenging to implement for researchers without expertise in neural networks. Question to the Author: 1. How do you plan to validate the interpretability of SimpleBits in realworld applications or with human evaluators Feedback: The paper presents a wellstructured methodology for input simplification backed by a detailed evaluation across different scenarios. The clear explanation of the SimpleBits method and its application in neural network behavior analysis make this paper a valuable contribution to the field of interpretability in deep learning. However further discussion or comparative analysis on the impact of simplification on different network architectures or tasks could enhance the papers depth. Overall I recommend acceptance with minor revisions to address the weak points outlined above.", "hqkhcFHOeKD": "1) Summary of the paper: The paper proposes a principled mathematical framework for designing marginbased loss functions in deep learningbased classification tasks. The main objective is to learn towards the largest margins to enhance the discriminative power of features. The authors introduce the concept of class margin for interclass separability and sample margin for intraclass compactness and use them to derive a generalized margin softmax loss. The proposed framework is validated through experimental results on various tasks including visual classification imbalanced classification person reidentification and face verification. 2) Strong and Weak Points: Strong Points:  Theoretical Foundation: The paper establishes a solid theoretical foundation by defining class and sample margins to guide the design of marginbased losses.  Rigorous Analysis: Through rigorous mathematical analysis the authors provide theoretical guarantees and explanations for the necessity of normalization in feature and prototype vectors.  Generalization: The proposed framework offers new insights that can guide the design of new tools and addresses the challenge of marginbased losses in a comprehensive manner.  Experimental Validation: Extensive experimental results demonstrate the effectiveness of the strategy across a variety of tasks showcasing improved performance in accuracy and margins. Weak Points:  Complexity: The paper involves complex mathematical derivations and theoretical analyses which may make it challenging for readers without a strong background in mathematics.  Practical Implementation: While the proposed theoretical framework is comprehensive the practical implementation of the derived losses may require additional study and experimentation.  Generalizability: The paper primarily focuses on balanced and classimbalanced cases and further exploration of extreme cases or realworld scenarios with varying degrees of imbalance could enhance the generalizability of the proposed framework.  Clarity: The paper contains dense mathematical notation and derivations that could benefit from clearer explanations to facilitate understanding for a wider audience. Questions and Feedback to the Authors:  Question: How do you envision the adoption and integration of the proposed principled mathematical framework into realworld applications and neural network architectures  Feedback: Providing additional insights or examples to simplify the understanding of the theoretical concepts and connecting them more explicitly to practical implications could enhance the papers accessibility. 3) Review Summary: The paper presents a rigorous and comprehensive approach to designing marginbased loss functions for deep learningbased classification tasks. By introducing class and sample margins as measures of interclass separability and intraclass compactness the authors develop a principled framework that aims to maximize discriminative power in feature representation. The experimental results demonstrate the effectiveness of the proposed strategy across various tasks showcasing improvements in accuracy and margins. While the paper excels in providing a strong theoretical foundation and rigorous analysis the complex nature of the mathematics involved may pose challenges for readers. Clearer explanations and practical implications could further enhance the accessibility and applicability of the proposed framework. Overall the paper offers valuable insights into addressing the limitations of existing marginbased losses through a principled approach paving the way for advancements in deep learningbased classification methodologies. Recommendation: I recommend accepting this paper for publication considering its contributions to the advancement of marginbased loss functions in deep learningbased classification tasks and the thoroughness of its theoretical and experimental analyses.", "tUa4REjGjTf": " Review of \"Analysis and Certification of Robustness for Ensemble Machine Learning Models with DiversityRegularized Training\"  1) Summary of the Paper: The paper aims to analyze and provide certified robustness for ensemble Machine Learning (ML) models through the proposed DiversityRegularized Training (DRT) approach. The study investigates the conditions necessary for certifiably robust ensemble ML models and introduces a novel training algorithm DRT to achieve stateoftheart certified L2robustness on popular datasets such as MNIST CIFAR10 and ImageNet. Theoretical analysis is conducted to establish the sufficient and necessary conditions for robust ensemble models focusing on Weighted Ensemble (WE) and MaxMargin Ensemble (MME) protocols. The paper demonstrates through extensive experiments that DRTenhanced ensembles consistently outperform existing single and ensemble ML models in terms of certified robustness.  2) Strengths and Weaknesses: Strong Points:  The paper addresses an important and timely issue of enhancing the certified robustness of ensemble ML models showcasing a novel approach  DiversityRegularized Training (DRT).  Thorough theoretical analysis is provided to establish the necessary and sufficient conditions for certifiably robust ensemble models particularly focusing on the Gradient Diversity and Confidence Margin Losses.  The empirical evaluation across multiple datasets validates the effectiveness of DRT in achieving significantly higher certified robustness compared to existing baselines.  Clear and detailed explanations of the theoretical framework training approach and experimental setup provide clarity to readers.  The reproducibility statement providing details on the experimental setup and availability of source code enhances the reproducibility of the study. Weak Points:  While the theoretical analysis and experimental results are comprehensive further discussions on the practical implications of the findings and potential realworld applications could add value.  The paper could benefit from a more indepth discussion on the limitations and potential drawbacks of the proposed approach along with addressing possible future research directions. Questions and Feedback to the Author:  Can you provide more insights into how the proposed DRT approach could be generalized to other types of attacks or perturbations beyond L2 distance  It would be beneficial to discuss potential limitations or edge cases where the DRT approach may not be as effective. Are there specific scenarios where this approach might not perform optimally  In future work consider exploring the scalability of DRT to larger ensemble sizes and ultralarge datasets and how it compares with other ensemble methods in terms of computational efficiency.  3) Review Summary: The paper makes a significant contribution by presenting a thorough analysis of the conditions for certifiably robust ensemble ML models and introducing DRT for training such models. The combination of theoretical foundations empirical evaluations and detailed explanations enhances the quality and credibility of the study. The proposed approach DRT showcases notable improvements in certified robustness across various datasets emphasizing the practical relevance of the research. Overall the paper is wellstructured presents a compelling argument and provides valuable insights for researchers in the field of robust machine learning. I recommend this paper for publication based on its rigorous methodology significant findings and clear presentation of results.", "ffS_Y258dZs": " Peer Review  1) Summary: The paper introduces a novel benchmark to investigate the ability of artificial agents to exhibit compositional learning behaviors. It focuses on the concept of compositional learning behaviors proposing a Symbolic Continuous Stimulus (SCS) representation as a versatile encoding method. The paper also discusses metareferential games for training agents and emphasizes systematic generalization. The study evaluates baseline results using stateoftheart RL agents highlighting challenges and the need for further research in developing more capable artificial agents.  2) Strengths:  Innovative Benchmark: The paper introduces a novel benchmark for studying artificial agents abilities to exhibit compositional learning behaviors filling a gap in current research.  Symbolic Continuous Stimulus Representation: The proposed SCS representation is a novel approach that allows for flexible instantiation of semantic structures without changing the representation shape.  Comprehensive Background: The paper provides extensive background information on language emergence compositionality and metareinforcement learning enhancing readers understanding.  Baseline Results: The study presents baseline results using stateoftheart RL agents demonstrating the feasibility and challenges of the benchmark.  3) Weaknesses:  Complexity and Jargon: The paper contains dense technical jargon and complex concepts which may hinder the accessibility and readability for a broader audience.  Lack of Comparison: While the paper outlines the proposed benchmark and presents baseline results a comparison with existing benchmarks or methods could provide additional context and insights.  Limited Discussion on Future Directions: The paper briefly touches on the need for further research and capability development but lacks a detailed discussion on specific next steps or future directions.  Question and Feedback to the Author: 1. Is there a plan to compare the proposed benchmark with existing benchmarks or methods to highlight its novelty and advantages 2. Could you provide more clarity on the practical implications of the study and how the proposed benchmark could be applied in realworld scenarios or AI applications 3. Considering the complexity of the concepts discussed do you plan to provide simplified explanations or visual aids to enhance reader understanding  4) Review Summary: The paper presents an innovative benchmark for investigating artificial agents abilities to exhibit compositional learning behaviors through a Symbolic Continuous Stimulus representation. The studys comprehensive background innovative approach and baseline results showcase the potential of the proposed benchmark. However the paper could benefit from improved accessibility more indepth discussions on future directions and potential practical applications. Overall the paper makes a valuable contribution to the field of artificial intelligence and warrants further exploration and development. Based on the strengths of the study in introducing a novel benchmark and the potential for advancing research in artificial intelligence I recommend acceptance after addressing the mentioned weaknesses and providing clarification on the questions raised.", "w7Nb5dSMM-": " 1) Summary of the Paper: The paper introduces a new class of Evolutionary Algorithms (EAs) called GillespieOrr EA (GOEA) and establishes an equivalence between GOEA and Stochastic Gradient Descent (SGD) for the optimization of artificial neural networks (ANNs). The authors explore the implications of this equivalence for Transfer Learning (TL) and model finetuning providing insights on optimization minima flatness generalization and interactions with ANN architectures. Additionally hypotheses are proposed based on evolutionary algorithms conceptual framework along with heuristics to accelerate finetuning processes.  2) Strengths and Weaknesses Questions and Feedback:  Strong Points:  Theoretical Rigor: The paper builds on strong theoretical foundations by establishing an equivalence between GOEA and SGD providing valuable insights into optimization mechanisms for ANNs.  Novel Contributions: Introducing GOEA and exploring its implications for TL and model finetuning represents a novel and valuable contribution to the field of machine learning.  Comprehensive Coverage: The paper covers various aspects of EA optimization from minima flatness to successful model finetuning strategies offering a thorough examination of the subject.  Weak Points:  Lack of Experimental Validation: While the theoretical framework is robust the paper could benefit from experimental validation to support the proposed hypotheses and heuristics.  Complexity and Accessibility: The technical depth and computational requirements for validation could limit the accessibility and practical application of the presented concepts for researchers with limited resources.  Questions and Feedback: 1. Experimental Validation: Have there been plans to conduct experiments to validate the hypotheses and heuristics proposed in the paper considering the substantial computational resources required 2. Practical Implementation: How feasible do the authors consider the practical implementation of the proposed heuristics and insights given the computational demands and complexities involved 3. Applicability in RealWorld Scenarios: How do the authors envision the adoption of GOEA and the derived insights in realworld scenarios particularly in industry applications where resource constraints may be a concern  3) Review Summary: The paper presents a wellresearched exploration into the equivalence between GOEA and SGD offering valuable insights into optimization techniques for ANNs. While the theoretical framework and implications for TL and model finetuning are compelling the lack of experimental validation and the significant computational demands represent potential limitations. Despite these concerns the depth of analysis and the novelty of the contributions make this paper a valuable addition to the field with potential for practical applications in optimization and transfer learning scenarios.  4) Recommendation: Considering the strong theoretical foundation and the novel insights provided by this paper I recommend its publication after addressing the questions raised regarding experimental validation practical implementation and realworld applicability. The theoretical contributions and potential impact on optimization strategies for ANNs make this work valuable for the machine learning community.", "uy602F8cTrh": " Summary: The paper presents a novel approach CausalDyna for improving the generalization ability of modelbased reinforcement learning agents in robotic manipulation tasks. The proposed algorithm leverages structural causal models to counterfactually reason about object properties enhancing sample efficiency and generalization. Experimental results in the CausalWorld roboticmanipulation environment demonstrate that CausalDyna outperforms stateoftheart modelbased and modelfree algorithms in terms of both sample efficiency and generalization.  Strengths: 1. Innovative Approach: The paper introduces a novel approach CausalDyna utilizing structural causal models for counterfactual reasoning to improve RL in realworld robotic manipulation tasks. 2. Comparative Evaluation: The paper thoroughly evaluates CausalDyna against stateoftheart algorithms demonstrating superior performance in terms of sample efficiency and generalization. 3. Experimental Rigor: The experiments conducted in the CausalWorld roboticmanipulation environment are welldesigned covering various scenarios to highlight the effectiveness of CausalDyna. 4. Clear Explanations: The paper provides clear explanations of the proposed method the training procedure and the experimental setup making it easy for readers to follow.  Weaknesses: 1. Potential Overfitting: The paper highlights the issue of overfitting in realworld environments with limited object properties but it would be beneficial to discuss potential risks of overfitting in the proposed algorithm and how they are addressed. 2. Model Limitations: The quality of counterfactual episodes generated by the world model is crucial and it would be valuable to delve deeper into the limitations or uncertainties associated with the world models predictions. 3. Scalability: While the results are promising scalability issues may arise when transitioning the proposed method to more complex robotic tasks or environments which could be acknowledged and addressed in future work.  Questions and Feedback to the Author: 1. Could you elaborate on the scalability of CausalDyna to more complex robotic manipulation tasks or environments How would the proposed algorithm handle increased complexity and variability in object properties 2. Have you considered the potential computational complexity or resource requirements of implementing CausalDyna in practical robotic systems How feasible is the deployment of this algorithm in realworld applications 3. How does CausalDyna adapt to changes in the environment dynamics or properties that were not encountered during training Is there a mechanism for continuous learning and adaptation  Review Summary: The paper introduces a novel algorithm CausalDyna that leverages structural causal models for counterfactual reasoning to enhance generalization and sample efficiency in realworld robotic manipulation tasks. The experimental results demonstrate the superior performance of CausalDyna compared to existing algorithms showcasing its potential in addressing the limitations of traditional reinforcement learning approaches. While the paper is wellstructured innovative and provides clear explanations further discussions on potential overfitting model limitations scalability and practical deployment considerations would strengthen the paper. Overall the proposed approach shows promise in advancing the field of reinforcement learning in robotics and further research on addressing identified weaknesses could enhance the robustness and applicability of CausalDyna.", "p4H9QlbJvx": "Review: 1. Summary of the paper: The paper investigates the value of inheriting weights in structured neural network pruning and challenges recent works that argue training from scratch can match or surpass finetuning a pruned model. The authors present empirical evidence that suggests the performance gap is due to improperly small finetuning learning rates. They explore the concept of dynamical isometry a property that affects gradient signal propagation to explain why larger finetuning learning rates lead to better performance outcomes. The study demonstrates that recovering dynamical isometry before finetuning is crucial and proposes a regularizationbased technique for dynamical isometry recovery on modern residual convolutional neural networks. 2. Strong points:  Comprehensive literature review: The paper provides an indepth review of structured pruning methods methodologies and their implications on network performance making it informative and wellresearched.  Novel theoretical insights: The exploration of dynamical isometry and its impact on pruning performance is a notable contribution offering a unique perspective to the pruning process.  Empirical validation: The authors present convincing empirical results that support their theoretical findings enhancing the credibility of their explanations.  Practical implications: The proposed regularizationbased technique for dynamical isometry recovery is simple yet effective providing a practical solution for improving pruning performance on modern convolutional neural networks. Weak points:  Lack of clarity in the presentation: The paper is dense with technical details and theoretical discussions which might make it challenging for readers unfamiliar with the topic to grasp the key points easily.  Limited generalizability: The focus on specific pruning methods and datasets may limit the generalizability of the findings to other network architectures or tasks.  Lack of broader discussion: While the paper delves into the technical intricacies of dynamical isometry and its impact on pruning a broader discussion on the implications for the field of machine learning or potential future research directions could enhance the papers impact. Questions and Feedback to the Authors: 1. Could you provide more insights into how the proposed regularizationbased technique for dynamical isometry recovery compares to existing methods in the literature 2. How do you plan to further validate the effectiveness of the proposed technique on a wider range of neural network architectures and datasets 3. Considering the practical implications of your study how do you envision the adoption of dynamical isometry recovery techniques in realworld applications of structured pruning 3. Review Summary: The paper offers a comprehensive investigation into the value of inheriting weights in structured neural network pruning shedding light on the importance of finetuning learning rates and dynamical isometry in this process. The theoretical insights into dynamical isometry provide a novel perspective on understanding performance gaps in pruning models. The empirical evidence presented supports the theoretical findings showcasing the effectiveness of a regularizationbased technique for dynamical isometry recovery. While the paper is wellresearched and insightful addressing clarity issues generalizability concerns and broadening the discussion on practical implications could enhance its impact and applicability in the machine learning community. Overall I recommend acceptance after addressing the minor weaknesses mentioned above. The paper makes a valuable contribution to the field of pruning methodologies and theoretical understanding in neural networks.", "z7DAilcTx7": "Peer Review:  1) Summary: The paper introduces the Adversarial Transport Framework which addresses adversarial examples in neural networks by proposing a systematic approach that combines distributional robustness optimization (DRO) and optimal transport theory. The framework aims to train robust classifiers by leveraging an optimal distribution of adversarial examples obtained via Langevin Monte Carlo sampling. The theoretical framework is applied to MNIST and CIFAR10 datasets demonstrating improved robustness and training speed compared to standard baselines such as PGD. 2) Strengths:  Innovative Approach: The paper introduces a novel framework that connects adversarial training with distributional robustness optimization showcasing theoretical foundations and practical implementation in training robust classifiers.  Theoretical Rigor: The paper provides clear definitions propositions and theorems to formalize the Adversarial Transport Framework highlighting the theoretical underpinnings supporting the proposed methodology.  Empirical Results: Experimental results on MNIST and CIFAR10 datasets demonstrate the superiority of the proposed Adversarial Transport approach in terms of robustness and training speed compared to standard baselines like PGD.  Efficiency Improvements: The paper introduces efficient techniques such as reusing past samples onestep Langevin MC and gradient ascent on the distribution combined with gradient descent on the classifier for faster and effective training. Weaknesses:  Clarity of Presentation: The paper contains highly technical content that may be challenging for readers not deeply familiar with optimal transport theory and distributional robustness optimization. Some sections could benefit from improved clarity and more intuitive explanations.  Evaluation Scope: While the empirical results demonstrate improvements on MNIST and CIFAR10 datasets a broader set of benchmarks or diverse datasets could enhance the assessment of the proposed Adversarial Transport Frameworks generalizability.  Practical Implementation Details: Further details on practical implementation aspects like hyperparameter tuning computational complexity and scalability could provide deeper insights for potential adopters of the framework. Questions and Feedback to the Authors:  Could you briefly explain how the proposed Adversarial Transport Framework could be extended or adapted to more complex datasets or architectures beyond MNIST and CIFAR10  How sensitive is the frameworks performance to hyperparameters and what considerations should practitioners keep in mind when applying it to different datasets or models  Could you provide insights into how the proposed framework aligns with interpretability and explainability requirements in adversarial robustness research 3) Review Summary: The paper presents a novel Adversarial Transport Framework that connects adversarial training with distributional robustness optimization offering a comprehensive theoretical foundation and practical implementation for training robust classifiers. While the paper showcases significant improvements in robustness and efficiency compared to standard baselines on MNIST and CIFAR10 datasets some sections could improve clarity for wider accessibility. Future work could focus on broader evaluation across diverse datasets and architectures along with practical considerations for implementation and hyperparameter tuning. Overall the proposed framework represents a promising advancement in addressing adversarial vulnerabilities in neural networks.  Overall recommendation: Based on the innovative methodology strong theoretical foundation and promising empirical results I recommend acceptance of the paper with minor revisions to enhance clarity and further discuss practical implementation aspects. The insights provided by the Adversarial Transport Framework make a valuable contribution to the field of adversarial robustness in neural networks.", "qrdbsZEZPZ": "1) Summary: The paper addresses the vulnerability of federated learning (FL) to poisoning attacks and explores leveraging differential privacy for robustness certification in FL. It proposes robustness certification criteria for both userlevel and instancelevel privacy in FL. The authors develop algorithms and conduct experiments on MNIST CIFAR10 and Sentiment140 datasets to verify their theories under various attacks. 2) Strengths:  The paper addresses an important and timely issue in the field of federated learning by focusing on robustness certification against poisoning attacks leveraging differential privacy.  Comprehensive theoretical analysis: The paper provides detailed definitions algorithms theorems and corollaries to support the proposed concepts.  Methodological contributions: The introduction of criteria for certified robustness in FL against poisoning attacks and algorithms to enhance privacy at both user and instance levels are significant contributions to the research area.  Empirical validation: Conducting extensive experiments on realworld datasets to verify the proposed certifications adds credibility to the theoretical claims.  Clear writing: The paper is wellstructured and easy to follow with detailed explanations of concepts approaches and results. Weaknesses:  Lack of generalizability: The focus on specific datasets (MNIST CIFAR10 Sentiment140) might limit the generalizability of the findings. Including more diverse datasets could strengthen the papers impact.  Limited discussion on practical implications: While the theoretical and empirical analyses are thorough a more indepth discussion on the practical implications of the findings for realworld FL systems could enhance the papers relevance.  Ethical considerations: The paper briefly mentions opensourced datasets and codes but could benefit from a more detailed discussion on ethical implications data privacy and fairness considerations in the context of FL. Questions and Feedback to the Author:  Can the proposed certification criteria be easily integrated into existing FL systems Are there any practical challenges in implementing the certified robustness concepts in realworld applications  Have you considered potential tradeoffs between privacy protection and model utility in FL systems when enhancing privacy guarantees for improved certified robustness  How do you plan to address the limitation of dataset generalizability and scalability in future research to make the proposed concepts applicable to a wider range of FL scenarios  It would be beneficial to elaborate on the specific ethical considerations related to differential privacy model robustness and data security in FL. Discussing implications for data privacy and fairness could enrich the papers discussion. 3) Review Summary: The paper makes significant contributions by introducing novel concepts for certified robustness in federated learning leveraging differential privacy. The comprehensive theoretical and empirical analyses provide a strong foundation for the proposed methodologies. However ensuring broader applicability and addressing ethical considerations could further strengthen the paper. Overall based on the thorough analysis clear presentation and valuable contributions I recommend accepting this paper for publication with minor revisions related to generalizability and ethical considerations.", "hcMvApxGSzZ": " Summary The paper introduces a novel steganography algorithm termed Fixed Neural Network Steganography (FNNS) that leverages the sensitivity of neural networks to small perturbations for hiding secret messages in images. FNNS achieves significantly lower error rates compared to existing methods particularly reaching a 0 error rate for hiding up to 3 bits per pixel (bpp) of information. The paper highlights the domain independence of FNNS robustness against steganalysis tools resistance to JPEG compression and introduces an application for anonymized image sharing using FNNS.  Strengths 1. Innovative Approach: The use of neural network sensitivity as a feature for steganography is a novel and innovative approach. 2. Error Rate: Achieving a 0 error rate for up to 3 bpp is a significant advancement in steganography. 3. Domain Independence: The methods ability to be domain independent provides flexibility and usability across various datasets. 4. Anonymized Image Sharing: The proposed application for face anonymization using FNNS is practical demonstrating the algorithms realworld applicability. 5. Robustness and Resistance: FNNSs resistance to steganalysis and robustness against JPEG compression add to its reliability and security.  Weaknesses 1. Optimization Time: The paper mentions longer optimization times especially for the random network variant which could limit practical scalability. 2. Image Quality: In some cases pixelation in steganographic images may occur especially with certain initialization methods or hyperparameters. 3. Error Rates Variation: While FNNSD performs well the paper notes that achieving a 0 error rate for certain datasets is more challenging.  Questions and Feedback to the Author 1. Optimization Time: Can you provide insights into potential strategies to optimize the FNNSR variant to reduce the computation time while maintaining accuracy 2. Error Rates Variation: Have you considered further experiments or strategies to address the challenge of achieving a 0 error rate for all datasets especially those where it is more difficult 3. Extension of Applications: Are there additional realworld applications you envision for FNNS beyond anonymized image sharing  Review Summary The paper presents a cuttingedge steganography algorithm FNNS that achieves remarkably low error rates and demonstrates robustness against steganalysis and JPEG compression. The methods innovative use of neural network perturbations for message hiding coupled with practical applications such as face anonymization showcases its potential for secure and practical image sharing. While the paper has notable strengths in its novelty and performance addressing issues such as optimization time and error rate variation could further solidify its impact and usability. I recommend acceptance with minor revisions to enhance clarity and address the identified weaknesses.", "nD9Pf-PjTbT": " Peer Review  1. Summary: The paper investigates the convergence behavior of the generalized belief propagation algorithm on graphs with motifs specifically focusing on ferromagnetic Ising models on loopy graphs with simple edges. It builds on existing literature on belief propagation and Bethe free energy providing insights into the behavior of the algorithm on nontreelike structures. The study demonstrates that under certain initialization conditions the generalized belief propagation converges to the global optimum of the Bethe free energy for ferromagnetic Ising models on graphs with motifs. The mathematical derivations and the optimization landscape analysis support the findings.  2. Strengths:  Original Contribution: The paper introduces a novel perspective by analyzing the convergence behavior of belief propagation on nontreelike graphs with motifs adding value to the existing body of work in the field.  Robust Methodology: The research methodology is sound drawing on established theories on belief propagation and Bethe free energy and the derivations are wellexplained providing clarity to readers.  Thorough Analysis: The mathematical derivations and the exploration of the optimization landscape are thorough enhancing the understanding of the convergence properties of the generalized belief propagation algorithm.  Discussion of Related Work: The paper effectively discusses existing literature clearly establishing the gap that this study aims to address.  3. Weaknesses:  Clarity of Exposition: The paper is heavy on mathematical formalisms which might pose a challenge for readers not deeply familiar with the topic. Consider simplifying some derivations or providing more intuitive explanations.  Experimentation: While the theoretical analysis is comprehensive the paper lacks empirical validation of the proposed convergence behavior. Incorporating numerical simulations or realworld examples could strengthen the findings.  Impact and Applications: It would be beneficial to elaborate more on the practical implications of the research findings. How could the insights gained be applied in realworld scenarios  Questions and Feedback to Authors: 1. Could you provide a more intuitive explanation of the core concepts and derivations for readers less versed in highly technical mathematics 2. Have you considered conducting numerical experiments to validate the convergence behavior observed in theoretical analysis How would such validation impact the robustness of your findings 3. It would be beneficial to expand the discussion on the practical implications of your research. How could the findings be applied in realworld scenarios beyond theoretical models  4. Review Summary: Overall the paper provides a valuable contribution to the literature by investigating the convergence behavior of belief propagation on nontreelike graphs with motifs. The rigorous mathematical analysis enhances the understanding of the algorithms performance on complex structures. However to improve the accessibility of the paper consider offering more intuitive explanations and potential realworld applications. Moreover incorporating numerical validations would enhance the reliability of the proposed convergence behavior. With these enhancements the paper has the potential to make a significant impact in the field of belief propagation algorithms on complex graphical structures.", "qyTBxTztIpQ": "Peer Review 1. Summary The paper introduces CrowdPlay a crowdsourcing framework that generates largescale human demonstration data to support research in imitation learning and offline reinforcement learning. The dataset includes human gameplay demonstrations in Atari 2600 games with multimodal behavior and both humanhuman and humanAI multiagent data. The paper presents detailed information on the architecture of CrowdPlay software components dataset scope task design recruitment channels and incentive experiments. It also evaluates the dataset using various analytical methods and benchmarks the performance of offline reinforcement learning algorithms on the gathered data. 2. Strengths and Weaknesses Strong Points:  Novelty: CrowdPlay addresses a critical gap in the intersection of crowdsourcing and reinforcement learning providing a valuable resource for advancing research on imitation learning and offline reinforcement learning.  Comprehensive Data Collection: The dataset includes diverse gameplay scenarios humanhuman interactions and multimodal behaviors enhancing the richness and complexity of the collected data.  Scalable Framework: The architecture and design of CrowdPlay demonstrate scalability for handling hundreds or thousands of concurrent users which is essential for largescale data collection.  RealTime Feedback and Incentives: The incorporation of realtime feedback and incentive models contributes to highquality data collection and participant engagement. Weak Points:  Data Quality Issues: The paper mentions challenges with unbalanced data and difficulties in collecting humanhuman multiagent data which could impact the datasets completeness and quality.  Limited Evaluation: While the paper provides indepth details on dataset creation architecture and incentive experiments a more extensive evaluation of the datasets effectiveness in improving offline learning could strengthen the paper.  Ethical Consideration: The paper briefly discusses ethical concerns related to fair compensation but could elaborate more on the ethical implications of crowdsourcing research data. Feedback to the Authors:  Could you provide more insights into how the datasets diversity and complexity contribute to improving offline reinforcement learning methods compared to existing datasets  Consider addressing the data quality challenges faced during data collection and strategies implemented to mitigate these issues to enhance transparency and reliability.  Further exploration and discussion on the ethical considerations surrounding fair compensation and participant engagement within crowdsourcing research could enrich the papers impact and relevance. 3. Review Summary Overall the paper presents a pioneering framework CrowdPlay that bridges the gap between crowdsourcing and reinforcement learning offering a valuable dataset for imitation learning and offline reinforcement learning research. The comprehensive coverage of dataset creation technical aspects experiment results and evaluation metrics demonstrates the papers robustness. However addressing data quality challenges providing deeper insights into the datasets impact on advancing offline learning and expanding on ethical considerations would further strengthen the papers contribution to the research community. Based on the strong contribution to the emerging field of imitation learning and reinforcement learning combined with thorough technical details and novel dataset characteristics I recommend accepting the paper after the authors revise the weak points and address the feedback provided. Recommendation: Accept after Revisions.", "vcUmUvQCloe": " PeerReview of \"Joint Shapley Values: Unifying and Extending Shapley\u2019s Value for Sets of Features in Machine Learning Attribution Problems\"  Summary The paper introduces the concept of Joint Shapley values which extend Shapleys axioms and intuitions to measure the average contribution of a set of features to a models prediction. It establishes the uniqueness of Joint Shapley values and compares them to existing interaction indices in games. The application of Joint Shapley values to ML attribution problems particularly with binary features is explored. The paper presents theoretical development proofs and experimental results on simulated data game theoretical models Boston housing dataset and movie review classification.  Strong Points 1. Theoretical Rigor: The paper provides a thorough theoretical foundation by extending Shapleys axioms to sets of features and proving the uniqueness of Joint Shapley values. 2. Innovative Approach: Introducing Joint Shapley values is a novel and insightful contribution to the field of ML feature importance and attribution. 3. Clarity and Structure: The paper is wellstructured with clear explanations proofs and illustrative examples that aid in understanding the concept of Joint Shapley values. 4. Practical Application: The application of Joint Shapley values to concrete datasets like the Boston housing data and movie reviews adds practical relevance to the theoretical concept.  Weak Points 1. Computational Complexity: The paper mentions high computational complexity for deriving Joint Shapley values. Providing more insights into strategies to mitigate this issue would enhance the papers utility. 2. Empirical Evaluation: While the experimental results are insightful more extensive validation on diverse datasets and comparisons with other stateoftheart methods would strengthen the paper\u2019s empirical contributions. 3. Discussion on Limitations: Addressing potential limitations or assumptions made in the model could improve the papers depth and facilitate better interpretation of the results.  Questions and Feedback to Authors 1. Could you elaborate on approaches or techniques to address the computational complexity associated with deriving Joint Shapley values especially for larger feature sets 2. How would you address potential biases or limitations in the application of Joint Shapley values when dealing with realworld complex datasets 3. Considering the practical implications how does the interpretability and scalability of Joint Shapley values compare to other prominent feature importance attribution methods in ML  Review Summary The paper presents an innovative concept of Joint Shapley values expanding Shapleys principles to evaluate the collective impact of feature sets in ML attribution. The theoretical groundwork is robust and the experiments demonstrate the practical utility of Joint Shapley values. However addressing computational complexity enhancing empirical validation and discussing potential limitations would further strengthen the paper\u2019s contribution. Overall the paper is technically sound wellstructured and makes a significant contribution to the field of ML feature attribution. Hence I recommend acceptance with minor revisions to address the identified weak points.", "vPK-G5HbnWg": " Summary: The paper introduces a new approach named Parallelizable Attentionbased Computation structure Encoder (PACE) for optimizing directed acyclic graphs (DAGs) using neural network encoders. PACE utilizes a Transformerbased framework with a dag2seq model to encode DAGs in parallel overcoming the sequential nature of existing DAG encoders. The proposed method is demonstrated to improve encoding efficiency training speed and downstream optimization performance compared to previous models.  Strengths: 1. Innovative Approach: The paper introduces a novel approach PACE to encode DAGs in parallel using Transformer architecture which is a significant advancement in the field. 2. Efficiency Improvement: PACE showcases superior encoding efficiency and training speed compared to traditional DAG encoders. 3. Experimental Validation: The paper provides a comprehensive experimental evaluation on various datasets demonstrating the effectiveness and superiority of PACE across different scenarios. 4. Clear Explanation: The paper effectively explains the motivation methodology and results making it easy for readers to understand the proposed model.  Weaknesses: 1. Complexity: The extensive technical details and terminologies used in the paper might be challenging for readers unfamiliar with the field to grasp the full scope and significance of the proposed method. 2. Limited Comparison: While the paper compares PACE with existing DAG encoders a broader comparison with other relevant methods or architectures could have further validated the superiority of PACE.  Questions to the Author: 1. How does PACE perform in terms of scalability when dealing with largescale DAG structures 2. Could you provide more insights into the interpretability of the learned latent spaces by PACE and how they benefit downstream tasks 3. Are there any realworld applications or use cases where PACE has been applied or shows promise  Feedback to the Author:  Consider providing a more simplified explanation of technical details for easier comprehension by a wider audience.  Expand the comparison section to include more diverse benchmark models for a more comprehensive evaluation.  Review Summary: The paper introduces a novel Parallelizable Attentionbased Computation structure Encoder (PACE) for efficiently optimizing directed acyclic graphs using Transformer architecture. The proposed method shows promising results in terms of encoding efficiency training speed and downstream optimization performance. The study is wellstructured with detailed experimental validations showcasing the superiority of PACE over existing methods. However the technical complexity and limited comparison with other methods could be addressed to enhance the papers impact. Overall the innovative approach and significant performance improvements demonstrated by PACE make it a valuable contribution to the field of DAG optimization.", "l3SDgUh7qZO": " Peer Review:  1) Summary: The paper introduces a novel deep face recognition framework called SphereFace2 which addresses the limitations of conventional deep face recognition methods trained using a softmaxbased multiclass classification framework. SphereFace2 is based on binary classification and pairwise learning paradigm which aims to bridge the gap between training and testing sets by constructing K binary classification tasks. The paper provides indepth discussions on the design principles of the loss function in SphereFace2 and demonstrates its effectiveness through ablation studies exploratory experiments evaluations on largescale benchmarks (IJB and MegaFace datasets) and robustness testing against label noise. The paper also highlights the efficient model parallelization capabilities of SphereFace2 and compares it with existing stateoftheart methods in terms of accuracy and model performance.  2) Strengths:  Innovative Approach: The paper proposes a novel binary classification framework SphereFace2 which addresses limitations of conventional methods by focusing on pairwise learning and binary classification leading to significant improvements in face recognition performance.  Comprehensive Experiments: The paper provides extensive experimental results on various datasets ablation studies and comparisons with existing methods demonstrating the superiority of SphereFace2 in face recognition tasks.  WellStructured and Detailed: The paper is wellstructured providing clear explanations of the proposed method design principles experimental setups results and comparisons. The detailed discussions on the loss function design principles and hyperparameter settings are particularly informative.  Robustness Evaluation: The paper conducts robustness testing against label noise and demonstrates the robustness of SphereFace2 in noisy label learning scenarios which adds value to the reliability of the proposed method.  Weaknesses:  Complexity: The paper involves complex technical discussions and mathematical formulations which may make it challenging for some readers especially those lacking a strong background in deep learning.  Visualization: While the paper provides thorough textual descriptions the inclusion of more visual aids like diagrams graphs or tables could enhance the clarity and understanding of the concepts presented especially in the design principles and hyperparameter analysis sections.  Impact Comparison: The paper could benefit from a more detailed discussion on the potential realworld applications and practical implications of SphereFace2 compared to existing stateoftheart methods highlighting its novelty and impact in face recognition tasks.  Questions and Feedback to the Author: 1. Can you provide more insights into the potential scalability of SphereFace2 for largescale datasets beyond the discussed model parallelization capabilities 2. How does SphereFace2 perform in scenarios with varying degrees of data imbalance among classes and does it show resilience to classwise label noise even in highly imbalanced datasets 3. Have you considered exploring the interpretability aspects of SphereFace2 such as visualizing the learned features or decision boundaries to gain more insights into the models working mechanism 4. It would be interesting to see a comparison of the computational efficiency and resource consumption of SphereFace2 with conventional softmaxbased methods especially in terms of memory usage and training time.  Review Summary: The paper presents SphereFace2 a novel binary classification framework for deep face recognition. Through extensive experiments and analysis the paper demonstrates the superiority of SphereFace2 over existing stateoftheart methods in terms of accuracy robustness to label noise and scalability for largescale datasets. The detailed discussion on design principles loss function hyperparameters and efficient model parallelization provides a comprehensive understanding of SphereFace2. While the paper is technically rich and wellstructured enhancing visual aids for better comprehension and exploring additional realworld applications could further strengthen its impact and appeal to a wider audience. Based on the innovative approach robust experimental results and thorough analysis provided I recommend accepting this paper for publication in a reputable conference or journal.", "zfmB5vgfaCt": " Summary of the Paper: The paper investigates the efficiency robustness of neural machine translation (NMT) systems by introducing a new attack approach called TranSlowDown. The study reveals that by slightly modifying input sentences adversaries can significantly increase the computational resources consumed by NMT systems impacting their efficiency. TranSlowDown is designed to generate adversarial examples that delay the generation of the endofsequence (EOS) token leading to longer outputs and increased computational overhead. The evaluation on three NMT systems demonstrates the effectiveness of TranSlowDown in increasing response latency energy consumption and battery power usage. The paper highlights the need for protecting NMT systems against efficiencyoriented threats.  Strengths of the Paper: 1. Novel Contribution: The paper addresses an important but less explored aspect of NMT systems by focusing on efficiency robustness and introducing a new attack approach. 2. Experimental Rigor: The systematic evaluation on three public NMT systems provides comprehensive insights into the impact of TranSlowDown on efficiency metrics. 3. RealWorld Implications: The study includes a case study on mobile devices demonstrating the practical implications of efficiency attacks on battery power consumption.  Weaknesses of the Paper: 1. Lack of Comparison: While the paper compares TranSlowDown with accuracybased attacks a comparison with other efficiencyoriented attacks or defense mechanisms would enhance the depth of the analysis. 2. Theoretical Explanation: The paper could benefit from further explaining the theoretical foundations behind the efficiency vulnerabilities of NMT systems and the rationale behind the TranSlowDown attack methodology.  Questions for the Authors: 1. How do you envision the practical implementation of defense mechanisms to mitigate efficiencybased attacks on NMT systems 2. Could TranSlowDown be adapted to target specific vulnerabilities in different NMT architectures or is its effectiveness limited to the models tested in this study 3. Have you considered the potential ethical implications of utilizing efficiency attacks in the domain of machine translation and the impact on users experience and trust  Feedback for the Authors: 1. Providing additional insights on the theoretical underpinnings of efficiency vulnerabilities in NMT systems and the generalizability of TranSlowDown across various NMT architectures would further enrich the paper. 2. Consider expanding the discussion on the implications of efficiency attacks on the broader landscape of AI security and the development of robust defense strategies. 3. It would be valuable to discuss potential future research directions in enhancing the efficiency robustness of NMT systems and exploring countermeasures against emerging efficiency attacks.  Review Summary: The paper makes a significant contribution to the field by shedding light on the vulnerability of NMT systems to efficiency attacks particularly through the TranSlowDown methodology. The experimental evaluation provides compelling evidence of the impact of this attack approach on NMT system efficiency. However further elaboration on the theoretical foundation comparative analysis and realworld implications could enhance the overall quality of the paper. The innovative nature of TranSlowDown and its potential to stimulate advancements in defending against efficiencyoriented threats underline the importance of this research. Consequently based on the provided strengths weaknesses questions and feedback I recommend acceptance with revisions to address the identified areas for improvement and enhance the clarity and impact of the findings.", "iRCUlgmdfHJ": " Summary: This paper investigates the representation bottleneck of deep neural networks (DNNs) in encoding interactions between input variables. The authors focus on the phenomenon that DNNs tend to encode too simple or too complex interactions but struggle to learn interactions of intermediate complexity. The paper provides theoretical proof for the existence of this bottleneck and proposes losses to guide DNNs in learning interactions of specific complexities. The authors analyze the representation capacities of DNNs with different orders of interactions and experimentally validate the effectiveness of their proposed method.  Strengths:  Theoretical Contribution: The paper provides a theoretical explanation for the representation bottleneck observed in DNNs which is a valuable contribution to the field.  Methodology: The use of multiorder interactions to analyze the complexity of interactions encoded in DNNs is a novel and insightful approach.  Experimental Validation: The experiments conducted to validate the proposed losses and analyze the representation capacities of DNNs are thorough and provide evidence for the effectiveness of the method.  Insightful Findings: The discovery of the representation bottleneck and the observation that highorder interactions encode more structural information are significant findings.  Weaknesses:  Analytical Complexity: The papers content and technical details may be challenging for readers not deeply familiar with the intricacies of deep learning and neural networks.  Experimental Setup: It would be beneficial to provide more details on the experimental setup and datasets used to ensure reproducibility and transparency.  Questions for the Authors: 1. Can you elaborate on how the proposed losses to guide DNNs in learning interactions of specific complexities can be generalized to different types of tasks beyond image classification 2. How do you envision the practical implications of addressing the representation bottleneck in DNNs for realworld applications of artificial intelligence 3. Could you discuss potential limitations or challenges in implementing the proposed method in largescale or more complex neural network architectures  Feedback:  Clarity and Accessibility: Consider enhancing the clarity and accessibility of the paper for a broader audience by simplifying complex technical jargon without compromising on the depth of the content.  RealWorld Applications: Providing concrete examples of how addressing the representation bottleneck can impact practical AI applications would strengthen the papers relevance and significance.  Future Directions: Discuss potential future research directions or extensions of the proposed method to further advance the understanding and improvement of feature representations in deep neural networks. Overall the paper presents a thoughtprovoking analysis of the representation bottleneck in DNNs and offers insightful solutions to address this issue. With further refinement and clarity in presentation the research findings have the potential to make a significant impact in the field of artificial intelligence and deep learning.", "zKbMQ2NY1y": " 1) Summary of the Paper: The paper revisits the Intermediate Level Attack (ILA) and proposes AugILA an improved method that enhances the transferability of existing attacks under the ILA framework through the application of image augmentation techniques. The authors evaluate the effect of common image augmentation techniques and explore novel augmentations with the aid of adversarial perturbations. AugILA incorporates typical image augmentation reverse adversarial update on the clean image and attack interpolation between two reference attacks. Experimental results demonstrate that AugILA outperforms ILA and its variants as well as stateoftheart transferbased attacks achieving high attack success rates on nine undefended models.  2) Evaluation of Strong and Weak Points Questions and Feedback:  Strong Points:  Thorough Experimentation: The paper conducts extensive experiments to analyze the effectiveness of different image augmentation techniques in enhancing attack transferability.  Novel Contribution: Introducing AugILA as an improved method that outperforms existing approaches in enhancing blackbox transferability.  Clear Presentation: The paper presents a detailed description of the proposed method and provides clear experimental results.  Weak Points:  Lack of Comparison Metrics: While the experimental results show the superiority of AugILA a comparison with more stateoftheart methods beyond the ones mentioned could provide a broader view of its effectiveness.  Generalization Concerns: The paper focuses on transferability across models trained on ImageNet limiting the generalization of the proposed method to other datasets or applications.  Questions for the Authors: 1. How does the proposed AugILA method perform in scenarios with different datasets or model architectures 2. Can the AugILA methodology be adapted to address specific types of adversarial attacks beyond the ones discussed in the paper 3. Have you considered the computational complexity of AugILA in comparison to other stateoftheart methods  Feedback to the Authors:  It would be beneficial to provide more insights into the scalability and generalizability of AugILA by testing it on a wider range of datasets and deep learning models.  Consider discussing potential limitations or challenges in implementing AugILA in realworld scenarios to provide a more holistic view of the methods applicability.  3) Review Summary: The paper presents a novel approach AugILA that enhances the transferability of attacks under the ILA framework through image augmentation techniques. The comprehensive experimental results demonstrate the superiority of AugILA over existing methods in achieving high attack success rates on various models. While the paper is wellstructured and provides clear details on the methodology and results addressing concerns about generalization performance metrics and scalability could further strengthen the contribution of AugILA in the field of adversarial attacks. Overall based on the detailed analysis and contributions of the paper I recommend acceptance pending the authors response to the questions and feedback provided.", "zPLQSnfd14w": " Summary The paper introduces generalization guarantees for metric learning with neural network embeddings. It focuses on nonlinear embeddings and provides bounds for different regimes based on the sparsity of the weight matrices. The paper explores the behavior of networks trained using stochastic gradient descent (SGD) on the MNIST and 20newsgroups datasets demonstrating both sparse and dense regimes occurring in realistic data.  Strong Points 1. Novel Contribution: The paper addresses an important gap in understanding generalization guarantees for nonlinear embeddings in metric learning. 2. Theoretical Analysis: The paper provides detailed theoretical analyses and proofs for the bounds on generalization guarantees in the context of neural network embeddings. 3. Empirical Validation: The empirical evaluation on real datasets like MNIST and 20newsgroups enhances the practical relevance of the findings. 4. Clarity and Structure: The paper is wellstructured and effectively conveys complex mathematical concepts in a clear manner.  Weak Points 1. Limited Practical Implications: While the theoretical analysis is thorough the practical implications or applications of the findings could be further elaborated. 2. Assumptions and Limitations: The discussion on the limitations or potential drawbacks of the proposed bounds especially in realworld applications could be explored in more detail.  Questions 1. How do the proposed bounds on generalization guarantees translate into practical guidelines for designing and training neural networks in metric learning tasks 2. Are there any considerations or modifications needed for extending these bounds to more complex neural network architectures beyond the ones considered in the paper 3. Have you considered comparing the proposed bounds with existing metrics or evaluation methods in the field of metric learning to showcase the advantages of the new approach  Feedback  The papers thorough theoretical analysis and empirical validation provide a solid foundation for understanding generalization guarantees in the context of metric learning with neural network embeddings.  Strengthening the discussion on practical implications and the relationship of the proposed bounds with realworld applications would further enhance the relevance of the research.  Addressing the limitations and potential extensions of the proposed bounds could lead to valuable insights for future research in this area. Overall the paper makes a significant contribution to the field of metric learning but by further discussing practical implications and limitations it can broaden its impact and appeal to a wider audience. Further clarifications on the applicability and scalability of the proposed bounds would strengthen the papers contribution.", "gRCCdgpVZf": " Review of \"ZeroShot Domain Adaptation with LowRank Tensor Structure\"  1. Summary: The paper presents a novel approach for zeroshot domain adaptation by introducing a model that utilizes a lowrank tensor structure to capture the relationships among different domains. The model consists of a domaininvariant latent representation layer and a domainspecific linear prediction layer with a lowrank tensor structure. The paper offers theoretical sample complexity bounds to characterize the prediction error on unseen domains. Experimental results on MNIST fiber sensing and GTOS datasets demonstrate the effectiveness of the proposed method.  2. Strengths:  Theoretical Rigor: The paper provides explicit sample complexity bounds and analysis for the proposed model.  Innovative Approach: The introduction of a lowrank tensor structure for domain adaptation is novel and can potentially provide better performance.  Experimental Validation: Results on various datasets showcase the effectiveness of the proposed model compared to existing methods.  Clarity and Organization: The paper is wellstructured and clearly explains the problem contributions and methodology.  3. Weaknesses:  Theoretical Assumptions: Some assumptions regarding the realizable learning problem and strong convexity might limit the practical applicability of the proposed method.  Complexity: The methods complexity may hinder its applicability in largescale datasets or realworld scenarios.  Evaluation Metrics: The paper could benefit from a more detailed comparison with existing approaches in terms of additional metrics beyond prediction accuracy.  Experimental Setup: The paper could provide more insight into hyperparameter tuning and additional experimental scenarios for a comprehensive evaluation.  Questions and Feedback: 1. Scalability Concerns: How does the proposed model scale with increasing dataset sizes or highdimensional data 2. Practical Implementation: What challenges might practitioners face in implementing the lowrank tensor structure in realworld applications 3. Generalization Capability: How well does the model generalize to unseen domains in extremely diverse or outlier situations not covered in the training data 4. Comparison with StateoftheArt: Can the paper provide a more extensive comparison with other recent domain adaptation methods in terms of interpretability and computational efficiency  Review Summary: The paper presents an innovative model for zeroshot domain adaptation using a lowrank tensor structure providing theoretical bounds and experimental validation across various datasets. While the proposed method demonstrates promising results addressing scalability concerns practical implementation challenges and further comparisons with stateoftheart methods can enhance the overall impact of the research. Overall the paper provides significant contributions to the field of domain adaptation but could benefit from additional analysis and considerations for realworld applicability.", "sRZ3GhmegS": " Summary: The paper introduces Contrastive BERT for RL (COBERL) an agent that aims to improve data efficiency in reinforcement learning tasks by combining a new contrastive loss with a hybrid LSTMtransformer architecture. It addresses the challenges of learning from raw highdimensional inputs in RL by leveraging techniques inspired by BERT and contrastive representation learning methods. The proposed agent COBERL is extensively tested across various domains including the Atari suite control tasks and 3D environments demonstrating improved data efficiency and often increased final score performance.  Strengths: 1. Innovative Approach: The paper introduces a novel agent COBERL that combines contrastive learning with a hybrid LSTMtransformer architecture showcasing an innovative approach to improving data efficiency in RL. 2. Extensive Evaluation: The agent is rigorously tested across a wide range of environments and tasks including Atari games control tasks and 3D environments demonstrating its versatility and effectiveness. 3. Empirical Results: The experimental results show consistent improvements in data efficiency and performance in various domains indicating the effectiveness of the proposed methods.  Weaknesses: 1. Clarity of Explanations: The technical details and methodology could be dense and challenging to grasp for readers not wellversed in reinforcement learning and neural network architectures. 2. Experimental Setup: While the experiments are thorough some details on hyperparameter tuning reproducibility and computational resources used can be further elaborated for transparency and to facilitate reproducibility by other researchers. 3. Comparison with Baselines: While the paper discusses performance comparisons with existing methods like R2D2 and GTrXL a more detailed comparison with a broader range of stateoftheart RL agents could provide a more comprehensive evaluation.  Questions and Feedback to the authors: 1. Could you provide more insights into the hyperparameter tuning process especially in scenarios where multiple hyperparameters were optimized to achieve the reported results 2. It would be beneficial to elaborate on the computational resources used for training COBERL including GPU specifications training time and any additional hardwaresoftware configurations that were crucial for the experiments. 3. Considering the comprehensive evaluation conducted how does COBERL compare to the latest advancements in RL agents in terms of sample efficiency scalability and generalizability across diverse environments  Review Summary: The paper presents a novel approach COBERL that combines contrastive learning with a hybrid LSTMtransformer architecture to enhance data efficiency in reinforcement learning tasks. The experimental results demonstrate notable improvements in efficiency and performance across various domains. While the paper showcases significant contributions to the field of RL addressing potential communication clarity issues and providing additional insights on the experimental setup could enhance the impact and reproducibility of the research. Overall based on the innovative approach and promising results I recommend considering this paper for publication pending minor revisions and clarifications.", "mqIeP6qPvta": " 1) Summary of the Paper: The paper introduces the FoveaTer model which combines a foveated architecture with a transformer network for object classification tasks. By pooling image features in a foveated manner and making sequential fixations guided by selfattention weights the model reduces computational complexity while maintaining accuracy. It also dynamically allocates computational resources based on image difficulty and shows robustness against adversarial attacks.  2) Evaluation of Strong and Weak Points: Strong Points:  Innovative Approach: The paper introduces a novel approach by combining foveated vision with transformer networks for image classification tasks.  Efficient Computational Model: The FoveaTer model efficiently reduces computational resources by using a foveated architecture and making sequential fixations.  Robustness: The model demonstrates robustness against adversarial attacks outperforming fullresolution models. Weak Points:  Theoretical Explanations: The paper lacks detailed theoretical explanations for certain decisions made in the model architecture.  Experimental Setup: More details on hyperparameters selection and training setup would enhance reproducibility.  Comparative Analysis: Further comparisons with other stateoftheart models or benchmark datasets could strengthen the evaluation. Question to the Author:  Could you elaborate on how the FoveaTer models fixation exploration strategy adapts to different image classes and allocates resources based on image complexity  How does the models foveated architecture handle occlusions or object variations across fixations Feedback to the Author:  Consider providing a more detailed discussion on the theoretical motivations behind the models design choices for better understanding.  Enhance reproducibility by providing comprehensive details on hyperparameters selection and training setup in the paper.  3) Review Summary: The paper presents a novel FoveaTer model that combines a foveated architecture with a transformer network demonstrating efficient scene exploration for object classification tasks. The model achieves high accuracy while significantly reducing computational complexity outperforming fullresolution models and showing robustness against adversarial attacks. The innovative approach and strong experimental results make a compelling case for the effectiveness of the FoveaTer model. However providing more theoretical insights experiment details and comparative analyses could further strengthen the paper. Overall based on the presented framework and results I recommend accepting the paper after addressing the outlined feedback for a more comprehensive and impactful contribution to the field.", "noaG7SrPVK0": " Summary: The paper addresses the challenge of generating counterfactual plans under model uncertainty in the context of machine learning explanations. It introduces an uncertainty quantification tool to compute the bounds of the probability of validity for a given counterfactual plan. Corrective methods are provided to adjust the plan and improve its validity measure. Additionally a COunterfactual Plan under Ambiguity (COPA) framework is proposed to construct counterfactual plans considering model uncertainty. The work is validated through numerical experiments on both synthetic and realworld datasets.  Strong Points: 1. Thorough Analysis: The paper provides a comprehensive study of counterfactual plans under model uncertainty addressing the need for robustness when model parameters shift. 2. Novel Contributions: The proposed uncertainty quantification tool corrective methods and the COPA framework introduce innovative approaches to enhance the validity of counterfactual plans. 3. Clear Presentation: The paper is wellstructured with a clear presentation of the problem methodology and experimental results facilitating understanding for readers. 4. Validation Through Experiments: The experiments conducted on both synthetic and realworld datasets provide concrete evidence of the effectiveness of the proposed methods. 5. Quantitative Metrics: The use of empirical validity lower validity bound proximity and diversity as evaluation metrics adds depth to the analysis of the proposed approaches.  Weak Points: 1. Complexity: The theoretical framework involving semidefinite programming and conic optimization may be challenging for readers without a strong background in optimization theory. 2. Algorithmic Details: While the paper outlines the proposed methods clearly additional details on algorithm implementation and computational complexity could be beneficial for reproducibility. 3. Generalization: The discussion on how the proposed methods could be extended to handle nonlinear models or other contexts beyond the linear classification setting would enhance the broader applicability of the research.  Questions and Feedback to the Author: 1. Clarity on Code Availability: Will the code for implementing the proposed methods be made publicly available to enable reproducibility and further research in this area 2. Scalability Considerations: Have you tested the scalability of the proposed methods with larger datasets or higherdimensional feature spaces to assess computational efficiency 3. Realworld Applicability: How do you envision the practical implementation of your proposed methods in realworld machine learning applications especially in domains requiring explainable AI  Review Summary: The paper presents a significant contribution to the field of machine learning explanations by addressing the challenge of generating robust counterfactual plans under model uncertainty. The novel uncertainty quantification tool corrective methods and the COPA framework show promising results in enhancing the validity of counterfactual plans. The comprehensive experimental validation further strengthens the credibility of the proposed approaches. The clear presentation of the research supported by quantitative metrics enhances the impact and relevance of the work. Addressing minor weaknesses related to complexity and generalization will further enhance the papers value. Overall based on the strong points and potential impact of the proposed methods I recommend this paper for publication.", "iMSjopcOn0p": "Summary: The paper presents a unified framework for MultiTask Multitrack Music Transcription (MT3) using a generalpurpose Transformer model. The authors demonstrate that their model referred to as MT3 can transcribe multiple instruments simultaneously across various datasets. They address the challenges of lowresource music datasets dataset diversity and inconsistent evaluation metrics. The MT3 model outperforms existing models on six datasets including both high and lowresource datasets achieving stateoftheart results. Strong Points: 1. Unified Training Framework: The use of a generalpurpose Transformer model for multitask music transcription is a significant strength showcasing the versatility and effectiveness of such an approach. 2. Benchmark Dataset Collection: The assembly of a diverse set of datasets for multitask AMT training provides a comprehensive evaluation platform and enhances the generalizability of the proposed model. 3. Consistent Evaluation: The introduction of standard test set splits and a new instrumentsensitive transcription metric improves evaluation consistency and facilitates fair performance comparison across different datasets. 4. SOTA Baseline: The MT3 model achieves stateoftheart performance on all six datasets outperforming existing datasetspecific models and professional DSPbased transcription software. 5. Improvement for LowResource AMT: The significant performance gain observed for lowresource datasets along with strong performance on highresource datasets demonstrates the models effectiveness especially for challenging scenarios. Weak Points: 1. Scalability: The paper touches on the challenge of scaling Transformer models for large audio sequences due to memory constraints which could limit the practical applicability of the proposed model in realworld scenarios. 2. Instrument Labeling: While the model performs well in predicting note onsets and offsets there might be room for improvement in instrument labeling accuracy especially with ambiguous instrument distinctions. 3. Model Interpretability: Although the model achieves impressive results providing insights into how the model makes transcription decisions could enhance its interpretability and applicability in music transcription applications. Questions to the Author: 1. Can you elaborate on the potential scalability challenges especially in processing longer audio sequences with Transformer models 2. How do you envision addressing the issue of instrument label granularity to improve instrument distinction accuracy in future models 3. Could you provide insights into potential strategies for integrating unsupervised or semisupervised learning to leverage unlabeled data for music transcription tasks Feedback: The paper presents a novel and impactful contribution to the field of MultiTask Multitrack Music Transcription through the development of the MT3 model. The comprehensive evaluation consistent metrics and significant performance gains on both high and lowresource datasets establish the superiority of the proposed approach. Addressing scalability concerns refining instrument labeling accuracy and enhancing model interpretability could further strengthen the models applicability and impact in realworld music transcription applications. Overall the paper is wellstructured rigorously researched and provides valuable insights that advance the stateoftheart in music transcription. Review Summary: I recommend accepting the paper for publication due to its innovative approach substantial experimental validation and superior results in MultiTask Multitrack Music Transcription. The proposed MT3 model outperforms existing methods particularly on lowresource datasets showcasing its robustness and adaptability. Addressing scalability instrument labeling accuracy and model interpretability can further enhance the models utility and advance future research in this domain. The papers thoroughness clarity and impactful results make it a valuable contribution to the field of music transcription.", "t2LJBsPxQM": " Summary of the Paper: The paper proposes a theoretical framework for enforcing exact orthogonality in convolutional neural networks addressing challenges in achieving orthogonality in deep architectures. The framework establishes the equivalence between orthogonal convolutions in the spatial domain and paraunitary systems in the spectral domain enabling precise parameterization of separable orthogonal convolutions. The study provides insights into designing deep orthogonal networks exploring skip connections initialization methods stride and dilation choices. Extensive experiments demonstrate the effectiveness of the proposed framework in scaling orthogonal networks to deep architectures outperforming shallower counterparts in terms of performance and memory efficiency.  Strengths of the Paper: 1. Innovative Framework: The proposed paraunitary framework offers a novel perspective on enforcing exact orthogonality in convolutional networks addressing challenges in achieving orthogonality in deep architectures. 2. Comprehensive Analysis: The paper thoroughly evaluates the frameworks effectiveness through extensive experiments showcasing its superiority in terms of performance robustness and memory efficiency. 3. Clear Presentation: The paper is wellstructured and clearly presents the theoretical framework design considerations experimental results and implications.  Weaknesses of the Paper: 1. Complexity: The theoretical aspects and mathematical formulations may be challenging for readers not familiar with spectral domain concepts or paraunitary systems. 2. Limited RealWorld Applications: While the study discusses the implications for deep architectures the practical application of the proposed framework in diverse realworld scenarios is not elaborated. 3. Technical Jargon: The paper contains technical terms and notations that may require domain expertise to fully comprehend possibly limiting accessibility.  Questions and Feedback to the Author: 1. Clarity on Practical Implementation: Can the authors elaborate on the practical implementation aspects of the proposed framework particularly in terms of deployment in realworld applications and scalability to different types of networks 2. Comparison with Existing Methods: How does the proposed method compare to existing approaches in terms of computational efficiency scalability and performance in handling complex network architectures 3. Generalization and Robustness: Have you conducted experiments to evaluate the frameworks generalization capabilities across various datasets and its robustness in adversarial settings 4. Future Directions: What are the next steps in your research Are there plans to extend the framework to address additional challenges or explore applications in other domains  Review Summary: The paper presents a pioneering theoretical framework for achieving exact orthogonality in convolutional neural networks offering valuable insights into designing deep orthogonal networks. Despite the complexity of the theoretical constructs the comprehensive analysis and experimental results convincingly demonstrate the frameworks effectiveness in scaling orthogonal networks to deep architectures. Addressing certain limitations and providing further clarity on practical implementations would strengthen the papers impact. Overall based on the innovative approach thorough evaluation and significant contributions to the field I recommend acceptance of the paper with minor revisions to enhance clarity and applicability.", "twgEkDwFTP": " Summary of the Paper The paper investigates the overfitting phenomenon in reweighting algorithms used for improving fairness in machine learning models. The authors provide a theoretical backing to the empirical results and prove that reweighting algorithms always overfit. They analyze the impact of regularization and demonstrate that large regularization is necessary to prevent overfitting and achieve high worstgroup test performance. The study covers linear models linearized neural networks and wide fullyconnected neural networks.  Strong Points: 1. Theoretical Rigor: The paper presents a rigorous theoretical analysis of the implicit biases of reweighting algorithms providing insights into why these algorithms tend to overfit in practice. 2. Empirical Validation: The authors conduct experiments on real datasets to validate their theoretical results enhancing the robustness and practical relevance of their findings. 3. Novel Contributions: The paper makes novel contributions by delving into the overfitting behavior of reweighting algorithms and analyzing the impact of regularization on worstgroup test performance. 4. Clarity and Structure: The paper is wellstructured with a clear introduction detailed theoretical analysis and empirical study making it easy to follow for readers.  Weak Points: 1. Complexity: The papers content is highly technical and might be challenging for readers without a strong background in machine learning theory. 2. Limited Discussion on Practical Implications: While the paper provides valuable theoretical insights it could benefit from a more detailed discussion on the practical implications of the findings for realworld applications.  Questions for the Authors: 1. Have you considered exploring alternative approaches to address the overfitting issue in reweighting algorithms apart from regularization For instance could ensemble methods or transfer learning be viable solutions 2. How do you envision practitioners incorporating the findings from your study into the development of fair machine learning models in diverse applications  Feedback for Authors:  Provide a clearer discussion on the practical implications of the studys findings including recommendations for practitioners working on fairness in machine learning.  Consider elaborating on the potential limitations or assumptions of the theoretical analysis to provide a more comprehensive view of the research.  Review Summary: The paper offers a thorough theoretical analysis of the overfitting behavior in reweighting algorithms for fairness in machine learning models. While the study provides valuable insights and empirical validation the complexity of the content might limit its accessibility to a broader audience. To enhance the impact and applicability of the research the authors should provide clearer practical implications and consider discussing alternative solutions to mitigate overfitting in reweighting algorithms. Overall the paper presents significant contributions to the field of fairness in machine learning warranting further exploration and practical application.", "u4C_qLuEpZ": "Summary: The paper proposes a new program analysis model that utilizes graph neural networks from the level of assembly code to solve complex program analysis tasks. It abstracts programs into multiple graphs extracts various program structure information and performs multidimensional analysis. The model builds program embeddings based on assembly code allowing for analysis of both sourcecoderelated and compilationrelated tasks. The methodology involves using graph neural networks to represent the semantics of different graph structures in the program such as control flow graph call graph and data flow graph. The proposed model achieves high accuracy in program classification and binary similarity detection tasks. Strengths: 1. Innovative Approach: The paper introduces a novel approach that integrates graph neural networks with assembly code for program analysis addressing both program semantics and compilationrelated issues. 2. Comprehensive Analysis: The model considers multiple program structures including control flow graphs call graphs and data flow graphs for a thorough understanding of program characteristics. 3. Performance Improvement: The proposed model achieves significantly higher accuracy in program classification and binary similarity detection compared to stateoftheart approaches showcasing its effectiveness. 4. Detailed Methodology: The paper provides a detailed methodology explaining how program embeddings are constructed and how different graphs are utilized for analysis tasks. Weaknesses: 1. Complexity of Graph Structures: The paper mentions the challenge of handling largescale graphs with numerous nodes and edges which may lead to long training times. It could provide more insights on how this challenge is addressed in practice. 2. Limited Empirical Evaluation: While the results show improved accuracy additional experiments on diverse datasets and comparisons with more models could strengthen the validation of the proposed approach. Question and Feedback to Author: 1. Scalability Concerns: How does the model handle scalability issues related to large graph structures especially considering the complexity of data flow graphs with thousands of nodes and edges 2. Generalizability: How well do you think the proposed model would perform on different program analysis tasks or datasets beyond the ones tested in this study Review Summary: The paper presents a sophisticated model that leverages graph neural networks on assembly code to address diverse program analysis tasks. The approach is innovative demonstrating significant accuracy improvements in program classification and binary similarity detection. While the methodology is detailed and the results are promising the paper could benefit from more empirical evaluations on varied datasets and a deeper exploration of scalability challenges. Overall the papers novel approach and strong performance make it a valuable contribution to the field of program analysis. Recommendation: I recommend accepting the paper with minor revisions to address scalability concerns and request additional comparative experiments for further validation.", "wv6g8fWLX2q": " Summary of the Paper: The paper introduces a new model called TimeAware Multipersistence SpatioSupra Graph Convolutional Networks (TAMPS2GCNets) which integrates multipersistence (MP) with timeaware deep learning for multivariate time series forecasting tasks. The model leverages topological data analysis tools to capture hidden timeconditioned properties in complex dynamic phenomena. Key components include a Dynamic EulerPoincar\u00e9 Surface summary for timeconditioned topological properties and a novel supragraph convolutional module for learning spatial and temporal correlations in highdimensional data. Extensive experiments on highway traffic flow Ethereum token prices and COVID19 hospitalizations demonstrate superior performance over stateoftheart methods.  Strengths: 1. Novel Integration of Research Directions: The paper bridges two emerging research directions multipersistence and timeaware deep learning to propose a unique model that enhances understanding of complex dynamic phenomena. 2. Innovative Model Components: The introduction of the Dynamic EulerPoincar\u00e9 Surface summary and supragraph convolutional module demonstrates innovation in capturing timeconditioned topological properties and learning spatialtemporal correlations. 3. Extensive Experimental Evaluation: The paper conducts thorough experiments on realworld datasets showcasing the superiority of the proposed model over existing methods in multivariate time series forecasting tasks. 4. Comparative Analysis: The ablation studies and comparison with various baselines provide a comprehensive evaluation of the proposed models components highlighting their individual contributions to the overall performance. 5. Ethical Considerations: The paper addresses foreseeable ethical implications of the methodology and foresees positive impacts on biosurveillance through accurate tracking of diseases like COVID19.  Weaknesses: 1. Complexity and Readability: The papers technical depth may pose challenges for readers unfamiliar with the specific domains of topological data analysis and deep learning. 2. Limited Discussion on Limitations: While strengths are well presented addressing potential limitations or constraints of the proposed model could provide a more balanced view.  Questions and Feedback for the Author: 1. Clarification on Implementation: Can the author provide insights into the implementation details of the TAMPS2GCNets model especially regarding the architecture and optimization techniques used 2. Generalization and Transferability: How adaptable is the proposed model to other domains beyond the ones tested in the experiments Have there been considerations on the models generalization and transferability to diverse datasets 3. Scalability: Given the computational efficiency of DEPS over other MP summaries has the scalability of the model been evaluated for larger datasets or realtime applications 4. Ethical Implications: In addition to the positive impacts anticipated have broader ethical considerations related to data privacy bias or interpretability been considered in the context of implementing this model in realworld scenarios  Review Summary: The paper presents a novel and innovative approach by combining multipersistence with timeaware deep learning in the context of multivariate time series forecasting. The proposed TAMPS2GCNets model showcases superior performance on realworld datasets emphasizing its effectiveness in capturing timeconditioned topological properties and learning spatialtemporal correlations. While the experimental results and comparative analysis demonstrate the strengths of the model further clarity on implementation details generalizability scalability and ethical considerations would enhance the comprehensive understanding and applicability of the proposed methodology. Overall the paper represents a significant contribution to the field and warrants consideration for publication after addressing minor points and providing additional insights into the mentioned aspects.", "t7y6MKiyiWx": "1) Summary of the paper: The paper introduces a novel neural network layer called Pyramidal Circuit which implements an orthogonal matrix multiplication. Inspired by quantum computing this layer enables gradient descent with perfect orthogonality providing efficient training for neural networks. The paper discusses the theoretical underpinnings design and training methods of the Pyramidal Circuit. It includes detailed explanations of how the circuit preserves orthogonality loading data feedforward pass training by updating gate angles and numerical experiments to validate its performance. 2) Strengths and Weaknesses Questions and Feedback: Strong points:  The paper proposes an innovative Pyramidal Circuit layer to achieve perfect orthogonality in neural networks enhancing accuracy and gradient stability.  Integration of quantuminspired concepts for classical neural network training is a unique and intriguing approach.  The theoretical insights regarding how the Pyramidal Circuit maps to orthogonal weight matrices are wellelaborated.  The numerical experiments conducted on MNIST dataset demonstrate the effectiveness of the proposed method in practice.  The classical implementation of the Pyramidal Circuit for efficient training and inference provides practical applicability. Weak points:  The paper could benefit from a clearer explanation of the potential advantages of using the proposed Pyramidal Circuit compared to existing methods.  More discussion on the limitations or challenges associated with the proposed method would enhance the papers thoroughness.  Providing a comparison with stateoftheart methods for ensuring orthogonality in neural networks could further strengthen the papers contribution.  Elaborating on the scalability and generalizability of the Pyramidal Circuit for larger datasets or complex architectures would be valuable. Questions and Feedback to the authors:  Could you elaborate further on the computational complexity of the Pyramidal Circuit in practice How does it scale with larger neural network architectures or datasets  It would be beneficial to discuss the tradeoffs between training efficiency and model performance when using the Pyramidal Circuit compared to traditional methods.  Have you considered the impact of noise or error rates in practical quantum implementations of the Pyramidal Circuit and how might this affect its performance  Including more indepth analyses of the experiment results and discussing the implications of the findings would enhance the papers contribution. 3) Review Summary: The paper introduces a novel Pyramidal Circuit layer for neural networks inspired by quantum computing to achieve perfect orthogonality. While the paper presents a unique and promising concept with wellexplained theoretical foundations some aspects need further clarification and elaboration. Strengths lie in the innovative approach practical implementation for classical training and successful numerical experiments. Addressing the weak points through further detail on advantages limitations comparisons and scalability would strengthen the paper. Overall the paper makes a significant contribution to the field of neural network training by introducing a novel method for achieving perfect orthogonality efficiently. Recommendation: Considering the innovative approach theoretical soundness and successful experiments I recommend this paper for publication with minor revisions to address the outlined points for further clarification and discussion.", "tG8QrhMwEqS": " Summary of the Paper: The paper introduces an adaptive activationbased structured pruning approach to automatically generate small accurate and hardwareefficient deep learning models for resourceconstrained edge devices. The method utilizes activationbased attention feature maps to identify and prune unimportant filters iteratively. Additionally adaptive pruning policies are proposed to meet various user objectives such as accuracy memory constraint and latency sensitivity. The method outperforms existing stateoftheart structured pruning works on CIFAR10 and ImageNet datasets achieving significant parameter and FLOPs reduction without accuracy drop.  Strong Points of the Paper: 1. Innovative Approach: Introducing an activationbased structured pruning method is a novel and effective way to identify unimportant filters for pruning resulting in small accurate models. 2. Automatic Pruning: The adaptive pruning policies cater to different user objectives automatically eliminating the need for manual exploration and tuning of hyperparameters. 3. Comprehensive Evaluation: The paper provides a thorough evaluation showing superior performance compared to existing pruning methods on popular datasets like CIFAR10 and ImageNet. 4. Detailed Methodology: The paper includes detailed algorithms and explanations on activationbased filter pruning adaptive iterative pruning and layeraware threshold adjustment enhancing the reproducibility of the proposed method.  Weak Points of the Paper: 1. Evaluation Dataset: While the results on CIFAR10 and ImageNet datasets are impressive using additional diverse datasets could strengthen the generalizability of the proposed method. 2. Comparative Analysis: Further comparative analysis with more recent stateoftheart pruning methods could provide a richer context for the performance evaluation.  Questions for the Author: 1. How does the proposed method compare in terms of computational efficiency during the pruning process compared to existing approaches 2. Have you considered the impact of variations in dataset sizes and complexities on the effectiveness of the proposed adaptive pruning policies 3. Can you provide insights into the potential challenges or limitations of implementing the proposed method in realworld edge computing scenarios  Feedback for the Author: 1. Include additional datasets for evaluation to enhance the generalizability of the method. 2. Consider providing a comparative analysis with more recent pruning methods for a more comprehensive evaluation. 3. Discuss potential challenges and practical considerations for deploying the proposed method in diverse edge computing environments.  Review Summary: The paper presents an innovative activationbased structured pruning method with adaptive pruning policies to automatically generate small accurate and hardwareefficient deep learning models. The comprehensive evaluation results demonstrate the superiority of the proposed method over existing stateoftheart approaches. Addressing minor limitations and providing additional analysis would further strengthen the papers contribution. Overall I recommend accepting the paper for publication based on its innovative approach robust methodology and impressive performance outcomes.", "v6s3HVjPerv": "Summary: The paper presents a user study comparing the effectiveness of various image classification model explanation techniques in identifying biases. The study involved a synthetic dataset generator named TWO4TWO and evaluated three explanation methods: baseline conceptbased and counterfactual explanations from an invertible neural network. The results indicated that the baseline outperformed conceptbased explanations while counterfactual explanations performed similarly to the baseline but allowed users to identify some attributes more accurately. Strengths: 1. Novel Contribution: The paper introduces a synthetic dataset generator TWO4TWO which allows control over biases and features. 2. Empirical User Study: The study design is robust involving a significant number of participants (N240) and provides findings based on user responses rather than technical evaluations. 3. OpenSourced Resources: The authors share their dataset explanation techniques model study design and instructions promoting reproducibility and future research. Weaknesses: 1. Limited External Validity: The study is based on a synthetic dataset potentially limiting generalizability to realworld scenarios. 2. Methodological Flaws: Some statistical analyses were not preregistered potentially introducing bias into the results. 3. User Understanding: Participants comprehension of the explanation techniques varied indicating the need for clearer explanations or additional training. Questions and Feedback to the Author: 1. External Validity: How do you plan to transition from synthetic datasets to realworld data to validate the findings further 2. Study Design: How will you address potential methodological issues in future studies such as ensuring complete adherence to preregistration for all statistical analyses 3. User Understanding: Have you considered additional methods to improve users understanding of complex explanation techniques Review Summary: Overall the paper presents a valuable study on comparing explanation techniques for image classification models. While the study design and results provide insights into user interaction with different explanation methods there are some limitations in generalizability and methodological rigor. To improve future research addressing these weaknesses and enhancing user understanding could lead to more robust findings and practical applications in the field of interpretable ML.", "vrW3tvDfOJQ": " Peer Review:  1) Summary of the Paper: The paper introduces an effective method named InverseVariance Reinforcement Learning (IVRL) within a Bayesian framework to address the sample inefficiency commonly observed in modelfree deep reinforcement learning (RL) algorithms. IVRL leverages uncertainty estimation methods through variance networks and variance ensembles to reduce the impact of noisy supervision during training. By using Batch InverseVariance (BIV) weighting IVRL significantly improves sample efficiency and performance on discrete and continuous control tasks\u2014the results demonstrate its superiority over existing stateoftheart approaches.  2) Strengths and Weaknesses:  Strong Points:  Systematic Analysis: Provides a thorough analysis of the sources of uncertainty in modelfree DRL algorithms highlighting the importance of estimating the variance of the supervision noise.  Novel Framework: Proposes a novel framework IVRL which combines variance networks and variance ensembles to enhance the efficiency of deep RL algorithms.  Experiments and Results: Demonstrates significant improvement in sample efficiency on both discrete and continuous control tasks when applied to DQN and SAC.  Weak Points:  Unclear Justification: While the paper addresses the efficiency improvement more comprehensive justification for the choice of specific components hyperparameters and their impact could enhance understanding.  Limited Comparison: Although the comparison with existing stateoftheart methods is present a deeper analysis of IVRLs performance against a broader range of baselines could further highlight its effectiveness.  Reproducibility: While the code and hyperparameters are provided greater emphasis on reproducibility and detailed instructions for replicating the experiments could enhance the papers credibility.  Questions and Feedback to the Author: 1. Clarification on Hyperparameters: Could you provide more insights into the selection and tuning of hyperparameters (e.g. learning rate batch size) for IVRL and how they influence the results 2. Generalization and Robustness: How does IVRL perform in scenarios with noisy environments or nonstationary dynamics Have you tested the method for robustness across a diverse set of environments beyond the ones mentioned 3. Scalability: Considering the computational expenses of IVRL due to ensemble methods are there strategies in place to make the approach more scalable for largerscale and realtime applications 4. Ethical Considerations: Besides environmental sustainability what other ethical considerations do you believe researchers and practitioners should keep in mind when applying IVRL to different domains or realworld applications  3) Review Summary: The paper presents a significant contribution in the field of deep reinforcement learning by introducing the IVRL framework which effectively addresses the sample inefficiency challenge in modelfree RL algorithms through uncertaintybased weights and variance estimation methods. The systematic analysis and empirical results demonstrate the effectiveness of IVRL in enhancing sample efficiency on diverse control tasks. While the paper showcases a novel approach with promising outcomes further exploration on hyperparameters generalization scalability and ethical considerations could enhance the comprehensiveness and applicability of IVRL. Overall based on the strong theoretical foundation innovative methodology and promising experimental results I recommend accepting the paper with minor revisions to address the outlined points for improvement.", "swbAS4OpXW": "Summary of the Paper: The paper presents a novel method GenDA for oneshot generative domain adaptation using a pretrained Generative Adversarial Network (GAN). The main goal is to transfer the knowledge from a source domain GAN model to a target domain with only one reference image while maintaining high synthesis quality and diversity. GenDA introduces an attribute adaptor and an attribute classifier to the generator and discriminator respectively to ensure the transfer of representative characters from the target image while retaining prior knowledge. Additionally a diversityconstraint training strategy is proposed to enhance synthesis quality. Extensive experiments on different datasets demonstrate that the proposed GenDA outperforms existing methods in terms of synthesis diversity and quality. Strong Points: 1. Innovative Approach: GenDA introduces a novel method for oneshot generative domain adaptation that significantly improves synthesis diversity and quality compared to existing methods. 2. Efficient Use of Prior Knowledge: Leveraging the pretrained models prior knowledge with the attribute adaptor and classifier showcases a clever way to maximize data efficiency. 3. Demonstrated Performance: Extensive experiments demonstrate the effectiveness of GenDA in adapting to diverse target domains with as few as one training sample. 4. Comparison and Evaluation: The paper includes thorough comparisons with stateoftheart alternatives and provides quantitative metrics to support the proposed methods superiority. Weak Points: 1. Limitations Acknowledgment: The paper highlights limitations related to the adaptability of the method to completely different domains and the challenge in transferring specific attributes accurately. It would be beneficial to explore potential solutions or alternative strategies to address these limitations. 2. Reproducibility: While the paper mentions plans to make the code and models publicly available providing a direct link or repository where readers can access the implementations would enhance reproducibility and transparency. 3. Visualization: More visual examples or comparison results could further illustrate the effectiveness and limitations of GenDA in different scenarios to enhance understanding and insight. Questions and Feedback to the Author: 1. Can the proposed GenDA method be extended to handle the transfer between vastly different domains How would the model adapt in cases where attributes have significant variations 2. How robust is GenDA to variations in image quality background and lighting conditions in the target domain Does the method have limitations in scenarios with lowquality reference images 3. Given the rapid convergence time of GenDA could potential tradeoffs in synthesis quality or diversity arise How does the method handle complex scenarios beyond the examples provided in the paper Review Summary: The paper presents a novel approach GenDA for oneshot generative domain adaptation using a pretrained GAN model. The method leverages an attribute adaptor and classifier to transfer representative characters from a target image while retaining prior knowledge showcasing substantial improvements in synthesis diversity and quality. The thorough experimental evaluations comparisons with existing methods and acknowledgment of limitations enhance the papers credibility. The proposed GenDA offers a promising solution for dataefficient GAN training in diverse scenarios. Recommending acceptance for publication with potential minor revisions to address limitations and enhance reproducibility.", "nc0ETaieux": "1) Summary of the Paper: This paper addresses a fundamental question in the theory of generative adversarial networks (GANs) regarding when GANs can learn the underlying distribution. The paper presents theoretical results showing that achieving population minimax optimal solutions to the Wasserstein GAN objective may be insufficient for distribution learning in the usual statistical sense. The paper establishes connections between GANs pseudorandom generators (PRGs) and cryptographic assumptions. It also discusses the challenges of achieving distribution learning objectives using standard statistical metrics like KL divergence and proposes using Wasserstein distance for a more practical approach. 2) Strengths and Weaknesses: Strong Points:  The paper tackles a fundamental and challenging question in GAN theory that has implications for realworld applications.  The theoretical results provide valuable insights into the limitations of achieving distribution learning objectives in GANs.  The paper establishes important connections between GANs PRGs and cryptographic assumptions which could lead to further research avenues.  Empirical validations provide practical confirmation of the theoretical findings.  The technical preliminaries and formal definitions are comprehensive aiding in understanding the complex concepts presented in the paper. Weak Points:  The papers technical content is highly specialized and may be challenging for nonexperts in the field to grasp fully.  The paper could benefit from more detailed explanations of specific theorems and lemmas for enhanced clarity.  While the empirical results validate the theoretical findings the scope of the experiments could be expanded for a more comprehensive empirical assessment.  Some sections could be further clarified for better readability especially for readers less familiar with the technical details. Question and Feedback to the Author: 1. Could you elaborate on the practical implications of your findings related to GANs and distribution learning in realworld applications 2. How do you envision applying the insights gained from the theoretical results in this paper to improve the training and performance of GANs in practice 3. Considering the deep connections established between GANs PRGs and cryptographic assumptions how do you foresee these insights shaping future directions in GAN research and related fields 3) Review Summary: The paper provides a comprehensive and insightful investigation into the challenges of achieving distribution learning in GANs shedding light on the limitations of current statistical metrics and theoretical assumptions. The establishment of connections between GANs PRGs and cryptographic concepts offers a fresh perspective on the computational landscape of GANs. While the technical depth of the paper may pose a challenge for readers outside the field the theoretical and empirical results presented contribute significantly to advancing the understanding of GANs. I recommend this paper for publication based on the rigorous theoretical analysis the relevance of the research question and the potential impact on the field of generative modeling.", "vds4SNooOe": " PeerReview of \"Learning FineGrained Embeddings with a SuperclassConditional Gaussian Mixture Model\"  1. Summary: The paper introduces a novel framework called the SuperclassConditional Gaussian Mixture Model (SCGM) for finegrained embedding learning in the context of crossgranularity fewshot learning scenarios. The SCGM model is designed to address the challenge of adapting coarsely pretrained models to finegrained tasks by explicitly modeling the generative process of samples from hierarchical subclasses. The paper presents a principled ExpectationMaximization algorithm for endtoend parameter learning and extensive experiments on benchmark and medical datasets to demonstrate the effectiveness of the proposed method.  2. Strengths and Weaknesses: Strong Points:  The paper addresses an important problem in finegrained prediction by proposing a novel SCGM method.  The framework is flexible and efficient adding only a few distributionrelated parameters to an encoder.  The proposed model outperforms stateoftheart methods on benchmark datasets demonstrating its effectiveness.  The method is theoretically grounded linking it to contrastive learning and providing a clear explanation of its effectiveness.  The extensive experiments including a reallife medical dataset showcase the applicability and superiority of SCGM. Weak Points:  The paper could provide more explanation on the choice of hyperparameters such as \\gamma \\sigma2 and \\lambda.  The comparison with additional relevant baselines or stateoftheart methods could enhance the comprehensiveness of the evaluation.  The computational costs and scalability of the SCGM approach should be discussed more comprehensively. Questions and Feedback to the Author: 1. Could you provide further insights into the selection process of hyperparameters such as \\gamma \\sigma2 and \\lambda How did you ensure they were optimal for the experiments 2. It would be beneficial to include a discussion on the computational costs and scalability of SCGM compared to other methods. How does the SCGM model perform in largescale scenarios 3. Have you considered evaluating SCGM against more recent or domainspecific methods beyond the ones mentioned in the paper  3. Review Summary: The paper presents a wellmotivated and structured approach to tackle the crossgranularity fewshot learning problem with the SCGM framework. The paper effectively addresses the limitations of existing methods by introducing a flexible and efficient model that explicitly models subclass variations. The theoretical grounding of SCGM with contrastive learning enhances the understanding of its effectiveness. Extensive experiments on diverse datasets and a reallife medical scenario provide strong empirical evidence of the superiority of SCGM over existing approaches. In conclusion the paper is wellwritten technically sound and contributes significantly to the field of finegrained embedding learning. The results and analysis support the authors claims demonstrating the potential impact and wide applicability of the SCGM method. Therefore based on the comprehensive evaluation of the paper I recommend its acceptance for publication as it provides valuable insights and advancements in the field of finegrained prediction and model adaptation. Overall the paper is structured and presented effectively offering a compelling contribution to the research domain. The robust theoretical foundation extensive experiments and superior performance of the proposed SCGM method make it a valuable addition to the existing literature.  As an experienced reviewer this detailed and structured peerreview provides a comprehensive evaluation of the strengths and weaknesses of the paper along with insightful questions and feedback to guide the authors in refining their work. The review summary offers a clear recommendation based on the thorough assessment of the paper.", "rwEv1SklKFt": " Summary: The paper presents a novel threat model for poisoned classifiers challenging the assumption that only the attacker possessing the trigger can control the classifier. The authors propose a humanintheloop attack approach to generate multiple effective alternative triggers without access to the original trigger or training data. By analyzing smoothed adversarial examples they construct new triggers through human interaction and demonstrate the effectiveness of their approach on highresolution datasets ImageNet and TrojAI. The paper also includes a user study showing the generality of their method in identifying poisoned classifiers.  Strengths: 1. Innovative Approach: The paper introduces a new threat model for poisoned classifiers expanding the understanding beyond the common assumption of secret backdoors controlled only by the attacker. 2. Humanintheloop Attack: The incorporation of human interaction in the attack methodology adds interpretability and helps users easily identify poisoned classifiers. 3. Empirical Validation: Extensive experiments on highresolution datasets demonstrate the effectiveness of the proposed attack method in generating alternative triggers compared to existing approaches. 4. User Study: The inclusion of a user study adds value to the research showing promising results in enabling users to identify poisoned classifiers.  Weaknesses: 1. Limited to Patchbased Triggers: The methods applicability is restricted to patchbased triggers limiting its effectiveness against more sophisticated backdoor attacks with different trigger forms. 2. Subjectivity in Human Interaction: The reliance on human interaction for trigger extraction introduces subjectivity and may lead to potential biases in trigger selection.  Questions to the Author: 1. Generalization to Diverse Trigger Forms: How feasible is it to extend the proposed methodology to handle diverse trigger forms beyond image patching such as socialmedia filters or image warping 2. Scalability of HumanintheLoop Approach: Have you considered the scalability of the humanintheloop approach in scenarios involving largescale datasets and complex classifiers  Feedback to the Author: The paper presents an innovative approach to address the vulnerability of poisoned classifiers and contributes significantly to the understanding of backdoor attacks. To enhance the papers impact consider discussing the limitations in handling various trigger forms and address the subjectivity in trigger extraction with more concrete guidelines or algorithms. Additionally expanding the discussion on scalability and practical applicability would further strengthen the study.  Review Summary: Overall the paper provides valuable insights into the threat model of poisoned classifiers and offers a novel humanintheloop attack methodology. The empirical validation user study and comparison with existing methods support the significance of the proposed approach. Addressing the limitations and ensuring objectivity in human interaction will enhance the papers contribution to the field of adversarial machine learning. Based on the strengths and contributions outlined I recommend acceptance after minor revisions to address the highlighted areas.", "tsg-Lf1MYp": " Review of \"Enhancing the Reliability of NeuralNetworkBased Classifiers through Natural Attributebased Shift Detection\" 1. Summary: The paper introduces a new task termed Natural Attributebased Shift (NAtS) detection to improve the reliability of neuralnetworkbased classifiers by identifying samples shifted by natural attributes such as age or brightness. The authors create benchmark datasets in vision language and medical domains and extensively evaluate existing outofdistribution (OOD) detection methods on NAtS datasets. They observe inconsistency in OOD methods performance and analyze the relationship between NAtS sample location and the methods performance. Based on this analysis they propose a modification to the training objective to improve OOD detection across NAtS categories. Experimental results show the efficacy of this modification in achieving consistent NAtS detection performance. 2. Strengths:  Original Contribution: The paper introduces a novel task of NAtS detection and provides benchmark datasets addressing a critical issue in deploying classifiers in realworld systems.  Comprehensive Evaluation: The extensive evaluation of existing OOD detection methods on NAtS datasets and the subsequent analysis provide valuable insights into the performance challenges in detecting NAtS samples.  Clear Methodology: The proposed modification to the training objective is wellexplained and experimental results demonstrate its effectiveness in achieving consistent OOD detection performance across NAtS categories. 3. Weaknesses:  Generalizability Concerns: The proposed modification to the training objective is evaluated on specific datasets additional experiments on a wider range of datasets would strengthen the generalizability of the approach.  Limited Discussion on Ethical Implications: While the paper briefly mentions the importance of reliable classifiers in safetycritical systems a more indepth discussion on the ethical implications of unreliable predictions in such systems could enhance the papers impact.  Dataset Availability: Providing access to the benchmark datasets created for NAtS detection would encourage further research and facilitate reproducibility of results. 4. Questions to Authors:  Have you considered exploring the impact of different hyperparameter settings for the proposed modification to the training objective on the NAtS detection performance  How do you envision the proposed NAtS detection approach being integrated into realworld applications especially in safetycritical systems 5. Feedback to Authors:  Enhancing Reproducibility: Providing access to the benchmark datasets and source code would enhance reproducibility and encourage further research in the field.  Exploring Ethical Implications: Consider discussing the ethical implications of unreliable predictions in safetycritical systems and how the proposed NAtS detection can mitigate these risks.  Generalization: Discuss the scalability and generalizability of the proposed modification to the training objective across different domains and datasets.  Review Summary: The paper presents a significant contribution in introducing the novel task of NAtS detection and providing benchmark datasets for evaluation. The comprehensive evaluation of existing OOD detection methods on NAtS datasets and the proposed modification to the training objective showcase the papers technical depth and innovation. However additional experiments on diverse datasets and a more robust discussion on ethical implications would further strengthen the paper. Considering the originality clear methodology and impactful results I recommend acceptance with minor revisions to address the weaknesses highlighted above.", "fy_XRVHqly": "PeerReview 1) Summary of the Paper: The paper introduces StructureaWAre Transformer (SWAT) a modular method for inhomogeneous MultiTask Reinforcement Learning (MTRL) that effectively incorporates agent morphology into a transformerbased policy. By proposing traversalbased positional embedding and graphbased relational embedding the authors aim to address the shortcomings of existing methods that overlook morphological information in favor of better communication efficiency. Experimental results demonstrate that SWAT outperforms stateoftheart methods in terms of sample efficiency and final performance in various MTRL benchmarks and transfer learning scenarios. 2) Strong Points:  Novel Approach: The introduction of structural embeddings to incorporate agent morphology in transformerbased policies is a novel and promising approach for improving performance in inhomogeneous MTRL.  Empirical Results: The paper provides comprehensive experimental results showing significant performance improvements of SWAT over existing methods in both multitask learning and transfer learning settings.  Thorough Analysis: The ablation study conducted to analyze the individual impact of positional and relational information on SWATs performance provides valuable insights into the effectiveness of the proposed structural embeddings.  Comparison and Behavior Analysis: The comparisons with other baselines and the behavior analysis showing how SWAT successfully adapted to specific tasks demonstrate the efficacy and generalizability of the proposed method. Weak Points:  Clarity and Readability: While the paper covers the technical aspects in depth the complex terminology and detailed explanations might be challenging for readers unfamiliar with the field. The authors could consider simplifying certain sections for better understanding.  Validation: While the results are impressive further validation through more diverse benchmark tasks or realworld applications could strengthen the papers contributions and generalizability.  Discussion on Limitations: It would be beneficial for the authors to discuss potential limitations or constraints of the proposed method providing a more balanced view of SWATs applicability across different scenarios. Questions and Feedback to the Author: 1. Can you provide more insights into the computational complexity of implementing SWAT compared to existing methods especially in scenarios with larger agent morphologies 2. Have you considered conducting user studies or realworld applications to validate SWATs performance in practical settings 3. How do you envision the scalability of SWAT when applied to highly complex tasks or scenarios with dynamic environments 3) Review Summary: The paper presents an innovative approach SWAT that effectively incorporates structural embeddings to enhance modular Reinforcement Learning in inhomogeneous settings. The empirical results demonstrate noteworthy performance improvements over existing methods highlighting the importance of leveraging morphological information in the transformerbased policy. Despite the technical depth and thorough analysis provided the paper could benefit from improved clarity in certain sections and further validation through diverse benchmark tasks or realworld applications. Overall based on the papers contributions empirical results and thorough analysis I recommend acceptance after addressing the minor concerns raised in the feedback.", "oSP1hwZB24": "Summary: The paper introduces a novel plugin operation Dynamic Parameterized Operation (DPO) to enhance feature relation modeling in clickthrough rate (CTR) prediction for recommendation systems. The DPO is designed to learn both explicit and implicit feature interactions instancewisely. The paper shows that incorporating DPO into DNN and Attention modules improves featurebased modeling adaptiveness and user behavior modeling with instancewise locality. The proposed method outperforms stateoftheart models in offline experiments on public and realworld datasets and has been successfully deployed in a large ecommerce companys ranking system. Strengths: 1. Innovative Approach: Introducing DPO to capture both explicit and implicit interactions in a dynamic manner is a novel approach that addresses limitations of existing methods. 2. Comprehensive Experiments: The paper conducts extensive experiments on both featurebased and sequencebased modeling on various datasets demonstrating the effectiveness and superiority of the proposed method. 3. RealWorld Application: Deployment of Dynamic Parameterized Networks in the production system of a large ecommerce company and showcasing significant improvements in CTR prediction validate the practical applicability of the proposed method. 4. Comparative Analysis: The paper provides detailed comparisons with stateoftheart methods on public datasets highlighting the superior performance of the Dynamic Parameterized Networks. 5. Detailed Methodology: The paper thoroughly explains the theoretical foundation operation variations and empirical evaluations providing a clear understanding of the proposed approach. Weaknesses: 1. Complexity: The proposed DPO introduces additional complexity which might impact scalability and computational efficiency in realworld applications. 2. Limited Discussion on Limitations: The paper does not extensively discuss the limitations or potential challenges of implementing the Dynamic Parameterized Networks in practical settings. 3. Scalability Concerns: The potential time and resource constraints associated with the proposed method especially in realtime systems require further exploration. 4. Abstraction of RealWorld Results: While the paper mentions deployment in a realworld system more specific details on the impact challenges faced and scalability issues in a live production environment would be beneficial. 5. Further Exploration: Additional studies on the implications of adopting DPO in different recommendation systems and the tradeoffs between model complexity and performance could strengthen the paper. Question to the Author: 1. Can you provide insights into any potential challenges faced during the deployment of Dynamic Parameterized Networks in a realworld production environment 2. How does the computational overhead and model complexity of the proposed DPO impact realtime inference speed and efficiency in practical scenarios 3. Have you conducted experiments or studied the tradeoffs between model performance gains and increased complexity when scaling the proposed method to larger datasets or applications Feedback to the Author: 1. Consider discussing the implications of the proposed method on deployment in realworld systems more extensively. 2. Evaluate and address potential scalability concerns computation efficiency issues and tradeoffs between model performance and complexity. 3. Provide more indepth insights into the limitations of the Dynamic Parameterized Networks and guidelines for practical adoption in recommendation systems. Review Summary: The paper presents a novel Dynamic Parameterized Operation (DPO) for capturing both explicit and implicit feature interactions in CTR prediction showcasing superior performance in offline and realworld datasets. While the innovative approach and comprehensive experimental validation are commendable the paper needs more indepth discussions on scalability computational efficiency and practical implications of deploying the proposed method. Addressing these aspects can strengthen the papers impact and contribute to the broader understanding of incorporating dynamic operations in recommendation systems. Therefore based on the strengths in innovation methodology and experimental validation coupled with the necessary improvements in discussing limitations and practical implications I recommend the paper for acceptance after revisions.", "nRCS3BfynGQ": "Peer Review  1) Summary: The paper introduces two novel graph network architectures that are equivariant to distance and angle preserving transformations in graph coordinate embeddings. The Distance Preserving Graph Network (DGN) is resilient to nonorthogonal transformations preserving distances while the Angle Preserving Graph Network (AGN) maintains angles between neighbouring nodes. These architectures enable invariance or equivariance to the Euclidean group conformal orthogonal group and conformal group. The study demonstrates the efficacy of these models on synthetic geometric datasets and highlights limitations when symmetries are absent. 2) Analysis: Strong Points:  The paper addresses a unique and important problem by proposing graph network architectures equivariant to distance and angle preserving transformations offering a novel approach to symmetry exploitation in deep learning.  The incorporation of group theory and symmetries in neural network design is both theoretically sound and practically relevant enhancing the field of deep learning.  The experimental results provide evidence of the effectiveness of the proposed architectures on synthetic datasets showcasing improved accuracy and data efficiency in the presence of symmetries.  The detailed mathematical formulations explanations and comparisons with existing architectures contribute to the overall coherence and depth of the paper.  The inclusion of computational complexity analysis provides additional insights for readers regarding the practical performance of the proposed models. Weak Points:  The complexity and depth of the concepts may be challenging for readers not wellversed in group theory or neural networks warranting a clearer elucidation of key concepts for better comprehension.  While the experimental results on synthetic datasets are insightful additional realworld applications or benchmarks could strengthen the papers relevance and practicality.  The limitations of the proposed architectures when symmetries are lacking could be further elaborated to provide a more thorough understanding of their constraints and implications.  The paper might benefit from a more detailed discussion on potential future directions or extensions of the proposed models to stimulate further research in the field. Question and Feedback to the Author: 1. Could you provide a more intuitive explanation or a concrete example to help readers grasp the concept of distance and angle preserving transformations in the context of graph coordinate embeddings 2. How do you envision the proposed architectures being applied to more diverse realworld datasets beyond the synthetic and benchmark datasets tested in the study 3. Could the paper benefit from including visual representations or illustrative examples to enhance the understanding of the proposed architectures and their symmetries for readers with varied levels of expertise in the field 3) Review Summary: The paper presents innovative graph network architectures that exploit distance and angle preserving transformations culminating in models equivariant to important symmetry groups. With sound theoretical foundations and empirical validation on synthetic datasets the proposed architectures exhibit enhanced accuracy and data efficiency in the presence of symmetries. While the paper is comprehensive in its approach and analysis addressing some readability challenges and augmenting the discussion on practical applications would further bolster its impact. Overall the paper offers valuable contributions to the field of deep learning and symmetric neural networks.  This review outlines the strengths and weaknesses of the paper provides thoughtful questions and feedback to the author and presents a comprehensive review summary with supporting arguments.", "l_amHf1oaK": " Summary: The paper presents a novel complete neural network verifier MNBAB which combines the strengths of tight multineuron relaxations and a BranchandBound (BaB) approach. MNBAB leverages GPUbased optimization and introduces a new branching heuristic ACS to significantly reduce the number of subproblems generated during the certification process. The verifier achieves up to 28 certification gains on challenging networks indicating meaningful progress towards handling more complex and accurate neural networks.  Strengths: 1. Innovative Approach: The paper introduces a novel method that combines the benefits of multineuron constraints and BaB methodology to address scalability and precision issues in neural network verification. 2. Extensive Evaluation: The paper conducts a comprehensive empirical evaluation to demonstrate the effectiveness of MNBAB on established benchmarks and networks with high accuracy. 3. Code Availability: The authors have shared all code and scripts to reproduce the experiments enhancing transparency and replicability.  Weaknesses: 1. Complexity of Presentation: The paper delves into intricate details of the proposed method potentially making it challenging for readers not deeply familiar with neural network verification. 2. Limited Comparison: While the paper compares MNBAB with stateoftheart methods it would benefit from a more detailed comparison with a broader range of verification tools to validate its superiority.  Questions: 1. Scalability: How does the proposed method scale with network size and complexity compared to existing verifiers especially on extremely large networks 2. Practical Relevance: Can you elaborate on the realworld applications or scenarios where MNBABs performance improvement of up to 28 could have significant implications  Feedback: 1. The paper would benefit from a more userfriendly introduction to make the method and its significance more accessible to a broader audience. 2. Adding a section discussing the implications of MNBABs results in practical scenarios or applications could enhance the papers impact and relevance.  Review Summary: The paper introduces a novel complete neural network verifier MNBAB which effectively combines multineuron constraints and BaB methodology to achieve a substantial improvement in certification accuracy. While the papers thorough evaluation showcases the methods superiority over stateoftheart verifiers some aspects like ease of understanding and realworld relevance could be further emphasized. Overall MNBAB represents a significant advancement in neural network verification demonstrating exceptional performance gains on challenging networks with high accuracy. Therefore based on the substantial contributions and empirical evidence provided I recommend accepting this paper for publication after addressing the minor weaknesses and incorporating the suggested improvements.", "hJk11f5yfy": " PeerReview of the Paper  1. Summary: The paper discusses the challenges of subpopulation shift in image classification tasks where the dataset shifts at a more granular level than traditional domain adaptation. The authors propose a novel conditional supervised training framework that incorporates hierarchical information to address subpopulation shift. The framework aims to improve network robustness and reduce misclassification impact using a graphical distance metric. Experimental results on custom datasets and the BREEDS LIVING17 benchmark demonstrate the efficacy of the proposed methodology.  2. Strengths and Weaknesses:  Strong Points:  Originality: The paper addresses an important challenge in image classification subpopulation shift using a novel conditional supervised training framework.  Methodological Approach: Incorporating hierarchical information in training as a means to mitigate subpopulation shift is a novel and logical approach.  Experimental Evaluation: The experimental results on custom datasets and the LIVING17 benchmark show substantial improvements in accuracy and misclassification impact over baseline models.  Weak Points:  Clarity of Explanations: Some parts of the methodology and experimental setup could be more clearly explained for better understanding.  Justification of Design Choices: More detailed justification of the decisions made in the design of the hierarchical structure and training framework would enhance the paper.  Comparison with Existing Methods: While related works are mentioned a deeper comparison with existing methods for subpopulation shift mitigation could add value.  Questions and Feedback to the Author:  Can you provide more intuition on how the conditional training framework effectively leverages hierarchical information for improved performance under subpopulation shift  Could you elaborate on the computational overhead or training time implications of incorporating hierarchical information in the learning process  Can you provide insights on the scalability of the proposed framework to larger datasets beyond the ones mentioned in the paper  3. Review Summary: The paper presents a novel conditional supervised training framework to address the challenge of subpopulation shift in image classification tasks. By incorporating hierarchical information and introducing a graphical distance metric the proposed methodology shows promising results in improving network robustness against subpopulation shift. While the experimental results demonstrate the effectiveness of the approach further clarity in explanations and deeper justifications of design choices would strengthen the paper. Overall the paper contributes to the research on handling subpopulation shift and provides a solid foundation for future work in this area.  Recommendation: Based on the innovative approach promising experimental results and the potential impact of addressing the challenging problem of subpopulation shift I recommend the paper for acceptance after addressing the suggested improvements in clarity and justification of design choices.", "rq1-7_lwisw": "Summary: The paper proposes the Object Concept Learning (OCL) task to enhance object understanding in artificial intelligence. OCL requires machines to infer object affordances based on attributes and categories while also explaining the causal relations between them. A densely annotated knowledge base is built to support OCL and a baseline model Object Concept Reasoning Network (OCRN) is introduced. The OCRN leverages causal intervention and concept instantiation to infer object knowledge effectively. Strengths: 1. Innovative Task: The OCL task introduced by the paper is novel and pushes the envelope for deeper object understanding. 2. Comprehensive Knowledge Base: The creation of a densely annotated knowledge base for categories attributes and affordances is a significant contribution to the field. 3. Causal Reasoning Focus: The emphasis on causal relations between object levels is a unique feature that adds depth to the proposed task. 4. Baseline Model Performance: The OCRN model shows effective performance in inferring object knowledge while adhering to causal relationships. Weaknesses: 1. Complexity: The proposed task of OCL is highly complex requiring sophisticated reasoning abilities that current AI systems may struggle to achieve. 2. Performance Comparison: The paper could provide more detailed comparisons with existing methods and benchmarks to showcase the superiority of the proposed approach. 3. Evaluation Clarity: Although the paper discusses metrics like mAP and TDE for evaluation it could benefit from more detailed explanations and results for better understanding. Questions: 1. How does the proposed OCRN model address the challenges of learning object affordances based on attributes and categories Can you provide more insights into the model architecture and reasoning mechanisms 2. What are the potential applications of the OCL task in realworld scenarios and how do you envision it impacting the field of artificial intelligence 3. How generalizable is the proposed approach to OCL across different datasets and tasks Are there any limitations or specific conditions under which the model performs best Feedback to the Author: 1. Clarify the model architecture and reasoning mechanisms of the OCRN model to provide a better understanding of how it performs causal reasoning for object concept learning. 2. Consider providing more detailed comparisons with existing methods and benchmarks to highlight the advantages of OCL and OCRN. 3. Discuss the practical implications and future directions of OCL research to inspire further studies on reasoningbased object understanding in artificial intelligence. In conclusion the paper offers a significant contribution to object understanding in AI through the OCL task and the OCRN model. However further discussions on implementation details model insights and realworld applications could enhance the clarity and impact of the proposed approach.", "o0ehFykKVtr": " Summary of the Paper: The paper proposes a \"robotaware control\" paradigm to facilitate the transferability of visual modelbased reinforcement learning policies across different robots without the need for extensive robotspecific data collection. The key idea is to exploit the inherent knowledge about the robot itself and separate the robot and world dynamics when training visual control policies. By disentangling visual observations utilizing robotspecific knowledge and designing robotaware models and planning costs the proposed method enables zeroshot transfer of visual manipulation skills from one robot to another. The paper showcases successful experiments on tabletop manipulation tasks with simulated and real robots demonstrating the effectiveness of the robotaware approach in boosting transferability.  Strong Points: 1. Innovative Proposal: The paper introduces a novel \"robotaware control\" paradigm that addresses the challenge of transferring visual control policies across different robots efficiently and effectively. 2. Comprehensive Experiments: The paper provides detailed experimental results on both simulated and realworld robots showcasing the effectiveness of the proposed method in enabling zeroshot transfer of visual manipulation skills. 3. Clear Presentation: The paper is wellstructured with detailed explanations of the proposed approach implementation details experimental setup and results. 4. Significant Contribution: The papers demonstration of zeroshot visual skill transfer between realworld robots and the ability to transfer after training on just one source robot represents a substantial advancement in robotic control research.  Weak Points: 1. Complexity: The proposed approach while innovative may be quite complex and challenging for implementation in realworld scenarios without adequate technical expertise. 2. Limited Evaluation: While the experiments are comprehensive the evaluation focuses on specific manipulation tasks. Further validation on a wider range of tasks or in more diverse environments could strengthen the papers conclusions.  Questions for the Author: 1. Scalability: How scalable is the proposed \"robotaware control\" approach when considering a larger number of robots with vastly different morphologies or capabilities 2. Generalization: How well does the method generalize to tasks beyond tabletop manipulation such as mobile robot navigation or complex object manipulation tasks  Feedback for the Author: 1. Comparative Analysis: Providing a more indepth comparative analysis with existing stateoftheart approaches for robot transfer learning would enhance the papers contribution and significance. 2. RealWorld Deployment: Addressing potential challenges or limitations that might arise when deploying the proposed method on real robots in dynamic and unstructured environments could make the research more practically applicable.  Review Summary: The paper presents a novel \"robotaware control\" paradigm that enables zeroshot transfer of visual manipulation skills between different robots. While the experimental results demonstrate the effectiveness of the method in tabletop manipulation tasks further exploration and validation across different scenarios would enhance the papers impact and potential for realworld deployment. Overall the papers contribution to the field of robot transfer learning is significant and with additional refinements it holds the potential to advance the stateoftheart in robotic control and transfer learning.", "rUwm9wCjURV": "Paper Summary: The paper addresses the challenge of developing agents that can learn to execute outofdistribution multitask instructions expressed in temporal logic using deep reinforcement learning. The authors propose a novel deep learning configuration called the latentgoal architecture which utilizes inductive biases to improve agents generalization performance. This new configuration involves generating latent representations of the current goal by combining human instructions and sensory inputs in separate channels. The paper evaluates the performance of multiple stateoftheart neural networks in executing formal instructions compares the proposed neural networks performance with existing architectures and conducts ablation studies to analyze the impact of different design choices. Strengths: 1. Innovative Approach: The introduction of the latentgoal architecture represents a novel approach to improving generalization in deep reinforcement learning agents executing multitask formal instructions. This innovative method adds to the existing literature on the topic. 2. Thorough Experimentation: The paper conducts comprehensive experiments to evaluate the proposed architectures performance against stateoftheart neural networks. The experiments cover different settings benchmarks and network configurations providing a robust analysis of the proposed approach. 3. Ablation Studies: The ablation studies help in understanding the key design choices and the impact of different components on the performance of the proposed architecture. This detailed analysis enhances the papers contribution by providing insights into the architectures effectiveness. Weaknesses: 1. Complexity of Content: The papers complexity and technical details may be a challenge for readers who are not wellversed in deep reinforcement learning formal instructions and neural network architectures. 2. Lack of Visualization: While the paper describes the architectures and experiments in detail the inclusion of visual aids or diagrams could enhance the clarity and help readers better understand the proposed latentgoal architecture and its components. Questions: 1. Can you provide more insight into the computational efficiency of the proposed latentgoal architecture compared to existing stateoftheart architectures 2. How do you envision the scalability of the proposed architecture for more complex tasks or environments with larger state spaces 3. Have you considered realworld applications or scenarios where the latentgoal architecture could be implemented and what challenges do you foresee in practical deployment Feedback to the Author: 1. Consider providing more visual aids such as diagrams or illustrations to help readers visualize the architecture and better understand the components and their interactions. 2. Clarify the computational efficiency and scalability aspects of the latentgoal architecture to highlight its practical applicability in realworld settings. 3. Discuss potential future directions for research including applying the latentgoal architecture in different domains or addressing specific challenges in deploying such architectures in practical scenarios. Review Summary: The paper presents an innovative latentgoal architecture within the context of deep reinforcement learning for executing formal instructions. The comprehensive evaluation thorough experimentation and detailed ablation studies demonstrate the effectiveness of the proposed architecture in improving agents generalization performance. The papers innovative approach and thorough analysis contribute significantly to the field. However addressing the complexity of content and providing more visual aids for clarity would enhance the papers accessibility to a broader audience. Overall I recommend accepting this paper for publication due to its significant contributions to the field and its potential to advance research in deep reinforcement learning and multitask instruction execution.", "y_op4lLLaWL": " PeerReview of \"Understanding the Dynamics of Variational Autoencoders Training on LowDimensional Manifolds\"  1. Summary: The paper explores the behavior of Variational Autoencoders (VAEs) trained on lowdimensional linear and nonlinear data manifolds. It investigates a twostage training algorithm for VAEs examining whether VAE training can recover the correct support of the data distribution. The study shows that for linear encodersdecoders the VAE training can recover the correct support of the data manifold. However for nonlinear data manifolds VAE training frequently learns a higherdimensional manifold than the ground truth. The paper also delves into the training dynamics and the implicit bias of gradient descent in linear VAEs.  2. Strengths and Weaknesses: Strong Points:  Theoretical and Practical Contribution: The paper provides insights into the behavior of VAEs when the data is supported on a lowdimensional manifold. This is a valuable contribution to understanding the dynamics of VAE training.  Experimental Validation: The study is well supported by experiments on both linear and nonlinear datasets enhancing the credibility of the findings. The comparison between ground truth and learned covariance eigenvalues provides valuable insights.  Rigorous Analysis: The paper includes detailed proofs of conclusions and results which strengthen the theoretical foundation of the study. Weak Points:  Complexity of Presentation: The paper contains highly technical content including theoretical derivations and proofs which can be challenging for nonexpert readers to follow.  Limitation of Results: The results show limitations in recovering the data density on the manifold which poses questions on the efficacy of VAEs for such data distributions. Consider discussing implications and potential improvements.  Discussion on Implicit Bias: While the paper delves into the implicit bias of gradient descent in linear VAEs extending this analysis to nonlinear VAEs could provide a more comprehensive understanding of the training dynamics.  Questions for the Authors: 1. Can you discuss the broader implications of the findings on VAE training dynamics for practical applications such as image generation or anomaly detection 2. Did you consider exploring alternative optimization strategies or network architectures to enhance the recovery of data density in nonlinear VAEs How do you see these strategies impacting the results 3. How might the insights from this study on VAE training dynamics inform future research directions in the field of generative modeling or unsupervised learning  Feedback to the Authors: 1. Clarity Improvements: Consider providing more accessible summaries and interpretations of the complex theoretical derivations for the benefit of a wider readership. 2. Discussion Expansion: Expand on the implications of the limitations observed in data density recovery on the effectiveness of VAEs in practical scenarios. 3. Future Directions: Consider elaborating on potential avenues for further research such as exploring novel training strategies or architectural modifications to address limitations identified in VAE training on lowdimensional manifolds.  Review Summary: The paper provides valuable insights into the behavior of Variational Autoencoders (VAEs) trained on lowdimensional linear and nonlinear data manifolds.", "zFlFjoyOW-z": " Summary: The paper introduces a framework called User Multi Interests Capsule Network (MICN) to learn interestbased item representations for recommendation systems. The MICN is designed as an auxiliary task to jointly learn itembased item representations and interestbased item representations without the need for major model redesign. The proposed approach is evaluated on various benchmarks and results show significant performance improvements compared to existing recommendation models. The paper extensively discusses the model architecture the dynamic routing mechanism of Capsule Networks for multiinterest representation and the experimental results on Amazon realworld datasets.  Strong Points: 1. Innovative Approach: The introduction of the MICN framework to learn interestbased item representations is novel and addresses the issue of capturing user interests effectively. 2. Model Agnostic: The framework is designed to be modelagnostic allowing easy integration with existing recommendation models without the need for extensive redesign. 3. Comprehensive Experiments: The paper conducts experiments on realworld datasets demonstrating the effectiveness of the proposed approach across different recommendation models and varying lengths of user behavior sequences. 4. Clear Explanation: The paper provides a detailed explanation of the model architecture dynamic routing mechanism and the methodology for learning interestbased item representations.  Weak Points: 1. Theoretical Justification: While the paper provides a detailed description of the model architecture and methodology the theoretical justification behind the effectiveness of the MICN framework could be further elaborated. 2. Comparison Metrics: It would be beneficial to include additional performance metrics besides AUC to provide a more comprehensive evaluation of the proposed approach. 3. Scalability: The scalability and computational efficiency of the proposed framework need to be discussed especially in largescale recommendation systems. 4. Future Directions: The paper could benefit from discussing potential future research directions or extensions of the proposed framework to enhance the overall impact of the study.  Questions and Feedback to the Author: 1. Can you provide more insights into the interpretability of the learned interestbased item representations using the MICN framework 2. How does the proposed approach handle coldstart users or items in the recommendation system 3. Have you considered exploring the scalability of the MICN framework for largescale recommendation systems in terms of computational resources and training time 4. It would be helpful to discuss the generalizability of the proposed approach across different domains beyond the benchmarks used in the experiments.  Review Summary: Overall the paper presents an innovative framework MICN for learning interestbased item representations in recommendation systems. The modelagnostic nature of the framework combined with the comprehensive experimental results on realworld datasets demonstrates its effectiveness in enhancing the performance of existing recommendation models. While the paper lacks some theoretical elaboration and additional performance metrics the clear explanation of the model architecture and the promising results make it a valuable contribution to the field of recommendation systems. Further discussions on scalability interpretability and future research directions can enhance the impact and relevance of the study. I recommend minor revisions addressing the weak points highlighted which will contribute to the overall quality of the paper.", "kj0_45Y4r9i": " Summary: The paper proposes a novel method Clustering by Discriminative Similarity (CDS) which learns discriminative similarity for data clustering. CDS is based on unsupervised similaritybased classifiers and minimizes generalization error to find optimal data partition. A new clustering method CDS via unsupervised kernel classification (CDSK) using a kernel as the similarity function is also introduced and evaluated through experimental results.  Strengths: 1. Innovative Approach: The paper introduces a novel method CDS which focuses on learning discriminative similarity for data clustering. It provides a fresh perspective on similaritybased clustering. 2. Theoretical Foundation: The paper is wellsupported by theoretical analysis providing a strong foundation for the proposed methods. 3. Empirical Evaluation: The experimental results demonstrate the effectiveness of the proposed CDSK method showcasing its performance compared to existing clustering algorithms. 4. Clarity of Presentation: The paper is wellorganized with clear sections that explain the problem setup contributions and methodology in a structured manner.  Weaknesses: 1. Complexity: The paper introduces multiple new concepts and methods which might be challenging for readers unfamiliar with the topic. Simplifying the presentation could improve accessibility. 2. Validation: While the experimental results show the effectiveness of CDSK additional comparisons with more stateoftheart methods could enhance the papers contribution. 3. Practical Applicability: While the theoretical underpinnings are solid the practical implications of the proposed method could be elaborated on to enhance the papers impact.  Questions and Feedback: 1. Clarification: Can you provide more insights into the practical implications of the proposed CDS and CDSK methods How can they be applied to realworld datasets or scenarios 2. Comparison: How does CDSK compare to more recent clustering methods in terms of scalability and performance on large and highdimensional datasets 3. Extensions: Are there plans to extend the proposed methods to handle streaming data or online clustering scenarios 4. Reproducibility: Providing implementation details or a publicly available code repository could enhance the reproducibility of the results for interested researchers.  Review Summary: The paper presents an innovative approach CDS and its application in the form of CDSK for data clustering. The theoretical grounding and experimental validation underscore the potential of the proposed methods. While the complexity of the methods and their practical applicability could be elaborated on the papers strong foundational framework and promising results support a positive recommendation for publication after addressing minor revisions and expanding discussions on realworld applications.", "gSdSJoenupI": " PeerReview 1. Summary: The paper proposes a novel framework called PolyLoss for understanding and designing loss functions in deep neural networks. PolyLoss views and designs loss functions as a linear combination of polynomial functions allowing for easy adjustment of polynomial coefficients to tailor loss functions for different tasks and datasets. The paper shows that the Poly1 formulation outperforms crossentropy loss and focal loss on various tasks including 2D image classification instance segmentation object detection and 3D object detection. Experimental results demonstrate the effectiveness of PolyLoss in improving the performance of deep learning models. 2. Strong and Weak Points: Strong Points:  Novel Framework: The PolyLoss framework introduces a fresh perspective on designing loss functions in neural networks.  Experimental Results: Extensive experiments showcase the effectiveness of PolyLoss across different tasks and datasets.  Simplicity: The Poly1 formulation is simple and requires minimal changes in code while delivering significant performance improvements.  Insights: The paper provides valuable insights into the importance of adjusting polynomial coefficients for optimal model performance.  Application in Various Tasks: The ability of PolyLoss to enhance performance in tasks like object detection and image classification highlights its versatility. Weak Points:  Complexity: The paper could have further simplified the explanation of complex mathematical concepts for better readability.  Lack of Baseline Comparisons: While the performance improvement of PolyLoss is highlighted more direct comparisons with other stateoftheart methods could have strengthened the analysis.  Generalization: The evaluation mainly focuses on specific models (e.g. EfficientNetV2 RSN) and datasets limiting the generalizability of the findings.  Parameter Tuning Details: More insights into the hyperparameter tuning process grid search techniques and computational costs could have enhanced the completeness of the experimental methodology. Questions and Feedback:  Question: Did you evaluate PolyLoss on any additional datasets apart from the ones mentioned in the paper to test its generalizability across diverse domains  Feedback: It would be beneficial to include a comparison with other recent works that focus on loss function design and optimization to highlight the uniqueness and superiority of the PolyLoss framework.  Question: How do you plan to address the limitation of the complexity introduced by a large number of polynomial coefficients when applying PolyLoss in realworld scenarios with extensive datasets 3. Review Summary: The paper presents a novel framework PolyLoss for designing loss functions in deep learning models showcasing its effectiveness in improving model performance across various tasks and datasets. While the experimental results are compelling the paper could benefit from further simplification of technical details additional comparative analyses and exploration of the frameworks scalability to different problem domains. Overall the PolyLoss framework shows promise in enhancing existing loss function design strategies and warrants further exploration and validation in diverse applications. Based on the strengths of the proposed framework its potential impact on improving model performance and the relevance of the findings to the deep learning community I recommend considering this paper for publication after addressing the outlined suggestions and questions.", "kezNJydWvE": " Summary: The paper proposes a novel approach for dynamic scene deblurring by introducing the concept of reblurring to improve the perceptual quality of the deblurred images. The key idea is to reconstruct the original blur by amplifying the remaining blur present in the deblurred image. The proposed method involves training a pair of deblurring and reblurring modules jointly to optimize the deblurring process. The paper presents experimental results that demonstrate the effectiveness of the reblurring losses in achieving sharper and visually pleasing deblurred images. The methodology is tested on largescale benchmarks and real images showing improvements in perceptual quality metrics such as NIQE and LPIPS scores.  Strong Points: 1. Innovative Concept: The introduction of the reblurring concept to improve deblurring quality is a novel and innovative approach. 2. Comprehensive Experiments: The paper conducts experiments on largescale datasets and real images to validate the proposed methods effectiveness. 3. Clearly Defined Methodology: The methodology for training the deblurring and reblurring modules is welldefined providing clear guidelines for implementation. 4. Detailed Analysis: The paper provides a thorough analysis of the proposed method including comparisons with existing approaches and visualization of results.  Weak Points: 1. Clarity of Presentation: The paper contains complex technical details that may be challenging for readers to grasp without a strong background in the field. 2. Lack of Abstraction: Some sections could benefit from more concise explanations and a higherlevel overview of the key findings.  Question to the Author:  Can you elaborate on how the proposed approach compares with stateoftheart methods in terms of computational efficiency and model complexity  How robust is the proposed method to variations in input data and different types of motion blur patterns  Feedback to the Author: The paper presents a significant advancement in the field of dynamic scene deblurring by introducing the innovative concept of reblurring to enhance the quality of deblurred images. The experimental results demonstrate the effectiveness of the proposed method in improving perceptual quality metrics across different architectures. However enhancing the clarity of presentation and providing more highlevel explanations could further improve the accessibility of the paper to a wider audience. Overall the paper provides valuable insights and contributes to the ongoing research efforts in dynamic scene deblurring algorithms. I recommend the paper for publication after addressing the minor issues identified in the review.", "gf9buGzMCa": "Summary: The paper focuses on the mathematical properties of deep neural networks specifically addressing the minimum width needed for universal approximation for different activation functions. It investigates network functions with widths less than or equal to the input dimension exploring conditions for universal approximation of continuous functions on compact subsets. The study proves a maximum principle and provides insights into the limitations and capabilities of networks with respect to approximation on certain compact sets. Strengths: 1. Theoretical Rigor: The paper is wellgrounded in mathematical theory providing detailed formal definitions and propositions to support the arguments. 2. Novel Contributions: The paper offers new insights into the limitations of neural network expressiveness for widths less than or equal to the input dimension extending the understanding of universal approximation in specific settings. 3. Comprehensive References: The paper cites relevant literature to contextualize its contributions and integrates previous research effectively into the discussion. 4. Numerical Experiment: The inclusion of a numerical experiment to substantiate the theoretical findings adds practical relevance to the study demonstrating the application of the proposed concepts. Weaknesses: 1. Complexity: The papers complexity may limit its accessibility to a broader audience especially those without a strong mathematical background. 2. Clarity: Some sections especially the prooforiented parts could be challenging for readers not wellversed in mathematical notation and concepts. 3. Discussion Expansion: The discussion on implications for generalization could be further elaborated to enhance the practical implications of the findings. 4. Visual Representation: While the numerical experiment results are mentioned visual representations in the form of tables or charts could enhance the clarity and interpretation of the results. Questions and Feedback: 1. How do the findings of the paper contribute to the broader understanding of neural network expressiveness and its implications for practical applications 2. Could you elaborate on potential future research directions or extensions to this study building on the insights and limitations identified in this work 3. How do the results of the numerical experiment align with the theoretical propositions of the paper and what are the implications for model performance in realworld scenarios 4. Considering the theoretical and practical aspects of your study how might the insights provided impact the design and training of neural networks in various machine learning tasks Review Summary: The paper presents a rigorous investigation into the limitations and capabilities of neural network functions with widths less than or equal to the input dimension for universal approximation on specific compact sets. While the theoretical framework is solid and the contributions are novel the complexity and lack of clarity in certain sections may hinder broader understanding. However the integration of a numerical experiment enhances the practical relevance of the findings. With some improvements in clarity discussion expansion and visual representation this study could significantly contribute to the advancement of understanding neural network expressiveness and its application in machine learning tasks. Based on the strengths and potential impact of the study I recommend further revision to enhance its accessibility and broaden its utility in the field.", "uut_j3UrRCg": "Summary: The paper introduces a novel architecture for lifelong learning of hierarchically structured tasks focusing on the ability to leverage what was learned from simple tasks to efficiently learn complex tasks. The architecture is based on the utilization of sketches and a localitysensitive hash table for memory management. Different versions of the architecture are proposed starting from a simple contextbased routing to a more complex hierarchical learning approach. Strengths: 1. Theoretical Analysis: The paper provides a strong theoretical foundation for the proposed architecture proving its capability to efficiently learn complex tasks by building upon previously learned tasks. The use of sketches and hash tables is a unique approach that adds value to the lifelong learning domain. 2. Innovative Architecture: The utilization of sketches and a localitysensitive hash table for memory management is innovative and provides a structured way to represent and process information enabling efficient learning of complex tasks. 3. Comprehensive Experiments: The experimental section includes diverse tasks such as learning intersections of halfspaces and recognizing 5digit numbers. The comparison between the endtoend approach and the modular approach provides empirical evidence of the effectiveness of the proposed architecture. Weaknesses: 1. Complexity: The architecture and algorithms described in the paper are highly complex which might pose challenges for implementation and practical application. A more detailed discussion on the scalability and efficiency of the proposed methods would be beneficial. 2. Lack of Realworld Applications: While the theoretical analysis and experimental results are comprehensive the paper could benefit from discussing realworld scenarios where the proposed architecture could be applied and its potential impact on practical applications. 3. Incomplete Discussion: Certain aspects such as the handling of logic and language tasks within the architecture are mentioned but not elaborated on. Including a more detailed discussion on these topics would enhance the completeness of the paper. Questions and Feedback to the Author: 1. Have you considered the computational complexity of the proposed architecture How does the complexity scale with the number of tasks and the size of the input data 2. Can you provide insights into how the architecture could be practically implemented and scaled for realworld applications considering the complexity of the algorithms and the memory requirements 3. It would be interesting to understand how the proposed architecture could adapt to dynamic environments where tasks evolve over time. Have you explored scenarios with evolving tasks in your experiments Review Summary: Overall the paper presents a novel architecture for lifelong learning with a strong theoretical foundation and innovative memory management techniques. The comprehensive experiments demonstrate the effectiveness of the proposed approach in learning complex tasks. However the complexity and practical implications of the architecture need further discussion. With additional clarity on realworld applications and scalability considerations the paper could make a significant contribution to the field of lifelong learning. Therefore I recommend minor revisions to address the clarity and completeness of the explanations and to further discuss the practical implications and scalability of the proposed architecture.", "tJCwZBHm-jW": "1) Summary of the Paper: The paper explores the potential of transferring 2D model architectures and weights for understanding 3D pointclouds. It investigates the feasibility of transferring pretrained 2D ConvNets to 3D ConvNets by inflating 2D convolution filters to 3D filters and finetuning them  termed as Finetuned ImagePretrained models (FIP). The paper demonstrates that minimal finetuning on input output and optionally batch normalization layers can yield competitive performance on 3D pointcloud classification. Furthermore fully finetuned models show significant improvements over existing pointcloud models. The study also investigates the data efficiency and training speed acceleration benefits of using imagepretrained models for pointcloud tasks. 2) Strong Points:  The paper presents a novel approach of transferring pretrained 2D ConvNets for 3D pointcloud understanding demonstrating competitive performance with minimal finetuning.  Empirical evaluation on various datasets and tasks (ModelNet 3D Warehouse S3DIS SemanticKITTI) supports the effectiveness of the proposed method.  Detailed experiments and comparisons with various baselines illustrate the benefits of using imagepretrained models for pointcloud recognition tasks.  The paper sheds light on the reasons behind the success of transferring imagepretrained models for pointcloud understanding through network dissection textureshape representation transferring and feature distribution distance analysis. Weak Points:  The paper lacks direct comparisons with other transfer learning methods or architectures that address the same problem domain.  While the empirical results are comprehensive some insights could be further elaborated or visualized for better understanding.  It would be beneficial to discuss potential limitations or scenarios where the proposed method may not be as effective. Question to the Authors:  Have you considered exploring the transferability of the proposed transfer learning approach across different types of 3D pointcloud tasks or datasets beyond the ones mentioned in the paper  How do you envision the scalability and generalizability of the proposed method to more diverse and complex 3D pointcloud recognition tasks in realworld applications  Could you provide insights into the computational resource requirements or scalability aspects of transferring pretrained models for 3D pointcloud understanding Feedback to the Authors:  The paper presents a compelling approach with strong empirical results and insightful analysis. Enhancing the discussion around potential limitations scalability and practical implications of the proposed method could further strengthen the papers impact and appeal to the research community. 3) Review Summary: The paper introduces an innovative method for transferring pretrained 2D ConvNets to understand 3D pointclouds showcasing competitive performance with minimal finetuning. The empirical evaluation across multiple datasets and tasks supports the efficacy of the proposed approach. The detailed analysis of the benefits data efficiency and training speed acceleration highlights the practical implications of using imagepretrained models for pointcloud recognition. Further elucidation on scalability limitations and practical considerations could amplify the impact and relevance of the research. Overall based on the compelling results and methodology the paper merits consideration for publication pending minor revisions to enhance clarity and completeness.", "keeCvPPd3vL": " Peer Review  1) Summary of the Paper: The paper discusses the interpretation and regularization of image generators within deep learning architectures particularly Generative Adversarial Networks (GANs). The authors propose a novel sparse modeling approach by relating generators to Convolutional Sparse Coding (CSC) and its MultiLayered version (MLCSC). They demonstrate that enforcing sparsityinspired regularization on generator activation layers leads to improved image synthesis. Furthermore they apply these insights to generators serving inverse problems specifically the Deep Image Prior (DIP) algorithm. The experiments conducted validate the effectiveness of the proposed regularization techniques in enhancing image synthesis and solving inverse problems.  2) Strong and Weak Points Questions and Feedback: Strong Points:  Novel Approach: The paper presents a novel interpretation of image generators in GANs through sparse modeling providing a new perspective on the synthesis process.  Effective Regularization: Enforcing sparsityinspired regularizations leads to improved image synthesis across various GAN architectures and inverse problems.  Extensive Experiments: The authors conduct thorough experiments on different GAN architectures and datasets showcasing the effectiveness and versatility of the proposed regularization techniques. Weak Points:  Clarity: Some sections particularly those discussing the mathematical formulations may be challenging for readers less familiar with sparse modeling and GANs.  Comparison with Existing Work: While the paper mentions differences between this work and prior research providing a more detailed comparative analysis with existing methods could strengthen the discussion. Questions: 1. How do the proposed regularization techniques impact the training convergence and stability of GANs especially in mitigating issues like mode collapse and vanishing gradients 2. Could you elaborate on the computational overhead introduced by the additional regularization layers and their impact on training efficiency Feedback:  Clarification on Implementation: Providing a clearer explanation of the practical implementation of the proposed sparsityinduced regularizations including hyperparameters and optimization strategies would enhance the papers applicability.  Discussion on Generalization: Discussing the generalization capabilities of the regularization techniques across different datasets and tasks could further highlight the robustness of the approach.  3) Review Summary: The paper presents an innovative approach to interpreting and regularizing image generators in GANs using sparse modeling techniques. The proposed regularization methods show significant improvements in image synthesis and solving inverse problems as demonstrated through comprehensive experiments. While the paper contributes valuable insights into understanding and enhancing generator architectures addressing clarity in complex sections and providing more indepth comparisons with existing approaches would strengthen the manuscript. Overall based on the novelty of the approach the effectiveness of the regularization techniques demonstrated in the experiments and the potential impact on improving GAN performance I recommend accepting this paper for publication with minor revisions to enhance clarity and discussion aspects.", "hRVZd5g-z7": "Peer Review:  Summary: The paper introduces a novel approach called ACDC (Atom Coefficient Decomposition and Coefficient Sharing) that presents a joint subspace view for convolutional neural networks (CNNs). The method aims to reduce parameter redundancy in deep network architectures while preserving model expressiveness. By sharing subspace coefficients across layers and filter groups ACDC provides structural regularization aiding in model interpretability training efficiency adaptation and model compaction. Extensive empirical evidence is presented to support the effectiveness of the proposed approach demonstrating notable parameter reductions with comparable or improved performance on image classification tasks across various datasets. Furthermore fewshot learning experiments show enhanced model adaptation on novel tasks with limited supervision.  Strengths:  Innovative Approach: The proposed ACDC method introduces a novel perspective on structurally regularizing CNN filters through shared subspace coefficients addressing parameter efficiency without sacrificing model performance.  Extensive Experimental Validation: The paper provides thorough empirical evidence supporting the effectiveness of ACDC showcasing significant parameter reductions with maintained or improved accuracies on various image classification datasets.  Flexibility and Compatibility: The method is shown to be compatible with diverse network architectures and easily implementable as plugandplay replacements to standard convolutions showcasing its remarkable flexibility.  Interpretability Discussion: Addressing interpretability concerns in deep learning the paper discusses the potential of ACDC for better interpretability through crosslayer shared coefficients and expands the Class Activation Mapping (CAM) method to all layers of a CNN demonstrating the interpretability benefits of the proposed approach.  Weaknesses:  Technical Complexity: The technical intricacies of atom coefficient decomposition and coefficient sharing might be challenging for readers not wellversed in deep learning concepts necessitating more detailed explanations or simpler analogies.  Limited Clarity on Implementation: While the paper presents the concept of ACDC effectively more explicit guidance on the practical implementation and integration of the method with existing deep learning frameworks could enhance the clarity for future users and researchers.  Oversight of Potential Drawbacks: The paper could benefit from a discussion on any potential limitations or scenarios where the proposed method may not be as effective providing a more comprehensive view of the approachs applicability.  Questions and Feedback: 1. Can you provide more insight into the computational efficiency benefits of ACDC particularly in terms of reduced FLOPs and improved training speed How does this efficiency translate into realworld applications 2. How does ACDC compare to stateoftheart methods in terms of model interpretability and adaptability to new tasks Are there specific scenarios where ACDC outperforms existing techniques in these aspects 3. It would be valuable to explore the robustness of ACDC to various data distributions or noise levels. Have you conducted experiments or considered scenarios where the methods performance might be impacted by noisy or diverse data inputs  Review Summary: The paper introduces a novel approach ACDC for enhancing parameter efficiency and model interpretability in deep learning models. The method demonstrates strong potential in reducing parameter redundancy while maintaining or improving model performance across image classification tasks. The flexibility compatibility and interpretability benefits of ACDC make it a promising addition to the deep learning toolkit. However further clarification on implementation details and potential limitations would strengthen the paper. Overall the innovative approach extensive experimental validation and interpretability discussions support a recommendation for acceptance after addressing the mentioned concerns.", "iEvAf8i6JjO": "Peer Review 1. Summary of the Paper: The paper proposes Trust Region Gradient Projection (TRGP) for continual learning to address catastrophic forgetting. TRGP uses a trust region to select correlated old tasks for knowledge transfer and implements scaled weight projection to leverage frozen weights of selected tasks without modifying the model. The method is evaluated on standard CL benchmarks showing substantial improvements over stateoftheart methods. 2. Strengths:  Innovative Approach: The introduction of trust region and scaled weight projection is a novel approach to facilitate forward knowledge transfer.  Comprehensive Evaluation: Extensive experiments on various datasets demonstrate superior performance compared to existing methods.  Robustness: The method shows stable accuracy across threshold values indicating robustness to parameter choice.  Fine Characterization: Layerwise trust region provides a fineresolution characterization of task correlations improving performance.  Comparative Analysis: Comparative analysis with memorybased regularizationbased and expansionbased methods highlights the superiority of TRGP. 3. Weaknesses:  Complexity: The method introduces additional complexity with the trust region and scaled weight projection potentially impacting practical implementation.  Theoretical Justification: While the paper provides intuitive explanations more theoretical justifications or proofs could enhance the understanding of the proposed method.  Limited Discussion: Further analysis on computational efficiency scalability to larger datasets and generalizability to different network architectures could strengthen the paper. Question and Feedback to the Author: 1. Question: How does the TRGP method perform in terms of computational complexity compared to other stateoftheart approaches 2. Feedback: Providing more insights into the theoretical underpinnings behind the trust region and scaled weight projection would enhance the papers credibility. Additionally discussing the applicability of TRGP to larger datasets or different network architectures would broaden the impact of the proposed method. Review Summary: The paper presents a novel approach TRGP for continual learning to address catastrophic forgetting. Its innovation lies in the trust region and scaled weight projection methods showcasing significant performance improvements over existing techniques. While the method demonstrates stability and robustness it would benefit from further theoretical justifications and indepth discussions on practical considerations. Overall the paper contributes a valuable method for promoting forward knowledge transfer in continual learning scenarios warranting consideration for publication pending minor revisions and additional discussions on theoretical aspects and practical implications.", "k6F-4Bw7LpV": " Summary of the Paper The paper introduces the concept of Distributional Generalization in the context of machine learning classifiers. It presents new empirical behaviors and quantitative conjectures that go beyond traditional measures like test error. The authors propose a formal conjecture named Feature Calibration which suggests that the joint distribution of classifier inputs and outputs on the test set closely resembles that of the train set. The paper provides various experiments to support this concept across different settings and classifier models emphasizing the importance of understanding the entire distribution of the classifiers behavior rather than just its test error.  Strong Points of the Paper  Novel Concept: The paper introduces a novel concept of Distributional Generalization offering a deeper understanding of classifier behaviors beyond traditional metrics.  Comprehensive Analysis: The study provides extensive experimental evidence across various machine learning settings illustrating the robustness of their proposed conjectures.  Theoretical Grounding: The authors ground their empirical findings with a formal conjecture Feature Calibration and relate it to classical generalization scaling limits and implicit bias in machine learning.  Weak Points of the Paper  Complexity: The paper dives into intricate theoretical formulations and definitions which might be challenging for readers not wellversed in machine learning theory.  Experimental Clarity: While extensive experiments are presented at times the connection between the experiments and the theoretical concepts could be further clarified to enhance the papers overall coherence.  Questions and Feedback to the Authors  Could the paper provide more practical examples or applications of Distributional Generalization in realworld scenarios  How can the concept of Feature Calibration be extended to noninterpolating classifiers as mentioned in the paper and do you plan to explore this in future research  Review Summary The paper presents a pioneering concept of Distributional Generalization in machine learning classifiers offering a more holistic understanding of their behavior beyond traditional metrics like test error. The empirical evidence and proposed conjectures provide a solid foundation for this new framework. While the papers complexity might be a barrier for some readers its theoretical grounding and comprehensive experiments make a compelling case for the importance of studying the entire distribution of classifier outputs. Therefore considering the innovative concept comprehensive analysis and significant implications for the field I recommend the paper for publication after addressing minor clarifications and extending the practical implications of the proposed concepts.", "vwLLQ-HwqhZ": "Summary: The paper focuses on the detrimental effect of Batch Normalization (BN) in online continual learning due to the crosstask normalization effect. The study proposes a novel Continual Normalization (CN) layer to mitigate this effect and improve overall performance in continual learning scenarios. Extensive experiments and evaluations across various benchmarks and protocols demonstrate the effectiveness of CN over traditional normalization layers. Strengths: 1. Innovative Approach: The paper addresses a crucial issue in continual learning proposing a unique solution in the form of Continual Normalization (CN) to enhance training resilience. 2. Thorough Analysis: The study provides a comprehensive analysis of the challenges posed by BN in online continual learning and conducts detailed experiments to validate the proposed solution. 3. Experimental Validation: The paper includes a series of experiments across different continual learning settings demonstrating the superiority of CN over traditional normalization layers. 4. Clear Presentation: The paper is wellstructured and effectively communicates the problem statement methodology and experimental results. Weaknesses: 1. Theoretical Justification: While the empirical results are convincing a deeper theoretical analysis of how CN effectively mitigates the crosstask normalization effect could enhance the paper. 2. Baseline Comparisons: Including a comparison with stateoftheart continual learning methods could strengthen the papers credibility further. 3. Complexity Analysis: More detailed insights into the computational overhead of CN compared to other normalization layers could provide a better understanding of its practical applicability. 4. Implementation Considerations: Providing insights on the ease of implementation and potential challenges in integrating CN into existing continual learning algorithms would be beneficial for practical adoption. Questions to the Author: 1. Can you provide more insights into the computational overhead of Continual Normalization (CN) compared to traditional normalization layers in terms of training time and resource utilization 2. How does CN generalize across different types of neural network architectures Have you tested CN with architectures other than ResNet to evaluate its applicability in diverse settings 3. Can you elaborate on the potential challenges or limitations in implementing CN in realworld continual learning applications Are there specific scenarios where CN might not be as effective 4. What are the future research directions or extensions of this work How do you plan to further validate the effectiveness of CN in more complex continual learning scenarios Review Summary: The paper presents a novel approach Continual Normalization (CN) to address the limitations of Batch Normalization in online continual learning and demonstrates its effectiveness through rigorous experiments. While the study is wellstructured and provides strong empirical evidence further theoretical analysis baseline comparisons and insights on implementation considerations could enhance the papers impact. Overall based on the innovative approach thorough analysis and positive experimental results I recommend accepting this paper with minor revisions to address the identified weaknesses and clarify the mentioned questions.", "xOeWOPFXrTh": "1) Summary of the Paper: The paper focuses on improving the accuracy of videobased cardiac measurements particularly in estimating higherorder dynamics related to cardiovascular health indicators. It introduces a novel deep learning architecture that explicitly incorporates secondderivative inputs and target signals in the training process. By enhancing the loss function to optimize for higherorder dynamics the model aims to better estimate left ventricle ejection time (LVET) intervals a critical parameter in cardiovascular assessment. 2) Evaluation of the Paper: Strong Points: a) Original Contribution: The paper introduces a novel approach by explicitly incorporating secondderivative information in the neural network training enhancing the accuracy of waveform morphology in cardiac measurements. b) Clinical Relevance: The focus on LVET and higherorder dynamics in cardiovascular assessment can have significant implications for clinical applications and improving patient care. c) Methodological Rigor: The systematic evaluation ablation study and comparison of different model configurations provide a thorough analysis of the proposed approach. d) Use of Diverse Data: Leveraging both synthetic data and a real human subject dataset adds validity and robustness to the studys findings. e) Clear Explanations: The paper provides detailed explanations of the optical and physiological principles underlying the videobased cardiac measurement process catering to both computer vision and healthcare professionals. Weak Points: a) Limited RealWorld Validation: While the paper extensively discusses the use of synthetic data and controlled realworld datasets more validation on a broader realworld patient population would strengthen the generalizability of the model. b) Lack of Comparison Metrics: The paper could benefit from including comparative metrics or results from existing methods in videobased cardiac measurement for benchmarking purposes. c) Future Direction: The paper could elaborate more on potential future applications or extensions of the proposed method beyond videobased cardiac measurements. Questions and Feedback to the Authors: 1) Could you provide more insights into the scalability and applicability of your approach in realworld clinical settings with diverse patient populations 2) Have you considered the computational complexity and efficiency of implementing the proposed model in realtime clinical monitoring scenarios 3) How do you see the integration of your model with existing healthcare systems or telehealth platforms for remote patient monitoring 3) Review Summary: The paper presents a compelling argument for the importance of explicitly incorporating higherorder dynamics such as second derivatives in videobased cardiac measurements. The proposed deep learning architecture shows promising results in improving the accuracy of estimating LVET intervals and waveform morphology which are crucial for clinical assessments. The comprehensive evaluation and systematic ablation study demonstrate the effectiveness of optimizing for higherorder signals. However more validation on diverse patient cohorts and comparison with existing methods would further strengthen the studys impact. Overall the paper makes an original and significant contribution to the field of computer vision in healthcare with potential implications for enhancing cardiac monitoring and diagnosis. Therefore based on the thorough analysis and impactful findings I recommend accepting this paper for publication after addressing the mentioned points.", "pVU7Gp7Nq4k": " Summary: The paper investigates a phenomenon referred to as \"representation mitosis\" in deep neural networks (DNNs) which offers an explanation for the performance improvement with overfitting in wide deep networks. The study examines the last hidden layer representations of various convolutional neural networks to reveal that if the last hidden representation is wide enough its neurons tend to split into redundant groups (\"clones\") carrying identical information. These clones act as statistically independent estimators of data features contributing to the benign overfitting observed in DNNs. The paper conducts quantitative analyses on nontrivial datasets and architectures to support its main findings.  Strong Points: 1. Novel Concept: Introducing the concept of \"representation mitosis\" to explain the behavior of deep neural networks sheds light on a puzzling aspect of DNNs. 2. Comprehensive Analysis: The paper provides detailed experimental results on various architectures and datasets showcasing the phenomenon of representation mitosis and its implications. 3. Methodological Rigor: The analysis methods used to study representation mitosis are welldefined and provide a clear understanding of the phenomenon. 4. Theoretical and Practical Implications: By linking theoretical insights to empirical observations the paper offers valuable insights into the behavior of wide deep neural networks.  Weak Points: 1. Lack of Comparative Analysis: While the paper provides indepth analysis of representation mitosis a comparative study with existing approaches or alternative explanations could enhance the robustness of the findings. 2. Generalization Challenge: The paper focuses on specific architectures and datasets and the generalizability of the concept of representation mitosis across a wider range of scenarios could be further explored.  Questions to the Author: 1. Could you elaborate on the potential implications of representation mitosis in practical applications of deep neural networks such as in realworld machine learning tasks 2. How do you foresee the concept of representation mitosis influencing future developments in neural network training and optimization algorithms  Feedback to the Author: 1. Consider discussing the practical implications and future research directions stemming from the concept of representation mitosis to provide a comprehensive perspective on the potential impact of the findings.  Review Summary: The paper presents a novel concept of \"representation mitosis\" in deep neural networks offering an intriguing explanation for the beneficial effect of overfitting in wide deep networks. Through a rigorous analysis of various architectures and datasets the paper provides compelling evidence for the existence and implications of representation mitosis. While the theoretical foundation and empirical support are strong further comparative analysis and exploration of the broader implications of representation mitosis could enhance the depth and significance of the findings. Overall the paper contributes valuable insights to the understanding of DNN behaviors and opens up new avenues for future research in neural network optimization. Thus based on the strength of the concept and the methodology employed I recommend acceptance with minor revisions to address the highlighted points.", "gEZrGCozdqR": " 1) Summary of the Paper: The paper presents a method called instruction tuning to enhance the zeroshot learning capabilities of language models. By finetuning a pretrained 137B parameter language model on a mixture of over 60 NLP datasets described through natural language instructions the resulting model named FLAN demonstrates substantial improvements in zeroshot performance on unseen tasks. FLAN outperforms the zeroshot and even fewshot GPT3 on various NLP task types. Ablation studies highlight the significance of the number of finetuning datasets model scale and natural language instructions for the success of instruction tuning.  2) Strengths and Weaknesses of the Paper: Strengths:  Innovative Methodology: The introduction of instruction tuning as a simple yet effective method to enhance zeroshot learning is innovative.  Strong Results: The results showcase FLANs superior performance compared to GPT3 on a variety of unseen tasks.  Comprehensive Evaluation: The paper evaluates FLAN across multiple NLP task types and includes ablation studies to dissect the impact of different factors. Weaknesses:  Subjectivity in Task Clustering: The assignment of tasks to clusters may introduce subjectivity which could affect the generalizability of the results.  Model Scale Limitation: The study focuses on a largescale model (137B parameters) which might limit the applicability of the method to smaller models.  Data Overlap Concern: The potential overlap of examples used for evaluation with the models pretraining data raises a concern about the generalization of FLAN. Questions and Feedback to the Author:  Can you elaborate on the methodology used for task clustering and how the clusters were determined  Have you considered experimenting with different model scales to analyze the scalability of the instruction tuning method  How does the potential overlap of examples with pretraining data impact the robustness of FLANs performance in realworld scenarios  3) Review Summary: The paper introduces a novel method instruction tuning to enhance zeroshot learning capabilities of language models. The results demonstrate the effectiveness of FLAN in outperforming GPT3 on various unseen tasks showcasing the potential of instruction tuning. However some limitations regarding subjectivity in task clustering scale dependency and data overlap in evaluation require further investigation. Overall the paper presents a promising avenue for improving zeroshot learning in language models and warrants further exploration and refinement in future research.", "qDx6DXD3Fzt": " Review:  Summary: The paper presents a novel method called ProoD which combines a certifiable outofdistribution (OOD) detector with a standard classifier to achieve provable guarantees for adversarially robust OOD detection. The method aims to address the problem of overconfident predictions by neural networks on OOD data. ProoD offers several advantages including guaranteed adversarially robust OOD detection prevention of asymptotic overconfidence independence from architecture constraints and no loss in prediction accuracy or standard OOD detection performance. The authors conduct experiments on CIFAR10 CIFAR100 and Restricted Imagenet datasets comparing ProoD with various baselines and demonstrating its effectiveness in achieving high accuracy and OOD detection performance while providing provable guarantees.  Strong Points: 1. Innovative Methodology: The paper introduces a novel approach ProoD that effectively combines a certifiable OOD detector with a standard classifier to address the challenges of overconfident predictions on OOD data. 2. Comprehensive Evaluation: The authors demonstrate the performance of ProoD through experiments on CIFAR10 CIFAR100 and Restricted Imagenet datasets comparing it with various baselines and showcasing its provable guarantees for OOD detection. 3. Valueadded Contribution: The paper contributes to the field by offering a method that achieves high accuracy and OOD detection performance while providing assurances for adversarial robustness and prevention of overconfidence in neural networks.  Weak Points: 1. Experimental Reproducibility: While the paper mentions providing the source code in the supplemental material for reproducibility the authors could offer more details or guidelines on the implementation and usage to facilitate easier replication of their experiments by other researchers. 2. Ethical Considerations: The authors acknowledge the use of a retracted dataset (80M Tiny Images) for comparison purposes. While the explanation is transparent further discussion on ethical considerations and alternative datasets could enhance the papers ethical perspective.  Questions: 1. Can you elaborate on the specific experimental setup used for comparing ProoD with baselines on the CIFAR10 CIFAR100 and Restricted Imagenet datasets 2. How scalable is the ProoD method to larger and more complex datasets like ImageNet in terms of both computational resources and performance metrics  Feedback to Author:  Enhance the reproducibility by providing detailed instructions on implementing and using the ProoD method including specific hyperparameters and setup information.  Consider expanding the discussion on ethical considerations related to dataset usage and provide insights into alternative datasets that can substitute 80M Tiny Images.  Review Summary: The paper introduces a novel method ProoD that effectively combines a certifiable OOD detector with a standard classifier to address overconfidence in neural networks on outofdistribution data. The extensive experiments demonstrate the methods efficacy in achieving high accuracy OOD detection performance and providing provable guarantees. Given the innovative approach and comprehensive evaluation I recommend the acceptance of this paper for publication.", "nKZvpGRdJlG": " Summary: The paper proposes a novel adversarial attack and defense framework called ROCO for combinatorial optimization solvers. The framework focuses on modifying the graph structure of combinatorial problems to evaluate the vulnerability of solvers and enhance their robustness. The study includes attack models using RL and traditional heuristic algorithms to modify edges in the graph as well as defense mechanisms to counter these attacks. Experimental results on three common combinatorial optimization tasks demonstrate the effectiveness of the proposed framework.  Strengths: 1. Innovative Concept: The paper introduces a novel concept of adversarial attack and defense for combinatorial optimization solvers shedding light on the vulnerability aspect that has been largely overlooked in the field. 2. Comprehensive Experiments: The paper presents detailed experiments on various combinatorial optimization tasks showcasing the effectiveness of both attack and defense strategies across different types of solvers and problem instances. 3. Clear Methodology: The proposed attack and defense models are welldefined making it easy to understand how the framework operates and how the alterations to the graph structure impact the solvers. 4. Comparative Analysis: The comparison of different attack and defense models as well as the analysis of results provides valuable insights into the performance and robustness of different solvers under adversarial conditions.  Weaknesses: 1. Lack of Realworld Scenarios: While the simulated datasets are used for experiments it would be beneficial to validate the framework on more realworld scenarios to assess its practical applicability and effectiveness in reallife combinatorial optimization problems. 2. Limited Discussion on Defenses: The paper could elaborate more on the defense mechanisms employed their limitations and potential enhancements to make the defense strategy more robust against sophisticated attacks. 3. Scalability Concerns: The scalability of the proposed framework especially in terms of the computational resources required for attacks and defenses on larger datasets or more complex problem instances needs to be discussed further. 4. Generalization to Other Solvers: Although the experiments cover a range of solvers more discussion on how the attack and defense strategies generalize to a broader set of combinatorial optimization solvers would enhance the papers depth.  Questions to the Author: 1. How do you plan to validate the effectiveness of the framework on realworld combinatorial optimization problems and what challenges do you foresee in the adoption of this framework in practical scenarios 2. Can you elaborate on the computational overhead incurred by the proposed attack and defense mechanisms especially when applied to largerscale combinatorial optimization tasks or more complex graph structures 3. How do you envision extending this research to address more diverse combinatorial optimization problems beyond the ones discussed in the paper and what are the key considerations for ensuring the frameworks adaptability to different problem domains  Feedback: Overall the paper presents a novel and intriguing perspective on adversarial attacks and defenses in combinatorial optimization backed by comprehensive experiments. Strengthening the validation on realworld scenarios deepening the discussion on defense mechanisms addressing scalability concerns and discussing the generalizability of the framework would further enhance the quality and impact of the research. The paper is wellstructured but further clarity and expansion in these areas would enrich the contribution significantly.", "kamUXjlAZuw": " Summary: The paper focuses on establishing a mathematical framework to address generalization issues in bias mitigation algorithms with a particular emphasis on fairness constraints. It introduces a PAC framework and an asymptotic framework to derive probabilistic upper bounds and limiting distributions for fairness tradeoffs across categories. The study addresses the challenges of learning fairness metrics without sufficient categorylevel samples through theoretical analysis and practical experiments on real datasets.  Strengths: 1. Theoretical Rigor: The paper demonstrates strong theoretical foundations by leveraging PAC inequalities and asymptotic frameworks to analyze fairness tradeoffs. 2. Novel Contributions: The paper introduces novel techniques such as addressing partial debiasing and quantifying sample efficiency thus contributing new insights to the field. 3. Experimental Evaluation: The experimental results on real datasets provide empirical validation of the theoretical findings enhancing the practical relevance of the research. 4. Clarity of Presentation: The paper is wellorganized with clearly defined sections making it accessible to readers with background knowledge in the field.  Weaknesses: 1. Complexity: The technical nature of the paper may limit its accessibility to a broader audience given the complexity of the mathematical frameworks and derivations. 2. Incomplete Experiments: While the experiments showcase interesting results the paper could benefit from a more comprehensive analysis of the experimental findings to draw stronger conclusions. 3. Clarity in Results Interpretation: The authors could improve the interpretation of the experimental results and their implications to make them more accessible to readers without a strong mathematical background.  Questions  Feedback: 1. Clarity for a General Audience: Could you provide additional contextualization or simplification to help readers without a deep mathematical background understand the implications of your findings 2. Further Experimentation: Are there plans to expand the experimental evaluation to include additional datasets or scenarios to strengthen the empirical validation of the proposed frameworks 3. Implications for RealWorld Applications: How do the theoretical insights and experimental findings translate to practical implications for the development and deployment of biasmitigating algorithms in realworld settings  Review Summary: The paper presents a comprehensive analysis of fairness tradeoffs in bias mitigation algorithms backed by strong theoretical frameworks and practical experiments. While the technical depth and complexity of the work might challenge some readers the novel contributions and thorough exploration of the subject matter justify further consideration for publication. With some improvements in the clarity of presentation and interpretation of results this research has the potential to make a significant contribution to the field of algorithmic fairness.", "fXHl76nO2AZ": " Summary of the Paper The paper introduces the Gradient Importance Learning (GIL) method which trains deep learning models such as multilayer perceptrons (MLPs) and long shortterm memories (LSTMs) to directly perform inference with incomplete data without the need for imputation. By leveraging reinforcement learning to adjust gradients during training the model can exploit the information underlying missingness patterns in the data. The approach is tested on realworld timeseries tabular data and the MNIST dataset showing superior performance compared to traditional imputationbased predictions using stateoftheart imputation methods.  Strengths of the Paper 1. Innovative Methodology:  The GIL method introduces a novel approach to training deep learning models with incomplete data without the need for imputation addressing a critical challenge in data analysis.  The use of reinforcement learning to adjust gradients for training models is a unique and effective strategy to handle missing data. 2. Performance Comparison:  The results demonstrate that the imputationfree predictions using GIL outperform traditional twostep imputationbased predictions with stateoftheart imputation methods highlighting the superiority of the proposed method. 3. Comprehensive Experiments:  The paper provides a thorough evaluation of the GIL method on realworld datasets including healthcare data and the MNIST dataset showcasing the robustness and effectiveness of the approach.  Weaknesses of the Paper 1. Lack of Analysis on Algorithm Complexity:  The paper does not delve into the computational complexity of the GIL method compared to traditional imputationbased approaches which could be important for practical implementation. 2. Limited Discussion on Scalability:  The scalability of the GIL method for large datasets or highdimensional data is not thoroughly discussed which could be a concern in realworld applications. 3. Interpretability Aspects:  While the performance results are promising more insights into the interpretability of the predictive models trained with GIL could enhance the understanding of the approach.  Questions for the Authors 1. Scalability: Have you considered the scalability of the GIL method for handling large datasets with high missingness rates How does the computational efficiency compare to traditional imputationbased methods 2. Generalizability: How well does the GIL method generalize to different types of datasets beyond the ones tested in the paper Have you explored other domains or applications to validate the generalizability of the approach  Feedback for the Authors Overall the paper presents a novel and promising approach for training deep learning models with incomplete data. The comprehensive experiments and performance comparisons demonstrate the effectiveness of the GIL method. To strengthen the paper consider elaborating on the scalability computational complexity and generalizability aspects of the approach. Additionally providing more insights into the interpretability of the models trained with GIL would enhance the practical utility of the method. Further discussions on these aspects and addressing the questions raised would add depth to the paper.  Review Summary Based on the innovative methodology comprehensive experiments showcasing superior performance and the unique contribution of the GIL method in addressing the challenge of missing data I recommend accepting the paper with revisions that address the aspects of scalability interpretability and generalizability for a more wellrounded and impactful research contribution.", "vxlAHR9AyZ6": "Summary of the Paper: The paper introduces a novel approach called \u03b1Weighted Federated Adversarial Training (\u03b1WFAT) to address the data privacy and governance issues while maintaining model robustness against adversarial attacks. The paper highlights the challenges of the straightforward combination of Federated Learning with Adversarial Training due to data heterogeneity among local clients. The key contribution is the introduction of an \u03b1weighted mechanism to relax the innermaximization of Adversarial Training in Federated Learning leading to improved model performance. Strengths of the Paper: 1. Innovative Approach: The paper introduces a novel approach to address the performance deterioration observed in the combined use of Federated Learning and Adversarial Training which is a significant contribution to the field. 2. Theoretical Analysis: The paper provides a detailed theoretical analysis of the proposed \u03b1weighted relaxation mechanism and its impact on the convergence of Federated Adversarial Training demonstrating a robust understanding of the method. 3. Empirical Evaluation: Extensive experiments are conducted on three benchmark datasets to validate the effectiveness of \u03b1WFAT showcasing its superiority over traditional Federated Adversarial Training methods under different optimization settings. 4. Ablation Studies: The paper conducts thorough ablation studies to analyze the impact of hyperparameters like \u03b1 and K\\xcc\\x82 on the performance of \u03b1WFAT providing valuable insights into the methods behavior. 5. Comparative Analysis: The paper compares \u03b1WFAT with existing methods like Federated Standard Training and other Adversarial Training techniques demonstrating its significant improvement in both natural and robust accuracy. Weaknesses of the Paper: 1. Lack of RealWorld Deployment: The paper lacks discussion on the realworld deployment and practical implications of the proposed method which could be essential for understanding its broader impact. 2. Limited Discussion on Scalability: There is a limited discussion on the scalability of the proposed method especially in scenarios with a large number of clients or complex models. Questions and Feedback to the Authors: 1. Can you provide more insights into how \u03b1WFAT can be practically implemented in realworld scenarios especially in industries or applications with stringent data privacy requirements 2. Have you considered conducting experiments on larger datasets or more diverse scenarios to further validate the robustness and scalability of \u03b1WFAT 3. How does the computational overhead of implementing \u03b1WFAT compare to traditional Federated Adversarial Training methods and are there any computational challenges that need to be addressed Review Summary: The paper presents an innovative approach \u03b1Weighted Federated Adversarial Training (\u03b1WFAT) to address the challenges in combining Federated Learning with Adversarial Training. The theoretical analysis and empirical results support the effectiveness of \u03b1WFAT in improving model performance compared to existing methods. The papers strong foundation in theory coupled with comprehensive experimental validation makes a compelling case for the adoption of \u03b1WFAT in the field of Federated Learning. Therefore based on the innovative approach wellsupported theoretical analysis and strong empirical results I recommend acceptance of this paper for publication.", "reFFte7mA0F": "1) Summary of the Paper: The paper discusses the challenges and solutions in ondemand ride pooling focusing on the RidePool Matching Problem (RMP) at a city scale. It introduces a new approach named Conditional Expectation based Value Decomposition (CEVD) to address the limitations of existing methods particularly Neural Approximate Dynamic Programming (NeurADP). The CEVD approach aims to improve the matching of user requests to vehicles by considering the future impact of current assignments and the impact of other agents actions on individual value through the use of conditional probabilities. The paper provides a detailed description of the RMP problem background on existing methods like NeurADP and outlines the algorithm and experiments comparing CEVD to NeurADP. 2) Strengths and Weaknesses with Questions and Feedback: Strong points:  Novel Approach: The paper introduces a novel CEVD approach that outperforms existing methods by taking into consideration the impact of other agent actions.  Scalability: The method aims to handle problems with hundreds or thousands of agents making it suitable for cityscale applications.  Experimental Results: The paper provides detailed experimental results comparing CEVD to NeurADP and showcases significant improvements in terms of requests served.  Realworld Dataset: The use of a realworld taxi dataset enhances the relevance and practicality of the study. Weak points:  Clarity and Readability: The paper is technically dense and may require specialized knowledge to fully understand some sections. Suggestions for simplifying complex concepts or providing more intuitive explanations could improve readability.  Limitations Discussion: While the paper discusses limitations of existing methods further exploration of potential limitations or challenges of the proposed CEVD approach would strengthen the discussion.  Comparative Analysis: While the paper compares CEVD to NeurADP more extensive comparisons with other stateoftheart methods or advanced variants could provide a broader perspective on the novel contributions of CEVD. Questions and Feedback: 1. Clarity: Can the paper be more accessible to readers without a deep background in the field Consider adding more intuitive explanations or examples to enhance clarity. 2. Scalability: How does the CEVD approach specifically address challenges related to scalability especially with a large number of agents in a cityscale ride pooling scenario 3. Generalization: How generalizable is the CEVD approach to different types of ride pooling scenarios or other multiagent systems beyond the described RMP problem 4. Practical Implementation: What challenges might arise in implementing the CEVD approach in realworld ride pooling platforms and how can these challenges be mitigated 3) Review Summary: The paper presents a novel approach CEVD to enhance ondemand ride pooling matching by considering the future impact of current assignments and the impact of other agent actions. With significant performance improvements demonstrated in cityscale experiments the CEVD method shows promise for enhancing ride pooling efficiency in realworld applications. While the paper is technically strong and provides valuable insights addressing clarity discussing potential limitations and exploring broader comparisons could further enhance the contribution of the research. Therefore based on the innovative approach promising experimental results and the potential to address scalability challenges I recommend accepting the paper with minor revisions to enhance clarity and discuss possible limitations and extensions of the proposed CEVD method.", "qY79G8jGsep": "Summary: The paper proposes a novel approach called DISSECT which generates Concept Traversals (CTs) to explain the inferences of deep learning models. DISSECT aims to disentangle important concepts effectively generate realistic examples and retain relevant information by jointly training a generator discriminator and concept disentangler. The paper demonstrates the effectiveness of DISSECT through experiments on synthetic and realistic datasets showing superior performance compared to existing baselines in terms of interpretability bias detection and artifact identification. Strong Points: 1. Innovative Approach: DISSECT offers a novel approach to generating Concept Traversals that effectively explain the inferences of deep learning models. 2. Performance: The paper demonstrates superior performance of DISSECT compared to existing baselines across various evaluation metrics such as Importance Realism Distinctness Substitutability and Stability. 3. Experimental Validation: The paper provides thorough experimental validation on synthetic and realistic datasets showcasing the effectiveness of DISSECT in detecting biases and identifying spurious artifacts. 4. Comprehensive Analysis: The paper includes a detailed discussion on related work method details experimental setup hyperparameter tuning ablation experiments and evaluation metrics. Weak Points: 1. Human Studies: The paper mentions a need for humansubject studies to validate the semantical meaningfulness of the discovered concepts. Direct human evaluation could provide additional robustness to the findings. 2. Impact of Generation Quality: The paper discusses the interaction between generation quality and explanation quality highlighting that realism is affected by imperfect generation quality. Further exploration on ways to improve realism without compromising other qualities could enhance the approach. Questions and Feedback: 1. Generalization: How well does DISSECT generalize to different types of datasets and models beyond the ones tested in the experiments 2. Usability: How userfriendly is the implementation of DISSECT for practitioners without technical expertise in explainable AI 3. Scalability: How does the approach scale with larger datasets and deep learning models with higher complexities 4. Robustness: What methods were employed to ensure the robustness and reliability of DISSECT in different scenarios and applications Review Summary: The paper presents an innovative and effective approach DISSECT for generating Concept Traversals to explain deep learning model inferences. The comprehensive experimental validation demonstrates its superiority over existing baselines in terms of interpretability bias detection and artifact identification. While the paper showcases strong performance metrics and provides a thorough analysis addressing potential limitations around humansubject studies and generation quality could further strengthen the approach. Overall the paper presents a significant contribution to the field of explainable AI with promising implications for realworld applications. Based on the strong empirical evidence provided I recommend acceptance of this paper for publication.", "sk63PSiUyci": " Summary of the Paper The paper introduces AISARAH a practical variant of SARAH for solving the unconstrained finitesum optimization problem prevalent in machine learning tasks. AISARAH adapts to local geometry implicitly computes stepsize and efficiently estimates local Lipschitz smoothness of stochastic functions. The algorithm is fully adaptive tunefree straightforward to implement and computationally efficient. The paper provides technical insight intuitive illustrations and empirical analysis demonstrating superior performance compared to classical counterparts and other stateoftheart firstorder methods.  Strengths of the Paper  Innovative Algorithm: AISARAH introduces a novel approach by dynamically determining stepsize based on local geometry which leads to efficient convergence in optimization.  Comprehensive Analysis: The paper includes extensive numerical experiments demonstrating the strong performance of AISARAH compared to SARAH SARAH and other methods showcasing its competitiveness.  Clear Exposition: The paper is wellstructured with detailed explanations technical insights and intuitive illustrations making it easy to understand the algorithm and its benefits.  Empirical Validation: The empirical results support the claims in the paper showing the effectiveness of AISARAH in practical applications.  WeaknessesGaps in the Paper  Lack of Theoretical Analysis: While the paper provides empirical evidence of AISARAHs performance a more indepth theoretical analysis regarding convergence properties could further strengthen the research.  Complexity Concerns: The algorithms computational complexity is briefly mentioned but may require further elaboration and discussion to assess its scalability to larger datasets or more complex problems.  Questions and Feedback for the Author 1. Theoretical Analysis: Have you considered providing a more comprehensive theoretical analysis especially regarding convergence guarantees and complexity bounds to further validate the effectiveness of AISARAH 2. Scalability: How does AISARAH perform in terms of computational efficiency and scalability when applied to larger datasets or more complex optimization problems 3. Comparison Metrics: Could you elaborate on the specific metrics used to compare the performance of AISARAH with other algorithms in the empirical analysis 4. Sensitivity Analysis: Is there a sensitivity analysis conducted on the hyperparameters used in AISARAH to assess its robustness and generalizability across different scenarios  Review Summary The paper presents an innovative algorithm AISARAH which offers a significant advancement in stochastic optimization by dynamically adjusting stepsizes based on local geometry. The comprehensive empirical analysis showcases its superiority over existing methods. However to strengthen the research further theoretical analysis and considerations on scalability may be warranted. Overall the clear presentation detailed experimentation and the novelty of the algorithm suggest that the paper contributes valuable insights to the field of optimization in machine learning.", "o86_622j0sb": " Summary: The paper proposes utilizing segmentation priors in blackbox attacks to enhance the imperceptibility of adversarial examples perturbations. The approach limits perturbations to the salient region of images aiming to make the attacks less noticeable while maintaining query efficiency and success rate. The authors introduce the Saliency Attack a new gradientfree blackbox attack that refines perturbations within the salient regions. Experimental results show that the proposed method generates imperceptible perturbations that are interpretable and robust against detectionbased defenses.  Strengths: 1. Innovative Approach: The use of segmentation priors to improve imperceptibility in blackbox attacks is novel and addresses a critical aspect of adversarial attacks. 2. Experimental Validation: The paper provides comprehensive experimental results demonstrating the effectiveness of the proposed approach including comparisons with stateoftheart blackbox attacks. 3. Robustness Testing: Testing against detectionbased defenses adds value by showing the efficacy of the proposed method in practical scenarios. 4. Detailed Methodology: The paper provides a detailed methodology section explaining the problem formulation salient object segmentation refining perturbations and the complete Saliency Attack process.  Weaknesses: 1. Theoretical Explanation: While the methodology is detailed additional theoretical explanation would benefit readers to understand the reasoning behind using segmentation priors for imperceptible adversarial attacks. 2. Comparison Metrics: While the paper uses common metrics like success rate and query efficiency more qualitative comparison with visual examples or qualitative user studies could enhance the understanding of imperceptibility. 3. Hyperparameter Sensitivity: The study mentions the initial block size as a hyperparameter but does not extensively discuss sensitivity analysis or optimization techniques for hyperparameter tuning. 4. Ablation Study Scope: The ablation study is limited to refining in salient nonsalient and whole regions. Exploring other variations or additional parameters for refinement could strengthen the study.  Questions and Feedback to the Author: 1. Can you provide more insights into how the imperceptibility was quantitatively evaluated Did you consider any human studies or visual inspections to validate the imperceptibility 2. Considering the potential ethical implications of creating imperceptible adversarial attacks how do you ensure the responsible use of such techniques 3. How generalizable is the Saliency Attack method across different datasets or types of models beyond the Inception v3 architecture and the ImageNet dataset 4. Have you considered further exploring the interpretability of the perturbations for understanding the models decisionmaking process  Review Summary: The paper presents an innovative approach to improving imperceptibility in blackbox adversarial attacks using segmentation priors. Through detailed methodology and robust experimental results the authors demonstrate the effectiveness of the proposed Saliency Attack for generating imperceptible and interpretable adversarial examples. While the paper has strengths in its novelty and experimental validation addressing theoretical explanations expanding comparison metrics optimizing hyperparameters and broadening the ablation study would enhance the studys impact. Overall the paper makes a valuable contribution to the field of adversarial attacks and merits publication pending minor revisions and clarifications.", "zaALYtvbRlH": " Summary: The paper introduces SPANDROP a data augmentation technique designed to help models identify the true supervision signal in long sequences with sparse supervision. The technique involves randomly ablating parts of the input sequence to create counterfactual examples. The authors propose a variant of SPANDROP based on the betaBernoulli distribution that provides more consistent learning objectives. The effectiveness of SPANDROP is demonstrated on synthetic FINDCATS tasks and natural language processing tasks. Experimental results show that SPANDROP improves data efficiency reduces overfitting mitigates position bias and enhances model performance on realworld datasets.  Strengths: 1. Innovative and Effective Technique: SPANDROP addresses the underspecification problem in long sequence learning by distilling sparse supervision signals effectively. 2. Theoretical Analysis: The paper provides a sound theoretical basis for SPANDROP validating its properties and its effectiveness in generating diverse counterfactual examples. 3. Comprehensive Experiments: The authors conduct experiments on synthetic tasks and realworld datasets demonstrating the benefits of SPANDROP in improving model performance under various conditions. 4. Comparison and Analysis: The comparison with existing techniques like masking and investigation into factors influencing SPANDROPs performance add depth to the study. 5. Clear and WellStructured: The paper is wellstructured with clear explanations and provides detailed methodology and results.  Weaknesses: 1. Generalization Gaps: The performance gains on realworld datasets are less significant compared to synthetic data indicating potential limitations in broader generalization. 2. Assumptions and Limitations: The method relies on known or estimable supporting facts which might not always be feasible in practical scenarios. 3. Limited Exploration: While SPANDROP is shown to be effective further exploration on the impact of different hyperparameters and span sizes could enhance the understanding of its applicability.  Questions and Feedback to Authors: 1. How do you address the potential challenges of estimating supporting facts in realworld scenarios where true supervision signals may be ambiguous or unknown 2. Have you considered the scalability and computational efficiency of SPANDROP when applied to largerscale datasets with highdimensional inputs 3. Can the proposed technique be easily integrated into existing neural models without significant modifications and have you tested the method on a wider range of tasks beyond the ones presented in the paper  Review Summary: The paper presents a novel data augmentation technique SPANDROP for distilling supervision signals from long sequences with sparse supervision. The method shows promising results in improving data efficiency reducing overfitting and mitigating position bias in various tasks. While the experimental validation is thorough further exploration on generalization to diverse NLP tasks and practical applicability could enhance the studys impact. Overall the paper is wellstructured supported by sound theoretical analysis and presents a valuable contribution to the field of long sequence learning and data augmentation. I recommend this paper for publication with minor revisions to address the identified weaknesses.", "oU3aTsmeRQV": "1) Summary The paper introduces a method named SelfEnsemble Adversarial Training (SEAT) to enhance the robustness of deep neural networks against adversarial attacks. SEAT leverages the states of history models through the training process by averaging weights to yield a more resilient classifier than individually trained models. The study compares SEAT with traditional ensemble methods and advanced defense techniques like TRADES MART and others. The methodology is tested on CIFAR10 and CIFAR100 datasets using ResNet18 and WRN3210 models. 2) Strengths and Weaknesses Questions and Feedback Strengths:  The study addresses a critical issue in deep learning\u2014adversarial attacks\u2014and proposes a novel method SEAT to enhance the robustness of deep neural networks.  The thorough theoretical analysis empirical evidence and visualizations effectively support the proposed method and help in understanding its implications.  The paper provides a comprehensive comparison with existing defense methods showcasing the superiority of SEAT in terms of robust accuracy against various attack methods.  The experimental results on CIFAR10 and CIFAR100 datasets demonstrate the effectiveness of SEAT in improving adversarial robustness.  The ablation study and analysis of the learning rate strategies ensembling frequency and initial safeguard length provide valuable insights into the working principles of SEAT. Weaknesses:  While the paper discusses the theoretical underpinnings of SEAT and its performance in experiments a more detailed discussion on the practical implications and limitations of the method could enhance the papers applicability.  The paper could benefit from a clearer explanation of the proposed method for readers unfamiliar with adversarial attacks and deep learning concepts.  Providing insights into the scalability and computational efficiency of SEAT especially when applied to larger datasets or more complex models would further strengthen the practical utility of the method. Questions: 1. How does the SEAT method perform in scenarios with largescale datasets or more complex neural network architectures 2. Could you elaborate on the computational requirements and training time comparison between SEAT and traditional ensemble techniques 3. What are the implications of SEAT in realworld applications beyond CIFAR datasets such as in industry or critical systems Feedback:  Consider clarifying the practical implications and potential applications of SEAT in realworld scenarios to provide a clearer context for readers.  Provide a detailed comparison of SEAT with other cuttingedge defense methods and discuss the tradeoffs and advantages of SEAT in different settings.  Include a discussion on the interpretability and explainability aspects of SEAT to shed light on how the method contributes to trust and transparency in deep learning models. 3) Review Summary The paper presents a novel method SelfEnsemble Adversarial Training (SEAT) aimed at improving the robustness of deep neural networks against adversarial attacks. The theoretical foundations empirical results and comparative analysis with existing defense methods demonstrate the efficacy of SEAT in enhancing adversarial robustness. The thorough investigation through experiments ablation studies and theoretical analyses supports the significance of SEAT. However the paper could benefit from additional insights into the practical implications scalability and realworld applications of SEAT. Overall considering the innovative approach solid theoretical foundations and promising experimental results I recommend acceptance of this paper with minor revisions to enhance clarity and practical implications.", "gpp7cf0xdfN": "SUMMARY The paper introduces a new adversarial learning paradigm called Reverse Engineering of Deceptions (RED) to address the problem of recovering adversarial perturbations from adversarial images. The paper formulates the RED problem identifies key principles for effective perturbation estimation and proposes a ClassDiscriminative Denoising based RED framework (CDDRED). The performance of CDDRED is extensively evaluated using various metrics including pixellevel reconstruction error predictionlevel alignment and attributionlevel adversary saliency region recovery. Overall CDDRED shows promising results in estimating adversarial perturbations and recovering original images compared to denoisingonly and stateoftheart methods. STRONG POINTS 1. Novel Contribution: The paper introduces a novel RED paradigm to address the challenge of recovering adversarial perturbations which is a new and unexplored area in the field of adversarial attacks. 2. Solid Formulation and Principles: The authors formulate the RED problem and identify key principles crucial for designing an effective RED approach providing a comprehensive framework for perturbation estimation. 3. Performance Evaluation: The paper extensively evaluates the proposed CDDRED framework using various evaluation metrics and demonstrates its effectiveness under different attack generation methods showing superior performance compared to baselines. WEAK POINTS 1. Complexity: The paper introduces a complex problem and solution framework which may be challenging for readers unfamiliar with adversarial attacks and machine learning concepts. 2. Assumptions and Limitations: The paper assumes certain conditions such as knowledge of the victim model or access to surrogate victim models which may not be practical in realworld scenarios. 3. Lack of Realworld Application: The paper focuses mainly on theoretical concepts and technical evaluation lacking discussion on realworld applications or implications of the proposed RED approach. QUESTION TO THE AUTHORS 1. How do you envision the practical application of the RED framework in realworld scenarios considering the complexities and assumptions involved in the current formulation FEEDBACK TO THE AUTHORS  Consider providing more practical usecases or scenarios where the RED framework can be applied effectively.  Simplify the presentation of the complex technical details to make the paper more accessible to a wider audience. REVIEW SUMMARY The paper introduces a novel adversarial learning paradigm RED to address the challenging problem of recovering adversarial perturbations from adversarial images. The formulation of the RED problem identification of crucial principles and proposal of the CDDRED framework demonstrate a solid contribution to the field of adversarial attacks. The extensive evaluation of CDDRED under various metrics and attack methods showcases its superiority over existing approaches. However the complex nature of the problem and assumptions made in the formulation could limit the practical applicability of the RED framework. Further clarifications on realworld applications and simplification of technical details would enhance the overall impact and accessibility of the research.", "kQMXLDF_z20": " PeerReview: 1. Summary of the Paper: The paper focuses on addressing the issue of oversmoothing in Graph Neural Networks (GNNs) which occurs when node representations become indistinguishable as the number of hidden layers in GNN models increases. The paper first surveys existing deoversmoothing methods and proposes three key metrics to evaluate such methods. Subsequently the authors introduce a new deoversmoothing method termed Topologyguided Graph Contrastive Layer (TGCL) which aims to alleviate oversmoothing by maintaining constant divergence between nodes easy determination of divergence and a modelagnostic approach. The effectiveness of TGCL is theoretically analyzed and validated through empirical experiments comparing its performance with stateoftheart baselines. 2. Strong Points:  Comprehensive Survey: The paper provides a thorough overview of the oversmoothing problem in GNNs and existing deoversmoothing strategies.  Novel Methodology: The proposed TGCL addresses the oversmoothing issue by introducing a contrastive learning approach which is an innovative solution.  Theoretical Analysis: The paper includes a detailed theoretical analysis of TGCL providing a solid foundation for the proposed method.  Empirical Validation: Extensive experiments are conducted to demonstrate the effectiveness of TGCL compared to stateoftheart baselines across multiple datasets.  Efficiency Analysis: The authors conduct efficiency analysis demonstrating linear scalability of the proposed method with increasing layers. 3. Weak Points:  Limited Comparison: The paper focuses on comparing the proposed TGCL with a few baselines. It could benefit from additional comparison with more diverse existing deoversmoothing methods.  Experimental Setup: While the experimental results are promising additional insights could be gained by varying other experimental factors like hyperparameters or dataset sizes.  Presentation Clarity: Some sections are dense with technical details and might be challenging for readers not deeply familiar with the topic.  Questions and Feedback to the Author:  Question 1: Have you considered exploring the performance of the proposed TGCL method with different hyperparameter settings (e.g. temperature in contrastive loss) to evaluate its sensitivity and robustness  Question 2: How do the computational costs of TGCL compare to other deoversmoothing methods especially in scenarios with varying graph sizes  Feedback: Providing more visualization aids or examples in the paper could assist readers in understanding the concepts better especially in complex theoretical sections.  Review Summary: The paper presents a novel deoversmoothing method TGCL to mitigate the oversmoothing issue in GNNs by maintaining constant divergence easy determination of divergence and a modelagnostic strategy. The study includes a detailed theoretical analysis comprehensive experimental validation efficiency analysis and relevant discussions. Overall the papers contributions are significant and the empirical results support the effectiveness of TGCL. However further clarity additional comparisons and insights into hyperparameter settings could enhance the papers impact. Considering the strong methodology and promising results I recommend acceptance after addressing the minor points raised above.", "faMcf0MDk0f": " Summary of the Paper: The paper introduces BoolNet a novel architecture for Binary Neural Networks (BNNs) aiming to balance accuracy and energy consumption. It addresses the tradeoff between accuracy and hardware efficiency by minimizing the use of 32bit components commonly found in existing BNN designs. BoolNet uses 1bit values for feature maps incorporates a Multislice strategy to maintain representational capacity and proposes progressive weight binarization during training. Experimental results on the ImageNet dataset demonstrate that BoolNet achieves 63.0 Top1 accuracy with a significant 2.95\u00d7 energy reduction compared to recent stateoftheart BNN architectures.  Strong Points of the Paper:  Original Contribution: The paper presents a novel BNN architecture BoolNet which addresses the fundamental problem of balancing accuracy and energy consumption in BNNs.  Comprehensive Analysis: The paper extensively studies the effects of 32bit components on BNNs and proposes effective strategies to replace or reduce such components.  Experimental Results: The experimental results demonstrate the effectiveness of BoolNet in achieving high accuracy with significantly lower energy consumption compared to stateoftheart BNNs.  Thorough Evaluation: The authors conduct a detailed ablation study on the proposed design changes training strategies and Multislice Binary Convolution providing insightful comparisons.  Energy Consumption Evaluation: The paper includes a thorough evaluation of energy consumption discussing the impact of memory access and computation on overall energy efficiency.  Weak Points of the Paper:  Complexity: The paper is highly technical and detailed which might make it challenging for readers without advanced knowledge in neural networks and hardware accelerators to grasp the content fully.  Lack of Visual Aids: While the paper includes diagrams to help explain concepts some sections would benefit from more visual aids to enhance understanding especially for complex concepts like the Multislice Binary Convolution.  Questions for the Authors: 1. Can you provide more insights into the decisionmaking process behind choosing k4 for the Multislice Binary Convolution and how this value was optimized for the best tradeoff between accuracy and energy efficiency 2. How do you plan to further optimize the proposed BoolNet architecture for realworld deployment in resourceconstrained devices such as mobile phones and IoT devices 3. Are there plans to extend the evaluation of BoolNet to other benchmark datasets or realworld applications to assess its generalizability and practical utility  Feedback for the Authors: The paper presents a significant contribution to the field by introducing BoolNet a novel BNN architecture with promising results in achieving high accuracy coupled with energy efficiency. The thorough analysis experimental evaluation and detailed discussion on the tradeoff between accuracy and energy consumption make the paper valuable to the research community. Addressing the weak points highlighted such as complexity and lack of visual aids could further enhance the clarity and accessibility of the paper. Overall I recommend the acceptance of this paper given its innovative approach thorough evaluation and impactful results in advancing the area of BNN research.", "pgir5f7ekAL": " Summary: The paper focuses on the problem of principal component analysis with generative modeling assumptions. The authors propose a quadratic estimator and a projected power method to solve the eigenvalue problem efficiently showing statistical rates of order \\xe2\\x88\\x9ak logL m. They conduct experiments on image datasets for spiked matrix and phase retrieval models demonstrating the superior performance of their method compared to classic power methods and truncated power methods.  Strengths: 1. Novel Contribution: The paper introduces a new method for principal component analysis with generative priors offering a fresh perspective on the problem. 2. Theoretical Rigor: The authors provide detailed theoretical analysis proofs and mathematical foundations to support the proposed methods. 3. Experimental Validation: The experimental section includes results on various datasets showcasing the superior performance of the proposed method over traditional approaches. 4. Detailed Comparisons: The comparisons with classic power methods and truncated power methods provide a clear understanding of the advantages of the proposed method.  Weaknesses: 1. Complexity: The mathematical and theoretical framework presented in the paper may be challenging for readers without a strong background in the field. 2. Generalization: The experiments were limited to specific datasets and scenarios limiting the generalizability of the findings. 3. Clarity: Some parts of the paper could benefit from improved clarity and explanations to enhance understanding especially for readers less familiar with the topic.  Questions: 1. Did you consider other alternative methods or models for comparison with the proposed approach Providing a broader comparative analysis would enhance the validation of the proposed method. 2. How would the proposed method perform on larger or more diverse datasets beyond the ones tested in the experiments 3. Have you considered the computational complexity and scalability of the proposed method for larger datasets or realworld applications  Feedback: 1. Clarity and Structure: Further clarification and restructuring of the paper especially in the introduction and discussion of results will benefit readers. 2. Expand Experimental Section: Consider expanding the experimental section to include more diverse datasets and scenarios to provide a comprehensive evaluation of the proposed method. 3. Address Generalizability: Discuss the generalizability of the proposed method and its applicability in realworld scenarios to enhance the practical value of the research.  Review Summary: The paper presents a novel approach to principal component analysis with generative modeling assumptions providing theoretical foundations and experimental validation. While the paper showcases significant strengths in originality and theoretical rigor improvements in clarity generalizability and broader comparative analysis can enhance its impact and applicability. Overall the research shows promise and merits further exploration and refinement.", "zzk231Ms1Ih": " Summary of the Paper: The paper presents a novel theory for understanding parametric tournament representations. It addresses two fundamental questions: 1) what structurally characterizes the class of tournaments that can be represented in d dimensions and 2) what is the minimum dimension needed to represent a given tournament. The authors introduce the concept of forbidden configurations flip classes and positive spans to characterize and analyze different rank tournament classes. They also connect their results to sign rank of matrices learning from pairwise comparisons and realworld tournament experiments.  Strong Points: 1. Innovative Approach: The paper introduces a novel theory for understanding tournament representations addressing important questions in the field. 2. Comprehensive Analysis: The authors provide a detailed analysis of forbidden configurations flip classes and positive spans to characterize different rank tournament classes. 3. Theoretical Contributions: The paper establishes connections between tournament representations and forbidden configurations providing insights into the structure of tournaments based on representation dimensions.  Weak Points: 1. Complexity: The paper delves into advanced mathematical concepts that may be challenging for readers unfamiliar with tournament theory and related mathematical frameworks. 2. Clarity: Some sections may require further clarification or examples to aid in understanding the concepts presented especially for readers outside the specific research domain.  Questions and Feedback to the Author:  Can the results presented in the paper be applied to practical scenarios involving tournament modeling such as sports rankings or preference learning algorithms  Could the paper benefit from more illustrative examples or case studies to demonstrate the practical implications of the theoretical findings  Are there any limitations or assumptions in the methodology employed that could affect the generalizability of the results to realworld applications  Review Summary: The paper makes significant contributions to the field of tournament representations by introducing a novel theory to understand parametric tournament structures. The comprehensive analysis of forbidden configurations flip classes and positive spans provides valuable insights into the underlying mathematics of tournament modeling. The theoretical findings presented in the paper offer a solid foundation for further research in the area. However further clarity in presentation and applicability to practical scenarios could enhance the understanding and relevance of the paper. Overall the paper showcases a rigorous and innovative approach to studying tournament representations opening up avenues for future exploration and application in various domains.", "gVRhIEajG1k": "Summary: The paper discusses the issue of adversarial transferability in deep neural networks (DNNs) and proposes a novel approach called Intrinsic Adversarial Attack (IAA) to enhance transferability by aligning adversarial attacks with the ground truth data distribution. The authors identify lowdensity data (LDD) that can easily be perturbed by small Gaussian noise and show that adversarial examples crafted from LDD samples have strong transferability. They introduce the concept of Alignment between Adversarial attack and Intrinsic attack (AAI) to quantify the effectiveness of transferability. The key contributions include proposing IAA which improves transferable attacks against multiple DNNs and outperforms existing methods. Strengths: 1. Innovative Concept: The paper introduces a novel perspective on adversarial transferability focusing on data distribution and proposing a unique method (IAA) to enhance transferability. 2. Comprehensive Experiments: The authors conduct a thorough experimental evaluation against various DNNs both normal and robust models showcasing the effectiveness of IAA in boosting transferability. 3. Clear and Detailed Explanations: The paper provides detailed explanations of the methodology algorithms and experimental results enhancing the clarity and understanding of the proposed approach. Weaknesses: 1. Lack of Open Source Code: While the paper mentions plans to open the source code for reproducibility the absence of a readily available codebase limits the immediate reproducibility and evaluation by the research community. 2. Complex Terminologies: The paper includes technical terms and detailed mathematical formulations that may be challenging for readers who are not deeply familiar with the topic. Question to the Author: 1. Can you provide more insights into the computational overhead of implementing the proposed IAA approach compared to existing methods How feasible is it to integrate these changes into practical applications Feedback to the Author: 1. Consider providing a more detailed explanation of the algorithm and the practical implementation steps to facilitate the understanding of readers. Review Summary: The paper presents a novel approach IAA to enhance adversarial transferability by aligning attack directions with the ground truth data distribution. The research is significant in addressing the security concerns related to adversarial attacks in DNNs. The experimental results demonstrate the effectiveness of IAA in boosting transferability across various models. Despite the strengths in innovation and comprehensive experiments providing a more accessible writing style and open sourcing the code would enhance the impact and reproducibility of the study. Overall based on the innovative approach and promising experimental results I would recommend the paper for publication pending revisions to address the identified weaknesses.", "mdUYT5QV0O": " Summary The paper proposes the Regularized Modernized Dual Averaging (RMDA) algorithm for training neural networks with a regularization term to promote desired structures such as structured sparsity or discretevalued parameters. RMDA achieves variance reduction and can identify the desired structure at a stationary point even in the presence of engineering complexities like data augmentation. The paper provides theoretical analysis algorithm details and experimental results showcasing the superior performance of RMDA compared to existing methods for training structured neural networks.  Strengths 1. Theoretical Foundation: The paper provides a strong theoretical foundation rooted in manifold identification from nonlinear optimization which allows for the identification of desired structures. 2. Novel Algorithm: RMDA is a novel algorithm that achieves variance reduction beyond finitesum problems with low computational cost and no additional hyperparameters making it suitable for largescale deep learning tasks. 3. Empirical Validation: The experimental results demonstrate the effectiveness of RMDA in identifying stationary structures without compromising model performance outperforming existing methods for structured sparsity training.  Weaknesses 1. Complexity: The paper has a high level of technicality making it challenging for readers without a deep understanding of optimization theory to grasp the concepts effectively. 2. Comparative Analysis: While the experimental results are comprehensive a more detailed comparison with a wider range of stateoftheart methods could enhance the papers impact.  Questions and Feedback to the Authors 1. Question: Could you provide more insights into the computational complexity of RMDA compared to other existing methods It would be interesting to understand how the algorithm scales with increasing model sizes. 2. Feedback: Providing a more indepth discussion on the interpretability of the identified structures and how they impact model generalization could further enrich the paper.  Review Summary The paper presents a novel algorithm RMDA for training structured neural networks addressing the challenge of identifying desired structures at stationary points. With strong theoretical underpinnings and promising empirical results RMDA outperforms existing methods for structured sparsity training. Despite the technical complexity the paper provides valuable contributions to the field of deep learning optimization. Therefore I recommend accepting the paper after addressing the minor weaknesses identified.", "gxk4-rVATDA": " 1) Summary of the paper: The paper proposes a novel algorithm for training neural network weights at the bitlevel. By learning individual bits representing weights on arbitrary bitdepths the algorithm uncovers sparse networks naturally without additional constraints. The study shows improved results compared to standard training techniques for fully connected networks and comparable performance for residual networks.  2) Strong Points: a) Novel Approach: The algorithm introduces a unique approach to training neural network weights at the bitlevel providing insights into sparsity and regularization. b) Experimental Results: Extensive experiments demonstrate the efficacy of the proposed algorithm across different networks and datasets. c) Selective Bit Training: The ability to selectively train specific bits provides valuable insights into the importance of different bits for network performance. d) Message Encoding: The demonstration of encoding messages in untrainable bits showcases the algorithms versatility and potential applications. e) Connection to Previous Works: The paper effectively relates its novel approach to existing pruning and quantization techniques. Weak Points: a) Lack of Theoretical Analysis: While the experimental results are extensive more theoretical analysis or proofs validating the algorithms efficacy would enhance the papers credibility. b) Comparison with StateoftheArt: Further benchmarking against stateoftheart pruning and quantization methods would strengthen the papers contributions. c) Complexity: The complexity of the proposed algorithm and its impact on computational resources should be discussed to provide a comprehensive evaluation. Questions and Feedback for Authors: 1. Can you provide more insights into the theoretical underpinnings of why the proposed bitwise training approach leads to network sparsity and regularization 2. How does the proposed algorithm compare in terms of inference speed and computational resources with standard training and other pruning methods 3. Have you considered analyzing the resilience of the algorithm to adversarial attacks or noise in the input data during training  3) Review Summary: The paper presents an innovative algorithm for training neural network weights at the bitlevel showcasing improved results compared to standard training techniques while naturally uncovering sparse networks. The experiments are extensive and provide valuable insights into the importance of different bits in network performance. The proposed approachs ability to selectively train specific bits and encode messages in untrainable bits demonstrates its versatility and potential applications. Connecting the novel algorithm to existing works enhances its significance. While the paper lacks indepth theoretical analysis and comparisons with stateoftheart methods its experimental results and innovative approach make a strong case for its contributions. Addressing the weak points and questions raised would strengthen the papers impact in the field of neural network training and pruning.  Recommendation: Based on the novelty of the algorithm comprehensive experimental results and valuable insights into bitlevel training I recommend acceptance of the paper for publication pending revisions addressing the identified weak points."}